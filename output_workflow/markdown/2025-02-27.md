> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-27

共有301篇相关领域论文, 另有40篇其他

## 无序系统和神经网络(cond-mat.dis-nn:Disordered Systems and Neural Networks)

### Spectral Analysis of Representational Similarity with Limited Neurons 
[[arxiv](https://arxiv.org/abs/2502.19648)] [[cool](https://papers.cool/arxiv/2502.19648)] [[pdf](https://arxiv.org/pdf/2502.19648)]
> **Authors**: Hyunmo Kang,Abdulkadir Canatar,SueYeon Chung
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 无序系统和神经网络,机器学习,神经元和认知
- **Abstract**: Measuring representational similarity between neural recordings and computational models is challenging due to constraints on the number of neurons that can be recorded simultaneously. In this work, we investigate how such limitations affect similarity measures, focusing on Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA). Leveraging tools from Random Matrix Theory, we develop a predictive spectral framework for these measures and demonstrate that finite neuron sampling systematically underestimates similarity due to eigenvector delocalization. To overcome this, we introduce a denoising method to infer population-level similarity, enabling accurate analysis even with small neuron samples. Our theory is validated on synthetic and real datasets, offering practical strategies for interpreting neural data under finite sampling constraints.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Efficient and Accurate Spatial Mixing of Machine Learned Interatomic Potentials for Materials Science 
[[arxiv](https://arxiv.org/abs/2502.19081)] [[cool](https://papers.cool/arxiv/2502.19081)] [[pdf](https://arxiv.org/pdf/2502.19081)]
> **Authors**: Fraser Birks,Thomas D Swinburne,James R Kermode
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 21 pages, 13 figures. To access the ML-MIX GitHub, click https://github.com/kermodegroup/ML-MIX
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Machine-learned interatomic potentials offer near first-principles accuracy but are computationally expensive, limiting their application in large-scale molecular dynamics simulations. Inspired by quantum mechanics/molecular mechanics methods, we present ML-MIX, an efficient and flexible LAMMPS package for accelerating simulations by spatially mixing interatomic potentials of different complexities. Through constrained linear fitting, we show it is possible to generate a 'cheap' approximate model which closely matches an 'expensive' reference in relevant regions of configuration space. We demonstrate the capability of ML-MIX through case-studies in Si, Fe, and W-He systems, achieving up to an 11x speedup on 8,000 atom systems without sacrificing accuracy on static and dynamic quantities, including calculation of minimum energy paths and dynamical simulations of defect diffusion. For larger domain sizes, we show that the achievable speedup of ML-MIX simulations is limited only by the relative speed of the cheap potential over the expensive potential. The ease of use and flexible nature of this method will extend the practical reach of MLIPs throughout computational materials science, enabling parsimonious application to large spatial and temporal domains.

## 人工智能(cs.AI:Artificial Intelligence)

### Agentic Mixture-of-Workflows for Multi-Modal Chemical Search 
[[arxiv](https://arxiv.org/abs/2502.19629)] [[cool](https://papers.cool/arxiv/2502.19629)] [[pdf](https://arxiv.org/pdf/2502.19629)]
> **Authors**: Tiffany J. Callahan,Nathaniel H. Park,Sara Capponi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: PDF includes supplemental material
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The vast and complex materials design space demands innovative strategies to integrate multidisciplinary scientific knowledge and optimize materials discovery. While large language models (LLMs) have demonstrated promising reasoning and automation capabilities across various domains, their application in materials science remains limited due to a lack of benchmarking standards and practical implementation frameworks. To address these challenges, we introduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented Generation (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic workflows employing distinct CRAG strategies using open-source LLMs. Unlike prior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration agent, enabling direct evaluation of multiple LLMs across the same problem domain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical reactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral retrieval. Our results demonstrate that CRAG-MoWs achieve performance comparable to GPT-4o while being preferred more frequently in comparative evaluations, highlighting the advantage of structured retrieval and multi-agent synthesis. By revealing performance variations across data types, CRAG-MoW provides a scalable, interpretable, and benchmark-driven approach to optimizing AI architectures for materials discovery. These insights are pivotal in addressing fundamental gaps in benchmarking LLMs and autonomous AI agents for scientific applications.

### Self-rewarding correction for mathematical reasoning 
[[arxiv](https://arxiv.org/abs/2502.19613)] [[cool](https://papers.cool/arxiv/2502.19613)] [[pdf](https://arxiv.org/pdf/2502.19613)]
> **Authors**: Wei Xiong,Hanning Zhang,Chenlu Ye,Lichang Chen,Nan Jiang,Tong Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.

### Program Synthesis Dialog Agents for Interactive Decision-Making 
[[arxiv](https://arxiv.org/abs/2502.19610)] [[cool](https://papers.cool/arxiv/2502.19610)] [[pdf](https://arxiv.org/pdf/2502.19610)]
> **Authors**: Matthew Toles,Nikhil Balwani,Rattandeep Singh,Valentina Giulia Sartori Rodriguez,Zhou Yu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Many real-world eligibility problems, ranging from medical diagnosis to tax planning, can be mapped to decision problems expressed in natural language, wherein a model must make a binary choice based on user features. Large-scale domains such as legal codes or frequently updated funding opportunities render human annotation (e.g., web forms or decision trees) impractical, highlighting the need for agents that can automatically assist in decision-making. Since relevant information is often only known to the user, it is crucial that these agents ask the right questions. As agents determine when to terminate a conversation, they face a trade-off between accuracy and the number of questions asked, a key metric for both user experience and cost. To evaluate this task, we propose BeNYfits, a new benchmark for determining user eligibility for multiple overlapping social benefits opportunities through interactive decision-making. Our experiments show that current language models struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a ReAct-style chain-of-thought. To address this, we introduce ProADA, a novel approach that leverages program synthesis to assist in decision-making by mapping dialog planning to a code generation problem and using gaps in structured data to determine the best next action. Our agent, ProADA, improves the F1 score to 55.6 while maintaining nearly the same number of dialog turns.

### Trustworthy Answers, Messier Data: Bridging the Gap in Low-Resource Retrieval-Augmented Generation for Domain Expert Systems 
[[arxiv](https://arxiv.org/abs/2502.19596)] [[cool](https://papers.cool/arxiv/2502.19596)] [[pdf](https://arxiv.org/pdf/2502.19596)]
> **Authors**: Nayoung Choi,Grace Byun,Andrew Chung,Ellie S. Paek,Shinsun Lee,Jinho D. Choi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,信息检索
- **Abstract**: RAG has become a key technique for enhancing LLMs by reducing hallucinations, especially in domain expert systems where LLMs may lack sufficient inherent knowledge. However, developing these systems in low-resource settings introduces several challenges: (1) handling heterogeneous data sources, (2) optimizing retrieval phase for trustworthy answers, and (3) evaluating generated answers across diverse aspects. To address these, we introduce a data generation pipeline that transforms raw multi-modal data into structured corpus and Q&A pairs, an advanced re-ranking phase improving retrieval precision, and a reference matching algorithm enhancing answer traceability. Applied to the automotive engineering domain, our system improves factual correctness (+1.94), informativeness (+1.16), and helpfulness (+1.67) over a non-RAG baseline, based on a 1-5 scale by an LLM judge. These results highlight the effectiveness of our approach across distinct aspects, with strong answer grounding and transparency.

### Repurposing the scientific literature with vision-language models 
[[arxiv](https://arxiv.org/abs/2502.19546)] [[cool](https://papers.cool/arxiv/2502.19546)] [[pdf](https://arxiv.org/pdf/2502.19546)]
> **Authors**: Anton Alyakin,Jaden Stryker,Daniel Alexander Alber,Karl L. Sangwon,Brandon Duderstadt,Akshay Save,David Kurland,Spencer Frome,Shrutika Singh,Jeff Zhang,Eunice Yang,Ki Yun Park,Cordelia Orillac,Aly A. Valliani,Sean Neifert,Albert Liu,Aneek Patel,Christopher Livia,Darryl Lau,Ilya Laufer,Peter A. Rozman,Eveline Teresa Hidalgo,Howard Riina,Rui Feng,Todd Hollon, et al. (6 additional authors not shown)
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互
- **Abstract**: Research in AI for Science often focuses on using AI technologies to augment components of the scientific process, or in some cases, the entire scientific method; how about AI for scientific publications? Peer-reviewed journals are foundational repositories of specialized knowledge, written in discipline-specific language that differs from general Internet content used to train most large language models (LLMs) and vision-language models (VLMs). We hypothesized that by combining a family of scientific journals with generative AI models, we could invent novel tools for scientific communication, education, and clinical care. We converted 23,000 articles from Neurosurgery Publications into a multimodal database - NeuroPubs - of 134 million words and 78,000 image-caption pairs to develop six datasets for building AI models. We showed that the content of NeuroPubs uniquely represents neurosurgery-specific clinical contexts compared with broader datasets and PubMed. For publishing, we employed generalist VLMs to automatically generate graphical abstracts from articles. Editorial board members rated 70% of these as ready for publication without further edits. For education, we generated 89,587 test questions in the style of the ABNS written board exam, which trainee and faculty neurosurgeons found indistinguishable from genuine examples 54% of the time. We used these questions alongside a curriculum learning process to track knowledge acquisition while training our 34 billion-parameter VLM (CNS-Obsidian). In a blinded, randomized controlled trial, we demonstrated the non-inferiority of CNS-Obsidian to GPT-4o (p = 0.1154) as a diagnostic copilot for a neurosurgical service. Our findings lay a novel foundation for AI with Science and establish a framework to elevate scientific communication using state-of-the-art generative artificial intelligence while maintaining rigorous quality standards.

### Opus: A Workflow Intention Framework for Complex Workflow Generation 
[[arxiv](https://arxiv.org/abs/2502.19532)] [[cool](https://papers.cool/arxiv/2502.19532)] [[pdf](https://arxiv.org/pdf/2502.19532)]
> **Authors**: Phillip Kingston,Théo Fagnoni,Mahsun Altin
> **First submission**: 2025-02-25
> **First announcement**: 2025-02-27
> **comment**: 1 Figure, 27 Pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper introduces Workflow Intention, a novel framework for identifying and encoding process objectives within complex business environments. Workflow Intention is the alignment of Input, Process and Output elements defining a Workflow's transformation objective interpreted from Workflow Signal inside Business Artefacts. It specifies how Input is processed to achieve desired Output, incorporating quality standards, business rules, compliance requirements and constraints. We adopt an end-to-end Business Artefact Encoder and Workflow Signal interpretation methodology involving four steps: Modality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion Attention then Intention Decoding. We provide training procedures and critical loss function definitions. In this paper we introduce the concepts of Workflow Signal and Workflow Intention, where Workflow Signal decomposed into Input, Process and Output elements is interpreted from Business Artefacts, and Workflow Intention is a complete triple of these elements. We introduce a mathematical framework for representing Workflow Signal as a vector and Workflow Intention as a tensor, formalizing properties of these objects. Finally, we propose a modular, scalable, trainable, attention-based multimodal generative system to resolve Workflow Intention from Business Artefacts.

### Building Knowledge Graphs Towards a Global Food Systems Datahub 
[[arxiv](https://arxiv.org/abs/2502.19507)] [[cool](https://papers.cool/arxiv/2502.19507)] [[pdf](https://arxiv.org/pdf/2502.19507)]
> **Authors**: Nirmal Gelal,Aastha Gautam,Sanaz Saki Norouzi,Nico Giordano,Claudio Dias da Silva Jr,Jean Ribert Francois,Kelsey Andersen Onofre,Katherine Nelson,Stacy Hutchinson,Xiaomao Lin,Stephen Welch,Romulo Lollato,Pascal Hitzler,Hande Küçük McGinty
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Sustainable agricultural production aligns with several sustainability goals established by the United Nations (UN). However, there is a lack of studies that comprehensively examine sustainable agricultural practices across various products and production methods. Such research could provide valuable insights into the diverse factors influencing the sustainability of specific crops and produce while also identifying practices and conditions that are universally applicable to all forms of agricultural production. While this research might help us better understand sustainability, the community would still need a consistent set of vocabularies. These consistent vocabularies, which represent the underlying datasets, can then be stored in a global food systems datahub. The standardized vocabularies might help encode important information for further statistical analyses and AI/ML approaches in the datasets, resulting in the research targeting sustainable agricultural production. A structured method of representing information in sustainability, especially for wheat production, is currently unavailable. In an attempt to address this gap, we are building a set of ontologies and Knowledge Graphs (KGs) that encode knowledge associated with sustainable wheat production using formal logic. The data for this set of knowledge graphs are collected from public data sources, experimental results collected at our experiments at Kansas State University, and a Sustainability Workshop that we organized earlier in the year, which helped us collect input from different stakeholders throughout the value chain of wheat. The modeling of the ontology (i.e., the schema) for the Knowledge Graph has been in progress with the help of our domain experts, following a modular structure using KNARM methodology. In this paper, we will present our preliminary results and schemas of our Knowledge Graph and ontologies.

### Conversational Planning for Personal Plans 
[[arxiv](https://arxiv.org/abs/2502.19500)] [[cool](https://papers.cool/arxiv/2502.19500)] [[pdf](https://arxiv.org/pdf/2502.19500)]
> **Authors**: Konstantina Christakopoulou,Iris Qu,John Canny,Andrew Goodridge,Cj Adams,Minmin Chen,Maja Matarić
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,机器学习
- **Abstract**: The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.

### TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding 
[[arxiv](https://arxiv.org/abs/2502.19400)] [[cool](https://papers.cool/arxiv/2502.19400)] [[pdf](https://arxiv.org/pdf/2502.19400)]
> **Authors**: Max Ku,Thomas Chong,Jonathan Leung,Krish Shah,Alvin Yu,Wenhu Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机视觉和模式识别,多媒体
- **Abstract**: Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.

### Joint Optimal Transport and Embedding for Network Alignment 
[[arxiv](https://arxiv.org/abs/2502.19334)] [[cool](https://papers.cool/arxiv/2502.19334)] [[pdf](https://arxiv.org/pdf/2502.19334)]
> **Authors**: Qi Yu,Zhichen Zeng,Yuchen Yan,Lei Ying,R. Srikant,Hanghang Tong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, 7 figures
- **标题**: None
- **领域**: 人工智能,机器学习,社交和信息网络
- **Abstract**: Network alignment, which aims to find node correspondence across different networks, is the cornerstone of various downstream multi-network and Web mining tasks. Most of the embedding-based methods indirectly model cross-network node relationships by contrasting positive and negative node pairs sampled from hand-crafted strategies, which are vulnerable to graph noises and lead to potential misalignment of nodes. Another line of work based on the optimal transport (OT) theory directly models cross-network node relationships and generates noise-reduced alignments. However, OT methods heavily rely on fixed, pre-defined cost functions that prohibit end-to-end training and are hard to generalize. In this paper, we aim to unify the embedding and OT-based methods in a mutually beneficial manner and propose a joint optimal transport and embedding framework for network alignment named JOENA. For one thing (OT for embedding), through a simple yet effective transformation, the noise-reduced OT mapping serves as an adaptive sampling strategy directly modeling all cross-network node pairs for robust embedding learning.For another (embedding for OT), on top of the learned embeddings, the OT cost can be gradually trained in an end-to-end fashion, which further enhances the alignment quality. With a unified objective, the mutual benefits of both methods can be achieved by an alternating optimization schema with guaranteed convergence. Extensive experiments on real-world networks validate the effectiveness and scalability of JOENA, achieving up to 16% improvement in MRR and 20x speedup compared with the state-of-the-art alignment methods.

### Complex LLM Planning via Automated Heuristics Discovery 
[[arxiv](https://arxiv.org/abs/2502.19295)] [[cool](https://papers.cool/arxiv/2502.19295)] [[pdf](https://arxiv.org/pdf/2502.19295)]
> **Authors**: Hongyi Ling,Shubham Parashar,Sambhav Khurana,Blake Olson,Anwesha Basu,Gaurangi Sinha,Zhengzhong Tu,James Caverlee,Shuiwang Ji
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We consider enhancing large language models (LLMs) for complex planning tasks. While existing methods allow LLMs to explore intermediate steps to make plans, they either depend on unreliable self-verification or external verifiers to evaluate these steps, which demand significant data and computations. Here, we propose automated heuristics discovery (AutoHD), a novel approach that enables LLMs to explicitly generate heuristic functions to guide inference-time search, allowing accurate evaluation of intermediate states. These heuristic functions are further refined through a heuristic evolution process, improving their robustness and effectiveness. Our proposed method requires no additional model training or fine-tuning, and the explicit definition of heuristic functions generated by the LLMs provides interpretability and insights into the reasoning process. Extensive experiments across diverse benchmarks demonstrate significant gains over multiple baselines, including nearly twice the accuracy on some datasets, establishing our approach as a reliable and interpretable solution for complex planning tasks.

### Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.19145)] [[cool](https://papers.cool/arxiv/2502.19145)] [[pdf](https://arxiv.org/pdf/2502.19145)]
> **Authors**: Pierre Peigne-Lefebvre,Mikolaj Kniejski,Filip Sondej,Matthieu David,Jason Hoelscher-Obermaier,Christian Schroeder de Witt,Esben Kran
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to AAAI 2025 Conference
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: As AI agents are increasingly adopted to collaborate on complex objectives, ensuring the security of autonomous multi-agent systems becomes crucial. We develop simulations of agents collaborating on shared objectives to study these security risks and security trade-offs. We focus on scenarios where an attacker compromises one agent, using it to steer the entire system toward misaligned outcomes by corrupting other agents. In this context, we observe infectious malicious prompts - the multi-hop spreading of malicious instructions. To mitigate this risk, we evaluated several strategies: two "vaccination" approaches that insert false memories of safely handling malicious input into the agents' memory stream, and two versions of a generic safety instruction strategy. While these defenses reduce the spread and fulfillment of malicious instructions in our experiments, they tend to decrease collaboration capability in the agent network. Our findings illustrate potential trade-off between security and collaborative efficiency in multi-agent systems, providing insights for designing more secure yet effective AI collaborations.

### A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management 
[[arxiv](https://arxiv.org/abs/2502.19135)] [[cool](https://papers.cool/arxiv/2502.19135)] [[pdf](https://arxiv.org/pdf/2502.19135)]
> **Authors**: Enrico Saccon,Ahmet Tikna,Davide De Martini,Edoardo Lamon,Luigi Palopoli,Marco Roveri
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互,机器人技术
- **Abstract**: This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning.

### Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation 
[[arxiv](https://arxiv.org/abs/2502.19091)] [[cool](https://papers.cool/arxiv/2502.19091)] [[pdf](https://arxiv.org/pdf/2502.19091)]
> **Authors**: Humza Sami,Mubashir ul Islam,Samy Charas,Asav Gandhi,Pierre-Emmanuel Gaillardon,Valerio Tenace
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统,软件工程
- **Abstract**: Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS. To address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities. Experimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%.

### Talking like Piping and Instrumentation Diagrams (P&IDs) 
[[arxiv](https://arxiv.org/abs/2502.18928)] [[cool](https://papers.cool/arxiv/2502.18928)] [[pdf](https://arxiv.org/pdf/2502.18928)]
> **Authors**: Achmad Anggawirya Alimin,Dominik P. Goldstein,Lukas Schulze Balhorn,Artur M. Schweidtmann
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P&IDs) using natural language. In particular, we represent P&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P&ID knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P&IDs using natural language. It extends LLM's ability to retrieve contextual data from P&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P&IDs, and AI-assisted HAZOP studies.

### Multi-LLM Collaborative Search for Complex Problem Solving 
[[arxiv](https://arxiv.org/abs/2502.18873)] [[cool](https://papers.cool/arxiv/2502.18873)] [[pdf](https://arxiv.org/pdf/2502.18873)]
> **Authors**: Sen Yang,Yafu Li,Wai Lam,Yu Cheng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.

### Towards an AI co-scientist 
[[arxiv](https://arxiv.org/abs/2502.18864)] [[cool](https://papers.cool/arxiv/2502.18864)] [[pdf](https://arxiv.org/pdf/2502.18864)]
> **Authors**: Juraj Gottweis,Wei-Hung Weng,Alexander Daryin,Tao Tu,Anil Palepu,Petar Sirkovic,Artiom Myaskovsky,Felix Weissenberger,Keran Rong,Ryutaro Tanno,Khaled Saab,Dan Popovici,Jacob Blum,Fan Zhang,Katherine Chou,Avinatan Hassidim,Burak Gokturk,Amin Vahdat,Pushmeet Kohli,Yossi Matias,Andrew Carroll,Kavita Kulkarni,Nenad Tomasev,Yuan Guan,Vikram Dhillon, et al. (9 additional authors not shown)
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 81 pages in total (main 38 pages, appendix 43 pages), 13 main figures, 40 appendix figures, 1 main table, 2 appendix tables, 143 main references, 7 appendix references
- **标题**: None
- **领域**: 人工智能,计算语言学,人机交互,机器学习,物理与社会,其他定量生物学
- **Abstract**: Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.

### Intelligence Test 
[[arxiv](https://arxiv.org/abs/2502.18858)] [[cool](https://papers.cool/arxiv/2502.18858)] [[pdf](https://arxiv.org/pdf/2502.18858)]
> **Authors**: Jingtao Zhan,Jiahao Zhao,Jiayu Li,Yiqun Liu,Bo Zhang,Qingyao Ai,Jiaxin Mao,Hongning Wang,Min Zhang,Shaoping Ma
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: How does intelligence emerge? We propose that intelligence is not a sudden gift or random occurrence, but rather a necessary trait for species to survive through Natural Selection. If a species passes the test of Natural Selection, it demonstrates the intelligence to survive in nature. Extending this perspective, we introduce Intelligence Test, a method to quantify the intelligence of any subject on any task. Like how species evolve by trial and error, Intelligence Test quantifies intelligence by the number of failed attempts before success. Fewer failures correspond to higher intelligence. When the expectation and variance of failure counts are both finite, it signals the achievement of an autonomous level of intelligence. Using Intelligence Test, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve a level of autonomy in simple tasks, they are still far from autonomous in more complex tasks, such as vision, search, recommendation, and language. While scaling model size might help, this would come at an astronomical cost. Projections suggest that achieving general autonomy would require unimaginable $10^{26}$ parameters. Even if Moore's Law continuously holds, such a parameter scale would take $70$ years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI. To further understand this phenomenon, we conduct a theoretical analysis. Our simulations suggest that human tasks possess a criticality property. As a result, autonomy requires a deep understanding of the task's underlying mechanisms. Current AI, however, does not fully grasp these mechanisms and instead relies on superficial mimicry, making it difficult to reach an autonomous level. We believe Intelligence Test can not only guide the future development of AI but also offer profound insights into the intelligence of humans ourselves.

### REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.18836)] [[cool](https://papers.cool/arxiv/2502.18836)] [[pdf](https://arxiv.org/pdf/2502.18836)]
> **Authors**: Longling Geng,Edward Y. Chang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 14 pages, 4 figures, 9 tables
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This benchmark suite provides a comprehensive evaluation framework for assessing both individual LLMs and multi-agent systems in real-world planning scenarios. The suite encompasses eleven designed problems that progress from basic to highly complex, incorporating key aspects such as multi-agent coordination, inter-agent dependencies, and dynamic environmental disruptions. Each problem can be scaled along three dimensions: the number of parallel planning threads, the complexity of inter-dependencies, and the frequency of unexpected disruptions requiring real-time adaptation. The benchmark includes detailed specifications, evaluation metrics, and baseline implementations using contemporary frameworks like LangGraph, enabling rigorous testing of both single-agent and multi-agent planning capabilities. Through standardized evaluation criteria and scalable complexity, this benchmark aims to drive progress in developing more robust and adaptable AI planning systems for real-world applications.

## 硬件架构(cs.AR:Hardware Architecture)

### HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration 
[[arxiv](https://arxiv.org/abs/2502.19662)] [[cool](https://papers.cool/arxiv/2502.19662)] [[pdf](https://arxiv.org/pdf/2502.19662)]
> **Authors**: Rohan Juneja,Shivam Aggarwal,Safeen Huda,Tulika Mitra,Li-Shiuan Peh
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: Quantization is critical for realizing efficient inference of LLMs. Traditional quantization methods are hardware-agnostic, limited to bit-width constraints, and lacking circuit-level insights, such as timing and energy characteristics of Multiply-Accumulate (MAC) units. We introduce HALO, a versatile framework that adapts to various hardware through a Hardware-Aware Post-Training Quantization (PTQ) approach. By leveraging MAC unit properties, HALO minimizes critical-path delays and enables dynamic frequency scaling. Deployed on LLM accelerators like TPUs and GPUs, HALO achieves on average 270% performance gains and 51% energy savings, all with minimal accuracy drop.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### FinTSB: A Comprehensive and Practical Benchmark for Financial Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18834)] [[cool](https://papers.cool/arxiv/2502.18834)] [[pdf](https://arxiv.org/pdf/2502.18834)]
> **Authors**: Yifan Hu,Yuante Li,Peiyuan Liu,Yuxia Zhu,Naiqi Li,Tao Dai,Shu-tao Xia,Dawei Cheng,Changjun Jiang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Financial time series (FinTS) record the behavior of human-brain-augmented decision-making, capturing valuable historical information that can be leveraged for profitable investment strategies. Not surprisingly, this area has attracted considerable attention from researchers, who have proposed a wide range of methods based on various backbones. However, the evaluation of the area often exhibits three systemic limitations: 1. Failure to account for the full spectrum of stock movement patterns observed in dynamic financial markets. (Diversity Gap), 2. The absence of unified assessment protocols undermines the validity of cross-study performance comparisons. (Standardization Deficit), and 3. Neglect of critical market structure factors, resulting in inflated performance metrics that lack practical applicability. (Real-World Mismatch). Addressing these limitations, we propose FinTSB, a comprehensive and practical benchmark for financial time series forecasting (FinTSF). To increase the variety, we categorize movement patterns into four specific parts, tokenize and pre-process the data, and assess the data quality based on some sequence characteristics. To eliminate biases due to different evaluation settings, we standardize the metrics across three dimensions and build a user-friendly, lightweight pipeline incorporating methods from various backbones. To accurately simulate real-world trading scenarios and facilitate practical implementation, we extensively model various regulatory constraints, including transaction fees, among others. Finally, we conduct extensive experiments on FinTSB, highlighting key insights to guide model selection under varying market conditions. Overall, FinTSB provides researchers with a novel and comprehensive platform for improving and evaluating FinTSF methods. The code is available at https://github.com/TongjiFinLab/FinTSBenchmark.

## 计算语言学(cs.CL:Computation and Language)

### PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation 
[[arxiv](https://arxiv.org/abs/2502.19756)] [[cool](https://papers.cool/arxiv/2502.19756)] [[pdf](https://arxiv.org/pdf/2502.19756)]
> **Authors**: Nathan Roll
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 6 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.

### Beneath the Surface: How Large Language Models Reflect Hidden Bias 
[[arxiv](https://arxiv.org/abs/2502.19749)] [[cool](https://papers.cool/arxiv/2502.19749)] [[pdf](https://arxiv.org/pdf/2502.19749)]
> **Authors**: Jinhao Pan,Chahat Raj,Ziyu Yao,Ziwei Zhu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The exceptional performance of Large Language Models (LLMs) often comes with the unintended propagation of social biases embedded in their training data. While existing benchmarks evaluate overt bias through direct term associations between bias concept terms and demographic terms, LLMs have become increasingly adept at avoiding biased responses, creating an illusion of neutrality. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a novel dataset designed to assess hidden bias that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response to overt bias, they continue to reinforce biases in nuanced settings. Data, code, and results are available at https://github.com/JP-25/Hidden-Bias-Benchmark.

### HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture 
[[arxiv](https://arxiv.org/abs/2502.19747)] [[cool](https://papers.cool/arxiv/2502.19747)] [[pdf](https://arxiv.org/pdf/2502.19747)]
> **Authors**: Taiqiang Wu,Chenchen Ding,Wenyong Zhou,Yuxin Cheng,Xincheng Feng,Shuqi Wang,Chufan Shi,Zhengwu Liu,Ngai Wong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 7 pages
- **标题**: None
- **领域**: 计算语言学,硬件架构
- **Abstract**: Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning method to adapt large language models (LLMs) for downstream tasks. In this paper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid compute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and LoRA onto SRAM). To address performance degradation from RRAM's inherent noise, we design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to train a LoRA branch that is both robust and accurate by aligning the training objectives under both ideal and noisy conditions. Experiments finetuning LLaMA 3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning tasks, achieving up to 22.7 improvement in average score while maintaining robustness at various noise levels.

### XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs 
[[arxiv](https://arxiv.org/abs/2502.19737)] [[cool](https://papers.cool/arxiv/2502.19737)] [[pdf](https://arxiv.org/pdf/2502.19737)]
> **Authors**: Linyang He,Ercong Nie,Sukru Samet Dindar,Arsalan Firoozi,Adrian Florea,Van Nguyen,Corentin Puffay,Riki Shimizu,Haotian Ye,Jonathan Brennan,Helmut Schmid,Hinrich Schütze,Nima Mesgarani
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce XCOMPS in this work, a multilingual conceptual minimal pair dataset covering 17 languages. Using this dataset, we evaluate LLMs' multilingual conceptual understanding through metalinguistic prompting, direct probability measurement, and neurolinguistic probing. By comparing base, instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs exhibit weaker conceptual understanding for low-resource languages, and accuracy varies across languages despite being tested on the same concept sets. 2) LLMs excel at distinguishing concept-property pairs that are visibly different but exhibit a marked performance drop when negative pairs share subtle semantic similarities. 3) Instruction tuning improves performance in concept understanding but does not enhance internal competence; knowledge distillation can enhance internal competence in conceptual understanding for low-resource languages with limited gains in explicit task performance. 4) More morphologically complex languages yield lower concept understanding scores and require deeper layers for conceptual reasoning.

### R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning 
[[arxiv](https://arxiv.org/abs/2502.19735)] [[cool](https://papers.cool/arxiv/2502.19735)] [[pdf](https://arxiv.org/pdf/2502.19735)]
> **Authors**: Minggui He,Yilun Liu,Shimin Tao,Yuanchang Luo,Hongyong Zeng,Chang Su,Li Zhang,Hongxia Ma,Daimeng Wei,Weibin Meng,Hao Yang,Boxing Chen,Osamu Yoshie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.

### Speculative Decoding and Beyond: An In-Depth Review of Techniques 
[[arxiv](https://arxiv.org/abs/2502.19732)] [[cool](https://papers.cool/arxiv/2502.19732)] [[pdf](https://arxiv.org/pdf/2502.19732)]
> **Authors**: Yunhai Hu,Zining Liu,Zhenyuan Dong,Tianfan Peng,Bradley McDanel,Sai Qian Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sequential dependencies present a fundamental bottleneck in deploying large-scale autoregressive models, particularly for real-time applications. While traditional optimization approaches like pruning and quantization often compromise model quality, recent advances in generation-refinement frameworks demonstrate that this trade-off can be significantly mitigated. This survey presents a comprehensive taxonomy of generation-refinement frameworks, analyzing methods across autoregressive sequence tasks. We categorize methods based on their generation strategies (from simple n-gram prediction to sophisticated draft models) and refinement mechanisms (including single-pass verification and iterative approaches). Through systematic analysis of both algorithmic innovations and system-level implementations, we examine deployment strategies across computing environments and explore applications spanning text, images, and speech generation. This systematic examination of both theoretical frameworks and practical implementations provides a foundation for future research in efficient autoregressive decoding.

### Preference Learning Unlocks LLMs' Psycho-Counseling Skills 
[[arxiv](https://arxiv.org/abs/2502.19731)] [[cool](https://papers.cool/arxiv/2502.19731)] [[pdf](https://arxiv.org/pdf/2502.19731)]
> **Authors**: Mian Zhang,Shaun M. Eack,Zhiyu Zoey Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.

### CNsum:Automatic Summarization for Chinese News Text 
[[arxiv](https://arxiv.org/abs/2502.19723)] [[cool](https://papers.cool/arxiv/2502.19723)] [[pdf](https://arxiv.org/pdf/2502.19723)]
> **Authors**: Yu Zhao,Songping Huang,Dongsheng Zhou,Zhaoyun Ding,Fei Wang,Aixin Nian
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: WASA 2022
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Obtaining valuable information from massive data efficiently has become our research goal in the era of Big Data. Text summarization technology has been continuously developed to meet this demand. Recent work has also shown that transformer-based pre-trained language models have achieved great success on various tasks in Natural Language Processing (NLP). Aiming at the problem of Chinese news text summary generation and the application of Transformer structure on Chinese, this paper proposes a Chinese news text summarization model (CNsum) based on Transformer structure, and tests it on Chinese datasets such as THUCNews. The results of the conducted experiments show that CNsum achieves better ROUGE score than the baseline models, which verifies the outperformance of the model.

### Few-Shot Multilingual Open-Domain QA from 5 Examples 
[[arxiv](https://arxiv.org/abs/2502.19722)] [[cool](https://papers.cool/arxiv/2502.19722)] [[pdf](https://arxiv.org/pdf/2502.19722)]
> **Authors**: Fan Jiang,Tom Drummond,Trevor Cohn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by TACL; pre-MIT Press publication version
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Recent approaches to multilingual open-domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a \emph{few-shot learning} approach to synthesise large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, \textsc{FsModQA}, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a \emph{cross-lingual prompting} strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.

### Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19721)] [[cool](https://papers.cool/arxiv/2502.19721)] [[pdf](https://arxiv.org/pdf/2502.19721)]
> **Authors**: Hannah Cyberey,Yangfeng Ji,David Evans
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.

### GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration 
[[arxiv](https://arxiv.org/abs/2502.19684)] [[cool](https://papers.cool/arxiv/2502.19684)] [[pdf](https://arxiv.org/pdf/2502.19684)]
> **Authors**: Yoo Yeon Sung,Eve Fleisig,Yu Hou,Ishan Upadhyay,Jordan Lee Boyd-Graber
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.

### Investigating Neurons and Heads in Transformer-based LLMs for Typographical Errors 
[[arxiv](https://arxiv.org/abs/2502.19669)] [[cool](https://papers.cool/arxiv/2502.19669)] [[pdf](https://arxiv.org/pdf/2502.19669)]
> **Authors**: Kohei Tsuji,Tatsuya Hiraoka,Yuchang Cheng,Eiji Aramaki,Tomoya Iwakura
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 14 pages, 10 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper investigates how LLMs encode inputs with typos. We hypothesize that specific neurons and attention heads recognize typos and fix them internally using local and global contexts. We introduce a method to identify typo neurons and typo heads that work actively when inputs contain typos. Our experimental results suggest the following: 1) LLMs can fix typos with local contexts when the typo neurons in either the early or late layers are activated, even if those in the other are not. 2) Typo neurons in the middle layers are responsible for the core of typo-fixing with global contexts. 3) Typo heads fix typos by widely considering the context not focusing on specific tokens. 4) Typo neurons and typo heads work not only for typo-fixing but also for understanding general contexts.

### Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19655)] [[cool](https://papers.cool/arxiv/2502.19655)] [[pdf](https://arxiv.org/pdf/2502.19655)]
> **Authors**: Sheng Zhang,Qianchu Liu,Guanghui Qin,Tristan Naumann,Hoifung Poon
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine.

### Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.19622)] [[cool](https://papers.cool/arxiv/2502.19622)] [[pdf](https://arxiv.org/pdf/2502.19622)]
> **Authors**: Yanan Chen,Ali Pesaranghader,Tanmana Sadhu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, 1 figure, 3 tables, 4 prompt/data templates
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in Large Language Models (LLMs) have raised interest in their formal reasoning capabilities, particularly in mathematics. While closed LLMs like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains unclear whether small to medium-sized open LLMs can achieve similar performance, questioning their reliability. To close this gap, we propose a post-training approach leveraging a mixture of opinions (MoO) from weaker ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that, each post-training sample is augmented with Chain-of-Thought (CoT) reasoning steps and answers from ancillary LLMs, enabling the main LLM to learn from diverse perspectives. We compare MoO with standard supervised fine-tuning (SFT), few-shot prompting, and the Mixture of Agents (MoA) method on mathematical reasoning benchmarks. Our results show that incorporating weaker LLMs' opinions improves mathematical reasoning by an average of 5%, highlighting the value of diverse perspectives in reasoning tasks.

### Is Your Paper Being Reviewed by an LLM? A New Benchmark Dataset and Approach for Detecting AI Text in Peer Review 
[[arxiv](https://arxiv.org/abs/2502.19614)] [[cool](https://papers.cool/arxiv/2502.19614)] [[pdf](https://arxiv.org/pdf/2502.19614)]
> **Authors**: Sungduk Yu,Man Luo,Avinash Madusu,Vasudev Lal,Phillip Howard
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Motivated by the shortcomings of existing methods, we propose a new detection approach which surpasses existing methods in the identification of AI written peer reviews. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI.

### Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization 
[[arxiv](https://arxiv.org/abs/2502.19612)] [[cool](https://papers.cool/arxiv/2502.19612)] [[pdf](https://arxiv.org/pdf/2502.19612)]
> **Authors**: Anwar Hossain Zahid,Monoshi Kumar Roy,Swarna Das
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 6 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: The proliferation of hate speech on social media is one of the serious issues that is bringing huge impacts to society: an escalation of violence, discrimination, and social fragmentation. The problem of detecting hate speech is intrinsically multifaceted due to cultural, linguistic, and contextual complexities and adversarial manipulations. In this study, we systematically investigate the performance of LLMs on detecting hate speech across multilingual datasets and diverse geographic contexts. Our work presents a new evaluation framework in three dimensions: binary classification of hate speech, geography-aware contextual detection, and robustness to adversarially generated text. Using a dataset of 1,000 comments from five diverse regions, we evaluate three state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder (6.7b). Codellama had the best binary classification recall with 70.6% and an F1-score of 52.18%, whereas DeepSeekCoder had the best performance in geographic sensitivity, correctly detecting 63 out of 265 locations. The tests for adversarial robustness also showed significant weaknesses; Llama2 misclassified 62.5% of manipulated samples. These results bring to light the trade-offs between accuracy, contextual understanding, and robustness in the current versions of LLMs. This work has thus set the stage for developing contextually aware, multilingual hate speech detection systems by underlining key strengths and limitations, therefore offering actionable insights for future research and real-world applications.

### Revisiting Word Embeddings in the LLM Era 
[[arxiv](https://arxiv.org/abs/2502.19607)] [[cool](https://papers.cool/arxiv/2502.19607)] [[pdf](https://arxiv.org/pdf/2502.19607)]
> **Authors**: Yash Mahajan,Matthew Freestone,Sathyanarayanan Aakur,Santu Karmaker
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have recently shown remarkable advancement in various NLP tasks. As such, a popular trend has emerged lately where NLP researchers extract word/sentence/document embeddings from these large decoder-only models and use them for various inference tasks with promising results. However, it is still unclear whether the performance improvement of LLM-induced embeddings is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This is the central question we investigate in the paper by systematically comparing classical decontextualized and contextualized word embeddings with the same for LLM-induced embeddings. Our results show that LLMs cluster semantically related words more tightly and perform better on analogy tasks in decontextualized settings. However, in contextualized settings, classical models like SimCSE often outperform LLMs in sentence-level similarity assessment tasks, highlighting their continued relevance for fine-grained semantics.

### A City of Millions: Mapping Literary Social Networks At Scale 
[[arxiv](https://arxiv.org/abs/2502.19590)] [[cool](https://papers.cool/arxiv/2502.19590)] [[pdf](https://arxiv.org/pdf/2502.19590)]
> **Authors**: Sil Hamilton,Rebecca M. M. Hicke,David Mimno,Matthew Wilkens
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,社交和信息网络
- **Abstract**: We release 70,509 high-quality social networks extracted from multilingual fiction and nonfiction narratives. We additionally provide metadata for ~30,000 of these texts (73% nonfiction and 27% fiction) written between 1800 and 1999 in 58 languages. This dataset provides information on historical social worlds at an unprecedented scale, including data for 1,192,855 individuals in 2,805,482 pair-wise relationships annotated for affinity and relationship type. We achieve this scale by automating previously manual methods of extracting social networks; specifically, we adapt an existing annotation task as a language model prompt, ensuring consistency at scale with the use of structured output. This dataset provides an unprecedented resource for the humanities and social sciences by providing data on cognitive models of social realities.

### NeoBERT: A Next-Generation BERT 
[[arxiv](https://arxiv.org/abs/2502.19587)] [[cool](https://papers.cool/arxiv/2502.19587)] [[pdf](https://arxiv.org/pdf/2502.19587)]
> **Authors**: Lola Le Breton,Quentin Fournier,Mariam El Mezouar,Sarath Chandar
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 19 pages, 5 figures, 9 tables. Submitted to TMLR
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.

### Where Are We? Evaluating LLM Performance on African Languages 
[[arxiv](https://arxiv.org/abs/2502.19582)] [[cool](https://papers.cool/arxiv/2502.19582)] [[pdf](https://arxiv.org/pdf/2502.19582)]
> **Authors**: Ife Adebara,Hawau Olamide Toyin,Nahom Tesfu Ghebremichael,AbdelRahim Elmadany,Muhammad Abdul-Mageed
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.

### Do Large Language Models Know How Much They Know? 
[[arxiv](https://arxiv.org/abs/2502.19573)] [[cool](https://papers.cool/arxiv/2502.19573)] [[pdf](https://arxiv.org/pdf/2502.19573)]
> **Authors**: Gabriele Prato,Jerry Huang,Prasannna Parthasarathi,Shagun Sodhani,Sarath Chandar
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.

### Stay Focused: Problem Drift in Multi-Agent Debate 
[[arxiv](https://arxiv.org/abs/2502.19559)] [[cool](https://papers.cool/arxiv/2502.19559)] [[pdf](https://arxiv.org/pdf/2502.19559)]
> **Authors**: Jonas Becker,Lars Benedikt Kaesberg,Andreas Stephan,Jan Philip Wahle,Terry Ruas,Bela Gipp
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 34 pages, 21 figures, 4 tables, under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations, particularly when scaling them to longer reasoning chains. In this study, we unveil a new issue of multi-agent debate: discussions drift away from the initial problem over multiple turns. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, we perform a human study with eight experts on discussions suffering from problem drift, who find the most common issues are a lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To systematically address the issue of problem drift, we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of problem drift cases. Our study can be seen as a first step to understanding a key limitation of multi-agent debate, highlighting pathways for improving their effectiveness in the future.

### Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones? 
[[arxiv](https://arxiv.org/abs/2502.19557)] [[cool](https://papers.cool/arxiv/2502.19557)] [[pdf](https://arxiv.org/pdf/2502.19557)]
> **Authors**: Yudi Zhang,Lu Wang,Meng Fang,Yali Du,Chenghua Huang,Jun Wang,Qingwei Lin,Mykola Pechenizkiy,Dongmei Zhang,Saravan Rajmohan,Qi Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 14 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Distilling large language models (LLMs) typically involves transferring the teacher model's responses through supervised fine-tuning (SFT). However, this approach neglects the potential to distill both data (output content) and reward signals (quality evaluations). Extracting reliable reward signals directly from teacher models is challenging, as LLMs are optimized for generation rather than evaluation, often resulting in biased or inconsistent assessments. To address this limitation, we propose a novel distillation pipeline that transfers both responses and rewards. Our method generates pseudo-rewards through a self-supervised mechanism that leverages the inherent structure of both teacher and student responses, enabling reward learning without explicit external evaluation. The reward model subsequently guides reinforcement learning (RL), allowing iterative refinement of the student model after an SFT warm-up phase. Experiments on GSM8K and MMLU-PRO demonstrate that our method consistently outperforms traditional SFT-based approaches, enabling student models to surpass the performance of their teachers. This work highlights the potential for scalable, efficient distillation through structured self-supervised reward learning, reducing dependence on external reward supervision.

### When Large Language Models Meet Speech: A Survey on Integration Approaches 
[[arxiv](https://arxiv.org/abs/2502.19548)] [[cool](https://papers.cool/arxiv/2502.19548)] [[pdf](https://arxiv.org/pdf/2502.19548)]
> **Authors**: Zhengdong Yang,Shuichiro Shimizu,Yahan Yu,Chenhui Chu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for

### Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents 
[[arxiv](https://arxiv.org/abs/2502.19545)] [[cool](https://papers.cool/arxiv/2502.19545)] [[pdf](https://arxiv.org/pdf/2502.19545)]
> **Authors**: Ashley Lewis,Michael White,Jing Liu,Toshiaki Koike-Akino,Kieran Parsons,Ye Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information-and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized "I don't know" responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.

### Cognitive networks highlight differences and similarities in the STEM mindsets of human and LLM-simulated trainees, experts and academics 
[[arxiv](https://arxiv.org/abs/2502.19529)] [[cool](https://papers.cool/arxiv/2502.19529)] [[pdf](https://arxiv.org/pdf/2502.19529)]
> **Authors**: Edith Haim,Lars van den Bergh,Cynthia S. Q. Siew,Yoed N. Kenett,Daniele Marinazzo,Massimo Stella
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Keywords: cognitivenetworkscience; mindset measurement; associative knowledge; artificial intelligence; simulated participants
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Understanding attitudes towards STEM means quantifying the cognitive and emotional ways in which individuals, and potentially large language models too, conceptualise such subjects. This study uses behavioural forma mentis networks (BFMNs) to investigate the STEM-focused mindset, i.e. ways of associating and perceiving ideas, of 177 human participants and 177 artificial humans simulated by GPT-3.5. Participants were split in 3 groups - trainees, experts and academics - to compare the influence of expertise level on their mindset. The results revealed that human forma mentis networks exhibited significantly higher clustering coefficients compared to GPT-3.5, indicating that human mindsets displayed a tendency to form and close triads of conceptual associations while recollecting STEM ideas. Human experts, in particular, demonstrated robust clustering coefficients, reflecting better integration of STEM concepts into their cognitive networks. In contrast, GPT-3.5 produced sparser mindsets. Furthermore, both human and GPT mindsets framed mathematics in neutral or positive terms, differently from STEM high schoolers, researchers and other large language models sampled in other works. This research contributes to understanding how mindset structure can provide cognitive insights about memory structure and machine limitations.

### Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2502.19416)] [[cool](https://papers.cool/arxiv/2502.19416)] [[pdf](https://arxiv.org/pdf/2502.19416)]
> **Authors**: Akshat Gupta,Christine Fang,Atahan Ozdemir,Maochuan Lu,Ahmed Alaa,Thomas Hartvigsen,Gopala Anumanchipalli
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin note: text overlap with arXiv:2502.01636
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.

### The Mighty ToRR: A Benchmark for Table Reasoning and Robustness 
[[arxiv](https://arxiv.org/abs/2502.19412)] [[cool](https://papers.cool/arxiv/2502.19412)] [[pdf](https://arxiv.org/pdf/2502.19412)]
> **Authors**: Shir Ashury-Tahan,Yifan Mai,Rajmohan C,Ariel Gera,Yotam Perlitz,Asaf Yehudai,Elron Bandel,Leshem Choshen,Eyal Shnarch,Percy Liang,Michal Shmueli-Scheuer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, that measures model performance and robustness on table-related tasks. The benchmark includes 10 datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that table understanding and reasoning tasks remain a significant challenge.

### Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19411)] [[cool](https://papers.cool/arxiv/2502.19411)] [[pdf](https://arxiv.org/pdf/2502.19411)]
> **Authors**: Dayu Yang,Tianyang Liu,Daoan Zhang,Antoine Simoulin,Xiaoyi Liu,Yuwei Cao,Zhaopu Teng,Xin Qian,Grey Yang,Jiebo Luo,Julian McAuley
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,软件工程
- **Abstract**: In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.

### DataMan: Data Manager for Pre-training Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19363)] [[cool](https://papers.cool/arxiv/2502.19363)] [[pdf](https://arxiv.org/pdf/2502.19363)]
> **Authors**: Ru Peng,Kexin Yang,Yawen Zeng,Junyang Lin,Dayiheng Liu,Junbo Zhao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ICLR2025 paper
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by ``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.

### Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning? 
[[arxiv](https://arxiv.org/abs/2502.19361)] [[cool](https://papers.cool/arxiv/2502.19361)] [[pdf](https://arxiv.org/pdf/2502.19361)]
> **Authors**: Yancheng He,Shilong Li,Jiaheng Liu,Weixun Wang,Xingyuan Bu,Ge Zhang,Zhongyuan Peng,Zhaoxiang Zhang,Zhicheng Zheng,Wenbo Su,Bo Zheng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: The first four authors contributed equally, 27 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.

### Controlled Diversity: Length-optimized Natural Language Generation 
[[arxiv](https://arxiv.org/abs/2502.19347)] [[cool](https://papers.cool/arxiv/2502.19347)] [[pdf](https://arxiv.org/pdf/2502.19347)]
> **Authors**: Diana Marie Schenke,Timo Baumann
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: LLMs are not generally able to adjust the length of their outputs based on strict length requirements, a capability that would improve their usefulness in applications that require adherence to diverse user and system requirements. We present an approach to train LLMs to acquire this capability by augmenting existing data and applying existing fine-tuning techniques, which we compare based on the trained models' adherence to the length requirement and overall response quality relative to the baseline model. Our results demonstrate that these techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements. Our results indicate that our method may change the response quality when using training data that was not generated by the baseline model. This allows simultaneous alignment to another training objective in certain scenarios, but is undesirable otherwise. Training on a dataset containing the model's own responses eliminates this issue.

### Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets 
[[arxiv](https://arxiv.org/abs/2502.19339)] [[cool](https://papers.cool/arxiv/2502.19339)] [[pdf](https://arxiv.org/pdf/2502.19339)]
> **Authors**: Tohida Rehman,Soumabha Ghosh,Kuntal Das,Souvik Bhattacharjee,Debarshi Kumar Sanyal,Samiran Chattopadhyay
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 5 pages, 2 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Text summarization plays a crucial role in natural language processing by condensing large volumes of text into concise and coherent summaries. As digital content continues to grow rapidly and the demand for effective information retrieval increases, text summarization has become a focal point of research in recent years. This study offers a thorough evaluation of four leading pre-trained and open-source large language models: BART, FLAN-T5, LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News Summary, XSum, and BBC News. The evaluation employs widely recognized automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess the models' capabilities in generating coherent and informative summaries. The results reveal the comparative strengths and limitations of these models in processing various text types.

### Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems 
[[arxiv](https://arxiv.org/abs/2502.19328)] [[cool](https://papers.cool/arxiv/2502.19328)] [[pdf](https://arxiv.org/pdf/2502.19328)]
> **Authors**: Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Bin Xu,Lei Hou,Juanzi Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 16 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).

### Shh, don't say that! Domain Certification in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19320)] [[cool](https://papers.cool/arxiv/2502.19320)] [[pdf](https://arxiv.org/pdf/2502.19320)]
> **Authors**: Cornelius Emde,Alasdair Paren,Preetham Arvind,Maxime Kayser,Tom Rainforth,Thomas Lukasiewicz,Bernard Ghanem,Philip H. S. Torr,Adel Bibi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, includes appendix Published in International Conference onLearningRepresentations (ICLR) 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,机器学习,机器学习
- **Abstract**: Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.

### CritiQ: Mining Data Quality Criteria from Human Preferences 
[[arxiv](https://arxiv.org/abs/2502.19279)] [[cool](https://papers.cool/arxiv/2502.19279)] [[pdf](https://arxiv.org/pdf/2502.19279)]
> **Authors**: Honglin Guo,Kai Lv,Qipeng Guo,Tianyi Liang,Zhiheng Xi,Demin Song,Qiuyinzhe Zhang,Yu Sun,Kai Chen,Xipeng Qiu,Tao Gui
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only $\sim$30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.

### Disentangled VAD Representations via a Variational Framework for Political Stance Detection 
[[arxiv](https://arxiv.org/abs/2502.19276)] [[cool](https://papers.cool/arxiv/2502.19276)] [[pdf](https://arxiv.org/pdf/2502.19276)]
> **Authors**: Beiyu Xu,Zhiwei Liu,Sophia Ananiadou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The stance detection task aims to categorise the stance regarding specified targets. Current methods face challenges in effectively integrating sentiment information for stance detection. Moreover, the role of highly granular sentiment labelling in stance detection has been largely overlooked. This study presents a novel stance detection framework utilizing a variational autoencoder (VAE) to disentangle latent emotional features-value, arousal, and dominance (VAD)-from political discourse on social media. This approach addresses limitations in current methods, particularly in in-target and cross-target stance detection scenarios. This research uses an advanced emotional annotation tool to annotate seven-class sentiment labels for P-STANCE. Evaluations on benchmark datasets, including P-STANCE and SemEval-2016, reveal that PoliStance-VAE achieves state-of-the-art performance, surpassing models like BERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable solution for stance detection, demonstrating the effectiveness of integrating nuanced emotional representations. This framework paves the way for advancements in natural language processing tasks, particularly those requiring detailed emotional understanding.

### Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization 
[[arxiv](https://arxiv.org/abs/2502.19261)] [[cool](https://papers.cool/arxiv/2502.19261)] [[pdf](https://arxiv.org/pdf/2502.19261)]
> **Authors**: Taishi Nakamura,Takuya Akiba,Kazuki Fujii,Yusuke Oda,Rio Yokota,Jun Suzuki
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: To appear at the 13th International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. Extensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more. As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs. All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE.

### Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases 
[[arxiv](https://arxiv.org/abs/2502.19249)] [[cool](https://papers.cool/arxiv/2502.19249)] [[pdf](https://arxiv.org/pdf/2502.19249)]
> **Authors**: Michael Y. Hu,Jackson Petty,Chuan Shi,William Merrill,Tal Linzen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Pretraining language models on formal languages can improve their acquisition of natural language, but it is unclear which features of the formal language impart an inductive bias that leads to effective transfer. Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when the formal language both captures dependency structures in natural language and remains within the computational limitations of the model architecture. Focusing on transformers, we find that formal languages with both these properties enable language models to achieve lower loss on natural language and better linguistic generalization compared to other languages. In fact, pre-pretraining, or training on formal-then-natural language, reduces loss more efficiently than the same amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. We also give mechanistic evidence of cross-task transfer from formal to natural language: attention heads acquired during formal language pretraining remain crucial for the model's performance on syntactic evaluations.

### Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time 
[[arxiv](https://arxiv.org/abs/2502.19230)] [[cool](https://papers.cool/arxiv/2502.19230)] [[pdf](https://arxiv.org/pdf/2502.19230)]
> **Authors**: Jiazheng Li,Yuxiang Zhou,Junru Lu,Gladys Tyen,Lin Gui,Cesare Aloisi,Yulan He
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) often struggle with complex reasoning scenarios. While preference optimization methods enhance reasoning performance through training, they often lack transparency in why one reasoning outcome is preferred over another. Verbal reflection techniques improve explainability but are limited in LLMs' critique and refinement capacity. To address these challenges, we introduce a contrastive reflection synthesis pipeline that enhances the accuracy and depth of LLM-generated reflections. We further propose a dual-model reasoning framework within a verbal reinforcement learning paradigm, decoupling inference-time self-reflection into specialized, trained models for reasoning critique and refinement. Extensive experiments show that our framework outperforms traditional preference optimization methods across all evaluation metrics. Our findings also show that "two heads are better than one", demonstrating that a collaborative Reasoner-Critic model achieves superior reasoning performance and transparency, compared to single-model approaches.

### Negation-Induced Forgetting in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19211)] [[cool](https://papers.cool/arxiv/2502.19211)] [[pdf](https://arxiv.org/pdf/2502.19211)]
> **Authors**: Francesca Capuano,Ellen Boschert,Barbara Kaup
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The study explores whether Large Language Models (LLMs) exhibit negation-induced forgetting (NIF), a cognitive phenomenon observed in humans where negating incorrect attributes of an object or event leads to diminished recall of this object or event compared to affirming correct attributes (Mayo et al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental framework to test this effect in ChatGPT-3.5, GPT-4o mini and Llama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with negated information being less likely to be recalled than affirmed information. GPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did not exhibit NIF. The findings provide initial evidence of negation-induced forgetting in some LLMs, suggesting that similar cognitive biases may emerge in these models. This work is a preliminary step in understanding how memory-related phenomena manifest in LLMs.

### Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.19209)] [[cool](https://papers.cool/arxiv/2502.19209)] [[pdf](https://arxiv.org/pdf/2502.19209)]
> **Authors**: Zhouyu Jiang,Mengshu Sun,Zhiqiang Zhang,Lei Liang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in Large Language Models (LLMs) but can still produce inconsistent or unsupported content. Although LLM-as-a-Judge is widely used for RAG hallucination detection due to its implementation simplicity, it faces two main challenges: the absence of comprehensive evaluation benchmarks and the lack of domain-optimized judge models. To bridge these gaps, we introduce \textbf{Bi'an}, a novel framework featuring a bilingual benchmark dataset and lightweight judge models. The dataset supports rigorous evaluation across multiple RAG scenarios, while the judge models are fine-tuned from compact open-source LLMs. Extensive experimental evaluations on Bi'anBench show our 14B model outperforms baseline models with over five times larger parameter scales and rivals state-of-the-art closed-source LLMs. We will release our data and models soon at https://github.com/OpenSPG/KAG.

### MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer's Detection 
[[arxiv](https://arxiv.org/abs/2502.19208)] [[cool](https://papers.cool/arxiv/2502.19208)] [[pdf](https://arxiv.org/pdf/2502.19208)]
> **Authors**: Arezo Shakeri,Mina Farmanbar,Krisztian Balog
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 11 pages, 3 Figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as the leading cause. Conversation-based AD detection offers a cost-effective alternative to clinical methods, as language dysfunction is an early biomarker of AD. However, most prior research has framed AD detection as a binary classification problem, limiting the ability to identify Mild Cognitive Impairment (MCI)-a crucial stage for early intervention. Also, studies primarily rely on single-language datasets, mainly in English, restricting cross-language generalizability. To address this gap, we make three key contributions. First, we introduce a novel, multilingual dataset for AD detection by unifying 16 publicly available dementia-related conversational datasets. This corpus spans English, Spanish, Chinese, and Greek and incorporates both audio and text data derived from a variety of cognitive assessment tasks. Second, we perform finer-grained classification, including MCI, and evaluate various classifiers using sparse and dense text representations. Third, we conduct experiments in monolingual and multilingual settings, finding that some languages benefit from multilingual training while others perform better independently. This study highlights the challenges in multilingual AD detection and enables future research on both language-specific approaches and techniques aimed at improving model generalization and robustness.

### FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge 
[[arxiv](https://arxiv.org/abs/2502.19207)] [[cool](https://papers.cool/arxiv/2502.19207)] [[pdf](https://arxiv.org/pdf/2502.19207)]
> **Authors**: Nakyeong Yang,Minsung Kim,Seunghyun Yoon,Joongbo Shin,Kyomin Jung
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 16 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.

### LiGT: Layout-infused Generative Transformer for Visual Question Answering on Vietnamese Receipts 
[[arxiv](https://arxiv.org/abs/2502.19202)] [[cool](https://papers.cool/arxiv/2502.19202)] [[pdf](https://arxiv.org/pdf/2502.19202)]
> **Authors**: Thanh-Phong Le,Trung Le Chi Phan,Nghia Hieu Nguyen,Kiet Van Nguyen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at IJDAR
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: \textbf{Purpose:} Document Visual Question Answering (document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English. \textbf{Methods:} In this paper, we present ReceiptVQA (\textbf{Receipt} \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \textbf{9,000+} receipt images and \textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused \textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules. \textbf{Results:} Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models. \textbf{Conclusion:} We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language.

### BIG-Bench Extra Hard 
[[arxiv](https://arxiv.org/abs/2502.19187)] [[cool](https://papers.cool/arxiv/2502.19187)] [[pdf](https://arxiv.org/pdf/2502.19187)]
> **Authors**: Mehran Kazemi,Bahare Fatemi,Hritik Bansal,John Palowitch,Chrysovalantis Anastasiou,Sanket Vaibhav Mehta,Lalit K. Jain,Virginia Aglietti,Disha Jindal,Peter Chen,Nishanth Dikkala,Gladys Tyen,Xin Liu,Uri Shalit,Silvia Chiappa,Kate Olszewska,Yi Tay,Vinh Q. Tran,Quoc V. Le,Orhan Firat
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.

### MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.19175)] [[cool](https://papers.cool/arxiv/2502.19175)] [[pdf](https://arxiv.org/pdf/2502.19175)]
> **Authors**: Daniel Rose,Chia-Chien Hung,Marco Lepri,Israa Alqassem,Kiril Gashteovski,Carolin Lawrence
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.

### TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency 
[[arxiv](https://arxiv.org/abs/2502.19163)] [[cool](https://papers.cool/arxiv/2502.19163)] [[pdf](https://arxiv.org/pdf/2502.19163)]
> **Authors**: Henry Peng Zou,Zhengyao Gu,Yue Zhou,Yankai Chen,Weizhi Zhang,Liancheng Fang,Yibo Wang,Yangning Li,Kay Liu,Philip S. Yu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,机器学习
- **Abstract**: Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.

### Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19160)] [[cool](https://papers.cool/arxiv/2502.19160)] [[pdf](https://arxiv.org/pdf/2502.19160)]
> **Authors**: Rebekka Görge,Michael Mock,Héctor Allende-Cid
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a sentence. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these indicators are present in these sentences and explain the strength of a stereotype. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated behaviors and characteristics. Using more few-shot examples within the prompts, significantly improves performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.

### When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning 
[[arxiv](https://arxiv.org/abs/2502.19158)] [[cool](https://papers.cool/arxiv/2502.19158)] [[pdf](https://arxiv.org/pdf/2502.19158)]
> **Authors**: Yijiang River Dong,Tiancheng Hu,Yinhong Liu,Ahmet Üstün,Nigel Collier
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.

### Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs 
[[arxiv](https://arxiv.org/abs/2502.19148)] [[cool](https://papers.cool/arxiv/2502.19148)] [[pdf](https://arxiv.org/pdf/2502.19148)]
> **Authors**: Zhaowei Zhang,Fengshuo Bai,Qizhi Chen,Chengdong Ma,Mingzhi Wang,Haoran Sun,Zilong Zheng,Yaodong Yang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by ICLR 2025, Project page: https://zowiezhang.github.io/projects/Amulet
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.

### Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement 
[[arxiv](https://arxiv.org/abs/2502.19127)] [[cool](https://papers.cool/arxiv/2502.19127)] [[pdf](https://arxiv.org/pdf/2502.19127)]
> **Authors**: Siyuan Zhang,Yichi Zhang,Yinpeng Dong,Hang Su
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 29 pages, 17 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. While post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its existing memory--the knowledge acquired from pre-training data. We introduce self-memory alignment (SMA), which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments show that SMA significantly improves LLMs' overall performance, with consistent enhancement across various benchmarks concerning factuality, as well as helpfulness and comprehensive skills.

### Improving customer service with automatic topic detection in user emails 
[[arxiv](https://arxiv.org/abs/2502.19115)] [[cool](https://papers.cool/arxiv/2502.19115)] [[pdf](https://arxiv.org/pdf/2502.19115)]
> **Authors**: Bojana Bašaragin,Darija Medvecki,Gorana Gojić,Milena Oparnica,Dragiša Mišković
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Paper submitted to the 15th International Conference on Information Society and Technology (ICIST), Kopaonik, Serbia, 9-12 March 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: This study introduces a novel Natural Language Processing pipeline that enhances customer service efficiency at Telekom Srbija, a leading Serbian telecommunications company, through automated email topic detection and labelling. Central to the pipeline is BERTopic, a modular architecture that allows unsupervised topic modelling. After a series of preprocessing and post-processing steps, we assign one of 12 topics and several additional labels to incoming emails, allowing customer service to filter and access them through a custom-made application. The model's performance was evaluated by assessing the speed and correctness of the automatically assigned topics across a test dataset of 100 customer emails. The pipeline shows broad applicability across languages, particularly for those that are low-resourced and morphologically rich. The system now operates in the company's production environment, streamlining customer service operations through automated email classification.

### Conformal Linguistic Calibration: Trading-off between Factuality and Specificity 
[[arxiv](https://arxiv.org/abs/2502.19110)] [[cool](https://papers.cool/arxiv/2502.19110)] [[pdf](https://arxiv.org/pdf/2502.19110)]
> **Authors**: Zhengping Jiang,Anqi Liu,Benjamin Van Durme
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Language model outputs are not always reliable; this prompts research into methods for adapting model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unifying view of both approaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic calibration as answer set prediction. We begin by presenting a unified framework that connects abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation that allows for controlling the level of imprecision in model responses. Experimental results show that our method produces calibrated outputs with conformal guarantees on factual accuracy. Furthermore, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.

### Evaluating Gender Bias in German Machine Translation 
[[arxiv](https://arxiv.org/abs/2502.19104)] [[cool](https://papers.cool/arxiv/2502.19104)] [[pdf](https://arxiv.org/pdf/2502.19104)]
> **Authors**: Michelle Kappl
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ISCA/ITG Workshop on Diversity in Large Speech andLanguageModels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We present WinoMTDE, a new gender bias evaluation test set designed to assess occupational stereotyping and underrepresentation in German machine translation (MT) systems. Building on the automatic evaluation method introduced by arXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with grammatical gender. The WinoMTDE dataset comprises 288 German sentences that are balanced in regard to gender, as well as stereotype, which was annotated using German labor statistics. We conduct a large-scale evaluation of five widely used MT systems and a large language model. Our results reveal persistent bias in most models, with the LLM outperforming traditional systems. The dataset and evaluation code are publicly available under https://github.com/michellekappl/mt_gender_german.

### LongEval: A Comprehensive Analysis of Long-Text Generation Through a Plan-based Paradigm 
[[arxiv](https://arxiv.org/abs/2502.19103)] [[cool](https://papers.cool/arxiv/2502.19103)] [[pdf](https://arxiv.org/pdf/2502.19103)]
> **Authors**: Siwei Wu,Yizhi Li,Xingwei Qu,Rishi Ravikumar,Yucheng Li,Tyler Loakman Shanghaoran Quan Xiaoyong Wei,Riza Batista-Navarro,Chenghua Lin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, yet their ability to generate long-form content remains poorly understood and evaluated. Our analysis reveals that current LLMs struggle with length requirements and information density in long-text generation, with performance deteriorating as text length increases. To quantitively locate such a performance degradation and provide further insights on model development, we present LongEval, a benchmark that evaluates long-text generation through both direct and plan-based generation paradigms, inspired by cognitive and linguistic writing models. The comprehensive experiments in this work reveal interesting findings such as that while model size correlates with generation ability, the small-scale model (e.g., LongWriter), well-trained on long texts, has comparable performance. All code and datasets are released in https://github.com/Wusiwei0410/LongEval.

### Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs 
[[arxiv](https://arxiv.org/abs/2502.19078)] [[cool](https://papers.cool/arxiv/2502.19078)] [[pdf](https://arxiv.org/pdf/2502.19078)]
> **Authors**: Yiheng Yang,Yujie Wang,Chi Ma,Lei Yu,Emmanuele Chersoni,Chu-Ren Huang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Dense large language models(LLMs) face critical efficiency bottlenecks as they rigidly activate all parameters regardless of input complexity. While existing sparsity methods(static pruning or dynamic activation) address this partially, they either lack adaptivity to contextual or model structural demands or incur prohibitive computational overhead. Inspired by human brain's dual-process mechanisms - predictive coding (N400) for backbone sparsity and structural reanalysis (P600) for complex context - we propose CLADA, a \textit{\textbf{C}ognitive-\textbf{L}oad-\textbf{A}ware \textbf{D}ynamic \textbf{A}ctivation} framework that synergizes statistical sparsity with semantic adaptability. Our key insight is that LLM activations exhibit two complementary patterns: 1) \textit{Global statistical sparsity} driven by sequence-level prefix information, and 2) \textit{Local semantic adaptability} modulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs a hierarchical thresholding strategy: a baseline from offline error-controlled optimization ensures 40\%+ sparsity, dynamically adjusted by real-time cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks demonstrate that CLADA achieves \textbf{~20\% average speedup with <2\% accuracy drop}, outperforming Griffin (5\%+ degradation) and TT (negligible speedup). Crucially, we establish the first formal connection between neurolinguistic event-related potential (ERP) components and LLM efficiency mechanisms through multi-level regression analysis ($R^2=0.17$ for sparsity-adaptation synergy). Requiring no retraining or architectural changes, CLADA offers a deployable solution for resource-aware LLM inference while advancing biologically-inspired AI design. Our code is available at \href{https://github.com/Oldify/CLADA}{CLADA}.

### Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics 
[[arxiv](https://arxiv.org/abs/2502.19074)] [[cool](https://papers.cool/arxiv/2502.19074)] [[pdf](https://arxiv.org/pdf/2502.19074)]
> **Authors**: Aloka Fernando,Surangika Ranathunga,Nisansa de Silva
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Parallel Data Curation (PDC) techniques aim to filter out noisy parallel sentences from the web-mined corpora. Prior research has demonstrated that ranking sentence pairs using similarity scores on sentence embeddings derived from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT systems with the top-ranked samples, produces superior NMT performance than when trained using the full dataset. However, previous research has shown that the choice of multiPLM significantly impacts the ranking quality. This paper investigates the reasons behind this disparity across multiPLMs. Using the web-mined corpora CCMatrix and CCAligned for En$\rightarrow$Si, En$\rightarrow$Ta and Si$\rightarrow$Ta, we show that different multiPLMs (LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which allows noisy sentences to creep into the top-ranked samples. We show that by employing a series of heuristics, this noise can be removed to a certain extent. This results in improving the results of NMT systems trained with web-mined corpora and reduces the disparity across multiPLMs.

### Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique 
[[arxiv](https://arxiv.org/abs/2502.19064)] [[cool](https://papers.cool/arxiv/2502.19064)] [[pdf](https://arxiv.org/pdf/2502.19064)]
> **Authors**: Piotr Sawicki,Marek Grześ,Dan Brown,Fabrício Góes
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The Consensual Assessment Technique (CAT) evaluates creativity through holistic expert judgments. We investigate the use of two advanced Large Language Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a methodology inspired by the CAT. Using a dataset of 90 poems, we found that these LLMs can surpass the results achieved by non-expert human judges at matching a ground truth based on publication venue, particularly when assessing smaller subsets of poems. Claude-3-Opus exhibited slightly superior performance than GPT-4o. We show that LLMs are viable tools for accurately assessing poetry, paving the way for their broader application into other creative domains.

### MathClean: A Benchmark for Synthetic Mathematical Data Cleaning 
[[arxiv](https://arxiv.org/abs/2502.19058)] [[cool](https://papers.cool/arxiv/2502.19058)] [[pdf](https://arxiv.org/pdf/2502.19058)]
> **Authors**: Hao Liang,Meiyi Qiang,Yuying Li,Zefeng He,Yongzhen Guo,Zhengzhou Zhu,Wentao Zhang,Bin Cui
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at https://github.com/YuYingLi0/MathClean.

### Binary Neural Networks for Large Language Model: A Survey 
[[arxiv](https://arxiv.org/abs/2502.19008)] [[cool](https://papers.cool/arxiv/2502.19008)] [[pdf](https://arxiv.org/pdf/2502.19008)]
> **Authors**: Liangdong Liu,Zhitong Zheng,Cong Wang,Tianhuang Su,Zhenyu Yang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 23 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.

### MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering 
[[arxiv](https://arxiv.org/abs/2502.18993)] [[cool](https://papers.cool/arxiv/2502.18993)] [[pdf](https://arxiv.org/pdf/2502.18993)]
> **Authors**: Teng Lin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数据库
- **Abstract**: Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.

### GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation 
[[arxiv](https://arxiv.org/abs/2502.18990)] [[cool](https://papers.cool/arxiv/2502.18990)] [[pdf](https://arxiv.org/pdf/2502.18990)]
> **Authors**: Jie He,Jennifer Neville,Mengting Wan,Longqi Yang,Hui Liu,Xiaofeng Xu,Xia Song,Jeff Z. Pan,Pei Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.

### PEToolLLM: Towards Personalized Tool Learning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18980)] [[cool](https://papers.cool/arxiv/2502.18980)] [[pdf](https://arxiv.org/pdf/2502.18980)]
> **Authors**: Qiancheng Xu,Yongqi Li,Heming Xia,Fan Liu,Min Yang,Wenjie Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.

### Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.18978)] [[cool](https://papers.cool/arxiv/2502.18978)] [[pdf](https://arxiv.org/pdf/2502.18978)]
> **Authors**: Hongyi Cal,Jie Li,Wenzhen Dong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.

### Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles 
[[arxiv](https://arxiv.org/abs/2502.18968)] [[cool](https://papers.cool/arxiv/2502.18968)] [[pdf](https://arxiv.org/pdf/2502.18968)]
> **Authors**: Kuang Wang,Xianfei Li,Shenghao Yang,Li Zhou,Feng Jiang,Haizhou Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.

### MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors 
[[arxiv](https://arxiv.org/abs/2502.18940)] [[cool](https://papers.cool/arxiv/2502.18940)] [[pdf](https://arxiv.org/pdf/2502.18940)]
> **Authors**: Jakub Macina,Nico Daheim,Ido Hakimi,Manu Kapur,Iryna Gurevych,Mrinmaya Sachan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: https://eth-lre.github.io/mathtutorbench
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.

### JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18935)] [[cool](https://papers.cool/arxiv/2502.18935)] [[pdf](https://arxiv.org/pdf/2502.18935)]
> **Authors**: Shuyi Liu,Simiao Cui,Haoran Bu,Yuming Shang,Xi Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, 5 figures, accepted at PAKDD 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at https://github.com/STAIR-BUPT/JailBench.

### Kanana: Compute-efficient Bilingual Language Models 
[[arxiv](https://arxiv.org/abs/2502.18934)] [[cool](https://papers.cool/arxiv/2502.18934)] [[pdf](https://arxiv.org/pdf/2502.18934)]
> **Authors**: Kanana LLM Team,Yunju Bak,Hojin Lee,Minho Ryu,Jiyeon Ham,Seungjae Jung,Daniel Wontae Nam,Taegyeong Eo,Donghun Lee,Doohae Jung,Boseop Kim,Nayeon Kim,Jaesun Park,Hyunho Kim,Hyunwoong Ko,Changmin Lee,Kyoung-Woon On,Seulye Baeg,Junrae Cho,Sunghee Jung,Jieun Kang,EungGyun Kim,Eunhwa Kim,Byeongil Ko,Daniel Lee, et al. (4 additional authors not shown)
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 40 pages, 15 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.

### END: Early Noise Dropping for Efficient and Effective Context Denoising 
[[arxiv](https://arxiv.org/abs/2502.18915)] [[cool](https://papers.cool/arxiv/2502.18915)] [[pdf](https://arxiv.org/pdf/2502.18915)]
> **Authors**: Hongye Jin,Pei Chen,Jingfeng Yang,Zhengyang Wang,Meng Jiang,Yifan Gao,Binxuan Huang,Xinyang Zhang,Zheng Li,Tianyi Liu,Huasheng Li,Bing Yin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.

### CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.18913)] [[cool](https://papers.cool/arxiv/2502.18913)] [[pdf](https://arxiv.org/pdf/2502.18913)]
> **Authors**: Jiaming Zhou,Yujie Guo,Shiwan Zhao,Haoqin Sun,Hui Wang,Jiabei He,Aobo Kong,Shiyao Wang,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.

### From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens 
[[arxiv](https://arxiv.org/abs/2502.18890)] [[cool](https://papers.cool/arxiv/2502.18890)] [[pdf](https://arxiv.org/pdf/2502.18890)]
> **Authors**: Tong Wu,Junzhe Shen,Zixia Jia,Yuxuan Wang,Zilong Zheng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.

### On Pruning State-Space LLMs 
[[arxiv](https://arxiv.org/abs/2502.18886)] [[cool](https://papers.cool/arxiv/2502.18886)] [[pdf](https://arxiv.org/pdf/2502.18886)]
> **Authors**: Tamer Ghattas,Michael Hassid,Roy Schwartz
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.

### Learning to Generate Structured Output with Schema Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.18878)] [[cool](https://papers.cool/arxiv/2502.18878)] [[pdf](https://arxiv.org/pdf/2502.18878)]
> **Authors**: Yaxi Lu,Haolun Li,Xin Cong,Zhong Zhang,Yesai Wu,Yankai Lin,Zhiyuan Liu,Fangming Liu,Maosong Sun
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.

### Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework 
[[arxiv](https://arxiv.org/abs/2502.18874)] [[cool](https://papers.cool/arxiv/2502.18874)] [[pdf](https://arxiv.org/pdf/2502.18874)]
> **Authors**: Kaishuai Xu,Tiezheng Yu,Wenjun Hou,Yi Cheng,Liangyou Li,Xin Jiang,Lifeng Shang,Qun Liu,Wenjie Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.

### A Causal Lens for Evaluating Faithfulness Metrics 
[[arxiv](https://arxiv.org/abs/2502.18848)] [[cool](https://papers.cool/arxiv/2502.18848)] [[pdf](https://arxiv.org/pdf/2502.18848)]
> **Authors**: Kerem Zaman,Shashank Srivastava
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 18 pages, 18 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,方法论
- **Abstract**: Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.

### Sliding Window Attention Training for Efficient Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18845)] [[cool](https://papers.cool/arxiv/2502.18845)] [[pdf](https://arxiv.org/pdf/2502.18845)]
> **Authors**: Zichuan Fu,Wentao Song,Yejing Wang,Xian Wu,Yefeng Zheng,Yingying Zhang,Derong Xu,Xuetao Wei,Tong Xu,Xiangyu Zhao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://anonymous.4open.science/r/SWAT-attention.

### Sentiment Analysis of Movie Reviews Using BERT 
[[arxiv](https://arxiv.org/abs/2502.18841)] [[cool](https://papers.cool/arxiv/2502.18841)] [[pdf](https://arxiv.org/pdf/2502.18841)]
> **Authors**: Gibson Nkhata,Usman Anjum,Justin Zhan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 7 pages, 3 figures, published in the proceedings The Fifteenth International Conference on Information, Process, and Knowledge Management (eKNOW 2023)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sentiment Analysis (SA) or opinion mining is analysis of emotions and opinions from any kind of text. SA helps in tracking peoples viewpoints and it is an important factor when it comes to social media monitoring product and brand recognition customer satisfaction customer loyalty advertising and promotions success and product acceptance. That is why SA is one of the active research areas in Natural Language Processing (NLP). SA is applied on data sourced from various media platforms to mine sentiment knowledge from them. Various approaches have been deployed in the literature to solve the problem. Most techniques devise complex and sophisticated frameworks in order to attain optimal accuracy. This work aims to finetune Bidirectional Encoder Representations from Transformers (BERT) with Bidirectional Long Short-Term Memory (BiLSTM) for movie reviews sentiment analysis and still provide better accuracy than the State-of-The-Art (SOTA) methods. The paper also shows how sentiment analysis can be applied if someone wants to recommend a certain movie for example by computing overall polarity of its sentiments predicted by the model. That is our proposed method serves as an upper-bound baseline in prediction of a predominant reaction to a movie. To compute overall polarity a heuristic algorithm is applied to BERTBiLSTM output vector. Our model can be extended to three-class four-class or any fine-grained classification and apply overall polarity computation again. This is intended to be exploited in future work.

## 密码学和安全(cs.CR:Cryptography and Security)

### Atlas: A Framework for ML Lifecycle Provenance & Transparency 
[[arxiv](https://arxiv.org/abs/2502.19567)] [[cool](https://papers.cool/arxiv/2502.19567)] [[pdf](https://arxiv.org/pdf/2502.19567)]
> **Authors**: Marcin Spoczynski,Marcela S. Melara,Sebastian Szyller
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The rapid adoption of open source machine learning (ML) datasets and models exposes today's AI applications to critical risks like data poisoning and supply chain attacks across the ML lifecycle. With growing regulatory pressure to address these issues through greater transparency, ML model vendors face challenges balancing these requirements against confidentiality for data and intellectual property needs. We propose Atlas, a framework that enables fully attestable ML pipelines. Atlas leverages open specifications for data and software supply chain provenance to collect verifiable records of model artifact authenticity and end-to-end lineage metadata. Atlas combines trusted hardware and transparency logs to enhance metadata integrity, preserve data confidentiality, and limit unauthorized access during ML pipeline operations, from training through deployment. Our prototype implementation of Atlas integrates several open-source tools to build an ML lifecycle transparency system, and assess the practicality of Atlas through two case study ML pipelines.

### No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data 
[[arxiv](https://arxiv.org/abs/2502.19537)] [[cool](https://papers.cool/arxiv/2502.19537)] [[pdf](https://arxiv.org/pdf/2502.19537)]
> **Authors**: Joshua Kazdan,Lisa Yu,Rylan Schaeffer,Chris Cundy,Sanmi Koyejo,Dvijotham Krishnamurthy
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Leading language model (LM) providers like OpenAI and Google offer fine-tuning APIs that allow customers to adapt LMs for specific use cases. To prevent misuse, these LM providers implement filtering mechanisms to block harmful fine-tuning data. Consequently, adversaries seeking to produce unsafe LMs via these APIs must craft adversarial training data that are not identifiably harmful. We make three contributions in this context: 1. We show that many existing attacks that use harmless data to create unsafe LMs rely on eliminating model refusals in the first few tokens of their responses. 2. We show that such prior attacks can be blocked by a simple defense that pre-fills the first few tokens from an aligned model before letting the fine-tuned model fill in the rest. 3. We describe a new data-poisoning attack, ``No, Of course I Can Execute'' (NOICE), which exploits an LM's formulaic refusal mechanism to elicit harmful responses. By training an LM to refuse benign requests on the basis of safety before fulfilling those requests regardless, we are able to jailbreak several open-source models and a closed-source model (GPT-4o). We show an attack success rate (ASR) of 57% against GPT-4o; our attack earned a Bug Bounty from OpenAI. Against open-source models protected by simple defenses, we improve ASRs by an average of 3.25 times compared to the best performing previous attacks that use only harmless data. NOICE demonstrates the exploitability of repetitive refusal mechanisms and broadens understanding of the threats closed-source models face from harmless data.

### Poster: Long PHP webshell files detection based on sliding window attention 
[[arxiv](https://arxiv.org/abs/2502.19257)] [[cool](https://papers.cool/arxiv/2502.19257)] [[pdf](https://arxiv.org/pdf/2502.19257)]
> **Authors**: Zhiqiang Wang,Haoyu Wang,Lu Hao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 3 pages(include 1 page poster), 1 figure. Accepted as a poster at the NDSS 2025. Poster list: http://www.ndss-symposium.org/ndss2025/accepted-posters/. Dataset/code available at http://github.com/w-32768/PHP-Webshell-Detection-via-Opcode-Analysis
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Webshell is a type of backdoor, and web applications are widely exposed to webshell injection attacks. Therefore, it is important to study webshell detection techniques. In this study, we propose a webshell detection method. We first convert PHP source code to opcodes and then extract Opcode Double-Tuples (ODTs). Next, we combine CodeBert and FastText models for feature representation and classification. To address the challenge that deep learning methods have difficulty detecting long webshell files, we introduce a sliding window attention mechanism. This approach effectively captures malicious behavior within long files. Experimental results show that our method reaches high accuracy in webshell detection, solving the problem of traditional methods that struggle to address new webshell variants and anti-detection techniques.

### Evaluating Membership Inference Attacks in heterogeneous-data setups 
[[arxiv](https://arxiv.org/abs/2502.18986)] [[cool](https://papers.cool/arxiv/2502.18986)] [[pdf](https://arxiv.org/pdf/2502.18986)]
> **Authors**: Bram van Dartel,Marc Damie,Florian Hahn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted in SiMLA workshop 2025 (co-located with ACNS)
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Among all privacy attacks against Machine Learning (ML), membership inference attacks (MIA) attracted the most attention. In these attacks, the attacker is given an ML model and a data point, and they must infer whether the data point was used for training. The attacker also has an auxiliary dataset to tune their inference algorithm. Attack papers commonly simulate setups in which the attacker's and the target's datasets are sampled from the same distribution. This setting is convenient to perform experiments, but it rarely holds in practice. ML literature commonly starts with similar simplifying assumptions (i.e., "i.i.d." datasets), and later generalizes the results to support heterogeneous data distributions. Similarly, our work makes a first step in the generalization of the MIA evaluation to heterogeneous data. First, we design a metric to measure the heterogeneity between any pair of tabular data distributions. This metric provides a continuous scale to analyze the phenomenon. Second, we compare two methodologies to simulate a data heterogeneity between the target and the attacker. These setups provide opposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our results show that the MIA accuracy depends on the experimental setup; and even if research on MIA considers heterogeneous data setups, we have no standardized baseline of how to simulate it. The lack of such a baseline for MIA experiments poses a significant challenge to risk assessments in real-world machine learning scenarios.

### Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.18943)] [[cool](https://papers.cool/arxiv/2502.18943)] [[pdf](https://arxiv.org/pdf/2502.18943)]
> **Authors**: Yu He,Boheng Li,Liu Liu,Zhongjie Ba,Wei Dong,Yiming Li,Zhan Qin,Kui Ren,Chun Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by USENIX Security 2025
- **标题**: None
- **领域**: 密码学和安全,计算语言学
- **Abstract**: Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\ie, \textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \textbf{PETAL}: a label-only membership inference attack based on \textbf{PE}r-\textbf{T}oken sem\textbf{A}ntic simi\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.

### Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code 
[[arxiv](https://arxiv.org/abs/2502.18851)] [[cool](https://papers.cool/arxiv/2502.18851)] [[pdf](https://arxiv.org/pdf/2502.18851)]
> **Authors**: Jungin Kim,Shinwoo Park,Yo-Sub Han
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation. In addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in https://github.com/inistory/STONE-watermarking/.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Snowball Adversarial Attack on Traffic Sign Classification 
[[arxiv](https://arxiv.org/abs/2502.19757)] [[cool](https://papers.cool/arxiv/2502.19757)] [[pdf](https://arxiv.org/pdf/2502.19757)]
> **Authors**: Anthony Etim,Jakub Szefer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Adversarial attacks on machine learning models often rely on small, imperceptible perturbations to mislead classifiers. Such strategy focuses on minimizing the visual perturbation for humans so they are not confused, and also maximizing the misclassification for machine learning algorithms. An orthogonal strategy for adversarial attacks is to create perturbations that are clearly visible but do not confuse humans, yet still maximize misclassification for machine learning algorithms. This work follows the later strategy, and demonstrates instance of it through the Snowball Adversarial Attack in the context of traffic sign recognition. The attack leverages the human brain's superior ability to recognize objects despite various occlusions, while machine learning algorithms are easily confused. The evaluation shows that the Snowball Adversarial Attack is robust across various images and is able to confuse state-of-the-art traffic sign recognition algorithm. The findings reveal that Snowball Adversarial Attack can significantly degrade model performance with minimal effort, raising important concerns about the vulnerabilities of deep neural networks and highlighting the necessity for improved defenses for image recognition machine learning models.

### Lightweight Contrastive Distilled Hashing for Online Cross-modal Retrieval 
[[arxiv](https://arxiv.org/abs/2502.19751)] [[cool](https://papers.cool/arxiv/2502.19751)] [[pdf](https://arxiv.org/pdf/2502.19751)]
> **Authors**: Jiaxing Li,Lin Jiang,Zeqi Ma,Kaihang Jiang,Xiaozhao Fang,Jie Wen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep online cross-modal hashing has gained much attention from researchers recently, as its promising applications with low storage requirement, fast retrieval efficiency and cross modality adaptive, etc. However, there still exists some technical hurdles that hinder its applications, e.g., 1) how to extract the coexistent semantic relevance of cross-modal data, 2) how to achieve competitive performance when handling the real time data streams, 3) how to transfer the knowledge learned from offline to online training in a lightweight manner. To address these problems, this paper proposes a lightweight contrastive distilled hashing (LCDH) for cross-modal retrieval, by innovatively bridging the offline and online cross-modal hashing by similarity matrix approximation in a knowledge distillation framework. Specifically, in the teacher network, LCDH first extracts the cross-modal features by the contrastive language-image pre-training (CLIP), which are further fed into an attention module for representation enhancement after feature fusion. Then, the output of the attention module is fed into a FC layer to obtain hash codes for aligning the sizes of similarity matrices for online and offline training. In the student network, LCDH extracts the visual and textual features by lightweight models, and then the features are fed into a FC layer to generate binary codes. Finally, by approximating the similarity matrices, the performance of online hashing in the lightweight student network can be enhanced by the supervision of coexistent semantic relevance that is distilled from the teacher network. Experimental results on three widely used datasets demonstrate that LCDH outperforms some state-of-the-art methods.

### Recent Advances on Generalizable Diffusion-generated Image Detection 
[[arxiv](https://arxiv.org/abs/2502.19716)] [[cool](https://papers.cool/arxiv/2502.19716)] [[pdf](https://arxiv.org/pdf/2502.19716)]
> **Authors**: Qijie Xu,Defang Chen,Jiawei Chen,Siwei Lyu,Can Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The rise of diffusion models has significantly improved the fidelity and diversity of generated images. With numerous benefits, these advancements also introduce new risks. Diffusion models can be exploited to create high-quality Deepfake images, which poses challenges for image authenticity verification. In recent years, research on generalizable diffusion-generated image detection has grown rapidly. However, a comprehensive review of this topic is still lacking. To bridge this gap, we present a systematic survey of recent advances and classify them into two main categories: (1) data-driven detection and (2) feature-driven detection. Existing detection methods are further classified into six fine-grained categories based on their underlying principles. Finally, we identify several open challenges and envision some future directions, with the hope of inspiring more research work on this important topic. Reviewed works in this survey can be found at https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection.

### Accurate Pose Estimation for Flight Platforms based on Divergent Multi-Aperture Imaging System 
[[arxiv](https://arxiv.org/abs/2502.19708)] [[cool](https://papers.cool/arxiv/2502.19708)] [[pdf](https://arxiv.org/pdf/2502.19708)]
> **Authors**: Shunkun Liang,Bin Li,Banglei Guan,Yang Shang,Xianwei Zhu,Qifeng Yu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-based pose estimation plays a crucial role in the autonomous navigation of flight platforms. However, the field of view and spatial resolution of the camera limit pose estimation accuracy. This paper designs a divergent multi-aperture imaging system (DMAIS), equivalent to a single imaging system to achieve simultaneous observation of a large field of view and high spatial resolution. The DMAIS overcomes traditional observation limitations, allowing accurate pose estimation for the flight platform. {Before conducting pose estimation, the DMAIS must be calibrated. To this end we propose a calibration method for DMAIS based on the 3D calibration field.} The calibration process determines the imaging parameters of the DMAIS, which allows us to model DMAIS as a generalized camera. Subsequently, a new algorithm for accurately determining the pose of flight platform is introduced. We transform the absolute pose estimation problem into a nonlinear minimization problem. New optimality conditions are established for solving this problem based on Lagrange multipliers. Finally, real calibration experiments show the effectiveness and accuracy of the proposed method. Results from real flight experiments validate the system's ability to achieve centimeter-level positioning accuracy and arc-minute-level orientation accuracy.

### Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses 
[[arxiv](https://arxiv.org/abs/2502.19707)] [[cool](https://papers.cool/arxiv/2502.19707)] [[pdf](https://arxiv.org/pdf/2502.19707)]
> **Authors**: Jianning Chi,Zelan Li,Geng Lin,MingYang Sun,Xiaosheng Yu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at https://github.com/bluehenglee/MLI-MSC.

### CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching 
[[arxiv](https://arxiv.org/abs/2502.19705)] [[cool](https://papers.cool/arxiv/2502.19705)] [[pdf](https://arxiv.org/pdf/2502.19705)]
> **Authors**: Juntao Liang,Jun Hou,Weijun Zhang,Yong Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Achieving both efficiency and strong discriminative ability in lightweight visual tracking is a challenge, especially on mobile and edge devices with limited computational resources. Conventional lightweight trackers often struggle with robustness under occlusion and interference, while deep trackers, when compressed to meet resource constraints, suffer from performance degradation. To address these issues, we introduce CFTrack, a lightweight tracker that integrates contrastive learning and feature matching to enhance discriminative feature representations. CFTrack dynamically assesses target similarity during prediction through a novel contrastive feature matching module optimized with an adaptive contrastive loss, thereby improving tracking accuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTrack surpasses many state-of-the-art lightweight trackers, operating at 136 frames per second on the NVIDIA Jetson NX platform. Results on the HOOT dataset further demonstrate CFTrack's strong discriminative ability under heavy occlusion.

### Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.19700)] [[cool](https://papers.cool/arxiv/2502.19700)] [[pdf](https://arxiv.org/pdf/2502.19700)]
> **Authors**: Yimin Zhu,Linlin Xu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Although data augmentation is an effective method to address the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC), most methodologies extend features in the latent space. Few, however, generate realistic and diverse samples using text information to balance the limited number of annotated samples. Recently, text-driven diffusion models have gained significant attention due to their remarkable ability to generate highly diverse images based on given text prompts in natural image synthesis. Therefore, this paper proposes a novel language-informed hyperspectral image synthesis method (Txt2HSI-LDM(VAE)) for addressing the ISSD problem of HSIC. First, for addressing the high-dimensional hyperspectral data, we use universal varitional autoencoeder (VAE) to map the hyperspectral into a low-dimensional latent space and get stable feature representation, which hugely reduce the inference parameter of diffusion model. Next, a semi-supervised diffusion model is designed for fully taking advantage of unlabeled data, beside, random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are also used for simulating the varying degrees of mixing of training data. Then, VAE decodes HSI from latent space generated by diffusion model with the conditional language as input, contributing to more realistic and diverse samples. In our experiments, we fully evaluate the effectiveness of synthetic samples from aspect of statistical characteristic and data distribution in 2D-PCA space. Additionally, cross-attention map is visualized on the pixel-level to prove that our proposed model can capture the spatial layout of and geometry of the generated hyperspectral image depend on the visual-linguistic alignment.

### Spatial-Spectral Diffusion Contrastive Representation Network for Hyperspectral Image Classification 
[[arxiv](https://arxiv.org/abs/2502.19699)] [[cool](https://papers.cool/arxiv/2502.19699)] [[pdf](https://arxiv.org/pdf/2502.19699)]
> **Authors**: Yimin Zhu,Linlin Xu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Although efficient extraction of discriminative spatial-spectral features is critical for hyperspectral images classification (HSIC), it is difficult to achieve these features due to factors such as the spatial-spectral heterogeneity and noise effect. This paper presents a Spatial-Spectral Diffusion Contrastive Representation Network (DiffCRN), based on denoising diffusion probabilistic model (DDPM) combined with contrastive learning (CL) for HSIC, with the following characteristics. First,to improve spatial-spectral feature representation, instead of adopting the UNets-like structure which is widely used for DDPM, we design a novel staged architecture with spatial self-attention denoising module (SSAD) and spectral group self-attention denoising module (SGSAD) in DiffCRN with improved efficiency for spectral-spatial feature learning. Second, to improve unsupervised feature learning efficiency, we design new DDPM model with logarithmic absolute error (LAE) loss and CL that improve the loss function effectiveness and increase the instance-level and inter-class discriminability. Third, to improve feature selection, we design a learnable approach based on pixel-level spectral angle mapping (SAM) for the selection of time steps in the proposed DDPM model in an adaptive and automatic manner. Last, to improve feature integration and classification, we design an Adaptive weighted addition modul (AWAM) and Cross time step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wise features and perform classification. Experiments conducted on widely used four HSI datasets demonstrate the improved performance of the proposed DiffCRN over the classical backbone models and state-of-the-art GAN, transformer models and other pretrained methods. The source code and pre-trained model will be made available publicly.

### Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion 
[[arxiv](https://arxiv.org/abs/2502.19697)] [[cool](https://papers.cool/arxiv/2502.19697)] [[pdf](https://arxiv.org/pdf/2502.19697)]
> **Authors**: Yuan Bian,Min Liu,Yunqi Yi,Xueping Wang,Yaonan Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Person re-identification (re-id) models are vital in security surveillance systems, requiring transferable adversarial attacks to explore the vulnerabilities of them. Recently, vision-language models (VLM) based attacks have shown superior transferability by attacking generalized image and textual features of VLM, but they lack comprehensive feature disruption due to the overemphasis on discriminative semantics in integral representation. In this paper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel method that leverages VLM's image-text alignment capability to explicitly disrupt fine-grained semantic features of pedestrian images by destroying attribute-specific textual embeddings. To obtain personalized textual descriptions for individual attributes, textual inversion networks are designed to map pedestrian images to pseudo tokens that represent semantic embeddings, trained in the contrastive learning manner with images and a predefined prompt template that explicitly describes the pedestrian attributes. Inverted benign and adversarial fine-grained textual semantics facilitate attacker in effectively conducting thorough disruptions, enhancing the transferability of adversarial examples. Extensive experiments show that AP-Attack achieves state-of-the-art transferability, significantly outperforming previous methods by 22.9% on mean Drop Rate in cross-model&dataset attack scenarios.

### BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance 
[[arxiv](https://arxiv.org/abs/2502.19694)] [[cool](https://papers.cool/arxiv/2502.19694)] [[pdf](https://arxiv.org/pdf/2502.19694)]
> **Authors**: Xin Ye,Burhaneddin Yaman,Sheng Cheng,Feng Tao,Abhirup Mallik,Liu Ren
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: CVPR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Bird's-eye-view (BEV) representations play a crucial role in autonomous driving tasks. Despite recent advancements in BEV generation, inherent noise, stemming from sensor limitations and the learning process, remains largely unaddressed, resulting in suboptimal BEV representations that adversely impact the performance of downstream tasks. To address this, we propose BEVDiffuser, a novel diffusion model that effectively denoises BEV feature maps using the ground-truth object layout as guidance. BEVDiffuser can be operated in a plug-and-play manner during training time to enhance existing BEV models without requiring any architectural modifications. Extensive experiments on the challenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoising and generation capabilities, which enable significant enhancement to existing BEV models, as evidenced by notable improvements of 12.3\% in mAP and 10.1\% in NDS achieved for 3D object detection without introducing additional computational complexity. Moreover, substantial improvements in long-tail object detection and under challenging weather and lighting conditions further validate BEVDiffuser's effectiveness in denoising and enhancing BEV representations.

### M-LLM Based Video Frame Selection for Efficient Video Understanding 
[[arxiv](https://arxiv.org/abs/2502.19680)] [[cool](https://papers.cool/arxiv/2502.19680)] [[pdf](https://arxiv.org/pdf/2502.19680)]
> **Authors**: Kai Hu,Feng Gao,Xiaohan Nie,Peng Zhou,Son Tran,Tal Neiman,Lingyun Wang,Mubarak Shah,Raffay Hamid,Bing Yin,Trishul Chilimbi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising results in video reasoning. Popular Multi-Modal Large Language Model (M-LLM) frameworks usually apply naive uniform sampling to reduce the number of video frames that are fed into an M-LLM, particularly for long context videos. However, it could lose crucial context in certain periods of a video, so that the downstream M-LLM may not have sufficient visual information to answer a question. To attack this pain point, we propose a light-weight M-LLM -based frame selection method that adaptively select frames that are more relevant to users' queries. In order to train the proposed frame selector, we introduce two supervision signals (i) Spatial signal, where single frame importance score by prompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by prompting Large Language Model (LLM) using the captions of all frame candidates. The selected frames are then digested by a frozen downstream video M-LLM for visual reasoning and question answering. Empirical results show that the proposed M-LLM video frame selector improves the performances various downstream video Large Language Model (video-LLM) across medium (ActivityNet, NExT-QA) and long (EgoSchema, LongVideoBench) context video question answering benchmarks.

### Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack 
[[arxiv](https://arxiv.org/abs/2502.19672)] [[cool](https://papers.cool/arxiv/2502.19672)] [[pdf](https://arxiv.org/pdf/2502.19672)]
> **Authors**: Chenhe Gu,Jindong Gu,Andong Hua,Yao Qin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: arXiv admin note: text overlap with arXiv:2403.09766
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Multimodal Large Language Models (MLLMs), built upon LLMs, have recently gained attention for their capabilities in image recognition and understanding. However, while MLLMs are vulnerable to adversarial attacks, the transferability of these attacks across different models remains limited, especially under targeted attack setting. Existing methods primarily focus on vision-specific perturbations but struggle with the complex nature of vision-language modality alignment. In this work, we introduce the Dynamic Vision-Language Alignment (DynVLA) Attack, a novel approach that injects dynamic perturbations into the vision-language connector to enhance generalization across diverse vision-language alignment of different models. Our experimental results show that DynVLA significantly improves the transferability of adversarial examples across various MLLMs, including BLIP2, InstructBLIP, MiniGPT4, LLaVA, and closed-source models such as Gemini.

### Noise-Injected Spiking Graph Convolution for Energy-Efficient 3D Point Cloud Denoising 
[[arxiv](https://arxiv.org/abs/2502.19660)] [[cool](https://papers.cool/arxiv/2502.19660)] [[pdf](https://arxiv.org/pdf/2502.19660)]
> **Authors**: Zikuan Li,Qiaoyun Wu,Jialin Zhang,Kaijun Zhang,Jun Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Spiking neural networks (SNNs), inspired by the spiking computation paradigm of the biological neural systems, have exhibited superior energy efficiency in 2D classification tasks over traditional artificial neural networks (ANNs). However, the regression potential of SNNs has not been well explored, especially in 3D point cloud processing.In this paper, we propose noise-injected spiking graph convolutional networks to leverage the full regression potential of SNNs in 3D point cloud denoising. Specifically, we first emulate the noise-injected neuronal dynamics to build noise-injected spiking neurons. On this basis, we design noise-injected spiking graph convolution for promoting disturbance-aware spiking representation learning on 3D points. Starting from the spiking graph convolution, we build two SNN-based denoising networks. One is a purely spiking graph convolutional network, which achieves low accuracy loss compared with some ANN-based alternatives, while resulting in significantly reduced energy consumption on two benchmark datasets, PU-Net and PC-Net. The other is a hybrid architecture that combines ANN-based learning with a high performance-efficiency trade-off in just a few time steps. Our work lights up SNN's potential for 3D point cloud denoising, injecting new perspectives of exploring the deployment on neuromorphic chips while paving the way for developing energy-efficient 3D data acquisition devices.

### MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19634)] [[cool](https://papers.cool/arxiv/2502.19634)] [[pdf](https://arxiv.org/pdf/2502.19634)]
> **Authors**: Jiazhen Pan,Che Liu,Junde Wu,Fenglin Liu,Jiayuan Zhu,Hongwei Bran Li,Chen Chen,Cheng Ouyang,Daniel Rueckert
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.

### 3D Nephrographic Image Synthesis in CT Urography with the Diffusion Model and Swin Transformer 
[[arxiv](https://arxiv.org/abs/2502.19623)] [[cool](https://papers.cool/arxiv/2502.19623)] [[pdf](https://arxiv.org/pdf/2502.19623)]
> **Authors**: Hongkun Yu,Syed Jamal Safdar Gardezi,E. Jason Abel,Daniel Shapiro,Meghan G. Lubner,Joshua Warner,Matthew Smith,Giuseppe Toia,Lu Mao,Pallavi Tiwari,Andrew L. Wentland
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 15 pages, 6 figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Purpose: This study aims to develop and validate a method for synthesizing 3D nephrographic phase images in CT urography (CTU) examinations using a diffusion model integrated with a Swin Transformer-based deep learning approach. Materials and Methods: This retrospective study was approved by the local Institutional Review Board. A dataset comprising 327 patients who underwent three-phase CTU (mean $\pm$ SD age, 63 $\pm$ 15 years; 174 males, 153 females) was curated for deep learning model development. The three phases for each patient were aligned with an affine registration algorithm. A custom deep learning model coined dsSNICT (diffusion model with a Swin transformer for synthetic nephrographic phase images in CT) was developed and implemented to synthesize the nephrographic images. Performance was assessed using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Mean Absolute Error (MAE), and Fréchet Video Distance (FVD). Qualitative evaluation by two fellowship-trained abdominal radiologists was performed. Results: The synthetic nephrographic images generated by our proposed approach achieved high PSNR (26.3 $\pm$ 4.4 dB), SSIM (0.84 $\pm$ 0.069), MAE (12.74 $\pm$ 5.22 HU), and FVD (1323). Two radiologists provided average scores of 3.5 for real images and 3.4 for synthetic images (P-value = 0.5) on a Likert scale of 1-5, indicating that our synthetic images closely resemble real images. Conclusion: The proposed approach effectively synthesizes high-quality 3D nephrographic phase images. This model can be used to reduce radiation dose in CTU by 33.3\% without compromising image quality, which thereby enhances the safety and diagnostic utility of CT urography.

### Tell me why: Visual foundation models as self-explainable classifiers 
[[arxiv](https://arxiv.org/abs/2502.19577)] [[cool](https://papers.cool/arxiv/2502.19577)] [[pdf](https://arxiv.org/pdf/2502.19577)]
> **Authors**: Hugues Turbé,Mina Bjelogrlic,Gianmarco Mengaldo,Christian Lovis
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: :68T10ACM Class:I.2.6; I.2.10; I.5.4
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.

### Dictionary-based Framework for Interpretable and Consistent Object Parsing 
[[arxiv](https://arxiv.org/abs/2502.19540)] [[cool](https://papers.cool/arxiv/2502.19540)] [[pdf](https://arxiv.org/pdf/2502.19540)]
> **Authors**: Tiezheng Zhang,Qihang Yu,Alan Yuille,Ju He
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this work, we present CoCal, an interpretable and consistent object parsing framework based on dictionary-based mask transformer. Designed around Contrastive Components and Logical Constraints, CoCal rethinks existing cluster-based mask transformer architectures used in segmentation; Specifically, CoCal utilizes a set of dictionary components, with each component being explicitly linked to a specific semantic class. To advance this concept, CoCal introduces a hierarchical formulation of dictionary components that aligns with the semantic hierarchy. This is achieved through the integration of both within-level contrastive components and cross-level logical constraints. Concretely, CoCal employs a component-wise contrastive algorithm at each semantic level, enabling the contrasting of dictionary components within the same class against those from different classes. Furthermore, CoCal addresses logical concerns by ensuring that the dictionary component representing a particular part is closer to its corresponding object component than to those of other objects through a cross-level contrastive learning objective. To further enhance our logical relation modeling, we implement a post-processing function inspired by the principle that a pixel assigned to a part should also be assigned to its corresponding object. With these innovations, CoCal establishes a new state-of-the-art performance on both PartImageNet and Pascal-Part-108, outperforming previous methods by a significant margin of 2.08% and 0.70% in part mIoU, respectively. Moreover, CoCal exhibits notable enhancements in object-level metrics across these benchmarks, highlighting its capacity to not only refine parsing at a finer level but also elevate the overall quality of object segmentation.

### Evaluating the Suitability of Different Intraoral Scan Resolutions for Deep Learning-Based Tooth Segmentation 
[[arxiv](https://arxiv.org/abs/2502.19515)] [[cool](https://papers.cool/arxiv/2502.19515)] [[pdf](https://arxiv.org/pdf/2502.19515)]
> **Authors**: Daron Weekley,Jace Duckworth,Anastasiia Sukhanova,Ananya Jana
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: accepted to 2025 ASEE North Central Section Annual Conference
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Intraoral scans are widely used in digital dentistry for tasks such as dental restoration, treatment planning, and orthodontic procedures. These scans contain detailed topological information, but manual annotation of these scans remains a time-consuming task. Deep learning-based methods have been developed to automate tasks such as tooth segmentation. A typical intraoral scan contains over 200,000 mesh cells, making direct processing computationally expensive. Models are often trained on downsampled versions, typically with 10,000 or 16,000 cells. Previous studies suggest that downsampling may degrade segmentation accuracy, but the extent of this degradation remains unclear. Understanding the extent of degradation is crucial for deploying ML models on edge devices. This study evaluates the extent of performance degradation with decreasing resolution. We train a deep learning model (PointMLP) on intraoral scans decimated to 16K, 10K, 8K, 6K, 4K, and 2K mesh cells. Models trained at lower resolutions are tested on high-resolution scans to assess performance. Our goal is to identify a resolution that balances computational efficiency and segmentation accuracy.

### CLIP-Optimized Multimodal Image Enhancement via ISP-CNN Fusion for Coal Mine IoVT under Uneven Illumination 
[[arxiv](https://arxiv.org/abs/2502.19450)] [[cool](https://papers.cool/arxiv/2502.19450)] [[pdf](https://arxiv.org/pdf/2502.19450)]
> **Authors**: Shuai Wang,Shihao Zhang,Jiaqi Wu,Zijian Tian,Wei Chen,Tongzhu Jin,Miaomiao Xue,Zehua Wang,Fei Richard Yu,Victor C. M. Leung
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Clear monitoring images are crucial for the safe operation of coal mine Internet of Video Things (IoVT) systems. However, low illumination and uneven brightness in underground environments significantly degrade image quality, posing challenges for enhancement methods that often rely on difficult-to-obtain paired reference images. Additionally, there is a trade-off between enhancement performance and computational efficiency on edge devices within IoVT systems.To address these issues, we propose a multimodal image enhancement method tailored for coal mine IoVT, utilizing an ISP-CNN fusion architecture optimized for uneven illumination. This two-stage strategy combines global enhancement with detail optimization, effectively improving image quality, especially in poorly lit areas. A CLIP-based multimodal iterative optimization allows for unsupervised training of the enhancement algorithm. By integrating traditional image signal processing (ISP) with convolutional neural networks (CNN), our approach reduces computational complexity while maintaining high performance, making it suitable for real-time deployment on edge devices.Experimental results demonstrate that our method effectively mitigates uneven brightness and enhances key image quality metrics, with PSNR improvements of 2.9%-4.9%, SSIM by 4.3%-11.4%, and VIF by 4.9%-17.8% compared to seven state-of-the-art algorithms. Simulated coal mine monitoring scenarios validate our method's ability to balance performance and computational demands, facilitating real-time enhancement and supporting safer mining operations.

### ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19409)] [[cool](https://papers.cool/arxiv/2502.19409)] [[pdf](https://arxiv.org/pdf/2502.19409)]
> **Authors**: Danae Sánchez Villegas,Ingo Ziegler,Desmond Elliott
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Code, dataset, and checkpoints are publicly available at https://github.com/danaesavi/ImageChain
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.

### Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions 
[[arxiv](https://arxiv.org/abs/2502.19293)] [[cool](https://papers.cool/arxiv/2502.19293)] [[pdf](https://arxiv.org/pdf/2502.19293)]
> **Authors**: Ruben T. Lucassen,Sander P. J. Moonemans,Tijn van de Luijtgaarden,Gerben E. Breimer,Willeke A. M. Blokx,Mitko Veta
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 11 pages, 2 figures. arXiv admin note: text overlap with arXiv:2502.19285
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Millions of melanocytic skin lesions are examined by pathologists each year, the majority of which concern common nevi (i.e., ordinary moles). While most of these lesions can be diagnosed in seconds, writing the corresponding pathology report is much more time-consuming. Automating part of the report writing could, therefore, alleviate the increasing workload of pathologists. In this work, we develop a vision-language model specifically for the pathology domain of cutaneous melanocytic lesions. The model follows the Contrastive Captioner framework and was trained and evaluated using a melanocytic lesion dataset of 42,512 H&E-stained whole slide images and 19,645 corresponding pathology reports. Our results show that the quality scores of model-generated reports were on par with pathologist-written reports for common nevi, assessed by an expert pathologist in a reader study. While report generation revealed to be more difficult for rare melanocytic lesion subtypes, the cross-modal retrieval performance for these cases was considerably better.

### On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation 
[[arxiv](https://arxiv.org/abs/2502.19285)] [[cool](https://papers.cool/arxiv/2502.19285)] [[pdf](https://arxiv.org/pdf/2502.19285)]
> **Authors**: Ruben T. Lucassen,Tijn van de Luijtgaarden,Sander P. J. Moonemans,Gerben E. Breimer,Willeke A. M. Blokx,Mitko Veta
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 11 pages, 1 figure
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Vision-language models in pathology enable multimodal case retrieval and automated report generation. Many of the models developed so far, however, have been trained on pathology reports that include information which cannot be inferred from paired whole slide images (e.g., patient history), potentially leading to hallucinated sentences in generated reports. To this end, we investigate how the selection of information from pathology reports for vision-language modeling affects the quality of the multimodal representations and generated reports. More concretely, we compare a model trained on full reports against a model trained on preprocessed reports that only include sentences describing the cell and tissue appearances based on the H&E-stained slides. For the experiments, we built upon the BLIP-2 framework and used a cutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images and 19,636 corresponding pathology reports. Model performance was assessed using image-to-text and text-to-image retrieval, as well as qualitative evaluation of the generated reports by an expert pathologist. Our results demonstrate that text preprocessing prevents hallucination in report generation. Despite the improvement in the quality of the generated reports, training the vision-language model on full reports showed better cross-modal retrieval performance.

### Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.19269)] [[cool](https://papers.cool/arxiv/2502.19269)] [[pdf](https://arxiv.org/pdf/2502.19269)]
> **Authors**: Jiawei Kong,Hao Fang,Sihang Guo,Chenxi Qing,Bin Chen,Bin Wang,Shu-Tao Xia
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit excellent representational capabilities for multimodal data, recent studies have shown that they are vulnerable to backdoor attacks. To alleviate the threat, existing defense strategies primarily focus on fine-tuning the entire suspicious model, yet offer only marginal resistance to state-of-the-art attacks and often result in a decrease in clean accuracy, particularly in data-limited scenarios. Their failure may be attributed to the mismatch between insufficient fine-tuning data and massive parameters in VLMs. To address this challenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, an efficient and effective method that operates on the text prompts to indirectly purify the poisoned VLMs. Specifically, we first employ the advanced contrastive learning via our carefully crafted positive and negative samples, to effectively invert the backdoor triggers that are potentially adopted by the attacker. Once the dummy trigger is established, we utilize the efficient prompt tuning technique to optimize these class-wise text prompts for modifying the model's decision boundary to further reclassify the feature regions of backdoor triggers. Extensive experiments demonstrate that CBPT significantly mitigates backdoor threats while preserving model utility, e.g. an average Clean Accuracy (CA) of 58.86\% and an Attack Success Rate (ASR) of 0.39\% across seven mainstream backdoor attacks. These results underscore the superiority of our prompt purifying design to strengthen model robustness against backdoor attacks.

### ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding 
[[arxiv](https://arxiv.org/abs/2502.19247)] [[cool](https://papers.cool/arxiv/2502.19247)] [[pdf](https://arxiv.org/pdf/2502.19247)]
> **Authors**: Qihang Peng,Henry Zheng,Gao Huang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, 3 figures. Accepted by CVPR2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions. A foundational task in this domain is ego-centric 3D visual grounding. However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions. Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks. We propose Proxy Transformation suitable for multimodal task to efficiently improve the point cloud manifold. Our method first leverages Deformable Point Clustering to identify the point cloud sub-manifolds in target regions. Then, we propose a Proxy Attention module that utilizes multimodal proxies to guide point cloud transformation. Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions. Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions. Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%. These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.

### Arbitrary Volumetric Refocusing of Dense and Sparse Light Fields 
[[arxiv](https://arxiv.org/abs/2502.19238)] [[cool](https://papers.cool/arxiv/2502.19238)] [[pdf](https://arxiv.org/pdf/2502.19238)]
> **Authors**: Tharindu Samarakoon,Kalana Abeywardena,Chamira U. S. Edussooriya
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 9 pages, 7 figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: A four-dimensional light field (LF) captures both textural and geometrical information of a scene in contrast to a two-dimensional image that captures only the textural information of a scene. Post-capture refocusing is an exciting application of LFs enabled by the geometric information captured. Previously proposed LF refocusing methods are mostly limited to the refocusing of single planar or volumetric region of a scene corresponding to a depth range and cannot simultaneously generate in-focus and out-of-focus regions having the same depth range. In this paper, we propose an end-to-end pipeline to simultaneously refocus multiple arbitrary planar or volumetric regions of a dense or a sparse LF. We employ pixel-dependent shifts with the typical shift-and-sum method to refocus an LF. The pixel-dependent shifts enables to refocus each pixel of an LF independently. For sparse LFs, the shift-and-sum method introduces ghosting artifacts due to the spatial undersampling. We employ a deep learning model based on U-Net architecture to almost completely eliminate the ghosting artifacts. The experimental results obtained with several LF datasets confirm the effectiveness of the proposed method. In particular, sparse LFs refocused with the proposed method archive structural similarity index higher than 0.9 despite having only 20% of data compared to dense LFs.

### EGR-Net: A Novel Embedding Gramian Representation CNN for Intelligent Fault Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.19199)] [[cool](https://papers.cool/arxiv/2502.19199)] [[pdf](https://arxiv.org/pdf/2502.19199)]
> **Authors**: Linshan Jia
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Feature extraction is crucial in intelligent fault diagnosis of rotating machinery. It is easier for convolutional neural networks(CNNs) to visually recognize and learn fault features by converting the complicated one-dimensional (1D) vibrational signals into two-dimensional (2D) images with simple textures. However, the existing representation methods for encoding 1D signals as images have two main problems, including complicated computation and low separability. Meanwhile, the existing 2D-CNN fault diagnosis methods taking 2D images as the only inputs still suffer from the inevitable information loss because of the conversion process. Considering the above issues, this paper proposes a new 1D-to-2D conversion method called Embedding Gramian Representation (EGR), which is easy to calculate and shows good separability. In EGR, 1D signals are projected in the embedding space and the intrinsic periodicity of vibrational signals is captured enabling the faulty characteristics contained in raw signals to be uncovered. Second, aiming at the information loss problem of existing CNN models with the single input of converted images, a double-branch EGR-based CNN, called EGR-Net, is proposed to learn faulty features from both raw signal feature maps and their corresponding EGRs. The bridge connection is designed to improve the feature learning interaction between the two branches. Widely used open domain gearbox dataset and bearing dataset are used to verify the effectiveness and efficiency of the proposed methods. EGR-Net is compared with traditional and state-of-the-art approaches, and the results show that the proposed method can deliver enhanced performance.

### A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs 
[[arxiv](https://arxiv.org/abs/2502.19159)] [[cool](https://papers.cool/arxiv/2502.19159)] [[pdf](https://arxiv.org/pdf/2502.19159)]
> **Authors**: Xuan Ding,Yao Zhu,Yunjian Zhang,Chuanlong Xie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. Howerver, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the "Patch-like" feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we proposes a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35\% pruning on the Vicuna-7B model, our method achieved a 1.654\% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.

### SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.19128)] [[cool](https://papers.cool/arxiv/2502.19128)] [[pdf](https://arxiv.org/pdf/2502.19128)]
> **Authors**: Junlong Ren,Hao Wu,Hui Xiong,Hao Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ICRA 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The cross-modal 3D retrieval task aims to achieve mutual matching between text descriptions and 3D shapes. This has the potential to enhance the interaction between natural language and the 3D environment, especially within the realms of robotics and embodied artificial intelligence (AI) applications. However, the scarcity and expensiveness of 3D data constrain the performance of existing cross-modal 3D retrieval methods. These methods heavily rely on features derived from the limited number of 3D shapes, resulting in poor generalization ability across diverse scenarios. To address this challenge, we introduce SCA3D, a novel 3D shape and caption online data augmentation method for cross-modal 3D retrieval. Our approach uses the LLaVA model to create a component library, captioning each segmented part of every 3D shape within the dataset. Notably, it facilitates the generation of extensive new 3D-text pairs containing new semantic features. We employ both inter and intra distances to align various components into a new 3D shape, ensuring that the components do not overlap and are closely fitted. Further, text templates are utilized to process the captions of each component and generate new text descriptions. Besides, we use unimodal encoders to extract embeddings for 3D shapes and texts based on the enriched dataset. We then calculate fine-grained cross-modal similarity using Earth Mover's Distance (EMD) and enhance cross-modal matching with contrastive learning, enabling bidirectional retrieval between texts and 3D shapes. Extensive experiments show our SCA3D outperforms previous works on the Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to 27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be found in https://github.com/3DAgentWorld/SCA3D.

### An anatomically-informed correspondence initialisation method to improve learning-based registration for radiotherapy 
[[arxiv](https://arxiv.org/abs/2502.19101)] [[cool](https://papers.cool/arxiv/2502.19101)] [[pdf](https://arxiv.org/pdf/2502.19101)]
> **Authors**: Edward G. A. Henderson,Marcel van Herk,Andrew F. Green,Eliana M. Vasquez Osorio
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Presented at the XXth International Conference on the use of Computers in Radiation therapy. Pages 99-102 in XXth ICCR Proceedings, found here https://udl.hal.science/hal-04720234v1
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose an anatomically-informed initialisation method for interpatient CT non-rigid registration (NRR), using a learning-based model to estimate correspondences between organ structures. A thin plate spline (TPS) deformation, set up using the correspondence predictions, is used to initialise the scans before a second NRR step. We compare two established NRR methods for the second step: a B-spline iterative optimisation-based algorithm and a deep learning-based approach. Registration performance is evaluated with and without the initialisation by assessing the similarity of propagated structures. Our proposed initialisation improved the registration performance of the learning-based method to more closely match the traditional iterative algorithm, with the mean distance-to-agreement reduced by 1.8mm for structures included in the TPS and 0.6mm for structures not included, while maintaining a substantial speed advantage (5 vs. 72 seconds).

### An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection 
[[arxiv](https://arxiv.org/abs/2502.19048)] [[cool](https://papers.cool/arxiv/2502.19048)] [[pdf](https://arxiv.org/pdf/2502.19048)]
> **Authors**: Tresor Y. Koffi,Youssef Mourchid,Mohammed Hindawi,Yohan Dupuis
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 17th International Conference onMachineVision (ICMV 2024) will take place in Edinburgh, UK during October 10-13, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. To support further research and development of building more reliable impact fall detection systems, we have made the improved 3D skeletons UP-Fall dataset publicly available at this link https://zenodo.org/records/12773013.

### FungalZSL: Zero-Shot Fungal Classification with Image Captioning Using a Synthetic Data Approach 
[[arxiv](https://arxiv.org/abs/2502.19038)] [[cool](https://papers.cool/arxiv/2502.19038)] [[pdf](https://arxiv.org/pdf/2502.19038)]
> **Authors**: Anju Rani,Daniel O. Arroyo,Petar Durdevic
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 11 pages, 5 Figures, 1 Table
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The effectiveness of zero-shot classification in large vision-language models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), depends on access to extensive, well-aligned text-image datasets. In this work, we introduce two complementary data sources, one generated by large language models (LLMs) to describe the stages of fungal growth and another comprising a diverse set of synthetic fungi images. These datasets are designed to enhance CLIPs zero-shot classification capabilities for fungi-related tasks. To ensure effective alignment between text and image data, we project them into CLIPs shared representation space, focusing on different fungal growth stages. We generate text using LLaMA3.2 to bridge modality gaps and synthetically create fungi images. Furthermore, we investigate knowledge transfer by comparing text outputs from different LLM techniques to refine classification across growth stages.

### Enhanced Neuromorphic Semantic Segmentation Latency through Stream Event 
[[arxiv](https://arxiv.org/abs/2502.18982)] [[cool](https://papers.cool/arxiv/2502.18982)] [[pdf](https://arxiv.org/pdf/2502.18982)]
> **Authors**: D. Hareb,J. Martinet,B. Miramond
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Achieving optimal semantic segmentation with frame-based vision sensors poses significant challenges for real-time systems like UAVs and self-driving cars, which require rapid and precise processing. Traditional frame-based methods often struggle to balance latency, accuracy, and energy efficiency. To address these challenges, we leverage event streams from event-based cameras-bio-inspired sensors that trigger events in response to changes in the scene. Specifically, we analyze the number of events triggered between successive frames, with a high number indicating significant changes and a low number indicating minimal changes. We exploit this event information to solve the semantic segmentation task by employing a Spiking Neural Network (SNN), a bio-inspired computing paradigm known for its low energy consumption. Our experiments on the DSEC dataset show that our approach significantly reduces latency with only a limited drop in accuracy. Additionally, by using SNNs, we achieve low power consumption, making our method suitable for energy-constrained real-time applications. To the best of our knowledge, our approach is the first to effectively balance reduced latency, minimal accuracy loss, and energy efficiency using events stream to enhance semantic segmentation in dynamic and resource-limited environments.

### Brain-inspired analogical mixture prototypes for few-shot class-incremental learning 
[[arxiv](https://arxiv.org/abs/2502.18923)] [[cool](https://papers.cool/arxiv/2502.18923)] [[pdf](https://arxiv.org/pdf/2502.18923)]
> **Authors**: Wanyi Li,Wei Wei,Yongkang Luo,Peng Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: under review
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Few-shot class-incremental learning (FSCIL) poses significant challenges for artificial neural networks due to the need to efficiently learn from limited data while retaining knowledge of previously learned tasks. Inspired by the brain's mechanisms for categorization and analogical learning, we propose a novel approach called Brain-inspired Analogical Mixture Prototypes (BAMP). BAMP has three components: mixed prototypical feature learning, statistical analogy, and soft voting. Starting from a pre-trained Vision Transformer (ViT), mixed prototypical feature learning represents each class using a mixture of prototypes and fine-tunes these representations during the base session. The statistical analogy calibrates the mean and covariance matrix of prototypes for new classes according to similarity to the base classes, and computes classification score with Mahalanobis distance. Soft voting combines both merits of statistical analogy and an off-shelf FSCIL method. Our experiments on benchmark datasets demonstrate that BAMP outperforms state-of-the-art on both traditional big start FSCIL setting and challenging small start FSCIL setting. The study suggests that brain-inspired analogical mixture prototypes can alleviate catastrophic forgetting and over-fitting problems in FSCIL.

### Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025 
[[arxiv](https://arxiv.org/abs/2502.18867)] [[cool](https://papers.cool/arxiv/2502.18867)] [[pdf](https://arxiv.org/pdf/2502.18867)]
> **Authors**: Akhil Penta,Vaibhav Adwani,Ankush Chopra
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.

### Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM 
[[arxiv](https://arxiv.org/abs/2502.18863)] [[cool](https://papers.cool/arxiv/2502.18863)] [[pdf](https://arxiv.org/pdf/2502.18863)]
> **Authors**: Junxiao Ma,Jingjing Wang,Jiamin Luo,Peiying Yu,Guodong Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm \textbf{M}ulti-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.

### BarkXAI: A Lightweight Post-Hoc Explainable Method for Tree Species Classification with Quantifiable Concepts 
[[arxiv](https://arxiv.org/abs/2502.18844)] [[cool](https://papers.cool/arxiv/2502.18844)] [[pdf](https://arxiv.org/pdf/2502.18844)]
> **Authors**: Yunmei Huang,Songlin Hou,Zachary Nelson Horve,Songlin Fei
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The precise identification of tree species is fundamental to forestry, conservation, and environmental monitoring. Though many studies have demonstrated that high accuracy can be achieved using bark-based species classification, these models often function as "black boxes", limiting interpretability, trust, and adoption in critical forestry applications. Attribution-based Explainable AI (XAI) methods have been used to address this issue in related works. However, XAI applications are often dependent on local features (such as a head shape or paw in animal applications) and cannot describe global visual features (such as ruggedness or smoothness) that are present in texture-dominant images such as tree bark. Concept-based XAI methods, on the other hand, offer explanations based on global visual features with concepts, but they tend to require large overhead in building external concept image datasets and the concepts can be vague and subjective without good means of precise quantification. To address these challenges, we propose a lightweight post-hoc method to interpret visual models for tree species classification using operators and quantifiable concepts. Our approach eliminates computational overhead, enables the quantification of complex concepts, and evaluates both concept importance and the model's reasoning process. To the best of our knowledge, our work is the first study to explain bark vision models in terms of global visual features with concepts. Using a human-annotated dataset as ground truth, our experiments demonstrate that our method significantly outperforms TCAV and Llama3.2 in concept importance ranking based on Kendall's Tau, highlighting its superior alignment with human perceptions.

## 计算机与社会(cs.CY:Computers and Society)

### Do LLMs exhibit demographic parity in responses to queries about Human Rights? 
[[arxiv](https://arxiv.org/abs/2502.19463)] [[cool](https://papers.cool/arxiv/2502.19463)] [[pdf](https://arxiv.org/pdf/2502.19463)]
> **Authors**: Rafiya Javed,Jackie Kay,David Yanni,Abdullah Zaini,Anushe Sheikh,Maribeth Rauh,Iason Gabriel,Laura Weidinger
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,社交和信息网络
- **Abstract**: This research describes a novel approach to evaluating hedging behaviour in large language models (LLMs), specifically in the context of human rights as defined in the Universal Declaration of Human Rights (UDHR). Hedging and non-affirmation are behaviours that express ambiguity or a lack of clear endorsement on specific statements. These behaviours are undesirable in certain contexts, such as queries about whether different groups are entitled to specific human rights; since all people are entitled to human rights. Here, we present the first systematic attempt to measure these behaviours in the context of human rights, with a particular focus on between-group comparisons. To this end, we design a novel prompt set on human rights in the context of different national or social identities. We develop metrics to capture hedging and non-affirmation behaviours and then measure whether LLMs exhibit demographic parity when responding to the queries. We present results on three leading LLMs and find that all models exhibit some demographic disparities in how they attribute human rights between different identity groups. Futhermore, there is high correlation between different models in terms of how disparity is distributed amongst identities, with identities that have high disparity in one model also facing high disparity in both the other models. While baseline rates of hedging and non-affirmation differ, these disparities are consistent across queries that vary in ambiguity and they are robust across variations of the precise query wording. Our findings highlight the need for work to explicitly align LLMs to human rights principles, and to ensure that LLMs endorse the human rights of all groups equally.

### Implementation of a Generative AI Assistant in K-12 Education: The CGScholar AI Helper Initiative 
[[arxiv](https://arxiv.org/abs/2502.19422)] [[cool](https://papers.cool/arxiv/2502.19422)] [[pdf](https://arxiv.org/pdf/2502.19422)]
> **Authors**: Vania Castro,Ana Karina de Oliveira Nascimento,Raigul Zheldibayeva,Duane Searsmith,Akash Saini,Bill Cope,Mary Kalantzis
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: This paper focuses on the piloting of the CGScholar AI Helper, a Generative AI (GenAI) assistant tool that aims to provide feedback on writing in high school contexts. The aim was to use GenAI to provide formative and summative feedback on students' texts in English Language Arts (ELA) and History. The trials discussed in this paper relate to Grade 11, a crucial learning phase when students are working towards college readiness. These trials took place in two very different schools in the Midwest of the United States, one in a low socio-economic background with low-performance outcomes and the other in a high socio-economic background with high-performance outcomes. The assistant tool used two main mechanisms "prompt engineering" based on participant teachers' assessment rubric and "fine-tuning" a Large Language Model (LLM) from a customized corpus of teaching materials using Retrieval Augmented Generation (RAG). This paper focuses on the CGScholar AI Helper's potential to enhance students' writing abilities and support teachers in ELA and other subject areas requiring written assignments.

### Provocations from the Humanities for Generative AI Research 
[[arxiv](https://arxiv.org/abs/2502.19190)] [[cool](https://papers.cool/arxiv/2502.19190)] [[pdf](https://arxiv.org/pdf/2502.19190)]
> **Authors**: Lauren Klein,Meredith Martin,André Brock,Maria Antoniak,Melanie Walsh,Jessica Marie Johnson,Lauren Tilton,David Mimno
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: working draft; final draft in preparation
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This paper presents a set of provocations for considering the uses, impact, and harms of generative AI from the perspective of humanities researchers. We provide a working definition of humanities research, summarize some of its most salient theories and methods, and apply these theories and methods to the current landscape of AI. Drawing from foundational work in critical data studies, along with relevant humanities scholarship, we elaborate eight claims with broad applicability to current conversations about generative AI: 1) Models make words, but people make meaning; 2) Generative AI requires an expanded definition of culture; 3) Generative AI can never be representative; 4) Bigger models are not always better models; 5) Not all training data is equivalent; 6) Openness is not an easy fix; 7) Limited access to compute enables corporate capture; and 8) AI universalism creates narrow human subjects. We conclude with a discussion of the importance of resisting the extraction of humanities research by computer science and related fields.

### The Shady Light of Art Automation 
[[arxiv](https://arxiv.org/abs/2502.19107)] [[cool](https://papers.cool/arxiv/2502.19107)] [[pdf](https://arxiv.org/pdf/2502.19107)]
> **Authors**: Dejan Grba
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to ISEA 2025
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Generative artificial intelligence (generative AI) has entered the mainstream culture and become a subject of extensive academic investigation. However, the character and background of its impact on art require subtler scrutiny and more nuanced contextualization. This paper summarizes a broader study of the roles that AI's conceptual and ideological substrata play in influencing art notions. The focus is on divergent but coalescing and often questionable ideas, values, and political views that generative AI and other art-related AI technologies propagate from the computer science and AI/tech industry to the contemporary art and culture. The paper maps the main areas of this complex relationship and concisely critiques their key aspects.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Optimal Approximate Matrix Multiplication over Sliding Windows 
[[arxiv](https://arxiv.org/abs/2502.18830)] [[cool](https://papers.cool/arxiv/2502.18830)] [[pdf](https://arxiv.org/pdf/2502.18830)]
> **Authors**: Ziqi Yao,Mingsong Chen,Cheng Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: We explore the problem of approximate matrix multiplication (AMM) within the sliding window model, where algorithms utilize limited space to perform large-scale matrix multiplication in a streaming manner. This model has garnered increasing attention in the fields of machine learning and data mining due to its ability to handle time sensitivity and reduce the impact of outdated data. However, despite recent advancements, determining the optimal space bound for this problem remains an open question. In this paper, we introduce the DS-COD algorithm for AMM over sliding windows. This novel and deterministic algorithm achieves optimal performance regarding the space-error tradeoff. We provide theoretical error bounds and the complexity analysis for the proposed algorithm, and establish the corresponding space lower bound for the AMM sliding window problem. Additionally, we present an adaptive version of DS-COD, termed aDS-COD, which improves computational efficiency and demonstrates superior empirical performance. Extensive experiments conducted on both synthetic and real-world datasets validate our theoretical findings and highlight the practical effectiveness of our methods.

## 图形(cs.GR:Graphics)

### Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting 
[[arxiv](https://arxiv.org/abs/2502.19459)] [[cool](https://papers.cool/arxiv/2502.19459)] [[pdf](https://arxiv.org/pdf/2502.19459)]
> **Authors**: Yu Liu,Baoxiong Jia,Ruijie Lu,Junfeng Ni,Song-Chun Zhu,Siyuan Huang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图形,机器学习,机器人技术
- **Abstract**: Building articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement.

### AniGaussian: Animatable Gaussian Avatar with Pose-guided Deformation 
[[arxiv](https://arxiv.org/abs/2502.19441)] [[cool](https://papers.cool/arxiv/2502.19441)] [[pdf](https://arxiv.org/pdf/2502.19441)]
> **Authors**: Mengtian Li,Shengxiang Yao,Chen Kai,Zhifeng Xie,Keyu Chen,Yu-Gang Jiang
> **First submission**: 2025-02-24
> **First announcement**: 2025-02-27
> **comment**: 13pages, 14 figures. arXiv admin note: text overlap with arXiv:2401.09720
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: Recent advancements in Gaussian-based human body reconstruction have achieved notable success in creating animatable avatars. However, there are ongoing challenges to fully exploit the SMPL model's prior knowledge and enhance the visual fidelity of these models to achieve more refined avatar reconstructions. In this paper, we introduce AniGaussian which addresses the above issues with two insights. First, we propose an innovative pose guided deformation strategy that effectively constrains the dynamic Gaussian avatar with SMPL pose guidance, ensuring that the reconstructed model not only captures the detailed surface nuances but also maintains anatomical correctness across a wide range of motions. Second, we tackle the expressiveness limitations of Gaussian models in representing dynamic human bodies. We incorporate rigid-based priors from previous works to enhance the dynamic transform capabilities of the Gaussian model. Furthermore, we introduce a split-with-scale strategy that significantly improves geometry quality. The ablative study experiment demonstrates the effectiveness of our innovative model design. Through extensive comparisons with existing methods, AniGaussian demonstrates superior performance in both qualitative result and quantitative metrics.

### Fatigue-PINN: Physics-Informed Fatigue-Driven Motion Modulation and Synthesis 
[[arxiv](https://arxiv.org/abs/2502.19056)] [[cool](https://papers.cool/arxiv/2502.19056)] [[pdf](https://arxiv.org/pdf/2502.19056)]
> **Authors**: Iliana Loi,Konstantinos Moustakas
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 13 pages, 9 pages. This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 图形,机器学习
- **Abstract**: Fatigue modeling is essential for motion synthesis tasks to model human motions under fatigued conditions and biomechanical engineering applications, such as investigating the variations in movement patterns and posture due to fatigue, defining injury risk mitigation and prevention strategies, formulating fatigue minimization schemes and creating improved ergonomic designs. Nevertheless, employing data-driven methods for synthesizing the impact of fatigue on motion, receives little to no attention in the literature. In this work, we present Fatigue-PINN, a deep learning framework based on Physics-Informed Neural Networks, for modeling fatigued human movements, while providing joint-specific fatigue configurations for adaptation and mitigation of motion artifacts on a joint level, resulting in more realistic animations. To account for muscle fatigue, we simulate the fatigue-induced fluctuations in the maximum exerted joint torques by leveraging a PINN adaptation of the Three-Compartment Controller model to exploit physics-domain knowledge for improving accuracy. This model also introduces parametric motion alignment with respect to joint-specific fatigue, hence avoiding sharp frame transitions. Our results indicate that Fatigue-PINN accurately simulates the effects of externally perceived fatigue on open-type human movements being consistent with findings from real-world experimental fatigue studies. Since fatigue is incorporated in torque space, Fatigue-PINN provides an end-to-end encoder-decoder-like architecture, to ensure transforming joint angles to joint torques and vice-versa, thus, being compatible with motion synthesis frameworks operating on joint angles.

## 人机交互(cs.HC:Human-Computer Interaction)

### Less or More: Towards Glanceable Explanations for LLM Recommendations Using Ultra-Small Devices 
[[arxiv](https://arxiv.org/abs/2502.19410)] [[cool](https://papers.cool/arxiv/2502.19410)] [[pdf](https://arxiv.org/pdf/2502.19410)]
> **Authors**: Xinru Wang,Mengjie Yu,Hannah Nguyen,Michael Iuzzolino,Tianyi Wang,Peiqi Tang,Natasha Lynova,Co Tran,Ting Zhang,Naveen Sendhilnathan,Hrvoje Benko,Haijun Xia,Tanya Jonker
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Large Language Models (LLMs) have shown remarkable potential in recommending everyday actions as personal AI assistants, while Explainable AI (XAI) techniques are being increasingly utilized to help users understand why a recommendation is given. Personal AI assistants today are often located on ultra-small devices such as smartwatches, which have limited screen space. The verbosity of LLM-generated explanations, however, makes it challenging to deliver glanceable LLM explanations on such ultra-small devices. To address this, we explored 1) spatially structuring an LLM's explanation text using defined contextual components during prompting and 2) presenting temporally adaptive explanations to users based on confidence levels. We conducted a user study to understand how these approaches impacted user experiences when interacting with LLM recommendations and explanations on ultra-small devices. The results showed that structured explanations reduced users' time to action and cognitive load when reading an explanation. Always-on structured explanations increased users' acceptance of AI recommendations. However, users were less satisfied with structured explanations compared to unstructured ones due to their lack of sufficient, readable details. Additionally, adaptively presenting structured explanations was less effective at improving user perceptions of the AI compared to the always-on structured explanations. Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.

### Reimagining Personal Data: Unlocking the Potential of AI-Generated Images in Personal Data Meaning-Making 
[[arxiv](https://arxiv.org/abs/2502.18853)] [[cool](https://papers.cool/arxiv/2502.18853)] [[pdf](https://arxiv.org/pdf/2502.18853)]
> **Authors**: Soobin Park,Hankyung Kim,Youn-kyung Lim
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 21 pages excluding reference and appendix. Accepted at ACM CHI 2025
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AI's GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users' in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users' engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on AI-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making.

## 信息检索(cs.IR:Information Retrieval)

### Multiview graph dual-attention deep learning and contrastive learning for multi-criteria recommender systems 
[[arxiv](https://arxiv.org/abs/2502.19271)] [[cool](https://papers.cool/arxiv/2502.19271)] [[pdf](https://arxiv.org/pdf/2502.19271)]
> **Authors**: Saman Forouzandeh,Pavel N. Krivitsky,Rohitash Chandra
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习,机器学习
- **Abstract**: Recommender systems leveraging deep learning models have been crucial for assisting users in selecting items aligned with their preferences and interests. However, a significant challenge persists in single-criteria recommender systems, which often overlook the diverse attributes of items that have been addressed by Multi-Criteria Recommender Systems (MCRS). Shared embedding vector for multi-criteria item ratings but have struggled to capture the nuanced relationships between users and items based on specific criteria. In this study, we present a novel representation for Multi-Criteria Recommender Systems (MCRS) based on a multi-edge bipartite graph, where each edge represents one criterion rating of items by users, and Multiview Dual Graph Attention Networks (MDGAT). Employing MDGAT is beneficial and important for adequately considering all relations between users and items, given the presence of both local (criterion-based) and global (multi-criteria) relations. Additionally, we define anchor points in each view based on similarity and employ local and global contrastive learning to distinguish between positive and negative samples across each view and the entire graph. We evaluate our method on two real-world datasets and assess its performance based on item rating predictions. The results demonstrate that our method achieves higher accuracy compared to the baseline method for predicting item ratings on the same datasets. MDGAT effectively capture the local and global impact of neighbours and the similarity between nodes.

## 信息论(cs.IT:Information Theory)

### AutoBS: Autonomous Base Station Deployment Framework with Reinforcement Learning and Digital Twin Network 
[[arxiv](https://arxiv.org/abs/2502.19647)] [[cool](https://papers.cool/arxiv/2502.19647)] [[pdf](https://arxiv.org/pdf/2502.19647)]
> **Authors**: Ju-Hyung Lee,Andreas F. Molisch
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,人工智能,机器学习,网络和互联网架构
- **Abstract**: This paper introduces AutoBS, a reinforcement learning (RL)-based framework for optimal base station (BS) deployment in 6G networks. AutoBS leverages the Proximal Policy Optimization (PPO) algorithm and fast, site-specific pathloss predictions from PMNet to efficiently learn deployment strategies that balance coverage and capacity. Numerical results demonstrate that AutoBS achieves 95% for a single BS, and 90% for multiple BSs, of the capacity provided by exhaustive search methods while reducing inference time from hours to milliseconds, making it highly suitable for real-time applications. AutoBS offers a scalable and automated solution for large-scale 6G networks, addressing the challenges of dynamic environments with minimal computational overhead.

## 机器学习(cs.LG:Machine Learning)

### Learning with Exact Invariances in Polynomial Time 
[[arxiv](https://arxiv.org/abs/2502.19758)] [[cool](https://papers.cool/arxiv/2502.19758)] [[pdf](https://arxiv.org/pdf/2502.19758)]
> **Authors**: Ashkan Soleymani,Behrooz Tahmasebi,Stefanie Jegelka,Patrick Jaillet
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (not approximate) invariances in this context. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.

### HALO: Robust Out-of-Distribution Detection via Joint Optimisation 
[[arxiv](https://arxiv.org/abs/2502.19755)] [[cool](https://papers.cool/arxiv/2502.19755)] [[pdf](https://arxiv.org/pdf/2502.19755)]
> **Authors**: Hugo Lyons Keenan,Sarah Erfani,Christopher Leckie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: SaTML 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Effective out-of-distribution (OOD) detection is crucial for the safe deployment of machine learning models in real-world scenarios. However, recent work has shown that OOD detection methods are vulnerable to adversarial attacks, potentially leading to critical failures in high-stakes applications. This discovery has motivated work on robust OOD detection methods that are capable of maintaining performance under various attack settings. Prior approaches have made progress on this problem but face a number of limitations: often only exhibiting robustness to attacks on OOD data or failing to maintain strong clean performance. In this work, we adapt an existing robust classification framework, TRADES, extending it to the problem of robust OOD detection and discovering a novel objective function. Recognising the critical importance of a strong clean/robust trade-off for OOD detection, we introduce an additional loss term which boosts classification and detection performance. Our approach, called HALO (Helper-based AdversariaL OOD detection), surpasses existing methods and achieves state-of-the-art performance across a number of datasets and attack settings. Extensive experiments demonstrate an average AUROC improvement of 3.15 in clean settings and 7.07 under adversarial attacks when compared to the next best method. Furthermore, HALO exhibits resistance to transferred attacks, offers tuneable performance through hyperparameter selection, and is compatible with existing OOD detection frameworks out-of-the-box, leaving open the possibility of future performance gains. Code is available at: https://github.com/hugo0076/HALO

### Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data 
[[arxiv](https://arxiv.org/abs/2502.19752)] [[cool](https://papers.cool/arxiv/2502.19752)] [[pdf](https://arxiv.org/pdf/2502.19752)]
> **Authors**: Pei-Yau Weng,Minh Hoang,Lam M. Nguyen,My T. Thai,Tsui-Wei Weng,Trong Nghia Hoang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at NeurIPS-24
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.

### CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer 
[[arxiv](https://arxiv.org/abs/2502.19750)] [[cool](https://papers.cool/arxiv/2502.19750)] [[pdf](https://arxiv.org/pdf/2502.19750)]
> **Authors**: Yang Liu,Zinan Zheng,Jiashun Cheng,Fugee Tsung,Deli Zhao,Yu Rong,Jia Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Accurate Subseasonal-to-Seasonal (S2S) climate forecasting is pivotal for decision-making including agriculture planning and disaster preparedness but is known to be challenging due to its chaotic nature. Although recent data-driven models have shown promising results, their performance is limited by inadequate consideration of geometric inductive biases. Usually, they treat the spherical weather data as planar images, resulting in an inaccurate representation of locations and spatial relations. In this work, we propose the geometric-inspired Circular Transformer (CirT) to model the cyclic characteristic of the graticule, consisting of two key designs: (1) Decomposing the weather data by latitude into circular patches that serve as input tokens to the Transformer; (2) Leveraging Fourier transform in self-attention to capture the global information and model the spatial periodicity. Extensive experiments on the Earth Reanalysis 5 (ERA5) reanalysis dataset demonstrate our model yields a significant improvement over the advanced data-driven models, including PanguWeather and GraphCast, as well as skillful ECMWF systems. Additionally, we empirically show the effectiveness of our model designs and high-quality prediction over spatial and temporal dimensions.

### BiRating -- Iterative averaging on a bipartite graph of Beat Saber scores, player skills, and map difficulties 
[[arxiv](https://arxiv.org/abs/2502.19742)] [[cool](https://papers.cool/arxiv/2502.19742)] [[pdf](https://arxiv.org/pdf/2502.19742)]
> **Authors**: Juan Casanova
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 30 pages, 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Difficulty estimation of Beat Saber maps is an interesting data analysis problem and valuable to the Beat Saber competitive scene. We present a simple algorithm that iteratively averages player skill and map difficulty estimations in a bipartite graph of players and maps, connected by scores, using scores only as input. This approach simultaneously estimates player skills and map difficulties, exploiting each of them to improve the estimation of the other, exploitng the relation of multiple scores by different players on the same map, or on different maps by the same player. While we have been unable to prove or characterize theoretical convergence, the implementation exhibits convergent behaviour to low estimation error in all instances, producing accurate results. An informal qualitative evaluation involving experienced Beat Saber community members was carried out, comparing the difficulty estimations output by our algorithm with their personal perspectives on the difficulties of different maps. There was a significant alignment with player perceived perceptions of difficulty and with other existing methods for estimating difficulty. Our approach showed significant improvement over existing methods in certain known problematic maps that are not typically accurately estimated, but also produces problematic estimations for certain families of maps where the assumptions on the meaning of scores were inadequate (e.g. not enough scores, or scores over optimized by players). The algorithm has important limitations, related to data quality and meaningfulness, assumptions on the domain problem, and theoretical convergence of the algorithm. Future work would significantly benefit from a better understanding of adequate ways to quantify map difficulty in Beat Saber, including multidimensionality of skill and difficulty, and the systematic biases present in score data.

### Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption 
[[arxiv](https://arxiv.org/abs/2502.19741)] [[cool](https://papers.cool/arxiv/2502.19741)] [[pdf](https://arxiv.org/pdf/2502.19741)]
> **Authors**: Weilin Chen,Ruichu Cai,Jie Qiao,Yuguang Yan,José Miguel Hernández-Lobato
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: arXiv admin note: substantial text overlap with arXiv:2405.03342
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Estimating causal effects under networked interference is a crucial yet challenging problem. Existing methods based on observational data mainly rely on the networked unconfoundedness assumption, which guarantees the identification of networked effects. However, the networked unconfoundedness assumption is usually violated due to the latent confounders in observational data, hindering the identification of networked effects. Interestingly, in such networked settings, interactions between units provide valuable information for recovering latent confounders. In this paper, we identify three types of latent confounders in networked inference that hinder identification: those affecting only the individual, those affecting only neighbors, and those influencing both. Specifically, we devise a networked effect estimator based on identifiable representation learning techniques. Theoretically, we establish the identifiability of all latent confounders, and leveraging the identified latent confounders, we provide the networked effect identification result. Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.

### Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training 
[[arxiv](https://arxiv.org/abs/2502.19726)] [[cool](https://papers.cool/arxiv/2502.19726)] [[pdf](https://arxiv.org/pdf/2502.19726)]
> **Authors**: Toan Tran,Ruixuan Liu,Li Xiong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose a lightweight yet effective empirical privacy defense for protecting training data of language modeling by leveraging the token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\% across various LLM architectures and datasets compared to the baselines.

### Accurate and Scalable Graph Neural Networks via Message Invariance 
[[arxiv](https://arxiv.org/abs/2502.19693)] [[cool](https://papers.cool/arxiv/2502.19693)] [[pdf](https://arxiv.org/pdf/2502.19693)]
> **Authors**: Zhihao Shi,Jie Wang,Zhiwei Zhuang,Xize Liang,Bin Li,Feng Wu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. For a sampled mini-batch of target nodes, the message passing process is divided into two parts: message passing between nodes within the batch (MP-IB) and message passing from nodes outside the batch to those within it (MP-OB). However, MP-OB recursively relies on higher-order out-of-batch neighbors, leading to an exponentially growing computational cost with respect to the number of layers. Due to the neighbor explosion, the whole message passing stores most nodes and edges on the GPU such that many GNNs are infeasible to large-scale graphs. To address this challenge, we propose an accurate and fast mini-batch approach for large graph transductive learning, namely topological compensation (TOP), which obtains the outputs of the whole message passing solely through MP-IB, without the costly MP-OB. The major pillar of TOP is a novel concept of message invariance, which defines message-invariant transformations to convert costly MP-OB into fast MP-IB. This ensures that the modified MP-IB has the same output as the whole message passing. Experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) with limited accuracy degradation.

### The Future Outcome Reasoning and Confidence Assessment Benchmark 
[[arxiv](https://arxiv.org/abs/2502.19676)] [[cool](https://papers.cool/arxiv/2502.19676)] [[pdf](https://arxiv.org/pdf/2502.19676)]
> **Authors**: Zhangdie Yuan,Zifeng Ding,Andreas Vlachos
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.

### Training Robust Graph Neural Networks by Modeling Noise Dependencies 
[[arxiv](https://arxiv.org/abs/2502.19670)] [[cool](https://papers.cool/arxiv/2502.19670)] [[pdf](https://arxiv.org/pdf/2502.19670)]
> **Authors**: Yeonjun In,Kanghoon Yoon,Sukwon Yun,Kibum Kim,Sungchul Kim,Chanyoung Park
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Work in progress
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In real-world applications, node features in graphs often contain noise from various sources, leading to significant performance degradation in GNNs. Although several methods have been developed to enhance robustness, they rely on the unrealistic assumption that noise in node features is independent of the graph structure and node labels, thereby limiting their applicability. To this end, we introduce a more realistic noise scenario, dependency-aware noise on graphs (DANG), where noise in node features create a chain of noise dependencies that propagates to the graph structure and node labels. We propose a novel robust GNN, DA-GNN, which captures the causal relationships among variables in the data generating process (DGP) of DANG using variational inference. In addition, we present new benchmark datasets that simulate DANG in real-world applications, enabling more practical research on robust GNNs. Extensive experiments demonstrate that DA-GNN consistently outperforms existing baselines across various noise scenarios, including both DANG and conventional noise models commonly considered in this field.

### Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization 
[[arxiv](https://arxiv.org/abs/2502.19665)] [[cool](https://papers.cool/arxiv/2502.19665)] [[pdf](https://arxiv.org/pdf/2502.19665)]
> **Authors**: Yuanchao Wang,Zhao-Rong Lai,Tianqi Zhong
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we extend IRM-TV to a Lagrangian multiplier model named OOD-TV-IRM. We find that the autonomous TV penalty hyperparameter is exactly the Lagrangian multiplier. Thus OOD-TV-IRM is essentially a primal-dual optimization model, where the primal optimization minimizes the entire invariant risk and the dual optimization strengthens the TV penalty. The objective is to reach a semi-Nash equilibrium where the balance between the training loss and OOD generalization is maintained. We also develop a convergent primal-dual algorithm that facilitates an adversarial learning scheme. Experimental results show that OOD-TV-IRM outperforms IRM-TV in most situations.

### Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation 
[[arxiv](https://arxiv.org/abs/2502.19657)] [[cool](https://papers.cool/arxiv/2502.19657)] [[pdf](https://arxiv.org/pdf/2502.19657)]
> **Authors**: Pavel Rumiantsev,Mark Coates
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Neural Architecture Search (NAS) is a powerful automatic alternative to manual design of a neural network. In the zero-shot version, a fast ranking function is used to compare architectures without training them. The outputs of the ranking functions often vary significantly due to different sources of randomness, including the evaluated architecture's weights' initialization or the batch of data used for calculations. A common approach to addressing the variation is to average a ranking function output over several evaluations. We propose taking into account the variation in a different manner, by viewing the ranking function output as a random variable representing a proxy performance metric. During the search process, we strive to construct a stochastic ordering of the performance metrics to determine the best architecture. Our experiments show that the proposed stochastic ordering can effectively boost performance of a search on standard benchmark search spaces.

### Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19652)] [[cool](https://papers.cool/arxiv/2502.19652)] [[pdf](https://arxiv.org/pdf/2502.19652)]
> **Authors**: Shangding Gu,Laixi Shi,Muning Wen,Ming Jin,Eric Mazumdar,Yuejie Chi,Adam Wierman,Costas Spanos
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement learning (RL) seeks to improve resilience against the complexity and variability in agent-environment sequential interactions. Despite the existence of a large number of RL benchmarks, there is a lack of standardized benchmarks for robust RL. Current robust RL policies often focus on a specific type of uncertainty and are evaluated in distinct, one-off environments. In this work, we introduce Robust-Gymnasium, a unified modular benchmark designed for robust RL that supports a wide variety of disruptions across all key RL components-agents' observed state and reward, agents' actions, and the environment. Offering over sixty diverse task environments spanning control and robotics, safe RL, and multi-agent RL, it provides an open-source and user-friendly tool for the community to assess current methods and foster the development of robust RL algorithms. In addition, we benchmark existing standard and robust RL algorithms within this framework, uncovering significant deficiencies in each and offering new insights.

### Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph Representation 
[[arxiv](https://arxiv.org/abs/2502.19651)] [[cool](https://papers.cool/arxiv/2502.19651)] [[pdf](https://arxiv.org/pdf/2502.19651)]
> **Authors**: Yuanyuan Xu,Wenjie Zhang,Ying Zhang,Xuemin Lin,Xiwei Xu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal edges alongside rich textual attributes. A prior approach to representing DyTAGs leverages pre-trained language models to encode text attributes and subsequently integrates them into dynamic graph models. However, it follows edge-centric modeling, as in dynamic graph learning, which is limited in local structures and fails to exploit the unique characteristics of DyTAGs, leading to suboptimal performance. We observe that DyTAGs inherently comprise three distinct modalities-temporal, textual, and structural-often exhibiting dispersed or even orthogonal distributions, with the first two largely overlooked in existing research. Building on this insight, we propose MoMent, a model-agnostic multi-modal framework that can seamlessly integrate with dynamic graph models for structural modality learning. The core idea is to shift from edge-centric to node-centric modeling, fully leveraging three modalities for node representation. Specifically, MoMent presents non-shared node-centric encoders based on the attention mechanism to capture global temporal and semantic contexts from temporal and textual modalities, together with local structure learning, thus generating modality-specific tokens. To prevent disjoint latent space, we propose a symmetric alignment loss, an auxiliary objective that aligns temporal and textual tokens, ensuring global temporal-semantic consistency with a theoretical guarantee. Last, we design a lightweight adaptor to fuse these tokens, generating comprehensive and cohesive node representations. We theoretically demonstrate that MoMent enhances discriminative power over exclusive edge-centric modeling. Extensive experiments across seven datasets and two downstream tasks show that MoMent achieves up to 33.62% improvement against the baseline using four dynamic graph models.

### Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.19649)] [[cool](https://papers.cool/arxiv/2502.19649)] [[pdf](https://arxiv.org/pdf/2502.19649)]
> **Authors**: Jan Wehner,Sahar Abdelnabi,Daniel Tan,David Krueger,Mario Fritz
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.

### cMIM: A Contrastive Mutual Information Framework for Unified Generative and Discriminative Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.19642)] [[cool](https://papers.cool/arxiv/2502.19642)] [[pdf](https://arxiv.org/pdf/2502.19642)]
> **Authors**: Micha Livne
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: A working draft
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Learning representations that are useful for unknown downstream tasks is a fundamental challenge in representation learning. Prominent approaches in this domain include contrastive learning, self-supervised masking, and denoising auto-encoders. In this paper, we introduce a novel method, termed contrastive Mutual Information Machine (cMIM), which aims to enhance the utility of learned representations for downstream tasks. cMIM integrates a new contrastive learning loss with the Mutual Information Machine (MIM) learning framework, a probabilistic auto-encoder that maximizes the mutual information between inputs and latent representations while clustering the latent codes. Despite MIM's potential, initial experiments indicated that the representations learned by MIM were less effective for discriminative downstream tasks compared to state-of-the-art (SOTA) models. The proposed cMIM method directly addresses this limitation. The main contributions of this work are twofold: (1) We propose a novel contrastive extension to MIM for learning discriminative representations which eliminates the need for data augmentation and is robust to variations in the number of negative examples (i.e., batch size). (2) We introduce a generic method for extracting informative embeddings from encoder-decoder models, which significantly improves performance in discriminative downstream tasks without requiring additional training. This method is applicable to any pre-trained encoder-decoder model. By presenting cMIM, we aim to offer a unified generative model that is effective for both generative and discriminative tasks. Our results demonstrate that the learned representations are valuable for downstream tasks while maintaining the generative capabilities of MIM.

### Developing robust methods to handle missing data in real-world applications effectively 
[[arxiv](https://arxiv.org/abs/2502.19635)] [[cool](https://papers.cool/arxiv/2502.19635)] [[pdf](https://arxiv.org/pdf/2502.19635)]
> **Authors**: Youran Zhou,Mohamed Reda Bouadjenek,Sunil Arya
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: This work was presented at the ECML PKDD 2024 PhD Forum. https://ecmlpkdd. org/2024/program-accepted-phd-forum/
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Missing data is a pervasive challenge spanning diverse data types, including tabular, sensor data, time-series, images and so on. Its origins are multifaceted, resulting in various missing mechanisms. Prior research in this field has predominantly revolved around the assumption of the Missing Completely At Random (MCAR) mechanism. However, Missing At Random (MAR) and Missing Not At Random (MNAR) mechanisms, though equally prevalent, have often remained underexplored despite their significant influence. This PhD project presents a comprehensive research agenda designed to investigate the implications of diverse missing data mechanisms. The principal aim is to devise robust methodologies capable of effectively handling missing data while accommodating the unique characteristics of MCAR, MAR, and MNAR mechanisms. By addressing these gaps, this research contributes to an enriched understanding of the challenges posed by missing data across various industries and data modalities. It seeks to provide practical solutions that enable the effective management of missing data, empowering researchers and practitioners to leverage incomplete datasets confidently.

### Treatment Non-Adherence Bias in Clinical Machine Learning: A Real-World Study on Hypertension Medication 
[[arxiv](https://arxiv.org/abs/2502.19625)] [[cool](https://papers.cool/arxiv/2502.19625)] [[pdf](https://arxiv.org/pdf/2502.19625)]
> **Authors**: Zhongyuan Liang,Arvind Suresh,Irene Y. Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine learning systems trained on electronic health records (EHRs) increasingly guide treatment decisions, but their reliability depends on the critical assumption that patients follow the prescribed treatments recorded in EHRs. Using EHR data from 3,623 hypertension patients, we investigate how treatment non-adherence introduces implicit bias that can fundamentally distort both causal inference and predictive modeling. By extracting patient adherence information from clinical notes using a large language model, we identify 786 patients (21.7%) with medication non-adherence. We further uncover key demographic and clinical factors associated with non-adherence, as well as patient-reported reasons including side effects and difficulties obtaining refills. Our findings demonstrate that this implicit bias can not only reverse estimated treatment effects, but also degrade model performance by up to 5% while disproportionately affecting vulnerable populations by exacerbating disparities in decision outcomes and model error rates. This highlights the importance of accounting for treatment non-adherence in developing responsible and equitable clinical machine learning systems.

### PRDP: Progressively Refined Differentiable Physics 
[[arxiv](https://arxiv.org/abs/2502.19611)] [[cool](https://papers.cool/arxiv/2502.19611)] [[pdf](https://arxiv.org/pdf/2502.19611)]
> **Authors**: Kanishk Bhatia,Felix Koehler,Nils Thuerey
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The physics solvers employed for neural network training are primarily iterative, and hence, differentiating through them introduces a severe computational burden as iterations grow large. Inspired by works in bilevel optimization, we show that full accuracy of the network is achievable through physics significantly coarser than fully converged solvers. We propose Progressively Refined Differentiable Physics (PRDP), an approach that identifies the level of physics refinement sufficient for full training accuracy. By beginning with coarse physics, adaptively refining it during training, and stopping refinement at the level adequate for training, it enables significant compute savings without sacrificing network accuracy. Our focus is on differentiating iterative linear solvers for sparsely discretized differential operators, which are fundamental to scientific computing. PRDP is applicable to both unrolled and implicit differentiation. We validate its performance on a variety of learning scenarios involving differentiable physics solvers such as inverse problems, autoregressive neural emulators, and correction-based neural-hybrid solvers. In the challenging example of emulating the Navier-Stokes equations, we reduce training time by 62%.

### Learning Ensembles of Interpretable Simple Structure 
[[arxiv](https://arxiv.org/abs/2502.19602)] [[cool](https://papers.cool/arxiv/2502.19602)] [[pdf](https://arxiv.org/pdf/2502.19602)]
> **Authors**: Gaurav Arwade,Sigurdur Olafsson
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Decision-making in complex systems often relies on machine learning models, yet highly accurate models such as XGBoost and neural networks can obscure the reasoning behind their predictions. In operations research applications, understanding how a decision is made is often as crucial as the decision itself. Traditional interpretable models, such as decision trees and logistic regression, provide transparency but may struggle with datasets containing intricate feature interactions. However, complexity in decision-making stem from interactions that are only relevant within certain subsets of data. Within these subsets, feature interactions may be simplified, forming simple structures where simple interpretable models can perform effectively. We propose a bottom-up simple structure-identifying algorithm that partitions data into interpretable subgroups known as simple structure, where feature interactions are minimized, allowing simple models to be trained within each subgroup. We demonstrate the robustness of the algorithm on synthetic data and show that the decision boundaries derived from simple structures are more interpretable and aligned with the intuition of the domain than those learned from a global model. By improving both explainability and predictive accuracy, our approach provides a principled framework for decision support in applications where model transparency is essential.

### Introduction to Sequence Modeling with Transformers 
[[arxiv](https://arxiv.org/abs/2502.19597)] [[cool](https://papers.cool/arxiv/2502.19597)] [[pdf](https://arxiv.org/pdf/2502.19597)]
> **Authors**: Joni-Kristian Kämäräinen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 1 figure
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding the transformer architecture and its workings is essential for machine learning (ML) engineers. However, truly understanding the transformer architecture can be demanding, even if you have a solid background in machine learning or deep learning. The main working horse is attention, which yields to the transformer encoder-decoder structure. However, putting attention aside leaves several programming components that are easy to implement but whose role for the whole is unclear. These components are 'tokenization', 'embedding' ('un-embedding'), 'masking', 'positional encoding', and 'padding'. The focus of this work is on understanding them. To keep things simple, the understanding is built incrementally by adding components one by one, and after each step investigating what is doable and what is undoable with the current model. Simple sequences of zeros (0) and ones (1) are used to study the workings of each step.

### Improving Representation Learning of Complex Critical Care Data with ICU-BERT 
[[arxiv](https://arxiv.org/abs/2502.19593)] [[cool](https://papers.cool/arxiv/2502.19593)] [[pdf](https://arxiv.org/pdf/2502.19593)]
> **Authors**: Ricardo Santos,André V. Carreiro,Xi Peng,Hugo Gamboa,Holger Fröhlich
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted for poster at GenAI4Health Workshop at AAAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The multivariate, asynchronous nature of real-world clinical data, such as that generated in Intensive Care Units (ICUs), challenges traditional AI-based decision-support systems. These often assume data regularity and feature independence and frequently rely on limited data scopes and manual feature engineering. The potential of generative AI technologies has not yet been fully exploited to analyze clinical data. We introduce ICU-BERT, a transformer-based model pre-trained on the MIMIC-IV database using a multi-task scheme to learn robust representations of complex ICU data with minimal preprocessing. ICU-BERT employs a multi-token input strategy, incorporating dense embeddings from a biomedical Large Language Model to learn a generalizable representation of complex and multivariate ICU data. With an initial evaluation of five tasks and four additional ICU datasets, ICU-BERT results indicate that ICU-BERT either compares to or surpasses current performance benchmarks by leveraging fine-tuning. By integrating structured and unstructured data, ICU-BERT advances the use of foundational models in medical informatics, offering an adaptable solution for clinical decision support across diverse applications.

### LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training via Efficient Zeroth-Order Adaptive SAM 
[[arxiv](https://arxiv.org/abs/2502.19571)] [[cool](https://papers.cool/arxiv/2502.19571)] [[pdf](https://arxiv.org/pdf/2502.19571)]
> **Authors**: Yehonathan Refael,Iftach Arbel,Ofir Lindenbaum,Tom Tirer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: We study robust parameter-efficient fine-tuning (PEFT) techniques designed to improve accuracy and generalization while operating within strict computational and memory hardware constraints, specifically focusing on large-language models (LLMs). Existing PEFT methods often lack robustness and fail to generalize effectively across diverse tasks, leading to suboptimal performance in real-world scenarios. To address this, we present a new highly computationally efficient framework called AdaZo-SAM, combining Adam and Sharpness-Aware Minimization (SAM) while requiring only a single-gradient computation in every iteration. This is achieved using a stochastic zeroth-order estimation to find SAM's ascent perturbation. We provide a convergence guarantee for AdaZo-SAM and show that it improves the generalization ability of state-of-the-art PEFT methods. Additionally, we design a low-rank gradient optimization method named LORENZA, which is a memory-efficient version of AdaZo-SAM. LORENZA utilizes a randomized SVD scheme to efficiently compute the subspace projection matrix and apply optimization steps onto the selected subspace. This technique enables full-parameter fine-tuning with adaptive low-rank gradient updates, achieving the same reduced memory consumption as gradient-low-rank-projection methods. We provide a convergence analysis of LORENZA and demonstrate its merits for pre-training and fine-tuning LLMs.

### PhenoProfiler: Advancing Phenotypic Learning for Image-based Drug Discovery 
[[arxiv](https://arxiv.org/abs/2502.19568)] [[cool](https://papers.cool/arxiv/2502.19568)] [[pdf](https://arxiv.org/pdf/2502.19568)]
> **Authors**: Bo Li,Bob Zhang,Chengyang Zhang,Minghao Zhou,Weiliang Huang,Shihang Wang,Qing Wang,Mengran Li,Yong Zhang,Qianqian Song
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,图像和视频处理
- **Abstract**: In the field of image-based drug discovery, capturing the phenotypic response of cells to various drug treatments and perturbations is a crucial step. However, existing methods require computationally extensive and complex multi-step procedures, which can introduce inefficiencies, limit generalizability, and increase potential errors. To address these challenges, we present PhenoProfiler, an innovative model designed to efficiently and effectively extract morphological representations, enabling the elucidation of phenotypic changes induced by treatments. PhenoProfiler is designed as an end-to-end tool that processes whole-slide multi-channel images directly into low-dimensional quantitative representations, eliminating the extensive computational steps required by existing methods. It also includes a multi-objective learning module to enhance robustness, accuracy, and generalization in morphological representation learning. PhenoProfiler is rigorously evaluated on large-scale publicly available datasets, including over 230,000 whole-slide multi-channel images in end-to-end scenarios and more than 8.42 million single-cell images in non-end-to-end settings. Across these benchmarks, PhenoProfiler consistently outperforms state-of-the-art methods by up to 20%, demonstrating substantial improvements in both accuracy and robustness. Furthermore, PhenoProfiler uses a tailored phenotype correction strategy to emphasize relative phenotypic changes under treatments, facilitating the detection of biologically meaningful signals. UMAP visualizations of treatment profiles demonstrate PhenoProfiler ability to effectively cluster treatments with similar biological annotations, thereby enhancing interpretability. These findings establish PhenoProfiler as a scalable, generalizable, and robust tool for phenotypic learning.

### Extremely Greedy Equivalence Search 
[[arxiv](https://arxiv.org/abs/2502.19551)] [[cool](https://papers.cool/arxiv/2502.19551)] [[pdf](https://arxiv.org/pdf/2502.19551)]
> **Authors**: Achille Nazaret,David Blei
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ef:40th Conference on Uncertainty in Artificial Intelligence, 2024
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The goal of causal discovery is to learn a directed acyclic graph from data. One of the most well-known methods for this problem is Greedy Equivalence Search (GES). GES searches for the graph by incrementally and greedily adding or removing edges to maximize a model selection criterion. It has strong theoretical guarantees on infinite data but can fail in practice on finite data. In this paper, we first identify some of the causes of GES's failure, finding that it can get blocked in local optima, especially in denser graphs. We then propose eXtremely Greedy Equivalent Search (XGES), which involves a new heuristic to improve the search strategy of GES while retaining its theoretical guarantees. In particular, XGES favors deleting edges early in the search over inserting edges, which reduces the possibility of the search ending in local optima. A further contribution of this work is an efficient algorithmic formulation of XGES (and GES). We benchmark XGES on simulated datasets with known ground truth. We find that XGES consistently outperforms GES in recovering the correct graphs, and it is 10 times faster. XGES implementations in Python and C++ are available at https://github.com/ANazaret/XGES.

### Generalist World Model Pre-Training for Efficient Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19544)] [[cool](https://papers.cool/arxiv/2502.19544)] [[pdf](https://arxiv.org/pdf/2502.19544)]
> **Authors**: Yi Zhao,Aidan Scannell,Yuxin Hou,Tianyu Cui,Le Chen,Dieter Büchler,Arno Solin,Juho Kannala,Joni Pajarinen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Sample-efficient robot learning is a longstanding goal in robotics. Inspired by the success of scaling in vision and language, the robotics community is now investigating large-scale offline datasets for robot learning. However, existing methods often require expert and/or reward-labeled task-specific data, which can be costly and limit their application in practice. In this paper, we consider a more realistic setting where the offline data consists of reward-free and non-expert multi-embodiment offline data. We show that generalist world model pre-training (WPT), together with retrieval-based experience rehearsal and execution guidance, enables efficient reinforcement learning (RL) and fast task adaptation with such non-curated data. In experiments over 72 visuomotor tasks, spanning 6 different embodiments, covering hard exploration, complex dynamics, and various visual properties, WPT achieves 35.65% and 35% higher aggregated score compared to widely used learning-from-scratch baselines, respectively.

### High-fidelity Multiphysics Modelling for Rapid Predictions Using Physics-informed Parallel Neural Operator 
[[arxiv](https://arxiv.org/abs/2502.19543)] [[cool](https://papers.cool/arxiv/2502.19543)] [[pdf](https://arxiv.org/pdf/2502.19543)]
> **Authors**: Biao Yuan,He Wang,Yanjie Song,Ana Heitor,Xiaohui Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 11 figures, 1 table, 36 equations
- **标题**: None
- **领域**: 机器学习,数值分析,计算物理
- **Abstract**: Modelling complex multiphysics systems governed by nonlinear and strongly coupled partial differential equations (PDEs) is a cornerstone in computational science and engineering. However, it remains a formidable challenge for traditional numerical solvers due to high computational cost, making them impractical for large-scale applications. Neural operators' reliance on data-driven training limits their applicability in real-world scenarios, as data is often scarce or expensive to obtain. Here, we propose a novel paradigm, physics-informed parallel neural operator (PIPNO), a scalable and unsupervised learning framework that enables data-free PDE modelling by leveraging only governing physical laws. The parallel kernel integration design, incorporating ensemble learning, significantly enhances both compatibility and computational efficiency, enabling scalable operator learning for nonlinear and strongly coupled PDEs. PIPNO efficiently captures nonlinear operator mappings across diverse physics, including geotechnical engineering, material science, electromagnetism, quantum mechanics, and fluid dynamics. The proposed method achieves high-fidelity and rapid predictions, outperforming existing operator learning approaches in modelling nonlinear and strongly coupled multiphysics systems. Therefore, PIPNO offers a powerful alternative to conventional solvers, broadening the applicability of neural operators for multiphysics modelling while ensuring efficiency, robustness, and scalability.

### Retrieval Augmented Anomaly Detection (RAAD): Nimble Model Adjustment Without Retraining 
[[arxiv](https://arxiv.org/abs/2502.19534)] [[cool](https://papers.cool/arxiv/2502.19534)] [[pdf](https://arxiv.org/pdf/2502.19534)]
> **Authors**: Sam Pastoriza,Iman Yousfi,Christopher Redino,Marc Vucovich,Abdul Rahman,Sal Aguinaga,Dhruv Nandakumar
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 6 pages, 3 figures. 2 tables, accepted at ISDFS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: We propose a novel mechanism for real-time (human-in-the-loop) feedback focused on false positive reduction to enhance anomaly detection models. It was designed for the lightweight deployment of a behavioral network anomaly detection model. This methodology is easily integrable to similar domains that require a premium on throughput while maintaining high precision. In this paper, we introduce Retrieval Augmented Anomaly Detection, a novel method taking inspiration from Retrieval Augmented Generation. Human annotated examples are sent to a vector store, which can modify model outputs on the very next processed batch for model inference. To demonstrate the generalization of this technique, we benchmarked several different model architectures and multiple data modalities, including images, text, and graph-based data.

### Analyzing Cost-Sensitive Surrogate Losses via $\mathcal{H}$-calibration 
[[arxiv](https://arxiv.org/abs/2502.19522)] [[cool](https://papers.cool/arxiv/2502.19522)] [[pdf](https://arxiv.org/pdf/2502.19522)]
> **Authors**: Sanket Shah,Milind Tambe,Jessie Finocchiaro
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper aims to understand whether machine learning models should be trained using cost-sensitive surrogates or cost-agnostic ones (e.g., cross-entropy). Analyzing this question through the lens of $\mathcal{H}$-calibration, we find that cost-sensitive surrogates can strictly outperform their cost-agnostic counterparts when learning small models under common distributional assumptions. Since these distributional assumptions are hard to verify in practice, we also show that cost-sensitive surrogates consistently outperform cost-agnostic surrogates on classification datasets from the UCI repository. Together, these make a strong case for using cost-sensitive surrogates in practice.

### Mixtraining: A Better Trade-Off Between Compute and Performance 
[[arxiv](https://arxiv.org/abs/2502.19513)] [[cool](https://papers.cool/arxiv/2502.19513)] [[pdf](https://arxiv.org/pdf/2502.19513)]
> **Authors**: Zexin Li,Jiancheng Zhang,Yinglun Zhu,Cong Liu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Incorporating self-supervised learning (SSL) before standard supervised learning (SL) has become a widely used strategy to enhance model performance, particularly in data-limited scenarios. However, this approach introduces a trade-off between computation and performance: while SSL helps with representation learning, it requires a separate, often time-consuming training phase, increasing computational overhead and limiting efficiency in resource-constrained settings. To address these challenges, we propose MixTraining, a novel framework that interleaves several SSL and SL epochs within a unified mixtraining training phase, featuring a smooth transition between two learning objectives. MixTraining enhances synergy between SSL and SL for improved accuracy and consolidates shared computation steps to reduce computation overhead. MixTraining is versatile and applicable to both single-task and multi-task learning scenarios. Extensive experiments demonstrate that MixTraining offers a superior compute-performance trade-off compared to conventional pipelines, achieving an 8.81% absolute accuracy gain (18.89% relative accuracy gain) on the TinyImageNet dataset while accelerating training by up to 1.29x with the ViT-Tiny model.

### TRIX: A More Expressive Model for Zero-shot Domain Transfer in Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.19512)] [[cool](https://papers.cool/arxiv/2502.19512)] [[pdf](https://arxiv.org/pdf/2502.19512)]
> **Authors**: Yucheng Zhang,Beatrice Bevilacqua,Mikhail Galkin,Bruno Ribeiro
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Fully inductive knowledge graph models can be trained on multiple domains and subsequently perform zero-shot knowledge graph completion (KGC) in new unseen domains. This is an important capability towards the goal of having foundation models for knowledge graphs. In this work, we introduce a more expressive and capable fully inductive model, dubbed TRIX, which not only yields strictly more expressive triplet embeddings (head entity, relation, tail entity) compared to state-of-the-art methods, but also introduces a new capability: directly handling both entity and relation prediction tasks in inductive settings. Empirically, we show that TRIX outperforms the state-of-the-art fully inductive models in zero-shot entity and relation predictions in new domains, and outperforms large-context LLMs in out-of-domain predictions. The source code is available at https://github.com/yuchengz99/TRIX.

### Models That Are Interpretable But Not Transparent 
[[arxiv](https://arxiv.org/abs/2502.19502)] [[cool](https://papers.cool/arxiv/2502.19502)] [[pdf](https://arxiv.org/pdf/2502.19502)]
> **Authors**: Chudi Zhong,Panyu Chen,Cynthia Rudin
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: AISTATS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Faithful explanations are essential for machine learning models in high-stakes applications. Inherently interpretable models are well-suited for these applications because they naturally provide faithful explanations by revealing their decision logic. However, model designers often need to keep these models proprietary to maintain their value. This creates a tension: we need models that are interpretable--allowing human decision-makers to understand and justify predictions, but not transparent, so that the model's decision boundary is not easily replicated by attackers. Shielding the model's decision boundary is particularly challenging alongside the requirement of completely faithful explanations, since such explanations reveal the true logic of the model for an entire subspace around each query point. This work provides an approach, FaithfulDefense, that creates model explanations for logical models that are completely faithful, yet reveal as little as possible about the decision boundary. FaithfulDefense is based on a maximum set cover formulation, and we provide multiple formulations for it, taking advantage of submodularity.

### On the Interpolation Effect of Score Smoothing 
[[arxiv](https://arxiv.org/abs/2502.19499)] [[cool](https://papers.cool/arxiv/2502.19499)] [[pdf](https://arxiv.org/pdf/2502.19499)]
> **Authors**: Zhengdao Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Score-based diffusion models have achieved remarkable progress in various domains with the ability to generate new data samples that do not exist in the training set. In this work, we examine the hypothesis that their generalization ability arises from an interpolation effect caused by a smoothing of the empirical score function. Focusing on settings where the training set lies uniformly in a one-dimensional linear subspace, we study the interplay between score smoothing and the denoising dynamics with mathematically solvable models. In particular, we demonstrate how a smoothed score function can lead to the generation of samples that interpolate among the training data within their subspace while avoiding full memorization. We also present evidence that learning score functions with regularized neural networks can have a similar effect on the denoising dynamics as score smoothing.

### Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation 
[[arxiv](https://arxiv.org/abs/2502.19414)] [[cool](https://papers.cool/arxiv/2502.19414)] [[pdf](https://arxiv.org/pdf/2502.19414)]
> **Authors**: Shiven Sinha,Shashwat Goel,Ponnurangam Kumaraguru,Jonas Geiping,Matthias Bethge,Ameya Prabhu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Technical Report
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Yet current benchmarks for LMs predominantly assess their ability to generate solutions rather than challenge them. We advocate for developing benchmarks that evaluate this inverse capability - creating counterexamples for subtly incorrect solutions. To demonstrate this approach, we start with the domain of algorithmic problem solving, where counterexamples can be evaluated automatically using code execution. Specifically, we introduce REFUTE, a dynamically updating benchmark that includes recent problems and incorrect submissions from programming competitions, where human experts successfully identified counterexamples. Our analysis finds that the best reasoning agents, even OpenAI o3-mini (high) with code execution feedback, can create counterexamples for only <9% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to 48% of these problems from scratch. We hope our work spurs progress in evaluating and enhancing LMs' ability to falsify incorrect solutions - a capability that is crucial for both accelerating research and making models self-improve through reliable reflective reasoning.

### Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs 
[[arxiv](https://arxiv.org/abs/2502.19413)] [[cool](https://papers.cool/arxiv/2502.19413)] [[pdf](https://arxiv.org/pdf/2502.19413)]
> **Authors**: Christoph Schuhmann,Gollam Rabby,Ameya Prabhu,Tawsif Ahmed,Andreas Hochlehnert,Huu Nguyen,Nick Akinci Heidrich,Ludwig Schmidt,Robert Kaczmarczyk,Sören Auer,Jenia Jitsev,Matthias Bethge
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Technical Report
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.

### Verde: Verification via Refereed Delegation for Machine Learning Programs 
[[arxiv](https://arxiv.org/abs/2502.19405)] [[cool](https://papers.cool/arxiv/2502.19405)] [[pdf](https://arxiv.org/pdf/2502.19405)]
> **Authors**: Arasu Arun,Adam St. Arnaud,Alexey Titov,Brian Wilcox,Viktor Kolobaric,Marc Brinkmann,Oguzhan Ersoy,Ben Fielding,Joseph Bonneau
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine learning programs, such as those performing inference, fine-tuning, and training of LLMs, are commonly delegated to untrusted compute providers. To provide correctness guarantees for the client, we propose adapting the cryptographic notion of refereed delegation to the machine learning setting. This approach enables a computationally limited client to delegate a program to multiple untrusted compute providers, with a guarantee of obtaining the correct result if at least one of them is honest. Refereed delegation of ML programs poses two technical hurdles: (1) an arbitration protocol to resolve disputes when compute providers disagree on the output, and (2) the ability to bitwise reproduce ML programs across different hardware setups, For (1), we design Verde, a dispute arbitration protocol that efficiently handles the large scale and graph-based computational model of modern ML programs. For (2), we build RepOps (Reproducible Operators), a library that eliminates hardware "non-determinism" by controlling the order of floating point operations performed on all hardware. Our implementation shows that refereed delegation achieves both strong guarantees for clients and practical overheads for compute providers.

### General Reasoning Requires Learning to Reason from the Get-go 
[[arxiv](https://arxiv.org/abs/2502.19402)] [[cool](https://papers.cool/arxiv/2502.19402)] [[pdf](https://arxiv.org/pdf/2502.19402)]
> **Authors**: Seungwook Han,Jyothish Pari,Samuel J. Gershman,Pulkit Agrawal
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 11 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs. To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a \textit{reasoning prior} for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.

### Residual Speech Embeddings for Tone Classification: Removing Linguistic Content to Enhance Paralinguistic Analysis 
[[arxiv](https://arxiv.org/abs/2502.19387)] [[cool](https://papers.cool/arxiv/2502.19387)] [[pdf](https://arxiv.org/pdf/2502.19387)]
> **Authors**: Hamdan Al Ahbabi,Gautier Marti,Saeed AlMarri,Ibrahim Elfadel
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Self-supervised learning models for speech processing, such as wav2vec2, HuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic and paralinguistic information, making it challenging to analyze tone independently of spoken content. In this work, we introduce a method for disentangling paralinguistic features from linguistic content by regressing speech embeddings onto their corresponding text embeddings and using the residuals as a representation of vocal tone. We evaluate this approach across multiple self-supervised speech embeddings, demonstrating that residual embeddings significantly improve tone classification performance compared to raw speech embeddings. Our results show that this method enhances linear separability, enabling improved classification even with simple models such as logistic regression. Visualization of the residual embeddings further confirms the successful removal of linguistic information while preserving tone-related features. These findings highlight the potential of residual embeddings for applications in sentiment analysis, speaker characterization, and paralinguistic speech processing.

### Efficient 4D fMRI ASD Classification using Spatial-Temporal-Omics-based Learning Framework 
[[arxiv](https://arxiv.org/abs/2502.19386)] [[cool](https://papers.cool/arxiv/2502.19386)] [[pdf](https://arxiv.org/pdf/2502.19386)]
> **Authors**: Ziqiao Weng,Weidong Cai,Bo Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at 2025 IEEE International Symposium on Biomedical Imaging (ISBI)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder impacting social and behavioral development. Resting-state fMRI, a non-invasive tool for capturing brain connectivity patterns, aids in early ASD diagnosis and differentiation from typical controls (TC). However, previous methods, which rely on either mean time series or full 4D data, are limited by a lack of spatial information or by high computational costs. This underscores the need for an efficient solution that preserves both spatial and temporal information. In this paper, we propose a novel, simple, and efficient spatial-temporal-omics learning framework designed to efficiently extract spatio-temporal features from fMRI for ASD classification. Our approach addresses these limitations by utilizing 3D time-domain derivatives as the spatial-temporal inter-voxel omics, which preserve full spatial resolution while capturing diverse statistical characteristics of the time series at each voxel. Meanwhile, functional connectivity features serve as the spatial-temporal inter-regional omics, capturing correlations across brain regions. Extensive experiments and ablation studies on the ABIDE dataset demonstrate that our framework significantly outperforms previous methods while maintaining computational efficiency. We believe our research offers valuable insights that will inform and advance future ASD studies, particularly in the realm of spatial-temporal-omics-based learning.

### HDEE: Heterogeneous Domain Expert Ensemble 
[[arxiv](https://arxiv.org/abs/2502.19385)] [[cool](https://papers.cool/arxiv/2502.19385)] [[pdf](https://arxiv.org/pdf/2502.19385)]
> **Authors**: Oğuzhan Ersoy,Jari Kolehmainen,Gabriel Passamani Andrade
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Training dense LLMs requires enormous amounts of data and centralized compute, which introduces fundamental bottlenecks and ever-growing costs for large models. Several studies aim to reduce this dependency on centralization by reducing the communication overhead of training dense models. Taking this idea of reducing communication overhead to a natural extreme, by training embarrassingly parallelizable ensembles of small independent experts, has been shown to outperform large dense models trained in traditional centralized settings. However, existing studies do not take into account underlying differences amongst data domains and treat them as monolithic, regardless of their underlying complexity, size, or distribution. In this paper, we explore the effects of introducing heterogeneity to these ensembles of domain expert models. Specifically, by allowing models within the ensemble to vary in size--as well as the number of training steps taken depending on the training data's domain--we study the effect heterogeneity has on these ensembles when evaluated against domains included in, and excluded from, the training set. We use the same compute budget to train heterogeneous ensembles and homogeneous baselines for comparison. We show that the heterogeneous ensembles achieve the lowest perplexity scores in $20$ out of the $21$ data domains used in the evaluation. Our code is available at https://github.com/gensyn-ai/hdee.

### Preference-Based Gradient Estimation for ML-Based Approximate Combinatorial Optimization 
[[arxiv](https://arxiv.org/abs/2502.19377)] [[cool](https://papers.cool/arxiv/2502.19377)] [[pdf](https://arxiv.org/pdf/2502.19377)]
> **Authors**: Arman Mielke,Uwe Bauknecht,Thilo Strauss,Mathias Niepert
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Preliminary work, under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Combinatorial optimization (CO) problems arise in a wide range of fields from medicine to logistics and manufacturing. While exact solutions are often not necessary, many applications require finding high-quality solutions quickly. For this purpose, we propose a data-driven approach to improve existing non-learned approximation algorithms for CO. We parameterize the approximation algorithm and train a graph neural network (GNN) to predict parameter values that lead to the best possible solutions. Our pipeline is trained end-to-end in a self-supervised fashion using gradient estimation, treating the approximation algorithm as a black box. We propose a novel gradient estimation scheme for this purpose, which we call preference-based gradient estimation. Our approach combines the benefits of the neural network and the non-learned approximation algorithm: The GNN leverages the information from the dataset to allow the approximation algorithm to find better solutions, while the approximation algorithm guarantees that the solution is feasible. We validate our approach on two well-known combinatorial optimization problems, the travelling salesman problem and the minimum k-cut problem, and show that our method is competitive with state of the art learned CO solvers.

### dCMF: Learning interpretable evolving patterns from temporal multiway data 
[[arxiv](https://arxiv.org/abs/2502.19367)] [[cool](https://papers.cool/arxiv/2502.19367)] [[pdf](https://arxiv.org/pdf/2502.19367)]
> **Authors**: Christos Chatzis,Carla Schenker,Jérémy E. Cohen,Evrim Acar
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Multiway datasets are commonly analyzed using unsupervised matrix and tensor factorization methods to reveal underlying patterns. Frequently, such datasets include timestamps and could correspond to, for example, health-related measurements of subjects collected over time. The temporal dimension is inherently different from the other dimensions, requiring methods that account for its intrinsic properties. Linear Dynamical Systems (LDS) are specifically designed to capture sequential dependencies in the observed data. In this work, we bridge the gap between tensor factorizations and dynamical modeling by exploring the relationship between LDS, Coupled Matrix Factorizations (CMF) and the PARAFAC2 model. We propose a time-aware coupled factorization model called d(ynamical)CMF that constrains the temporal evolution of the latent factors to adhere to a specific LDS structure. Using synthetic datasets, we compare the performance of dCMF with PARAFAC2 and t(emporal)PARAFAC2 which incorporates temporal smoothness. Our results show that dCMF and PARAFAC2-based approaches perform similarly when capturing smoothly evolving patterns that adhere to the PARAFAC2 structure. However, dCMF outperforms alternatives when the patterns evolve smoothly but deviate from the PARAFAC2 structure. Furthermore, we demonstrate that the proposed dCMF method enables to capture more complex dynamics when additional prior information about the temporal evolution is incorporated.

### Deep Learning For Time Series Analysis With Application On Human Motion 
[[arxiv](https://arxiv.org/abs/2502.19364)] [[cool](https://papers.cool/arxiv/2502.19364)] [[pdf](https://arxiv.org/pdf/2502.19364)]
> **Authors**: Ali Ismail-Fawaz
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series data, defined by equally spaced points over time, is essential in fields like medicine, telecommunications, and energy. Analyzing it involves tasks such as classification, clustering, prototyping, and regression. Classification identifies normal vs. abnormal movements in skeleton-based motion sequences, clustering detects stock market behavior patterns, prototyping expands physical therapy datasets, and regression predicts patient recovery. Deep learning has recently gained traction in time series analysis due to its success in other domains. This thesis leverages deep learning to enhance classification with feature engineering, introduce foundation models, and develop a compact yet state-of-the-art architecture. We also address limited labeled data with self-supervised learning. Our contributions apply to real-world tasks, including human motion analysis for action recognition and rehabilitation. We introduce a generative model for human motion data, valuable for cinematic production and gaming. For prototyping, we propose a shape-based synthetic sample generation method to support regression models when data is scarce. Lastly, we critically evaluate discriminative and generative models, identifying limitations in current methodologies and advocating for a robust, standardized evaluation framework. Our experiments on public datasets provide novel insights and methodologies, advancing time series analysis with practical applications.

### Physics-Based Hybrid Machine Learning for Critical Heat Flux Prediction with Uncertainty Quantification 
[[arxiv](https://arxiv.org/abs/2502.19357)] [[cool](https://papers.cool/arxiv/2502.19357)] [[pdf](https://arxiv.org/pdf/2502.19357)]
> **Authors**: Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Submitted to the International Journal of Heat and Mass Transfer
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Critical heat flux is a key quantity in boiling system modeling due to its impact on heat transfer and component temperature and performance. This study investigates the development and validation of an uncertainty-aware hybrid modeling approach that combines machine learning with physics-based models in the prediction of critical heat flux in nuclear reactors for cases of dryout. Two empirical correlations, Biasi and Bowring, were employed with three machine learning uncertainty quantification techniques: deep neural network ensembles, Bayesian neural networks, and deep Gaussian processes. A pure machine learning model without a base model served as a baseline for comparison. This study examines the performance and uncertainty of the models under both plentiful and limited training data scenarios using parity plots, uncertainty distributions, and calibration curves. The results indicate that the Biasi hybrid deep neural network ensemble achieved the most favorable performance (with a mean absolute relative error of 1.846% and stable uncertainty estimates), particularly in the plentiful data scenario. The Bayesian neural network models showed slightly higher error and uncertainty but superior calibration. By contrast, deep Gaussian process models underperformed by most metrics. All hybrid models outperformed pure machine learning configurations, demonstrating resistance against data scarcity.

### Recurrent Auto-Encoders for Enhanced Deep Reinforcement Learning in Wilderness Search and Rescue Planning 
[[arxiv](https://arxiv.org/abs/2502.19356)] [[cool](https://papers.cool/arxiv/2502.19356)] [[pdf](https://arxiv.org/pdf/2502.19356)]
> **Authors**: Jan-Hendrik Ewers,David Anderson,Douglas Thomson
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Submitted toMachineLearningwith Applications
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: Wilderness search and rescue operations are often carried out over vast landscapes. The search efforts, however, must be undertaken in minimum time to maximize the chance of survival of the victim. Whilst the advent of cheap multicopters in recent years has changed the way search operations are handled, it has not solved the challenges of the massive areas at hand. The problem therefore is not one of complete coverage, but one of maximizing the information gathered in the limited time available. In this work we propose that a combination of a recurrent autoencoder and deep reinforcement learning is a more efficient solution to the search problem than previous pure deep reinforcement learning or optimisation approaches. The autoencoder training paradigm efficiently maximizes the information throughput of the encoder into its latent space representation which deep reinforcement learning is primed to leverage. Without the overhead of independently solving the problem that the recurrent autoencoder is designed for, it is more efficient in learning the control task. We further implement three additional architectures for a comprehensive comparison of the main proposed architecture. Similarly, we apply both soft actor-critic and proximal policy optimisation to provide an insight into the performance of both in a highly non-linear and complex application with a large observation Results show that the proposed architecture is vastly superior to the benchmarks, with soft actor-critic achieving the best performance. This model further outperformed work from the literature whilst having below a fifth of the total learnable parameters and training in a quarter of the time.

### CryptoPulse: Short-Term Cryptocurrency Forecasting with Dual-Prediction and Cross-Correlated Market Indicators 
[[arxiv](https://arxiv.org/abs/2502.19349)] [[cool](https://papers.cool/arxiv/2502.19349)] [[pdf](https://arxiv.org/pdf/2502.19349)]
> **Authors**: Amit Kumar,Taoran Ji
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10
- **标题**: None
- **领域**: 机器学习,证券定价
- **Abstract**: Cryptocurrencies fluctuate in markets with high price volatility, posing significant challenges for investors. To aid in informed decision-making, systems predicting cryptocurrency market movements have been developed, typically focusing on historical patterns. However, these methods often overlook three critical factors influencing market dynamics: 1) the macro investing environment, reflected in major cryptocurrency fluctuations affecting collaborative investor behaviors; 2) overall market sentiment, heavily influenced by news impacting investor strategies; and 3) technical indicators, offering insights into overbought or oversold conditions, momentum, and market trends, which are crucial for short-term price movements. This paper proposes a dual prediction mechanism that forecasts the next day's closing price by incorporating macroeconomic fluctuations, technical indicators, and individual cryptocurrency price changes. Additionally, a novel refinement mechanism enhances predictions through market sentiment-based rescaling and fusion. Experiments demonstrate that the proposed model achieves state-of-the-art performance, consistently outperforming ten comparison methods.

### Consistent Amortized Clustering via Generative Flow Networks 
[[arxiv](https://arxiv.org/abs/2502.19337)] [[cool](https://papers.cool/arxiv/2502.19337)] [[pdf](https://arxiv.org/pdf/2502.19337)]
> **Authors**: Irit Chelly,Roy Uziel,Oren Freifeld,Ari Pakman
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to AISTATS 2025 on January 21, 2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov chain runs and the need for explicit data likelihoods. Existing methods which label each data point sequentially, like the Neural Clustering Process, often lead to cluster assignments highly dependent on the data order. Alternatively, methods that sequentially create full clusters, do not provide assignment probabilities. In this paper, we introduce GFNCP, a novel framework for amortized clustering. GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward. We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.

### I Know What I Don't Know: Improving Model Cascades Through Confidence Tuning 
[[arxiv](https://arxiv.org/abs/2502.19335)] [[cool](https://papers.cool/arxiv/2502.19335)] [[pdf](https://arxiv.org/pdf/2502.19335)]
> **Authors**: Stephan Rabanser,Nathalie Rauschmayr,Achin Kulshrestha,Petra Poklukar,Wittawat Jitkrittum,Sean Augenstein,Congchao Wang,Federico Tombari
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large-scale machine learning models deliver strong performance across a wide range of tasks but come with significant computational and resource constraints. To mitigate these challenges, local smaller models are often deployed alongside larger models, relying on routing and deferral mechanisms to offload complex tasks. However, existing approaches inadequately balance the capabilities of these models, often resulting in unnecessary deferrals or sub-optimal resource usage. In this work we introduce a novel loss function called Gatekeeper for calibrating smaller models in cascade setups. Our approach fine-tunes the smaller model to confidently handle tasks it can perform correctly while deferring complex tasks to the larger model. Moreover, it incorporates a mechanism for managing the trade-off between model performance and deferral accuracy, and is broadly applicable across various tasks and domains without any architectural changes. We evaluate our method on encoder-only, decoder-only, and encoder-decoder architectures. Experiments across image classification, language modeling, and vision-language tasks show that our approach substantially improves deferral performance.

### Partition Tree Weighting for Non-Stationary Stochastic Bandits 
[[arxiv](https://arxiv.org/abs/2502.19325)] [[cool](https://papers.cool/arxiv/2502.19325)] [[pdf](https://arxiv.org/pdf/2502.19325)]
> **Authors**: Joel Veness,Marcus Hutter,Andras Gyorgy,Jordi Grau-Moya
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper considers a generalisation of universal source coding for interaction data, namely data streams that have actions interleaved with observations. Our goal will be to construct a coding distribution that is both universal \emph{and} can be used as a control policy. Allowing for action generation needs careful treatment, as naive approaches which do not distinguish between actions and observations run into the self-delusion problem in universal settings. We showcase our perspective in the context of the challenging non-stationary stochastic Bernoulli bandit problem. Our main contribution is an efficient and high performing algorithm for this problem that generalises the Partition Tree Weighting universal source coding technique for passive prediction to the control setting.

### FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users 
[[arxiv](https://arxiv.org/abs/2502.19312)] [[cool](https://papers.cool/arxiv/2502.19312)] [[pdf](https://arxiv.org/pdf/2502.19312)]
> **Authors**: Anikait Singh,Sheryl Hsu,Kyle Hsu,Eric Mitchell,Stefano Ermon,Tatsunori Hashimoto,Archit Sharma,Chelsea Finn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Website: https://fewshot-preference-optimization.github.io/
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,人机交互,机器学习
- **Abstract**: Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.

### Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency 
[[arxiv](https://arxiv.org/abs/2502.19307)] [[cool](https://papers.cool/arxiv/2502.19307)] [[pdf](https://arxiv.org/pdf/2502.19307)]
> **Authors**: Michael Somma,Thomas Gallien,Branka Stojanovic
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection.

### Corporate Fraud Detection in Rich-yet-Noisy Financial Graph 
[[arxiv](https://arxiv.org/abs/2502.19305)] [[cool](https://papers.cool/arxiv/2502.19305)] [[pdf](https://arxiv.org/pdf/2502.19305)]
> **Authors**: Shiqi Wang,Zhibo Zhang,Libing Fang,Cam-Tu Nguyen,Wenzhon Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,风险管理,统计金融
- **Abstract**: Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.

### Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond 
[[arxiv](https://arxiv.org/abs/2502.19301)] [[cool](https://papers.cool/arxiv/2502.19301)] [[pdf](https://arxiv.org/pdf/2502.19301)]
> **Authors**: Qizhou Wang,Jin Peng Zhou,Zhanke Zhou,Saebyeol Shin,Bo Han,Kilian Q. Weinberger
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM unlearning, focusing on erasing targeted undesirable knowledge without compromising the integrity of other, non-targeted responses. Existing studies have introduced various unlearning objectives to pursue LLM unlearning without necessitating complete retraining. However, each of these objectives has unique properties, and no unified framework is currently available to comprehend them thoroughly. To fill the gap, we propose a toolkit of the gradient effect (G-effect), quantifying the impacts of unlearning objectives on model performance from a gradient perspective. A notable advantage is its broad ability to detail the unlearning impacts from various aspects across instances, updating steps, and LLM layers. Accordingly, the G-effect offers new insights into identifying drawbacks of existing unlearning objectives, further motivating us to explore a series of new solutions for their mitigation and improvements. Finally, we outline promising directions that merit further studies, aiming at contributing to the community to advance this important field.

### Global Graph Propagation with Hierarchical Information Transfer for Incomplete Contrastive Multi-view Clustering 
[[arxiv](https://arxiv.org/abs/2502.19291)] [[cool](https://papers.cool/arxiv/2502.19291)] [[pdf](https://arxiv.org/pdf/2502.19291)]
> **Authors**: Guoqing Chao,Kaixin Xu,Xijiong Xie,Yongyong Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Incomplete multi-view clustering has become one of the important research problems due to the extensive missing multi-view data in the real world. Although the existing methods have made great progress, there are still some problems: 1) most methods cannot effectively mine the information hidden in the missing data; 2) most methods typically divide representation learning and clustering into two separate stages, but this may affect the clustering performance as the clustering results directly depend on the learned representation. To address these problems, we propose a novel incomplete multi-view clustering method with hierarchical information transfer. Firstly, we design the view-specific Graph Convolutional Networks (GCN) to obtain the representation encoding the graph structure, which is then fused into the consensus representation. Secondly, considering that one layer of GCN transfers one-order neighbor node information, the global graph propagation with the consensus representation is proposed to handle the missing data and learn deep representation. Finally, we design a weight-sharing pseudo-classifier with contrastive learning to obtain an end-to-end framework that combines view-specific representation learning, global graph propagation with hierarchical information transfer, and contrastive clustering for joint optimization. Extensive experiments conducted on several commonly-used datasets demonstrate the effectiveness and superiority of our method in comparison with other state-of-the-art approaches. The code is available at https://github.com/KelvinXuu/GHICMC.

### Efficient Federated Search for Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.19280)] [[cool](https://papers.cool/arxiv/2502.19280)] [[pdf](https://arxiv.org/pdf/2502.19280)]
> **Authors**: Rachid Guerraoui,Anne-Marie Kermarrec,Diana Petrescu,Rafael Pires,Mathis Randl,Martijn de Vos
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: To appear in the proceedings of EuroMLSys'25
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,信息检索
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various domains but remain susceptible to hallucinations and inconsistencies, limiting their reliability. Retrieval-augmented generation (RAG) mitigates these issues by grounding model responses in external knowledge sources. Existing RAG workflows often leverage a single vector database, which is impractical in the common setting where information is distributed across multiple repositories. We introduce RAGRoute, a novel mechanism for federated RAG search. RAGRoute dynamically selects relevant data sources at query time using a lightweight neural network classifier. By not querying every data source, this approach significantly reduces query overhead, improves retrieval efficiency, and minimizes the retrieval of irrelevant information. We evaluate RAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness in retrieving relevant documents while reducing the number of queries. RAGRoute reduces the total number of queries up to 77.5% and communication volume up to 76.2%.

### Can RLHF be More Efficient with Imperfect Reward Models? A Policy Coverage Perspective 
[[arxiv](https://arxiv.org/abs/2502.19255)] [[cool](https://papers.cool/arxiv/2502.19255)] [[pdf](https://arxiv.org/pdf/2502.19255)]
> **Authors**: Jiawei Huang,Bingcong Li,Christoph Dann,Niao He
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 35 Pages
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Sample efficiency is critical for online Reinforcement Learning from Human Feedback (RLHF). While existing works investigate sample-efficient online exploration strategies, the potential of utilizing misspecified yet relevant reward models to accelerate learning remains underexplored. This paper studies how to transfer knowledge from those imperfect reward models in online RLHF. We start by identifying a novel property of the KL-regularized RLHF objective: \emph{a policy's ability to cover the optimal policy is captured by its sub-optimality}. Building on this insight, we propose a theoretical transfer learning algorithm with provable benefits compared to standard online learning. Our approach achieves low regret in the early stage by quickly adapting to the best available source reward models without prior knowledge of their quality, and over time, it attains an $\tilde{O}(\sqrt{T})$ regret bound \emph{independent} of structural complexity measures. Inspired by our theoretical findings, we develop an empirical algorithm with improved computational efficiency, and demonstrate its effectiveness empirically in summarization tasks.

### Set and functional prediction: randomness, exchangeability, and conformal 
[[arxiv](https://arxiv.org/abs/2502.19254)] [[cool](https://papers.cool/arxiv/2502.19254)] [[pdf](https://arxiv.org/pdf/2502.19254)]
> **Authors**: Vladimir Vovk
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习,统计理论
- **Abstract**: This paper continues the study of the efficiency of conformal prediction as compared with more general randomness prediction and exchangeability prediction. It does not restrict itself to the case of classification, and our results will also be applicable to the case of regression. The price to pay is that efficiency will be attained only on average, albeit with respect to a wide range of probability measures on the label space.

### GraphBridge: Towards Arbitrary Transfer Learning in GNNs 
[[arxiv](https://arxiv.org/abs/2502.19252)] [[cool](https://papers.cool/arxiv/2502.19252)] [[pdf](https://arxiv.org/pdf/2502.19252)]
> **Authors**: Li Ju,Xingyi Yang,Qi Li,Xinchao Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 3 figures, 6 tables, to be published in ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph neural networks (GNNs) are conventionally trained on a per-domain, per-task basis. It creates a significant barrier in transferring the acquired knowledge to different, heterogeneous data setups. This paper introduces GraphBridge, a novel framework to enable knowledge transfer across disparate tasks and domains in GNNs, circumventing the need for modifications to task configurations or graph structures. Specifically, GraphBridge allows for the augmentation of any pre-trained GNN with prediction heads and a bridging network that connects the input to the output layer. This architecture not only preserves the intrinsic knowledge of the original model but also supports outputs of arbitrary dimensions. To mitigate the negative transfer problem, GraphBridg merges the source model with a concurrently trained model, thereby reducing the source bias when applied to the target domain. Our method is thoroughly evaluated across diverse transfer learning scenarios, including Graph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical validation, conducted over 16 datasets representative of these scenarios, confirms the framework's capacity for task- and domain-agnostic transfer learning within graph-like data, marking a significant advancement in the field of GNNs.

### INFO-SEDD: Continuous Time Markov Chains as Scalable Information Metrics Estimators 
[[arxiv](https://arxiv.org/abs/2502.19183)] [[cool](https://papers.cool/arxiv/2502.19183)] [[pdf](https://arxiv.org/pdf/2502.19183)]
> **Authors**: Alberto Foresti,Giulio Franzese,Pietro Michiardi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Information-theoretic quantities play a crucial role in understanding non-linear relationships between random variables and are widely used across scientific disciplines. However, estimating these quantities remains an open problem, particularly in the case of high-dimensional discrete distributions. Current approaches typically rely on embedding discrete data into a continuous space and applying neural estimators originally designed for continuous distributions, a process that may not fully capture the discrete nature of the underlying data. We consider Continuous-Time Markov Chains (CTMCs), stochastic processes on discrete state-spaces which have gained popularity due to their generative modeling applications. In this work, we introduce INFO-SEDD, a novel method for estimating information-theoretic quantities of discrete data, including mutual information and entropy. Our approach requires the training of a single parametric model, offering significant computational and memory advantages. Additionally, it seamlessly integrates with pretrained networks, allowing for efficient reuse of pretrained generative models. To evaluate our approach, we construct a challenging synthetic benchmark. Our experiments demonstrate that INFO-SEDD is robust and outperforms neural competitors that rely on embedding techniques. Moreover, we validate our method on a real-world task: estimating the entropy of an Ising model. Overall, INFO-SEDD outperforms competing methods and shows scalability to high-dimensional scenarios, paving the way for new applications where estimating MI between discrete distribution is the focus. The promising results in this complex, high-dimensional scenario highlight INFO-SEDD as a powerful new estimator in the toolkit for information-theoretical analysis.

### AutoML for Multi-Class Anomaly Compensation of Sensor Drift 
[[arxiv](https://arxiv.org/abs/2502.19180)] [[cool](https://papers.cool/arxiv/2502.19180)] [[pdf](https://arxiv.org/pdf/2502.19180)]
> **Authors**: Melanie Schaller,Mathis Kruse,Antonio Ortega,Marius Lindauer,Bodo Rosenhahn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: To be published in Measurement Journal
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Addressing sensor drift is essential in industrial measurement systems, where precise data output is necessary for maintaining accuracy and reliability in monitoring processes, as it progressively degrades the performance of machine learning models over time. Our findings indicate that the standard cross-validation method used in existing model training overestimates performance by inadequately accounting for drift. This is primarily because typical cross-validation techniques allow data instances to appear in both training and testing sets, thereby distorting the accuracy of the predictive evaluation. As a result, these models are unable to precisely predict future drift effects, compromising their ability to generalize and adapt to evolving data conditions. This paper presents two solutions: (1) a novel sensor drift compensation learning paradigm for validating models, and (2) automated machine learning (AutoML) techniques to enhance classification performance and compensate sensor drift. By employing strategies such as data balancing, meta-learning, automated ensemble learning, hyperparameter optimization, feature selection, and boosting, our AutoML-DC (Drift Compensation) model significantly improves classification performance against sensor drift. AutoML-DC further adapts effectively to varying drift severities.

### A Model-Centric Review of Deep Learning for Protein Design 
[[arxiv](https://arxiv.org/abs/2502.19173)] [[cool](https://papers.cool/arxiv/2502.19173)] [[pdf](https://arxiv.org/pdf/2502.19173)]
> **Authors**: Gregory W. Kyro,Tianyin Qiu,Victor S. Batista
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,生物分子
- **Abstract**: Deep learning has transformed protein design, enabling accurate structure prediction, sequence optimization, and de novo protein generation. Advances in single-chain protein structure prediction via AlphaFold2, RoseTTAFold, ESMFold, and others have achieved near-experimental accuracy, inspiring successive work extended to biomolecular complexes via AlphaFold Multimer, RoseTTAFold All-Atom, AlphaFold 3, Chai-1, Boltz-1 and others. Generative models such as ProtGPT2, ProteinMPNN, and RFdiffusion have enabled sequence and backbone design beyond natural evolution-based limitations. More recently, joint sequence-structure co-design models, including ESM3, have integrated both modalities into a unified framework, resulting in improved designability. Despite these advances, challenges still exist pertaining to modeling sequence-structure-function relationships and ensuring robust generalization beyond the regions of protein space spanned by the training data. Future advances will likely focus on joint sequence-structure-function co-design frameworks that are able to model the fitness landscape more effectively than models that treat these modalities independently. Current capabilities, coupled with the dizzying rate of progress, suggest that the field will soon enable rapid, rational design of proteins with tailored structures and functions that transcend the limitations imposed by natural evolution. In this review, we discuss the current capabilities of deep learning methods for protein design, focusing on some of the most revolutionary and capable models with respect to their functionality and the applications that they enable, leading up to the current challenges of the field and the optimal path forward.

### On the Byzantine Fault Tolerance of signSGD with Majority Vote 
[[arxiv](https://arxiv.org/abs/2502.19170)] [[cool](https://papers.cool/arxiv/2502.19170)] [[pdf](https://arxiv.org/pdf/2502.19170)]
> **Authors**: Emanuele Mengoli,Luzius Moll,Virgilio Strozzi,El-Mahdi El-Mhamdi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In distributed learning, sign-based compression algorithms such as signSGD with majority vote provide a lightweight alternative to SGD with an additional advantage: fault tolerance (almost) for free. However, for signSGD with majority vote, this fault tolerance has been shown to cover only the case of weaker adversaries, i.e., ones that are not omniscient or cannot collude to base their attack on common knowledge and strategy. In this work, we close this gap and provide new insights into how signSGD with majority vote can be resilient against omniscient and colluding adversaries, which craft an attack after communicating with other adversaries, thus having better information to perform the most damaging attack based on a common optimal strategy. Our core contribution is in providing a proof that begins by defining the omniscience framework and the strongest possible damage against signSGD with majority vote without imposing any restrictions on the attacker. Thanks to the filtering effect of the sign-based method, we upper-bound the space of attacks to the optimal strategy for maximizing damage by an attacker. Hence, we derive an explicit probabilistic bound in terms of incorrect aggregation without resorting to unknown constants, providing a convergence bound on signSGD with majority vote in the presence of Byzantine attackers, along with a precise convergence rate. Our findings are supported by experiments on the MNIST dataset in a distributed learning environment with adversaries of varying strength.

### Generalizable deep learning for photoplethysmography-based blood pressure estimation -- A Benchmarking Study 
[[arxiv](https://arxiv.org/abs/2502.19167)] [[cool](https://papers.cool/arxiv/2502.19167)] [[pdf](https://arxiv.org/pdf/2502.19167)]
> **Authors**: Mohammad Moulaeifard,Peter H. Charlton,Nils Strodthoff
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 20 pages, 5 figures, code available at https://github.com/AI4HealthUOL/ppg-ood-generalization
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Photoplethysmography (PPG)-based blood pressure (BP) estimation represents a promising alternative to cuff-based BP measurements. Recently, an increasing number of deep learning models have been proposed to infer BP from the raw PPG waveform. However, these models have been predominantly evaluated on in-distribution test sets, which immediately raises the question of the generalizability of these models to external datasets. To investigate this question, we trained five deep learning models on the recently released PulseDB dataset, provided in-distribution benchmarking results on this dataset, and then assessed out-of-distribution performance on several external datasets. The best model (XResNet1d101) achieved in-distribution MAEs of 9.4 and 6.0 mmHg for systolic and diastolic BP respectively on PulseDB (with subject-specific calibration), and 14.0 and 8.5 mmHg respectively without calibration. Equivalent MAEs on external test datasets without calibration ranged from 15.0 to 25.1 mmHg (SBP) and 7.0 to 10.4 mmHg (DBP). Our results indicate that the performance is strongly influenced by the differences in BP distributions between datasets. We investigated a simple way of improving performance through sample-based domain adaptation and put forward recommendations for training models with good generalization properties. With this work, we hope to educate more researchers for the importance and challenges of out-of-distribution generalization.

### Design of Cavity Backed Slotted Antenna using Machine Learning Regression Model 
[[arxiv](https://arxiv.org/abs/2502.19164)] [[cool](https://papers.cool/arxiv/2502.19164)] [[pdf](https://arxiv.org/pdf/2502.19164)]
> **Authors**: Vijay Kumar Sutrakar,Anjana PK,Rohit Bisariya,Soumya KK,Gopal Chawan M
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理,应用物理
- **Abstract**: In this paper, a regression-based machine learning model is used for the design of cavity backed slotted antenna. This type of antenna is commonly used in military and aviation communication systems. Initial reflection coefficient data of cavity backed slotted antenna is generated using electromagnetic solver. These reflection coefficient data is then used as input for training regression-based machine learning model. The model is trained to predict the dimensions of cavity backed slotted antenna based on the input reflection coefficient for a wide frequency band varying from 1 GHz to 8 GHz. This approach allows for rapid prediction of optimal antenna configurations, reducing the need for repeated physical testing and manual adjustments, may lead to significant amount of design and development cost saving. The proposed model also demonstrates its versatility in predicting multi frequency resonance across 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential for leveraging machine learning in advanced antenna design, enhancing efficiency and accuracy in practical applications such as radar, military identification systems and secure communication networks.

### Design of Resistive Frequency Selective Surface based Radar Absorbing Structure-A Deep Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.19151)] [[cool](https://papers.cool/arxiv/2502.19151)] [[pdf](https://arxiv.org/pdf/2502.19151)]
> **Authors**: Vijay Kumar Sutrakar,Nikhil Morge,Anjana PK,Abhilash PV
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学,应用物理
- **Abstract**: In this paper, deep learning-based approach for the design of radar absorbing structure using resistive frequency selective surface is proposed. In the present design, reflection coefficient is used as input of deep learning model and the Jerusalem cross based unit cell dimensions is predicted as outcome. Sequential neural network based deep learning model with adaptive moment estimation optimizer is used for designing multi frequency band absorbers. The model is used for designing radar absorber from L to Ka band depending on unit cell parameters and thickness. The outcome of deep learning model is further compared with full-wave simulation software and an excellent match is obtained. The proposed model can be used for the low-cost design of various radar absorbing structures using a single unit cell and thickness across the band of frequencies.

### Random Similarity Isolation Forests 
[[arxiv](https://arxiv.org/abs/2502.19122)] [[cool](https://papers.cool/arxiv/2502.19122)] [[pdf](https://arxiv.org/pdf/2502.19122)]
> **Authors**: Sebastian Chwilczyński,Dariusz Brzezinski
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With predictive models becoming prevalent, companies are expanding the types of data they gather. As a result, the collected datasets consist not only of simple numerical features but also more complex objects such as time series, images, or graphs. Such multi-modal data have the potential to improve performance in predictive tasks like outlier detection, where the goal is to identify objects deviating from the main data distribution. However, current outlier detection algorithms are dedicated to individual types of data. Consequently, working with mixed types of data requires either fusing multiple data-specific models or transforming all of the representations into a single format, both of which can hinder predictive performance. In this paper, we propose a multi-modal outlier detection algorithm called Random Similarity Isolation Forest. Our method combines the notions of isolation and similarity-based projection to handle datasets with mixtures of features of arbitrary data types. Experiments performed on 47 benchmark datasets demonstrate that Random Similarity Isolation Forest outperforms five state-of-the-art competitors. Our study shows that the use of multiple modalities can indeed improve the detection of anomalies and highlights the need for new outlier detection benchmarks tailored for multi-modal algorithms.

### Chemical knowledge-informed framework for privacy-aware retrosynthesis learning 
[[arxiv](https://arxiv.org/abs/2502.19119)] [[cool](https://papers.cool/arxiv/2502.19119)] [[pdf](https://arxiv.org/pdf/2502.19119)]
> **Authors**: Guikun Chen,Xu Zhang,Yi Yang,Wenguan Wang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Chemical reaction data is a pivotal asset, driving advances in competitive fields such as pharmaceuticals, materials science, and industrial chemistry. Its proprietary nature renders it sensitive, as it often includes confidential insights and competitive advantages organizations strive to protect. However, in contrast to this need for confidentiality, the current standard training paradigm for machine learning-based retrosynthesis gathers reaction data from multiple sources into one single edge to train prediction models. This paradigm poses considerable privacy risks as it necessitates broad data availability across organizational boundaries and frequent data transmission between entities, potentially exposing proprietary information to unauthorized access or interception during storage and transfer. In the present study, we introduce the chemical knowledge-informed framework (CKIF), a privacy-preserving approach for learning retrosynthesis models. CKIF enables distributed training across multiple chemical organizations without compromising the confidentiality of proprietary reaction data. Instead of gathering raw reaction data, CKIF learns retrosynthesis models through iterative, chemical knowledge-informed aggregation of model parameters. In particular, the chemical properties of predicted reactants are leveraged to quantitatively assess the observable behaviors of individual models, which in turn determines the adaptive weights used for model aggregation. On a variety of reaction datasets, CKIF outperforms several strong baselines by a clear margin (e.g., ~20% performance improvement over FedAvg on USPTO-50K), showing its feasibility and superiority to stimulate further research on privacy-preserving retrosynthesis.

### Software demodulation of weak radio signals using convolutional neural network 
[[arxiv](https://arxiv.org/abs/2502.19097)] [[cool](https://papers.cool/arxiv/2502.19097)] [[pdf](https://arxiv.org/pdf/2502.19097)]
> **Authors**: Mykola Kozlenko,Ihor Lazarovych,Valerii Tkachuk,Vira Vialkova
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 4 pages, 8 figures. This is the preprint version of the paper published in 2020 IEEE 7th International Conference on Energy Smart Systems (ESS). The final version is available at IEEE Xplore: https://ieeexplore.ieee.org/document/9160035. arXiv admin note: text overlap with arXiv:2502.16371
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: In this paper we proposed the use of JT65A radio communication protocol for data exchange in wide-area monitoring systems in electric power systems. We investigated the software demodulation of the multiple frequency shift keying weak signals transmitted with JT65A communication protocol using deep convolutional neural network. We presented the demodulation performance in form of symbol and bit error rates. We focused on the interference immunity of the protocol over an additive white Gaussian noise with average signal-to-noise ratios in the range from -30 dB to 0 dB, which was obtained for the first time. We proved that the interference immunity is about 1.5 dB less than the theoretical limit of non-coherent demodulation of orthogonal MFSK signals.

### MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning Framework for Few-Shot Modulation Recognition 
[[arxiv](https://arxiv.org/abs/2502.19071)] [[cool](https://papers.cool/arxiv/2502.19071)] [[pdf](https://arxiv.org/pdf/2502.19071)]
> **Authors**: Dongwei Xu,Yutao Zhu,Yao Lu,Youpeng Feng,Yun Lin,Qi Xuan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With the rapid advancements in wireless communication technology, automatic modulation recognition (AMR) plays a critical role in ensuring communication security and reliability. However, numerous challenges, including higher performance demands, difficulty in data acquisition under specific scenarios, limited sample size, and low-quality labeled data, hinder its development. Few-shot learning (FSL) offers an effective solution by enabling models to achieve satisfactory performance with only a limited number of labeled samples. While most FSL techniques are applied in the field of computer vision, they are not directly applicable to wireless signal processing. This study does not propose a new FSL-specific signal model but introduces a framework called MCLRL. This framework combines multi-domain contrastive learning with reinforcement learning. Multi-domain representations of signals enhance feature richness, while integrating contrastive learning and reinforcement learning architectures enables the extraction of deep features for classification. In downstream tasks, the model achieves excellent performance using only a few samples and minimal training cycles. Experimental results show that the MCLRL framework effectively extracts key features from signals, performs well in FSL tasks, and maintains flexibility in signal model selection.

### A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks 
[[arxiv](https://arxiv.org/abs/2502.19070)] [[cool](https://papers.cool/arxiv/2502.19070)] [[pdf](https://arxiv.org/pdf/2502.19070)]
> **Authors**: Haoyang Li,Li Bai,Qingqing Ye,Haibo Hu,Yaxin Xiao,Huadi Zheng,Jianliang Xu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to be appeared in 39th Annual AAAI Conference on Artificial Intelligence (AAAI-25)
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Model Inversion (MI) attacks, which reconstruct the training dataset of neural networks, pose significant privacy concerns in machine learning. Recent MI attacks have managed to reconstruct realistic label-level private data, such as the general appearance of a target person from all training images labeled on him. Beyond label-level privacy, in this paper we show sample-level privacy, the private information of a single target sample, is also important but under-explored in the MI literature due to the limitations of existing evaluation metrics. To address this gap, this study introduces a novel metric tailored for training-sample analysis, namely, the Diversity and Distance Composite Score (DDCS), which evaluates the reconstruction fidelity of each training sample by encompassing various MI attack attributes. This, in turn, enhances the precision of sample-level privacy assessments. Leveraging DDCS as a new evaluative lens, we observe that many training samples remain resilient against even the most advanced MI attack. As such, we further propose a transfer learning framework that augments the generative capabilities of MI attackers through the integration of entropy loss and natural gradient descent. Extensive experiments verify the effectiveness of our framework on improving state-of-the-art MI attacks over various metrics including DDCS, coverage and FID. Finally, we demonstrate that DDCS can also be useful for MI defense, by identifying samples susceptible to MI attacks in an unsupervised manner.

### Foundation Inference Models for Stochastic Differential Equations: A Transformer-based Approach for Zero-shot Function Estimation 
[[arxiv](https://arxiv.org/abs/2502.19049)] [[cool](https://papers.cool/arxiv/2502.19049)] [[pdf](https://arxiv.org/pdf/2502.19049)]
> **Authors**: Patrick Seifner,Kostadin Cvejoski,David Berghaus,Cesar Ojeda,Ramses J. Sanchez
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Stochastic differential equations (SDEs) describe dynamical systems where deterministic flows, governed by a drift function, are superimposed with random fluctuations dictated by a diffusion function. The accurate estimation (or discovery) of these functions from data is a central problem in machine learning, with wide application across natural and social sciences alike. Yet current solutions are brittle, and typically rely on symbolic regression or Bayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation Inference Model for SDEs), a transformer-based recognition model capable of performing accurate zero-shot estimation of the drift and diffusion functions of SDEs, from noisy and sparse observations on empirical processes of different dimensionalities. Leveraging concepts from amortized inference and neural operators, we train FIM-SDE in a supervised fashion, to map a large set of noisy and discretely observed SDE paths to their corresponding drift and diffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE achieves robust zero-shot function estimation (i.e. without any parameter fine-tuning) across a wide range of synthetic and real-world processes, from canonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf bifurcations) to human motion recordings and oil price and wind speed fluctuations.

### A HEART for the environment: Transformer-Based Spatiotemporal Modeling for Air Quality Prediction 
[[arxiv](https://arxiv.org/abs/2502.19042)] [[cool](https://papers.cool/arxiv/2502.19042)] [[pdf](https://arxiv.org/pdf/2502.19042)]
> **Authors**: Norbert Bodendorfer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: :68T07; 86A10; 37M10; 62M20ACM Class:I.2.6; G.3; J.2
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Accurate and reliable air pollution forecasting is crucial for effective environmental management and policy-making. llull-environment is a sophisticated and scalable forecasting system for air pollution, inspired by previous models currently operational in Madrid and Valladolid (Spain). It contains (among other key components) an encoder-decoder convolutional neural network to forecast mean pollution levels for four key pollutants (NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and other contextual features. This paper investigates the augmentation of this neural network with an attention mechanism to improve predictive accuracy. The proposed attention mechanism pre-processes tensors containing the input features before passing them to the existing mean forecasting model. The resulting model is a combination of several architectures and ideas and can be described as a "Hybrid Enhanced Autoregressive Transformer", or HEART. The effectiveness of the approach is evaluated by comparing the mean square error (MSE) across different attention layouts against the system without such a mechanism. We observe a significant reduction in MSE of up to 22%, with an average of 7.5% across tested cities and pollutants. The performance of a given attention mechanism turns out to depend on the pollutant, highlighting the differences in their creation and dissipation processes. Our findings are not restricted to optimizing air quality prediction models, but are applicable generally to (fixed length) time series forecasting.

### Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning 
[[arxiv](https://arxiv.org/abs/2502.19009)] [[cool](https://papers.cool/arxiv/2502.19009)] [[pdf](https://arxiv.org/pdf/2502.19009)]
> **Authors**: Jaehyeon Son,Soochan Lee,Gunhee Kim
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent studies have shown that Transformers can perform in-context reinforcement learning (RL) by imitating existing RL algorithms, enabling sample-efficient adaptation to unseen tasks without parameter updates. However, these models also inherit the suboptimal behaviors of the RL algorithms they imitate. This issue primarily arises due to the gradual update rule employed by those algorithms. Model-based planning offers a promising solution to this limitation by allowing the models to simulate potential outcomes before taking action, providing an additional mechanism to deviate from the suboptimal behavior. Rather than learning a separate dynamics model, we propose Distillation for In-Context Planning (DICP), an in-context model-based RL framework where Transformers simultaneously learn environment dynamics and improve policy in-context. We evaluate DICP across a range of discrete and continuous environments, including Darkroom variants and Meta-World. Our results show that DICP achieves state-of-the-art performance while requiring significantly fewer environment interactions than baselines, which include both model-free counterparts and existing meta-RL methods.

### Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret in Noise-Free Gaussian Process Bandits 
[[arxiv](https://arxiv.org/abs/2502.19006)] [[cool](https://papers.cool/arxiv/2502.19006)] [[pdf](https://arxiv.org/pdf/2502.19006)]
> **Authors**: Shogo Iwazaki
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study the noise-free Gaussian Process (GP) bandits problem, in which the learner seeks to minimize regret through noise-free observations of the black-box objective function lying on the known reproducing kernel Hilbert space (RKHS). Gaussian process upper confidence bound (GP-UCB) is the well-known GP-bandits algorithm whose query points are adaptively chosen based on the GP-based upper confidence bound score. Although several existing works have reported the practical success of GP-UCB, the current theoretical results indicate its suboptimal performance. However, GP-UCB tends to perform well empirically compared with other nearly optimal noise-free algorithms that rely on a non-adaptive sampling scheme of query points. This paper resolves this gap between theoretical and empirical performance by showing the nearly optimal regret upper bound of noise-free GP-UCB. Specifically, our analysis shows the first constant cumulative regret in the noise-free settings for the squared exponential kernel and Matérn kernel with some degree of smoothness.

### The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training 
[[arxiv](https://arxiv.org/abs/2502.19002)] [[cool](https://papers.cool/arxiv/2502.19002)] [[pdf](https://arxiv.org/pdf/2502.19002)]
> **Authors**: Jinbo Wang,Mingze Wang,Zhanpeng Zhou,Junchi Yan,Weinan E,Lei Wu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 23 pages
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制,机器学习
- **Abstract**: Transformers consist of diverse building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feedforward networks. Thus, understanding the differences and interactions among these blocks is important. In this paper, we uncover a clear Sharpness Disparity across these blocks, which emerges early in training and intriguingly persists throughout the training process. Motivated by this finding, we propose Blockwise Learning Rate (LR), a strategy that tailors the LR to each block's sharpness, accelerating large language model (LLM) pre-training. By integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and datasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory saving. These results underscore the potential of exploiting the sharpness disparity to improve LLM training.

### Long-term Causal Inference via Modeling Sequential Latent Confounding 
[[arxiv](https://arxiv.org/abs/2502.18994)] [[cool](https://papers.cool/arxiv/2502.18994)] [[pdf](https://arxiv.org/pdf/2502.18994)]
> **Authors**: Weilin Chen,Ruichu Cai,Yuguang Yan,Zhifeng Hao,José Miguel Hernández-Lobato
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Long-term causal inference is an important but challenging problem across various scientific domains. To solve the latent confounding problem in long-term observational studies, existing methods leverage short-term experimental data. Ghassami et al. propose an approach based on the Conditional Additive Equi-Confounding Bias (CAECB) assumption, which asserts that the confounding bias in the short-term outcome is equal to that in the long-term outcome, so that the long-term confounding bias and the causal effects can be identified. While effective in certain cases, this assumption is limited to scenarios with a one-dimensional short-term outcome. In this paper, we introduce a novel assumption that extends the CAECB assumption to accommodate temporal short-term outcomes. Our proposed assumption states a functional relationship between sequential confounding biases across temporal short-term outcomes, under which we theoretically establish the identification of long-term causal effects. Based on the identification result, we develop an estimator and conduct a theoretical analysis of its asymptotic properties. Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.

### Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.18975)] [[cool](https://papers.cool/arxiv/2502.18975)] [[pdf](https://arxiv.org/pdf/2502.18975)]
> **Authors**: Martin Surner,Abdelmajid Khelil,Ludwig Bothmann
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.

### (Mis)Fitting: A Survey of Scaling Laws 
[[arxiv](https://arxiv.org/abs/2502.18969)] [[cool](https://papers.cool/arxiv/2502.18969)] [[pdf](https://arxiv.org/pdf/2502.18969)]
> **Authors**: Margaret Li,Sneha Kudugunta,Luke Zettlemoyer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 41 pages, 3 figure, first two authors contributed equally. ICLR, 2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,方法论
- **Abstract**: Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.

### One Set to Rule Them All: How to Obtain General Chemical Conditions via Bayesian Optimization over Curried Functions 
[[arxiv](https://arxiv.org/abs/2502.18966)] [[cool](https://papers.cool/arxiv/2502.18966)] [[pdf](https://arxiv.org/pdf/2502.18966)]
> **Authors**: Stefan P. Schmid,Ella Miray Rajaonson,Cher Tian Ser,Mohammad Haddadnia,Shi Xuan Leong,Alán Aspuru-Guzik,Agustinus Kristiadi,Kjell Jorner,Felix Strieth-Kalthoff
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: :J.2
- **标题**: None
- **领域**: 机器学习
- **Abstract**: General parameters are highly desirable in the natural sciences - e.g., chemical reaction conditions that enable high yields across a range of related transformations. This has a significant practical impact since those general parameters can be transferred to related tasks without the need for laborious and time-intensive re-optimization. While Bayesian optimization (BO) is widely applied to find optimal parameter sets for specific tasks, it has remained underused in experiment planning towards such general optima. In this work, we consider the real-world problem of condition optimization for chemical reactions to study how performing generality-oriented BO can accelerate the identification of general optima, and whether these optima also translate to unseen examples. This is achieved through a careful formulation of the problem as an optimization over curried functions, as well as systematic evaluations of generality-oriented strategies for optimization tasks on real-world experimental data. We find that for generality-oriented optimization, simple myopic optimization strategies that decouple parameter and task selection perform comparably to more complex ones, and that effective optimization is merely determined by an effective exploration of both parameter and task space.

### Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination 
[[arxiv](https://arxiv.org/abs/2502.18960)] [[cool](https://papers.cool/arxiv/2502.18960)] [[pdf](https://arxiv.org/pdf/2502.18960)]
> **Authors**: Weilin Chen,Ruichu Cai,Junjie Wan,Zeqin Yang,José Miguel Hernández-Lobato
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Long-term causal inference has drawn increasing attention in many scientific domains. Existing methods mainly focus on estimating average long-term causal effects by combining long-term observational data and short-term experimental data. However, it is still understudied how to robustly and effectively estimate heterogeneous long-term causal effects, significantly limiting practical applications. In this paper, we propose several two-stage style nonparametric estimators for heterogeneous long-term causal effect estimation, including propensity-based, regression-based, and multiple robust estimators. We conduct a comprehensive theoretical analysis of their asymptotic properties under mild assumptions, with the ultimate goal of building a better understanding of the conditions under which some estimators can be expected to perform better. Extensive experiments across several semi-synthetic and real-world datasets validate the theoretical results and demonstrate the effectiveness of the proposed estimators.

### Fourier Multi-Component and Multi-Layer Neural Networks: Unlocking High-Frequency Potential 
[[arxiv](https://arxiv.org/abs/2502.18959)] [[cool](https://papers.cool/arxiv/2502.18959)] [[pdf](https://arxiv.org/pdf/2502.18959)]
> **Authors**: Shijun Zhang,Hongkai Zhao,Yimin Zhong,Haomin Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Our code and implementation details are available at https://github.com/ShijunZhangMath/FMMNN
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The two most critical ingredients of a neural network are its structure and the activation function employed, and more importantly, the proper alignment of these two that is conducive to the effective representation and learning in practice. In this work, we introduce a surprisingly effective synergy, termed the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN), and demonstrate its surprising adaptability and efficiency in capturing high-frequency components. First, we theoretically establish that FMMNNs have exponential expressive power in terms of approximation capacity. Next, we analyze the optimization landscape of FMMNNs and show that it is significantly more favorable compared to fully connected neural networks. Finally, systematic and extensive numerical experiments validate our findings, demonstrating that FMMNNs consistently achieve superior accuracy and efficiency across various tasks, particularly impressive when high-frequency components are present.

### Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset 
[[arxiv](https://arxiv.org/abs/2502.18955)] [[cool](https://papers.cool/arxiv/2502.18955)] [[pdf](https://arxiv.org/pdf/2502.18955)]
> **Authors**: Yiqin Yang,Quanwei Wang,Chenghao Li,Hao Hu,Chengjie Wu,Yuhua Jiang,Dianyu Zhong,Ziyou Zhang,Qianchuan Zhao,Chongjie Zhang,Xu Bo
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: ef:Published on ICLR 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Offline reinforcement learning (RL) represents a significant shift in RL research, allowing agents to learn from pre-collected datasets without further interaction with the environment. A key, yet underexplored, challenge in offline RL is selecting an optimal subset of the offline dataset that enhances both algorithm performance and training efficiency. Reducing dataset size can also reveal the minimal data requirements necessary for solving similar problems. In response to this challenge, we introduce ReDOR (Reduced Datasets for Offline RL), a method that frames dataset selection as a gradient approximation optimization problem. We demonstrate that the widely used actor-critic framework in RL can be reformulated as a submodular optimization objective, enabling efficient subset selection. To achieve this, we adapt orthogonal matching pursuit (OMP), incorporating several novel modifications tailored for offline RL. Our experimental results show that the data subsets identified by ReDOR not only boost algorithm performance but also do so with significantly lower computational complexity.

### BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting 
[[arxiv](https://arxiv.org/abs/2502.18925)] [[cool](https://papers.cool/arxiv/2502.18925)] [[pdf](https://arxiv.org/pdf/2502.18925)]
> **Authors**: Weiyan Wang,Xingjian Shi,Ruiqi Shu,Yuan Gao,Rui Ray Chen,Kun Wang,Fan Xu,Jinbao Xue,Shuaipeng Li,Yangyu Tao,Di Wang,Hao Wu,Xiaomeng Huang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.

### CLLoRA: An Approach to Measure the Effects of the Context Length for LLM Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.18910)] [[cool](https://papers.cool/arxiv/2502.18910)] [[pdf](https://arxiv.org/pdf/2502.18910)]
> **Authors**: Ping Zhang,Zhaorui Zhang,Sheng Di,Yao Xin,Benben Liu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Large language model fine-tuning has been identified as an efficient approach to applying the pre-trained Large language models to other domains. To guarantee data privacy for different data owners, models are often fine-tuned in federated learning environments across different data owners, which often involve data heterogeneity issues and affect the fine-tuning performance. In addition, the length of the context for the training data has been identified as a major factor that affects the LLM's model performance. To efficiently measure how the context length affects the LLM's model performance in heterogeneous federated learning environments, we propose CLLoRA. CLLoRA utilizes the parameter-efficient fine-tuning approach LoRA based on different kinds of LLMs with varying sizes as the fine-tuning approach to investigate whether the quality and length of contexts can serve as standards for measuring non-IID context. The findings indicate that an imbalance in context quality not only affects local training on clients but also impacts the global model's performance. However, context length has a minimal effect on local training but a more significant influence on the global model. These results provide insights into how context quality and length affect the model performance for LLM fine-tuning in federated learning environments.

### A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic 
[[arxiv](https://arxiv.org/abs/2502.18909)] [[cool](https://papers.cool/arxiv/2502.18909)] [[pdf](https://arxiv.org/pdf/2502.18909)]
> **Authors**: Matin Shokri,Ramin Hasibi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 10 pages, 4 figures. arXiv admin note: text overlap with arXiv:1901.00204
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Network Traffic Classification (NTC) is one of the most important tasks in network management. The imbalanced nature of classes on the internet presents a critical challenge in classification tasks. For example, some classes of applications are much more prevalent than others, such as HTTP. As a result, machine learning classification models do not perform well on those classes with fewer data. To address this problem, we propose a pipeline to balance the dataset and classify it using a robust and accurate embedding technique. First, we generate artificial data using Long Short-Term Memory (LSTM) networks and Kernel Density Estimation (KDE). Next, we propose replacing one-hot encoding for categorical features with a novel embedding framework based on the "Flow as a Sentence" perspective, which we name FS-Embedding. This framework treats the source and destination ports, along with the packet's direction, as one word in a flow, then trains an embedding vector space based on these new features through the learning classification task. Finally, we compare our pipeline with the training of a Convolutional Recurrent Neural Network (CRNN) and Transformers, both with imbalanced and sampled datasets, as well as with the one-hot encoding approach. We demonstrate that the proposed augmentation pipeline, combined with FS-Embedding, increases convergence speed and leads to a significant reduction in the number of model parameters, all while maintaining the same performance in terms of accuracy.

### VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model 
[[arxiv](https://arxiv.org/abs/2502.18906)] [[cool](https://papers.cool/arxiv/2502.18906)] [[pdf](https://arxiv.org/pdf/2502.18906)]
> **Authors**: Jiani Zheng,Lu Wang,Fangkai Yang,Chaoyun Zhang,Lingrui Mei,Wenjie Yin,Qingwei Lin,Dongmei Zhang,Saravan Rajmohan,Qi Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 20pages,5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user's goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods.

### Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance 
[[arxiv](https://arxiv.org/abs/2502.18891)] [[cool](https://papers.cool/arxiv/2502.18891)] [[pdf](https://arxiv.org/pdf/2502.18891)]
> **Authors**: Ziyuan Zhong,Junyang Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 18 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into N equivalent training subsets and N prediction subsets using a supervised model, followed by independent predictions from N separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state of the art models. The key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm's robustness and adaptability.

### A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops 
[[arxiv](https://arxiv.org/abs/2502.18865)] [[cool](https://papers.cool/arxiv/2502.18865)] [[pdf](https://arxiv.org/pdf/2502.18865)]
> **Authors**: Shi Fu,Yingjie Wang,Yuzhu Chen,Xinmei Tian,Dacheng Tao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical results have been strikingly inconsistent: some models degrade or even collapse, while others successfully avoid these failures, leaving a significant gap in theoretical understanding to explain this discrepancy. This paper introduces the intriguing notion of recursive stability and presents the first theoretical generalization analysis, revealing how both model architecture and the proportion between real and synthetic data influence the success of STLs. We further extend this analysis to transformers in in-context learning, showing that even a constant-sized proportion of real data ensures convergence, while also providing insights into optimal synthetic data sizing.

### Investigating Generalization of One-shot LLM Steering Vectors 
[[arxiv](https://arxiv.org/abs/2502.18862)] [[cool](https://papers.cool/arxiv/2502.18862)] [[pdf](https://arxiv.org/pdf/2502.18862)]
> **Authors**: Jacob Dunefsky,Arman Cohan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 20 pages, 7 figures. Code is available at https://github.com/jacobdunefsky/one-shot-steering-repro
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure.

### TabGLM: Tabular Graph Language Model for Learning Transferable Representations Through Multi-Modal Consistency Minimization 
[[arxiv](https://arxiv.org/abs/2502.18847)] [[cool](https://papers.cool/arxiv/2502.18847)] [[pdf](https://arxiv.org/pdf/2502.18847)]
> **Authors**: Anay Majee,Maria Xenochristou,Wei-Peng Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Handling heterogeneous data in tabular datasets poses a significant challenge for deep learning models. While attention-based architectures and self-supervised learning have achieved notable success, their application to tabular data remains less effective over linear and tree based models. Although several breakthroughs have been achieved by models which transform tables into uni-modal transformations like image, language and graph, these models often underperform in the presence of feature heterogeneity. To address this gap, we introduce TabGLM (Tabular Graph Language Model), a novel multi-modal architecture designed to model both structural and semantic information from a table. TabGLM transforms each row of a table into a fully connected graph and serialized text, which are then encoded using a graph neural network (GNN) and a text encoder, respectively. By aligning these representations through a joint, multi-modal, self-supervised learning objective, TabGLM leverages complementary information from both modalities, thereby enhancing feature learning. TabGLM's flexible graph-text pipeline efficiently processes heterogeneous datasets with significantly fewer parameters over existing Deep Learning approaches. Evaluations across 25 benchmark datasets demonstrate substantial performance gains, with TabGLM achieving an average AUC-ROC improvement of up to 5.56% over State-of-the-Art (SoTA) tabular learning methods.

### Adversarial Combinatorial Semi-bandits with Graph Feedback 
[[arxiv](https://arxiv.org/abs/2502.18826)] [[cool](https://papers.cool/arxiv/2502.18826)] [[pdf](https://arxiv.org/pdf/2502.18826)]
> **Authors**: Yuxiao Wen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\widetildeΘ(S\sqrt{T}+\sqrt{αST})$, where $S$ is the size of the combinatorial decisions and $α$ is the independence number of $G$. This result interpolates between the known regrets $\widetildeΘ(S\sqrt{T})$ under full information (i.e., $G$ is complete) and $\widetildeΘ(\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations.

## 多代理系统(cs.MA:Multiagent Systems)

### Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.19717)] [[cool](https://papers.cool/arxiv/2502.19717)] [[pdf](https://arxiv.org/pdf/2502.19717)]
> **Authors**: Xinran Li,Xiaolu Wang,Chenjia Bai,Jun Zhang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted by the Thirteenth International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links-a task that becomes increasingly complex as the number of agents grows-we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The code is publicly available at https://github.com/LXXXXR/ExpoComm.

### Combining Planning and Reinforcement Learning for Solving Relational Multiagent Domains 
[[arxiv](https://arxiv.org/abs/2502.19297)] [[cool](https://papers.cool/arxiv/2502.19297)] [[pdf](https://arxiv.org/pdf/2502.19297)]
> **Authors**: Nikhilesh Prabhakar,Ranveer Singh,Harsha Kokel,Sriraam Natarajan,Prasad Tadepalli
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Multiagent Reinforcement Learning (MARL) poses significant challenges due to the exponential growth of state and action spaces and the non-stationary nature of multiagent environments. This results in notable sample inefficiency and hinders generalization across diverse tasks. The complexity is further pronounced in relational settings, where domain knowledge is crucial but often underutilized by existing MARL algorithms. To overcome these hurdles, we propose integrating relational planners as centralized controllers with efficient state abstractions and reinforcement learning. This approach proves to be sample-efficient and facilitates effective task transfer and generalization.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### Evolutionary Algorithms Approach For Search Based On Semantic Document Similarity 
[[arxiv](https://arxiv.org/abs/2502.19437)] [[cool](https://papers.cool/arxiv/2502.19437)] [[pdf](https://arxiv.org/pdf/2502.19437)]
> **Authors**: Chandrashekar Muniyappa,Eujin Kim
> **First submission**: 2025-02-20
> **First announcement**: 2025-02-27
> **comment**: :68T50ACM Class:I.2.7; I.2.11
- **标题**: None
- **领域**: 神经和进化计算,人工智能,机器学习
- **Abstract**: Advancements in cloud computing and distributed computing have fostered research activities in Computer science. As a result, researchers have made significant progress in Neural Networks, Evolutionary Computing Algorithms like Genetic, and Differential evolution algorithms. These algorithms are used to develop clustering, recommendation, and question-and-answering systems using various text representation and similarity measurement techniques. In this research paper, Universal Sentence Encoder (USE) is used to capture the semantic similarity of text; And the transfer learning technique is used to apply Genetic Algorithm (GA) and Differential Evolution (DE) algorithms to search and retrieve relevant top N documents based on user query. The proposed approach is applied to the Stanford Question and Answer (SQuAD) Dataset to identify a user query. Finally, through experiments, we prove that text documents can be efficiently represented as sentence embedding vectors using USE to capture the semantic similarity, and by comparing the results of the Manhattan Distance, GA, and DE algorithms we prove that the evolutionary algorithms are good at finding the top N results than the traditional ranking approach.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### A Multi-Agent DRL-Based Framework for Optimal Resource Allocation and Twin Migration in the Multi-Tier Vehicular Metaverse 
[[arxiv](https://arxiv.org/abs/2502.19004)] [[cool](https://papers.cool/arxiv/2502.19004)] [[pdf](https://arxiv.org/pdf/2502.19004)]
> **Authors**: Nahom Abishu Hayla,A. Mohammed Seid,Aiman Erbad,Tilahun M. Getu,Ala Al-Fuqaha,Mohsen Guizani
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 15 pages, 16 figures
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,计算机科学与博弈论
- **Abstract**: Although multi-tier vehicular Metaverse promises to transform vehicles into essential nodes -- within an interconnected digital ecosystem -- using efficient resource allocation and seamless vehicular twin (VT) migration, this can hardly be achieved by the existing techniques operating in a highly dynamic vehicular environment, since they can hardly balance multi-objective optimization problems such as latency reduction, resource utilization, and user experience (UX). To address these challenges, we introduce a novel multi-tier resource allocation and VT migration framework that integrates Graph Convolutional Networks (GCNs), a hierarchical Stackelberg game-based incentive mechanism, and Multi-Agent Deep Reinforcement Learning (MADRL). The GCN-based model captures both spatial and temporal dependencies within the vehicular network; the Stackelberg game-based incentive mechanism fosters cooperation between vehicles and infrastructure; and the MADRL algorithm jointly optimizes resource allocation and VT migration in real time. By modeling this dynamic and multi-tier vehicular Metaverse as a Markov Decision Process (MDP), we develop a MADRL-based algorithm dubbed the Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MO-MADDPG), which can effectively balances the various conflicting objectives. Extensive simulations validate the effectiveness of this algorithm that is demonstrated to enhance scalability, reliability, and efficiency while considerably improving latency, resource utilization, migration cost, and overall UX by 12.8%, 9.7%, 14.2%, and 16.1%, respectively.

## 机器人技术(cs.RO:Robotics)

### Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success 
[[arxiv](https://arxiv.org/abs/2502.19645)] [[cool](https://papers.cool/arxiv/2502.19645)] [[pdf](https://arxiv.org/pdf/2502.19645)]
> **Authors**: Moo Jin Kim,Chelsea Finn,Percy Liang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Website: https://openvla-oft.github.io/
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($π_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.

### Sensor-Invariant Tactile Representation 
[[arxiv](https://arxiv.org/abs/2502.19638)] [[cool](https://papers.cool/arxiv/2502.19638)] [[pdf](https://arxiv.org/pdf/2502.19638)]
> **Authors**: Harsh Gupta,Yuchen Mo,Shengmiao Jin,Wenzhen Yuan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted to ICLR'25
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. However, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. This limitation hinders the ability to transfer models or knowledge learned from one sensor to another. To address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. Our approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. Experimental results demonstrate the method's effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field.

### Diffusion-based Planning with Learned Viability Filters 
[[arxiv](https://arxiv.org/abs/2502.19564)] [[cool](https://papers.cool/arxiv/2502.19564)] [[pdf](https://arxiv.org/pdf/2502.19564)]
> **Authors**: Nicholas Ioannidis,Daniele Reda,Setareh Cohan,Michiel van de Panne
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Diffusion models can be used as a motion planner by sampling from a distribution of possible futures. However, the samples may not satisfy hard constraints that exist only implicitly in the training data, e.g., avoiding falls or not colliding with a wall. We propose learned viability filters that efficiently predict the future success of any given plan, i.e., diffusion sample, and thereby enforce an implicit future-success constraint. Multiple viability filters can also be composed together. We demonstrate the approach on detailed footstep planning for challenging 3D human locomotion tasks, showing the effectiveness of viability filters in performing online planning and control for box-climbing, step-over walls, and obstacle avoidance. We further show that using viability filters is significantly faster than guidance-based diffusion prediction.

### Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models 
[[arxiv](https://arxiv.org/abs/2502.19417)] [[cool](https://papers.cool/arxiv/2502.19417)] [[pdf](https://arxiv.org/pdf/2502.19417)]
> **Authors**: Lucy Xiaoyang Shi,Brian Ichter,Michael Equi,Liyiming Ke,Karl Pertsch,Quan Vuong,James Tanner,Anna Walling,Haohuan Wang,Niccolo Fusai,Adrian Li-Bell,Danny Driess,Lachy Groom,Sergey Levine,Chelsea Finn
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that's not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.

### ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration 
[[arxiv](https://arxiv.org/abs/2502.19250)] [[cool](https://papers.cool/arxiv/2502.19250)] [[pdf](https://arxiv.org/pdf/2502.19250)]
> **Authors**: Minjie Zhu,Yichen Zhu,Jinming Li,Zhongyi Zhou,Junjie Wen,Xiaoyu Liu,Chaomin Shen,Yaxin Peng,Feifei Feng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Project page at https://objectvla.github.io/
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Imitation learning has proven to be highly effective in teaching robots dexterous manipulation skills. However, it typically relies on large amounts of human demonstration data, which limits its scalability and applicability in dynamic, real-world environments. One key challenge in this context is object generalization, where a robot trained to perform a task with one object, such as "hand over the apple," struggles to transfer its skills to a semantically similar but visually different object, such as "hand over the peach." This gap in generalization to new objects beyond those in the same category has yet to be adequately addressed in previous work on end-to-end visuomotor policy learning. In this paper, we present a simple yet effective approach for achieving object generalization through Vision-Language-Action (VLA) models, referred to as \textbf{ObjectVLA}. Our model enables robots to generalize learned skills to novel objects without requiring explicit human demonstrations for each new target object. By leveraging vision-language pair data, our method provides a lightweight and scalable way to inject knowledge about the target object, establishing an implicit link between the object and the desired action. We evaluate ObjectVLA on a real robotic platform, demonstrating its ability to generalize across 100 novel objects with a 64\% success rate in selecting objects not seen during training. Furthermore, we propose a more accessible method for enhancing object generalization in VLA models, using a smartphone to capture a few images and fine-tune the pre-trained model. These results highlight the effectiveness of our approach in enabling object-level generalization and reducing the need for extensive human demonstrations, paving the way for more flexible and scalable robotic learning systems.

### SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images 
[[arxiv](https://arxiv.org/abs/2502.18932)] [[cool](https://papers.cool/arxiv/2502.18932)] [[pdf](https://arxiv.org/pdf/2502.18932)]
> **Authors**: Yangfan Xu,Qu Hao,Lilian Zhang,Jun Mao,Xiaofeng He,Wenqi Wu,Changhao Chen
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, but traditional RGB camera systems struggle in low-light conditions, driving interest in thermal SLAM, which excels in such environments. However, thermal imaging faces challenges like low contrast, high noise, and limited large-scale annotated datasets, restricting the use of deep learning in outdoor scenarios. We present DarkSLAM, a noval deep learning-based monocular thermal SLAM system designed for large-scale localization and reconstruction in complex lighting conditions.Our approach incorporates the Efficient Channel Attention (ECA) mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanism in depth estimation to enhance pose accuracy and mitigate thermal depth degradation. Additionally, the system includes thermal depth-based loop closure detection and pose optimization, ensuring robust performance in low-texture thermal scenes. Extensive outdoor experiments demonstrate that DarkSLAM significantly outperforms existing methods like SC-Sfm-Learner and Shin et al., delivering precise localization and 3D dense mapping even in challenging nighttime environments.

### Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2502.18842)] [[cool](https://papers.cool/arxiv/2502.18842)] [[pdf](https://arxiv.org/pdf/2502.18842)]
> **Authors**: Muhammad A. Muttaqien,Tomohiro Motoda,Ryo Hanai,Domae Yukiyasu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 2025 IEEE/SICE International Symposium on System Integration
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: This paper introduces a novel pipeline to enhance the precision of object masking for robotic manipulation within the specific domain of masking products in convenience stores. The approach integrates two advanced AI models, CLIP and SAM, focusing on their synergistic combination and the effective use of multimodal data (image and text). Emphasis is placed on utilizing gradient-based attention mechanisms and customized datasets to fine-tune performance. While CLIP, SAM, and Grad- CAM are established components, their integration within this structured pipeline represents a significant contribution to the field. The resulting segmented masks, generated through this combined approach, can be effectively utilized as inputs for robotic systems, enabling more precise and adaptive object manipulation in the context of convenience store products.

## 声音(cs.SD:Sound)

### DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.18952)] [[cool](https://papers.cool/arxiv/2502.18952)] [[pdf](https://arxiv.org/pdf/2502.18952)]
> **Authors**: Lei Zhao,Sizhou Chen,Linfeng Feng,Xiao-Lei Zhang,Xuelong Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Text-to-audio (TTA), which generates audio signals from textual descriptions, has received huge attention in recent years. However, recent works focused on text to monaural audio only. As we know, spatial audio provides more immersive auditory experience than monaural audio, e.g. in virtual reality. To address this issue, we propose a text-to-spatial-audio (TTSA) generation framework named DualSpec.Specifically, it first trains variational autoencoders (VAEs) for extracting the latent acoustic representations from sound event audio. Then, given text that describes sound events and event directions, the proposed method uses the encoder of a pretrained large language model to transform the text into text features. Finally, it trains a diffusion model from the latent acoustic representations and text features for the spatial audio generation. In the inference stage, only the text description is needed to generate spatial audio. Particularly, to improve the synthesis quality and azimuth accuracy of the spatial sound events simultaneously, we propose to use two kinds of acoustic features. One is the Mel spectrograms which is good for improving the synthesis quality, and the other is the short-time Fourier transform spectrograms which is good at improving the azimuth accuracy. We provide a pipeline of constructing spatial audio dataset with text prompts, for the training of the VAEs and diffusion model. We also introduce new spatial-aware evaluation metrics to quantify the azimuth errors of the generated spatial audio recordings. Experimental results demonstrate that the proposed method can generate spatial audio with high directional and event consistency.

### Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality Text-to-Speech Method based on Contextual Semantic Understanding 
[[arxiv](https://arxiv.org/abs/2502.18889)] [[cool](https://papers.cool/arxiv/2502.18889)] [[pdf](https://arxiv.org/pdf/2502.18889)]
> **Authors**: Tianyun Liu
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,人机交互,机器学习,音频和语音处理
- **Abstract**: Traditional text-to-speech (TTS) methods primarily focus on establishing a mapping between phonemes and mel-spectrograms. However, during the phoneme encoding stage, there is often a lack of real mel-spectrogram auxiliary information, which results in the encoding process lacking true semantic understanding. At the same time, traditional TTS systems often struggle to balance the inference speed of the model with the quality of the synthesized speech. Methods that generate high-quality synthesized speech tend to have slower inference speeds, while faster inference methods often sacrifice speech quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip architecture. This method uses the Clip framework to establish a connection between text content and real mel-spectrograms during the text encoding stage, enabling the text encoder to directly learn the true semantics of the global context, thereby ensuring the quality of the synthesized speech. In terms of model architecture, I adopt the basic structure of Transformer, which allows Clip-TTS to achieve fast inference speeds. Experimental results show that on the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves state-of-the-art MOS scores, and it also performs excellently on multi-emotion datasets.Audio samples are available at: https://ltydd1314.github.io/.

## 软件工程(cs.SE:Software Engineering)

### Bridging the PLC Binary Analysis Gap: A Cross-Compiler Dataset and Neural Framework for Industrial Control Systems 
[[arxiv](https://arxiv.org/abs/2502.19725)] [[cool](https://papers.cool/arxiv/2502.19725)] [[pdf](https://arxiv.org/pdf/2502.19725)]
> **Authors**: Yonatan Gizachew Achamyeleh,Shih-Yuan Yu,Gustavo Quirós Araya,Mohammad Abdullah Al Faruque
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Industrial Control Systems (ICS) rely heavily on Programmable Logic Controllers (PLCs) to manage critical infrastructure, yet analyzing PLC executables remains challenging due to diverse proprietary compilers and limited access to source code. To bridge this gap, we introduce PLC-BEAD, a comprehensive dataset containing 2431 compiled binaries from 700+ PLC programs across four major industrial compilers (CoDeSys, GEB, OpenPLC-V2, OpenPLC-V3). This novel dataset uniquely pairs each binary with its original Structured Text source code and standardized functionality labels, enabling both binary-level and source-level analysis. We demonstrate the dataset's utility through PLCEmbed, a transformer-based framework for binary code analysis that achieves 93\% accuracy in compiler provenance identification and 42\% accuracy in fine-grained functionality classification across 22 industrial control categories. Through comprehensive ablation studies, we analyze how compiler optimization levels, code patterns, and class distributions influence model performance. We provide detailed documentation of the dataset creation process, labeling taxonomy, and benchmark protocols to ensure reproducibility. Both PLC-BEAD and PLCEmbed are released as open-source resources to foster research in PLC security, reverse engineering, and ICS forensics, establishing new baselines for data-driven approaches to industrial cybersecurity.

### Accessing LLMs for Front-end Software Architecture Knowledge 
[[arxiv](https://arxiv.org/abs/2502.19518)] [[cool](https://papers.cool/arxiv/2502.19518)] [[pdf](https://arxiv.org/pdf/2502.19518)]
> **Authors**: L. P. Franciscatto Guerra,N. Ernst
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 4 pages, 1 figure, to appear in the International Workshop on Designing Software at ICSE 2025
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools.

### Learning Code-Edit Embedding to Model Student Debugging Behavior 
[[arxiv](https://arxiv.org/abs/2502.19407)] [[cool](https://papers.cool/arxiv/2502.19407)] [[pdf](https://arxiv.org/pdf/2502.19407)]
> **Authors**: Hasnain Heickal,Andrew Lan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.

### CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.19166)] [[cool](https://papers.cool/arxiv/2502.19166)] [[pdf](https://arxiv.org/pdf/2502.19166)]
> **Authors**: Kaiwen Yan,Hongcheng Guo,Xuanqing Shi,Jingyi Xu,Yaonan Gu,Zhoujun Li
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation.

### Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with PseudoEval 
[[arxiv](https://arxiv.org/abs/2502.19149)] [[cool](https://papers.cool/arxiv/2502.19149)] [[pdf](https://arxiv.org/pdf/2502.19149)]
> **Authors**: Jiarong Wu,Songqiang Chen,Jialun Cao,Hau Ching Lo,Shing-Chi Cheung
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: Existing code generation benchmarks for Large Language Models (LLMs) such as HumanEval and MBPP are designed to study LLMs' end-to-end performance, where the benchmarks feed a problem description in natural language as input and examine the generated code in specific programming languages. However, the evaluation scores revealed in this way provide a little hint as to the bottleneck of the code generation -- whether LLMs are struggling with their problem-solving capability or language-coding capability. To answer this question, we construct PseudoEval, a multilingual code generation benchmark that provides a solution written in pseudocode as input. By doing so, the bottleneck of code generation in various programming languages could be isolated and identified. Our study yields several interesting findings. For example, we identify that the bottleneck of LLMs in Python programming is problem-solving, while Rust is struggling relatively more in language-coding. Also, our study indicates that problem-solving capability may transfer across programming languages, while language-coding needs more language-specific effort, especially for undertrained programming languages. Finally, we release the pipeline of constructing PseudoEval to facilitate the extension to existing benchmarks. PseudoEval is available at: https://anonymous.4open.science/r/PseudocodeACL25-7B74.

### XSS Adversarial Attacks Based on Deep Reinforcement Learning: A Replication and Extension Study 
[[arxiv](https://arxiv.org/abs/2502.19095)] [[cool](https://papers.cool/arxiv/2502.19095)] [[pdf](https://arxiv.org/pdf/2502.19095)]
> **Authors**: Samuele Pasini,Gianluca Maragliano,Jinhan Kim,Paolo Tonella
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全
- **Abstract**: Cross-site scripting (XSS) poses a significant threat to web application security. While Deep Learning (DL) has shown remarkable success in detecting XSS attacks, it remains vulnerable to adversarial attacks due to the discontinuous nature of its input-output mapping. These adversarial attacks employ mutation-based strategies for different components of XSS attack vectors, allowing adversarial agents to iteratively select mutations to evade detection. Our work replicates a state-of-the-art XSS adversarial attack, highlighting threats to validity in the reference work and extending it toward a more effective evaluation strategy. Moreover, we introduce an XSS Oracle to mitigate these threats. The experimental results show that our approach achieves an escape rate above 96% when the threats to validity of the replicated technique are addressed.

### IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across Indic Languages 
[[arxiv](https://arxiv.org/abs/2502.19067)] [[cool](https://papers.cool/arxiv/2502.19067)] [[pdf](https://arxiv.org/pdf/2502.19067)]
> **Authors**: Ujjwal Singh,Aditi Sharma,Nikhil Gupta,Deepakshi,Vivek Kumar Jha
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation from natural language prompts, revolutionizing software development workflows. As we advance towards agent-based development paradigms, these models form the cornerstone of next-generation software development lifecycles. However, current benchmarks for evaluating multilingual code generation capabilities are predominantly English-centric, limiting their applicability across the global developer community. To address this limitation, we present IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\% of the world's population. Our benchmark bridges these languages with 12 programming languages, creating a robust evaluation framework. This work is particularly significant given India's representation of one-eighth of the global population and the crucial role Indic languages play in Indian society. IndicEval-XL represents a significant step toward expanding the linguistic diversity in code generation systems and evaluation frameworks. By developing resources that support multiple languages, we aim to make AI-powered development tools more inclusive and accessible to developers of various linguistic backgrounds. To facilitate further research and development in this direction, we make our dataset and evaluation benchmark publicly available at https://github.com/telekom/IndicEval-XL

### Automated Code Generation and Validation for Software Components of Microcontrollers 
[[arxiv](https://arxiv.org/abs/2502.18905)] [[cool](https://papers.cool/arxiv/2502.18905)] [[pdf](https://arxiv.org/pdf/2502.18905)]
> **Authors**: Sebastian Haug,Christoph Böhm,Daniel Mayer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Sebastian Haug: This paper, spanning 12 pages with 5 figures, presents my work on automated code generation and validation for STM32F407 microcontroller software components. Developed as part of a research project at Munich University of Applied Sciences and AGSOTEC GmbH, it leverages AST and RAG to streamline embedded development. Includes glossary and bibliography as supplementary materials
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: This paper proposes a method for generating software components for embedded systems, integrating seamlessly into existing implementations without developer intervention. We demonstrate this by automatically generating hardware abstraction layer (HAL) code for GPIO operations on the STM32F407 microcontroller. Using Abstract Syntax Trees (AST) for code analysis and Retrieval-Augmented Generation (RAG) for component generation, our approach enables autonomous code completion for embedded applications.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Extending the Hegselmann-Krause Model of Opinion Dynamics to include AI Oracles 
[[arxiv](https://arxiv.org/abs/2502.19701)] [[cool](https://papers.cool/arxiv/2502.19701)] [[pdf](https://arxiv.org/pdf/2502.19701)]
> **Authors**: Allen G. Rodrigo
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能,计算机与社会
- **Abstract**: The Hegselmann-Krause (HK) model of opinion dynamics describes how opinions held by individuals in a community change over time in response to the opinions of others and their access to the true value, T, to which these opinions relate. Here, I extend the simple HK model to incorporate an Artificially Intelligent (AI) Oracle that averages the opinions of members of the community. Agent-based simulations show that (1) if individuals only have access to the Oracle (and not T), and incorporate the Oracle's opinion as they update their opinions, then all opinions will converge on a common value; (2) in contrast, if all individuals also have access to T, then all opinions will ultimately converge to T, but the presence of an Oracle may delay the time to convergence; (3) if only some individuals have access to T, opinions may not converge to T, but under certain conditions, universal access to the Oracle will guarantee convergence to T; and (4) whether or not the Oracle only accesses the opinions of individuals who have access to T, or whether it accesses the opinions of everyone in the community, makes no marked difference to the extent to which the average opinion differs from T.

### Simulation of Language Evolution under Regulated Social Media Platforms: A Synergistic Approach of Large Language Models and Genetic Algorithms 
[[arxiv](https://arxiv.org/abs/2502.19193)] [[cool](https://papers.cool/arxiv/2502.19193)] [[pdf](https://arxiv.org/pdf/2502.19193)]
> **Authors**: Jinyu Cai,Yusei Ishimizu,Mingyue Zhang,Munan Li,Jialong Li,Kenji Tei
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: The manuscript has been submitted to IEEE Transactions on Computational Social Systems
- **标题**: None
- **领域**: 社交和信息网络,人工智能,神经和进化计算
- **Abstract**: Social media platforms frequently impose restrictive policies to moderate user content, prompting the emergence of creative evasion language strategies. This paper presents a multi-agent framework based on Large Language Models (LLMs) to simulate the iterative evolution of language strategies under regulatory constraints. In this framework, participant agents, as social media users, continuously evolve their language expression, while supervisory agents emulate platform-level regulation by assessing policy violations. To achieve a more faithful simulation, we employ a dual design of language strategies (constraint and expression) to differentiate conflicting goals and utilize an LLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of language strategies. The framework is evaluated using two distinct scenarios: an abstract password game and a realistic simulated illegal pet trade scenario. Experimental results demonstrate that as the number of dialogue rounds increases, both the number of uninterrupted dialogue turns and the accuracy of information transmission improve significantly. Furthermore, a user study with 40 participants validates the real-world relevance of the generated dialogues and strategies. Moreover, ablation studies validate the importance of the GA, emphasizing its contribution to long-term adaptability and improved overall results.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2502.18924)] [[cool](https://papers.cool/arxiv/2502.18924)] [[pdf](https://arxiv.org/pdf/2502.18924)]
> **Authors**: Ziyue Jiang,Yi Ren,Ruiqi Li,Shengpeng Ji,Zhenhui Ye,Chen Zhang,Bai Jionghao,Xiaoda Yang,Jialong Zuo,Yu Zhang,Rui Liu,Xiang Yin,Zhou Zhao
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习,声音
- **Abstract**: While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{S-DiT}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to S-DiT to reduce the difficulty of alignment learning without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that S-DiT achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.

## 图像和视频处理(eess.IV:Image and Video Processing)

### A Residual Multi-task Network for Joint Classification and Regression in Medical Imaging 
[[arxiv](https://arxiv.org/abs/2502.19692)] [[cool](https://papers.cool/arxiv/2502.19692)] [[pdf](https://arxiv.org/pdf/2502.19692)]
> **Authors**: Junji Lin,Yi Zhang,Yunyue Pan,Yuli Chen,Chengchang Pan,Honggang Qi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Detection and classification of pulmonary nodules is a challenge in medical image analysis due to the variety of shapes and sizes of nodules and their high concealment. Despite the success of traditional deep learning methods in image classification, deep networks still struggle to perfectly capture subtle changes in lung nodule detection. Therefore, we propose a residual multi-task network (Res-MTNet) model, which combines multi-task learning and residual learning, and improves feature representation ability by sharing feature extraction layer and introducing residual connections. Multi-task learning enables the model to handle multiple tasks simultaneously, while the residual module solves the problem of disappearing gradients, ensuring stable training of deeper networks and facilitating information sharing between tasks. Res-MTNet enhances the robustness and accuracy of the model, providing a more reliable lung nodule analysis tool for clinical medicine and telemedicine.

### Dual-branch Graph Feature Learning for NLOS Imaging 
[[arxiv](https://arxiv.org/abs/2502.19683)] [[cool](https://papers.cool/arxiv/2502.19683)] [[pdf](https://arxiv.org/pdf/2502.19683)]
> **Authors**: Xiongfei Su,Tianyi Zhu,Lina Liu,Zheng Chen,Yulun Zhang,Siyuan Li,Juntian Ye,Feihu Xu,Xin Yuan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The domain of non-line-of-sight (NLOS) imaging is advancing rapidly, offering the capability to reveal occluded scenes that are not directly visible. However, contemporary NLOS systems face several significant challenges: (1) The computational and storage requirements are profound due to the inherent three-dimensional grid data structure, which restricts practical application. (2) The simultaneous reconstruction of albedo and depth information requires a delicate balance using hyperparameters in the loss function, rendering the concurrent reconstruction of texture and depth information difficult. This paper introduces the innovative methodology, \xnet, which integrates an albedo-focused reconstruction branch dedicated to albedo information recovery and a depth-focused reconstruction branch that extracts geometrical structure, to overcome these obstacles. The dual-branch framework segregates content delivery to the respective reconstructions, thereby enhancing the quality of the retrieved data. To our knowledge, we are the first to employ the GNN as a fundamental component to transform dense NLOS grid data into sparse structural features for efficient reconstruction. Comprehensive experiments demonstrate that our method attains the highest level of performance among existing methods across synthetic and real data. https://github.com/Nicholassu/DG-NLOS.

### GONet: A Generalizable Deep Learning Model for Glaucoma Detection 
[[arxiv](https://arxiv.org/abs/2502.19514)] [[cool](https://papers.cool/arxiv/2502.19514)] [[pdf](https://arxiv.org/pdf/2502.19514)]
> **Authors**: Or Abramovich,Hadas Pizem,Jonathan Fhima,Eran Berkowitz,Ben Gofrit,Meishar Meisel,Meital Baskin,Jan Van Eijgen,Ingeborg Stalmans,Eytan Z. Blumenthal,Joachim A. Behar
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 9 pages, 4 figures, submitted to IEEE Transactions on Biomedical Engineering
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Glaucomatous optic neuropathy (GON) is a prevalent ocular disease that can lead to irreversible vision loss if not detected early and treated. The traditional diagnostic approach for GON involves a set of ophthalmic examinations, which are time-consuming and require a visit to an ophthalmologist. Recent deep learning models for automating GON detection from digital fundus images (DFI) have shown promise but often suffer from limited generalizability across different ethnicities, disease groups and examination settings. To address these limitations, we introduce GONet, a robust deep learning model developed using seven independent datasets, including over 119,000 DFIs with gold-standard annotations and from patients of diverse geographic backgrounds. GONet consists of a DINOv2 pre-trained self-supervised vision transformers fine-tuned using a multisource domain strategy. GONet demonstrated high out-of-distribution generalizability, with an AUC of 0.85-0.99 in target domains. GONet performance was similar or superior to state-of-the-art works and was significantly superior to the cup-to-disc ratio, by up to 21.6%. GONet is available at [URL provided on publication]. We also contribute a new dataset consisting of 768 DFI with GON labels as open access.

### SPU-IMR: Self-supervised Arbitrary-scale Point Cloud Upsampling via Iterative Mask-recovery Network 
[[arxiv](https://arxiv.org/abs/2502.19452)] [[cool](https://papers.cool/arxiv/2502.19452)] [[pdf](https://arxiv.org/pdf/2502.19452)]
> **Authors**: Ziming Nie,Qiao Wu,Chenlei Lv,Siwen Quan,Zhaoshuai Qi,Muze Wang,Jiaqi Yang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Point cloud upsampling aims to generate dense and uniformly distributed point sets from sparse point clouds. Existing point cloud upsampling methods typically approach the task as an interpolation problem. They achieve upsampling by performing local interpolation between point clouds or in the feature space, then regressing the interpolated points to appropriate positions. By contrast, our proposed method treats point cloud upsampling as a global shape completion problem. Specifically, our method first divides the point cloud into multiple patches. Then, a masking operation is applied to remove some patches, leaving visible point cloud patches. Finally, our custom-designed neural network iterative completes the missing sections of the point cloud through the visible parts. During testing, by selecting different mask sequences, we can restore various complete patches. A sufficiently dense upsampled point cloud can be obtained by merging all the completed patches. We demonstrate the superior performance of our method through both quantitative and qualitative experiments, showing overall superiority against both existing self-supervised and supervised methods.

### Multispectral to Hyperspectral using Pretrained Foundational model 
[[arxiv](https://arxiv.org/abs/2502.19451)] [[cool](https://papers.cool/arxiv/2502.19451)] [[pdf](https://arxiv.org/pdf/2502.19451)]
> **Authors**: Ruben Gonzalez,Conrad M Albrecht,Nassim Ait Ali Braham,Devyani Lambhate,Joao Lucas de Sousa Almeida,Paolo Fraccaro,Benedikt Blumenstiel,Thomas Brunschwiler,Ranjini Bangalore
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能
- **Abstract**: Hyperspectral imaging provides detailed spectral information, offering significant potential for monitoring greenhouse gases like CH4 and NO2. However, its application is constrained by limited spatial coverage and infrequent revisit times. In contrast, multispectral imaging delivers broader spatial and temporal coverage but lacks the spectral granularity required for precise GHG detection. To address these challenges, this study proposes Spectral and Spatial-Spectral transformer models that reconstruct hyperspectral data from multispectral inputs. The models in this paper are pretrained on EnMAP and EMIT datasets and fine-tuned on spatio-temporally aligned (Sentinel-2, EnMAP) and (HLS-S30, EMIT) image pairs respectively. Our model has the potential to enhance atmospheric monitoring by combining the strengths of hyperspectral and multispectral imaging systems.

### Deep Learning-Based Transfer Learning for Classification of Cassava Disease 
[[arxiv](https://arxiv.org/abs/2502.19351)] [[cool](https://papers.cool/arxiv/2502.19351)] [[pdf](https://arxiv.org/pdf/2502.19351)]
> **Authors**: Ademir G. Costa Junior,Fábio S. da Silva,Ricardo Rios
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 pages, in Portugueselanguage, 3 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: This paper presents a performance comparison among four Convolutional Neural Network architectures (EfficientNet-B3, InceptionV3, ResNet50, and VGG16) for classifying cassava disease images. The images were sourced from an imbalanced dataset from a competition. Appropriate metrics were employed to address class imbalance. The results indicate that EfficientNet-B3 achieved on this task accuracy of 87.7%, precision of 87.8%, revocation of 87.8% and F1-Score of 87.7%. These findings suggest that EfficientNet-B3 could be a valuable tool to support Digital Agriculture.

### Deep learning and classical computer vision techniques in medical image analysis: Case studies on brain MRI tissue segmentation, lung CT COPD registration, and skin lesion classification 
[[arxiv](https://arxiv.org/abs/2502.19258)] [[cool](https://papers.cool/arxiv/2502.19258)] [[pdf](https://arxiv.org/pdf/2502.19258)]
> **Authors**: Anyimadu Daniel Tweneboah,Suleiman Taofik Ahmed,Hossain Mohammad Imran
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 27 pages, 18 figures
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Medical imaging spans diverse tasks and modalities which play a pivotal role in disease diagnosis, treatment planning, and monitoring. This study presents a novel exploration, being the first to systematically evaluate segmentation, registration, and classification tasks across multiple imaging modalities. Integrating both classical and deep learning (DL) approaches in addressing brain MRI tissue segmentation, lung CT image registration, and skin lesion classification from dermoscopic images, we demonstrate the complementary strengths of these methodologies in diverse applications. For brain tissue segmentation, 3D DL models outperformed 2D and patch-based models, specifically nnU-Net achieving Dice of 0.9397, with 3D U-Net models on ResNet34 backbone, offering competitive results with Dice 0.8946. Multi-Atlas methods provided robust alternatives for cases where DL methods are not feasible, achieving average Dice of 0.7267. In lung CT registration, classical Elastix-based methods outperformed DL models, achieving a minimum Target Registration Error (TRE) of 6.68 mm, highlighting the effectiveness of parameter tuning. HighResNet performed best among DL models with a TRE of 7.40 mm. For skin lesion classification, ensembles of DL models like InceptionResNetV2 and ResNet50 excelled, achieving up to 90.44%, and 93.62% accuracies for binary and multiclass classification respectively. Also, adopting One-vs-All method, DL attained accuracies of 94.64% (mel vs. others), 95.35% (bcc vs. others), and 96.93% (scc vs. others), while ML models specifically Multi-Layer Perceptron (MLP) on handcrafted features offered interpretable alternatives with 85.04% accuracy using SMOTE for class imbalance correction on the multi-class task and 83.27% on the binary-class task. Links to source code are available on request.

### Multi-level Attention-guided Graph Neural Network for Image Restoration 
[[arxiv](https://arxiv.org/abs/2502.19181)] [[cool](https://papers.cool/arxiv/2502.19181)] [[pdf](https://arxiv.org/pdf/2502.19181)]
> **Authors**: Jiatao Jiang,Zhen Cui,Chunyan Xu,Jian Yang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: In recent years, deep learning has achieved remarkable success in the field of image restoration. However, most convolutional neural network-based methods typically focus on a single scale, neglecting the incorporation of multi-scale information. In image restoration tasks, local features of an image are often insufficient, necessitating the integration of global features to complement them. Although recent neural network algorithms have made significant strides in feature extraction, many models do not explicitly model global features or consider the relationship between global and local features. This paper proposes multi-level attention-guided graph neural network. The proposed network explicitly constructs element block graphs and element graphs within feature maps using multi-attention mechanisms to extract both local structural features and global representation information of the image. Since the network struggles to effectively extract global information during image degradation, the structural information of local feature blocks can be used to correct and supplement the global information. Similarly, when element block information in the feature map is missing, it can be refined using global element representation information. The graph within the network learns real-time dynamic connections through the multi-attention mechanism, and information is propagated and aggregated via graph convolution algorithms. By combining local element block information and global element representation information from the feature map, the algorithm can more effectively restore missing information in the image. Experimental results on several classic image restoration tasks demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance.

### RetinaRegen: A Hybrid Model for Readability and Detail Restoration in Fundus Images 
[[arxiv](https://arxiv.org/abs/2502.19153)] [[cool](https://papers.cool/arxiv/2502.19153)] [[pdf](https://arxiv.org/pdf/2502.19153)]
> **Authors**: Yuhan Tang,Yudian Wang,Weizhen Li,Ye Yue,Chengchang Pan,Honggang Qi
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Fundus image quality is crucial for diagnosing eye diseases, but real-world conditions often result in blurred or unreadable images, increasing diagnostic uncertainty. To address these challenges, this study proposes RetinaRegen, a hybrid model for retinal image restoration that integrates a readability classifi-cation model, a Diffusion Model, and a Variational Autoencoder (VAE). Ex-periments on the SynFundus-1M dataset show that the proposed method achieves a PSNR of 27.4521, an SSIM of 0.9556, and an LPIPS of 0.1911 for the readability labels of the optic disc (RO) region. These results demonstrate superior performance in restoring key regions, offering an effective solution to enhance fundus image quality and support clinical diagnosis.

### From Traditional to Deep Learning Approaches in Whole Slide Image Registration: A Methodological Review 
[[arxiv](https://arxiv.org/abs/2502.19123)] [[cool](https://papers.cool/arxiv/2502.19123)] [[pdf](https://arxiv.org/pdf/2502.19123)]
> **Authors**: Behnaz Elhaminia,Abdullah Alsalemi,Esha Nasir,Mostafa Jahanifar,Ruqayya Awan,Lawrence S. Young,Nasir M. Rajpoot,Fayyaz Minhas,Shan E Ahmed Raza
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Whole slide image (WSI) registration is an essential task for analysing the tumour microenvironment (TME) in histopathology. It involves the alignment of spatial information between WSIs of the same section or serial sections of a tissue sample. The tissue sections are usually stained with single or multiple biomarkers before imaging, and the goal is to identify neighbouring nuclei along the Z-axis for creating a 3D image or identifying subclasses of cells in the TME. This task is considerably more challenging compared to radiology image registration, such as magnetic resonance imaging or computed tomography, due to various factors. These include gigapixel size of images, variations in appearance between differently stained tissues, changes in structure and morphology between non-consecutive sections, and the presence of artefacts, tears, and deformations. Currently, there is a noticeable gap in the literature regarding a review of the current approaches and their limitations, as well as the challenges and opportunities they present. We aim to provide a comprehensive understanding of the available approaches and their application for various purposes. Furthermore, we investigate current deep learning methods used for WSI registration, emphasising their diverse methodologies. We examine the available datasets and explore tools and software employed in the field. Finally, we identify open challenges and potential future trends in this area of research.

## 信号处理(eess.SP:Signal Processing)

### SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.19668)] [[cool](https://papers.cool/arxiv/2502.19668)] [[pdf](https://arxiv.org/pdf/2502.19668)]
> **Authors**: Mingsheng Cai,Jiuming Jiang,Wenhao Huang,Che Liu,Rossella Arcucci
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,计算语言学,机器学习
- **Abstract**: Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a $\textbf{Su}$pervised $\textbf{Pre}$-training framework for $\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME applies Large Language Models (LLMs) to extract structured clinical entities from free-text ECG reports, filter out noise and irrelevant content, enhance clinical representation learning, and build a high-quality, fine-grained labeled dataset. By using text-based cardiac queries instead of traditional categorical labels, SuPreME enables zero-shot classification of unseen diseases without additional fine-tuning. We evaluate SuPreME on six downstream datasets covering 127 cardiac conditions, achieving superior zero-shot AUC performance over state-of-the-art eSSL and multimodal methods by over 1.96\%. Results demonstrate the effectiveness of SuPreME in leveraging structured, clinically relevant knowledge for high-quality ECG representations. All code and data will be released upon acceptance.

### Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces 
[[arxiv](https://arxiv.org/abs/2502.19281)] [[cool](https://papers.cool/arxiv/2502.19281)] [[pdf](https://arxiv.org/pdf/2502.19281)]
> **Authors**: Jiyuan Wang,Weishan Ye,Jialin He,Li Zhang,Gan Huang,Zhuliang Yu,Zhen Liang
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习
- **Abstract**: With the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application.

## 优化与控制(math.OC:Optimization and Control)

### Langevin Multiplicative Weights Update with Applications in Polynomial Portfolio Management 
[[arxiv](https://arxiv.org/abs/2502.19210)] [[cool](https://papers.cool/arxiv/2502.19210)] [[pdf](https://arxiv.org/pdf/2502.19210)]
> **Authors**: Yi Feng,Xiao Wang,Tian Xie
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Accepted for AAAI-2025
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: We consider nonconvex optimization problem over simplex, and more generally, a product of simplices. We provide an algorithm, Langevin Multiplicative Weights Update (LMWU) for solving global optimization problems by adding a noise scaling with the non-Euclidean geometry in the simplex. Non-convex optimization has been extensively studied by machine learning community due to its application in various scenarios such as neural network approximation and finding Nash equilibrium. Despite recent progresses on provable guarantee of escaping and avoiding saddle point (convergence to local minima) and global convergence of Langevin gradient based method without constraints, the global optimization with constraints is less studied. We show that LMWU algorithm is provably convergent to interior global minima with a non-asymptotic convergence analysis. We verify the efficiency of the proposed algorithm in real data set from polynomial portfolio management, where optimization of a highly non-linear objective function plays a crucial role.

## 可能性(math.PR:Probability)

### Stationary distribution of node2vec random walks on household models 
[[arxiv](https://arxiv.org/abs/2502.19039)] [[cool](https://papers.cool/arxiv/2502.19039)] [[pdf](https://arxiv.org/pdf/2502.19039)]
> **Authors**: Lars Schroeder,Clara Stegehuis
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 可能性,机器学习,社交和信息网络
- **Abstract**: The node2vec random walk has proven to be a key tool in network embedding algorithms. These random walks are tuneable, and their transition probabilities depend on the previous visited node and on the triangles containing the current and the previously visited node. Even though these walks are widely used in practice, most mathematical properties of node2vec walks are largely unexplored, including their stationary distribution. We study the node2vec random walk on community-structured household model graphs. We prove an explicit description of the stationary distribution of node2vec walks in terms of the walk parameters. We then show that by tuning the walk parameters, the stationary distribution can interpolate between uniform, size-biased, or the simple random walk stationary distributions, demonstrating the wide range of possible walks. We further explore these effects on some specific graph settings.

## 应用物理(physics.app-ph:Applied Physics)

### Blending Optimal Control and Biologically Plausible Learning for Noise-Robust Physical Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.19053)] [[cool](https://papers.cool/arxiv/2502.19053)] [[pdf](https://arxiv.org/pdf/2502.19053)]
> **Authors**: Satoshi Sunada,Tomoaki Niiyama,Kazutaka Kanno,Rin Nogami,André Röhm,Takato Awano,Atsushi Uchida
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 28 pages, 10 figures
- **标题**: None
- **领域**: 应用物理,新兴技术,机器学习,神经和进化计算
- **Abstract**: The rapidly increasing computational demands for artificial intelligence (AI) have spurred the exploration of computing principles beyond conventional digital computers. Physical neural networks (PNNs) offer efficient neuromorphic information processing by harnessing the innate computational power of physical processes; however, training their weight parameters is computationally expensive. We propose a training approach for substantially reducing this training cost. Our training approach merges an optimal control method for continuous-time dynamical systems with a biologically plausible training method--direct feedback alignment. In addition to the reduction of training time, this approach achieves robust processing even under measurement errors and noise without requiring detailed system information. The effectiveness was numerically and experimentally verified in an optoelectronic delay system. Our approach significantly extends the range of physical systems practically usable as PNNs.

## 化学物理(physics.chem-ph:Chemical Physics)

### Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems 
[[arxiv](https://arxiv.org/abs/2502.19227)] [[cool](https://papers.cool/arxiv/2502.19227)] [[pdf](https://arxiv.org/pdf/2502.19227)]
> **Authors**: Yunyang Li,Zaishuo Xia,Lin Huang,Xinran Wei,Han Yang,Sam Harshe,Zun Wang,Chang Liu,Jia Zhang,Bin Shao,Mark B. Gerstein
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 化学物理,材料科学,人工智能,机器学习
- **Abstract**: Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems.

### Graph Neural Networks embedded into Margules model for vapor-liquid equilibria prediction 
[[arxiv](https://arxiv.org/abs/2502.18998)] [[cool](https://papers.cool/arxiv/2502.18998)] [[pdf](https://arxiv.org/pdf/2502.18998)]
> **Authors**: Edgar Ivan Sanchez Medina,Kai Sundmacher
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 化学物理,机器学习
- **Abstract**: Predictive thermodynamic models are crucial for the early stages of product and process design. In this paper the performance of Graph Neural Networks (GNNs) embedded into a relatively simple excess Gibbs energy model, the extended Margules model, for predicting vapor-liquid equilibrium is analyzed. By comparing its performance against the established UNIFAC-Dortmund model it has been shown that GNNs embedded in Margules achieves an overall lower accuracy. However, higher accuracy is observed in the case of various types of binary mixtures. Moreover, since group contribution methods, like UNIFAC, are limited due to feasibility of molecular fragmentation or availability of parameters, the GNN in Margules model offers an alternative for VLE estimation. The findings establish a baseline for the predictive accuracy that simple excess Gibbs energy models combined with GNNs trained solely on infinite dilution data can achieve.

## 地球物理学(physics.geo-ph:Geophysics)

### Data-Driven and Theory-Guided Pseudo-Spectral Seismic Imaging Using Deep Neural Network Architectures 
[[arxiv](https://arxiv.org/abs/2502.18852)] [[cool](https://papers.cool/arxiv/2502.18852)] [[pdf](https://arxiv.org/pdf/2502.18852)]
> **Authors**: Christopher Zerafa
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 163 pages, 91 figures, 16 tables, PhD Thesis, https://www.um.edu.mt/library/oar/handle/123456789/91968
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: Full Waveform Inversion (FWI) reconstructs high-resolution subsurface models via multi-variate optimization but faces challenges with solver selection and data availability. Deep Learning (DL) offers a promising alternative, bridging data-driven and physics-based methods. While FWI in DL has been explored in the time domain, the pseudo-spectral approach remains underutilized, despite its success in classical FWI. This thesis integrates pseudo-spectral FWI into DL, formulating both data-driven and theory-guided approaches using Deep Neural Networks (DNNs) and Recurrent Neural Networks (RNNs). These methods were theoretically derived, tested on synthetic and Marmousi datasets, and compared with deterministic and time-domain approaches. Results show that data-driven pseudo-spectral DNNs outperform classical FWI in deeper and over-thrust regions due to their global approximation capability. Theory-guided RNNs yield greater accuracy, with lower error and better fault identification. While DNNs excel in velocity contrast recovery, RNNs provide superior edge definition and stability in shallow and deep sections. Beyond enhancing FWI performance, this research identifies broader applications of DL-based inversion and outlines future directions for these frameworks.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### FPGA-Accelerated SpeckleNN with SNL for Real-time X-ray Single-Particle Imaging 
[[arxiv](https://arxiv.org/abs/2502.19734)] [[cool](https://papers.cool/arxiv/2502.19734)] [[pdf](https://arxiv.org/pdf/2502.19734)]
> **Authors**: Abhilasha Dave,Cong Wang,James Russell,Ryan Herbst,Jana Thayer
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 仪器仪表和探测器,机器学习,图像和视频处理
- **Abstract**: We implement a specialized version of our SpeckleNN model for real-time speckle pattern classification in X-ray Single-Particle Imaging (SPI) using the SLAC Neural Network Library (SNL) on an FPGA. This hardware is optimized for inference near detectors in high-throughput X-ray free-electron laser (XFEL) facilities like the Linac Coherent Light Source (LCLS). To fit FPGA constraints, we optimized SpeckleNN, reducing parameters from 5.6M to 64.6K (98.8% reduction) with 90% accuracy. We also compressed the latent space from 128 to 50 dimensions. Deployed on a KCU1500 FPGA, the model used 71% of DSPs, 75% of LUTs, and 48% of FFs, with an average power consumption of 9.4W. The FPGA achieved 45.015us inference latency at 200 MHz. On an NVIDIA A100 GPU, the same inference consumed ~73W and had a 400us latency. Our FPGA version achieved an 8.9x speedup and 7.8x power reduction over the GPU. Key advancements include model specialization and dynamic weight loading through SNL, eliminating time-consuming FPGA re-synthesis for fast, continuous deployment of (re)trained models. These innovations enable real-time adaptive classification and efficient speckle pattern vetoing, making SpeckleNN ideal for XFEL facilities. This implementation accelerates SPI experiments and enhances adaptability to evolving conditions.

## 生物分子(q-bio.BM:Biomolecules)

### Fast and Accurate Antibody Sequence Design via Structure Retrieval 
[[arxiv](https://arxiv.org/abs/2502.19395)] [[cool](https://papers.cool/arxiv/2502.19395)] [[pdf](https://arxiv.org/pdf/2502.19395)]
> **Authors**: Xingyi Zhang,Kun Xie,Ningqiao Huang,Wei Liu,Peilin Zhao,Sibo Wang,Kangfei Zhao,Biaobin Jiang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,机器学习
- **Abstract**: Recent advancements in protein design have leveraged diffusion models to generate structural scaffolds, followed by a process known as protein inverse folding, which involves sequence inference on these scaffolds. However, these methodologies face significant challenges when applied to hyper-variable structures such as antibody Complementarity-Determining Regions (CDRs), where sequence inference frequently results in non-functional sequences due to hallucinations. Distinguished from prevailing protein inverse folding approaches, this paper introduces Igseek, a novel structure-retrieval framework that infers CDR sequences by retrieving similar structures from a natural antibody database. Specifically, Igseek employs a simple yet effective multi-channel equivariant graph neural network to generate high-quality geometric representations of CDR backbone structures. Subsequently, it aligns sequences of structurally similar CDRs and utilizes structurally conserved sequence motifs to enhance inference accuracy. Our experiments demonstrate that Igseek not only proves to be highly efficient in structural retrieval but also outperforms state-of-the-art approaches in sequence recovery for both antibodies and T-Cell Receptors, offering a new retrieval-based perspective for therapeutic protein design.

### Towards More Accurate Full-Atom Antibody Co-Design 
[[arxiv](https://arxiv.org/abs/2502.19391)] [[cool](https://papers.cool/arxiv/2502.19391)] [[pdf](https://arxiv.org/pdf/2502.19391)]
> **Authors**: Jiayang Wu,Xingyi Zhang,Xiangyu Dong,Kun Xie,Ziqi Liu,Wensheng Gan,Sibo Wang,Le Song
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,机器学习
- **Abstract**: Antibody co-design represents a critical frontier in drug development, where accurate prediction of both 1D sequence and 3D structure of complementarity-determining regions (CDRs) is essential for targeting specific epitopes. Despite recent advances in equivariant graph neural networks for antibody design, current approaches often fall short in capturing the intricate interactions that govern antibody-antigen recognition and binding specificity. In this work, we present Igformer, a novel end-to-end framework that addresses these limitations through innovative modeling of antibody-antigen binding interfaces. Our approach refines the inter-graph representation by integrating personalized propagation with global attention mechanisms, enabling comprehensive capture of the intricate interplay between local chemical interactions and global conformational dependencies that characterize effective antibody-antigen binding. Through extensive validation on epitope-binding CDR design and structure prediction tasks, Igformer demonstrates significant improvements over existing methods, suggesting that explicit modeling of multi-scale residue interactions can substantially advance computational antibody design for therapeutic applications.

### SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation 
[[arxiv](https://arxiv.org/abs/2502.18875)] [[cool](https://papers.cool/arxiv/2502.18875)] [[pdf](https://arxiv.org/pdf/2502.18875)]
> **Authors**: Fanglei Xue,Meihan Zhang,Shuqi Li,Xinyu Gao,James A. Wohlschlegel,Wenbing Huang,Yi Yang,Weixian Deng
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,人工智能,机器学习
- **Abstract**: Targeted protein degradation (TPD) induced by small molecules has emerged as a rapidly evolving modality in drug discovery, targeting proteins traditionally considered "undruggable". Proteolysis-targeting chimeras (PROTACs) and molecular glue degraders (MGDs) are the primary small molecules that induce TPD. Both types of molecules form a ternary complex linking an E3 ligase with a target protein, a crucial step for drug discovery. While significant advances have been made in binary structure prediction for proteins and small molecules, ternary structure prediction remains challenging due to obscure interaction mechanisms and insufficient training data. Traditional methods relying on manually assigned rules perform poorly and are computationally demanding due to extensive random sampling. In this work, we introduce DeepTernary, a novel deep learning-based approach that directly predicts ternary structures in an end-to-end manner using an encoder-decoder architecture. DeepTernary leverages an SE(3)-equivariant graph neural network (GNN) with both intra-graph and ternary inter-graph attention mechanisms to capture intricate ternary interactions from our collected high-quality training dataset, TernaryDB. The proposed query-based Pocket Points Decoder extracts the 3D structure of the final binding ternary complex from learned ternary embeddings, demonstrating state-of-the-art accuracy and speed in existing PROTAC benchmarks without prior knowledge from known PROTACs. It also achieves notable accuracy on the more challenging MGD benchmark under the blind docking protocol. Remarkably, our experiments reveal that the buried surface area calculated from predicted structures correlates with experimentally obtained degradation potency-related metrics. Consequently, DeepTernary shows potential in effectively assisting and accelerating the development of TPDs for previously undruggable targets.

## 基因组学(q-bio.GN:Genomics)

### scMamba: A Pre-Trained Model for Single-Nucleus RNA Sequencing Analysis in Neurodegenerative Disorders 
[[arxiv](https://arxiv.org/abs/2502.19429)] [[cool](https://papers.cool/arxiv/2502.19429)] [[pdf](https://arxiv.org/pdf/2502.19429)]
> **Authors**: Gyutaek Oh,Baekgyu Choi,Seyoung Jin,Inkyung Jung,Jong Chul Ye
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-27
> **comment**: 41 pages, 12 figures
- **标题**: None
- **领域**: 基因组学,机器学习
- **Abstract**: Single-nucleus RNA sequencing (snRNA-seq) has significantly advanced our understanding of the disease etiology of neurodegenerative disorders. However, the low quality of specimens derived from postmortem brain tissues, combined with the high variability caused by disease heterogeneity, makes it challenging to integrate snRNA-seq data from multiple sources for precise analyses. To address these challenges, we present scMamba, a pre-trained model designed to improve the quality and utility of snRNA-seq analysis, with a particular focus on neurodegenerative diseases. Inspired by the recent Mamba model, scMamba introduces a novel architecture that incorporates a linear adapter layer, gene embeddings, and bidirectional Mamba blocks, enabling efficient processing of snRNA-seq data while preserving information from the raw input. Notably, scMamba learns generalizable features of cells and genes through pre-training on snRNA-seq data, without relying on dimension reduction or selection of highly variable genes. We demonstrate that scMamba outperforms benchmark methods in various downstream tasks, including cell type annotation, doublet detection, imputation, and the identification of differentially expressed genes.

## 分子网络(q-bio.MN:Molecular Networks)

### Modelling Chemical Reaction Networks using Neural Ordinary Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.19397)] [[cool](https://papers.cool/arxiv/2502.19397)] [[pdf](https://arxiv.org/pdf/2502.19397)]
> **Authors**: Anna C. M. Thöni,William E. Robinson,Yoram Bachrach,Wilhelm T. S. Huck,Tal Kachman
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 分子网络,机器学习
- **Abstract**: In chemical reaction network theory, ordinary differential equations are used to model the temporal change of chemical species concentration. As the functional form of these ordinary differential equations systems is derived from an empirical model of the reaction network, it may be incomplete. Our approach aims to elucidate these hidden insights in the reaction network by combining dynamic modelling with deep learning in the form of neural ordinary differential equations. Our contributions not only help to identify the shortcomings of existing empirical models but also assist the design of future reaction networks.

## 一般财务(q-fin.GN:General Finance)

### A Method for Evaluating the Interpretability of Machine Learning Models in Predicting Bond Default Risk Based on LIME and SHAP 
[[arxiv](https://arxiv.org/abs/2502.19615)] [[cool](https://papers.cool/arxiv/2502.19615)] [[pdf](https://arxiv.org/pdf/2502.19615)]
> **Authors**: Yan Zhang,Lin Chen,Yixiang Tian
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 12 Pages,9 figures
- **标题**: None
- **领域**: 一般财务,机器学习
- **Abstract**: Interpretability analysis methods for artificial intelligence models, such as LIME and SHAP, are widely used, though they primarily serve as post-model for analyzing model outputs. While it is commonly believed that the transparency and interpretability of AI models diminish as their complexity increases, currently there is no standardized method for assessing the inherent interpretability of the models themselves. This paper uses bond market default prediction as a case study, applying commonly used machine learning algorithms within AI models. First, the classification performance of these algorithms in default prediction is evaluated. Then, leveraging LIME and SHAP to assess the contribution of sample features to prediction outcomes, the paper proposes a novel method for evaluating the interpretability of the models themselves. The results of this analysis are consistent with the intuitive understanding and logical expectations regarding the interpretability of these models.

## 量子物理学(quant-ph:Quantum Physics)

### Quantum Annealing Feature Selection on Light-weight Medical Image Datasets 
[[arxiv](https://arxiv.org/abs/2502.19201)] [[cool](https://papers.cool/arxiv/2502.19201)] [[pdf](https://arxiv.org/pdf/2502.19201)]
> **Authors**: Merlin A. Nau,Luca A. Nutricati,Bruno Camino,Paul A. Warburton,Andreas K. Maier
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: We investigate the use of quantum computing algorithms on real quantum hardware to tackle the computationally intensive task of feature selection for light-weight medical image datasets. Feature selection is often formulated as a k of n selection problem, where the complexity grows binomially with increasing k and n. As problem sizes grow, classical approaches struggle to scale efficiently. Quantum computers, particularly quantum annealers, are well-suited for such problems, offering potential advantages in specific formulations. We present a method to solve larger feature selection instances than previously presented on commercial quantum annealers. Our approach combines a linear Ising penalty mechanism with subsampling and thresholding techniques to enhance scalability. The method is tested in a toy problem where feature selection identifies pixel masks used to reconstruct small-scale medical images. The results indicate that quantum annealing-based feature selection is effective for this simplified use case, demonstrating its potential in high-dimensional optimization tasks. However, its applicability to broader, real-world problems remains uncertain, given the current limitations of quantum computing hardware.

## 方法论(stat.ME:Methodology)

### AI-Powered Bayesian Inference 
[[arxiv](https://arxiv.org/abs/2502.19231)] [[cool](https://papers.cool/arxiv/2502.19231)] [[pdf](https://arxiv.org/pdf/2502.19231)]
> **Authors**: Veronika Ročková,Sean O'Hagan
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Research note, 27 pages, 3 figures
- **标题**: None
- **领域**: 方法论,人工智能,机器学习
- **Abstract**: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.

## 机器学习(stat.ML:Machine Learning)

### Advancing calibration for stochastic agent-based models in epidemiology with Stein variational inference and Gaussian process surrogates 
[[arxiv](https://arxiv.org/abs/2502.19550)] [[cool](https://papers.cool/arxiv/2502.19550)] [[pdf](https://arxiv.org/pdf/2502.19550)]
> **Authors**: Connor Robertson,Cosmin Safta,Nicholson Collier,Jonathan Ozik,Jaideep Ray
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Accurate calibration of stochastic agent-based models (ABMs) in epidemiology is crucial to make them useful in public health policy decisions and interventions. Traditional calibration methods, e.g., Markov Chain Monte Carlo (MCMC), that yield a probability density function for the parameters being calibrated, are often computationally expensive. When applied to ABMs which are highly parametrized, the calibration process becomes computationally infeasible. This paper investigates the utility of Stein Variational Inference (SVI) as an alternative calibration technique for stochastic epidemiological ABMs approximated by Gaussian process (GP) surrogates. SVI leverages gradient information to iteratively update a set of particles in the space of parameters being calibrated, offering potential advantages in scalability and efficiency for high-dimensional ABMs. The ensemble of particles yields a joint probability density function for the parameters and serves as the calibration. We compare the performance of SVI and MCMC in calibrating CityCOVID, a stochastic epidemiological ABM, focusing on predictive accuracy and calibration effectiveness. Our results demonstrate that SVI maintains predictive accuracy and calibration effectiveness comparable to MCMC, making it a viable alternative for complex epidemiological models. We also present the practical challenges of using a gradient-based calibration such as SVI which include careful tuning of hyperparameters and monitoring of the particle dynamics.

### Practical Evaluation of Copula-based Survival Metrics: Beyond the Independent Censoring Assumption 
[[arxiv](https://arxiv.org/abs/2502.19460)] [[cool](https://papers.cool/arxiv/2502.19460)] [[pdf](https://arxiv.org/pdf/2502.19460)]
> **Authors**: Christian Marius Lillelund,Shi-ang Qi,Russell Greiner
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Conventional survival metrics, such as Harrell's concordance index and the Brier Score, rely on the independent censoring assumption for valid inference in the presence of right-censored data. However, when instances are censored for reasons related to the event of interest, this assumption no longer holds, as this kind of dependent censoring biases the marginal survival estimates of popular nonparametric estimators. In this paper, we propose three copula-based metrics to evaluate survival models in the presence of dependent censoring, and design a framework to create realistic, semi-synthetic datasets with dependent censoring to facilitate the evaluation of the metrics. Our empirical analyses in synthetic and semi-synthetic datasets show that our metrics can give error estimates that are closer to the true error, mainly in terms of predictive accuracy.

### Enhancing Gradient-based Discrete Sampling via Parallel Tempering 
[[arxiv](https://arxiv.org/abs/2502.19240)] [[cool](https://papers.cool/arxiv/2502.19240)] [[pdf](https://arxiv.org/pdf/2502.19240)]
> **Authors**: Luxu Liang,Yuhang Jia,Feng Zhou
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: 24 pages, 5 figures. arXiv admin note: text overlap with arXiv:2402.17699 by other authors
- **标题**: None
- **领域**: 机器学习,机器学习,应用领域
- **Abstract**: While gradient-based discrete samplers are effective in sampling from complex distributions, they are susceptible to getting trapped in local minima, particularly in high-dimensional, multimodal discrete distributions, owing to the discontinuities inherent in these landscapes. To circumvent this issue, we combine parallel tempering, also known as replica exchange, with the discrete Langevin proposal and develop the Parallel Tempering enhanced Discrete Langevin Proposal (PTDLP), which are simulated at a series of temperatures. Significant energy differences prompt sample swaps, which are governed by a Metropolis criterion specifically designed for discrete sampling to ensure detailed balance is maintained. Additionally, we introduce an automatic scheme to determine the optimal temperature schedule and the number of chains, ensuring adaptability across diverse tasks with minimal tuning. Theoretically, we establish that our algorithm converges non-asymptotically to the target energy and exhibits faster mixing compared to a single chain. Empirical results further emphasize the superiority of our method in sampling from complex, multimodal discrete distributions, including synthetic problems, restricted Boltzmann machines, and deep energy-based models.

### Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood 
[[arxiv](https://arxiv.org/abs/2502.19086)] [[cool](https://papers.cool/arxiv/2502.19086)] [[pdf](https://arxiv.org/pdf/2502.19086)]
> **Authors**: Stefano Damato,Dario Azzimonti,Giorgio Corani
> **First submission**: 2025-02-26
> **First announcement**: 2025-02-27
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,机器学习,应用领域
- **Abstract**: We introduce the use of Gaussian Processes (GPs) for the probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which is both zero-inflated and heavy tailed, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.

## 其他论文

- [Static task mapping for heterogeneous systems based on series-parallel decompositions](https://arxiv.org/abs/2502.19745)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Do Expressions Change Decisions? Exploring the Impact of AI's Explanation Tone on Decision-Making](https://arxiv.org/abs/2502.19730)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Teaching Dense Retrieval Models to Specialize with Listwise Distillation and LLM Data Augmentation](https://arxiv.org/abs/2502.19712)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [AoECR: AI-ization of Elderly Care Robot](https://arxiv.org/abs/2502.19706)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO in whitelist
- [Old Experience Helps: Leveraging Survey Methodology to Improve AI Text Annotation Reliability in Social Sciences](https://arxiv.org/abs/2502.19679)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.DL in whitelist
- [Joint Power Allocation and Phase Shift Design for Stacked Intelligent Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL](https://arxiv.org/abs/2502.19675)
  - **标题**: None
  - **Filtered Reason**: none of eess.SP,cs.IT in whitelist
- [PCL: Prompt-based Continual Learning for User Modeling in Recommender Systems](https://arxiv.org/abs/2502.19628)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Comprehensive Digital Forensics and Risk Mitigation Strategy for Modern Enterprises](https://arxiv.org/abs/2502.19621)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.CR in whitelist
- [RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping](https://arxiv.org/abs/2502.19592)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Low Rank Matrix Rigidity: Tight Lower Bounds and Hardness Amplification](https://arxiv.org/abs/2502.19580)
  - **标题**: None
  - **Filtered Reason**: none of cs.CC in whitelist
- [Assessing Autonomous Inspection Regimes: Active Versus Passive Satellite Inspection](https://arxiv.org/abs/2502.19556)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Static Vs. Agentic Game Master AI for Facilitating Solo Role-Playing Experiences](https://arxiv.org/abs/2502.19519)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [SVGEditBench V2: A Benchmark for Instruction-based SVG Editing](https://arxiv.org/abs/2502.19453)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Will the Technological Singularity Come Soon? Modeling the Dynamics of Artificial Intelligence Development via Multi-Logistic Growth Process](https://arxiv.org/abs/2502.19425)
  - **标题**: None
  - **Filtered Reason**: none of physics.soc-ph,cs.CY in whitelist
- [Are Large Language Models Ready for Business Integration? A Study on Generative AI Adoption](https://arxiv.org/abs/2502.19423)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Hybrid Robot Learning for Automatic Robot Motion Planning in Manufacturing](https://arxiv.org/abs/2502.19340)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Agent-centric Information Access](https://arxiv.org/abs/2502.19298)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed Visual-Ability Families](https://arxiv.org/abs/2502.19263)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure](https://arxiv.org/abs/2502.19242)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [A Multicast-Capable AXI Crossbar for Many-core Machine Learning Accelerators](https://arxiv.org/abs/2502.19215)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [UQABench: Evaluating User Embedding for Prompting LLMs in Personalized Question Answering](https://arxiv.org/abs/2502.19178)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Increasing the Task Flexibility of Heavy-Duty Manipulators Using Visual 6D Pose Estimation of Objects](https://arxiv.org/abs/2502.19169)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Language-Parametric Reference Synthesis (Extended)](https://arxiv.org/abs/2502.19143)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL in whitelist
- [DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM Co-Decomposition](https://arxiv.org/abs/2502.19133)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [A Categorical Unification for Multi-Model Data: Part I Categorical Model and Normal Forms](https://arxiv.org/abs/2502.19131)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [FedCDC: A Collaborative Framework for Data Consumers in Federated Learning Market](https://arxiv.org/abs/2502.19109)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [A 106K Multi-Topic Multilingual Conversational User Dataset with Emoticons](https://arxiv.org/abs/2502.19108)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.MM in whitelist
- [Language-Driven Opinion Dynamics in Agent-Based Simulations with LLMs](https://arxiv.org/abs/2502.19098)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs](https://arxiv.org/abs/2502.19041)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2502.18992)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [3D-TrIM: A Memory-Efficient Spatial Computing Architecture for Convolution Workloads](https://arxiv.org/abs/2502.18983)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Bidirectionalization For The Common People](https://arxiv.org/abs/2502.18954)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [A Reliable, Time-Predictable Heterogeneous SoC for AI-Enhanced Mixed-Criticality Edge Applications](https://arxiv.org/abs/2502.18953)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.AR in whitelist
- [ClassInvGen: Class Invariant Synthesis using Large Language Models](https://arxiv.org/abs/2502.18917)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.SE in whitelist
- [An Empirical Study on Commit Message Generation using LLMs via In-Context Learning](https://arxiv.org/abs/2502.18904)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Distributed Online Task Assignment via Inexact ADMM for unplanned online tasks and its Applications to Security](https://arxiv.org/abs/2502.18893)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA in whitelist
- [Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection](https://arxiv.org/abs/2502.18883)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration](https://arxiv.org/abs/2502.18881)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Adaptive Shielding via Parametric Safety Proofs](https://arxiv.org/abs/2502.18879)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL in whitelist
- [Adaptive and Accessible User Interfaces for Seniors Through Model-Driven Engineering](https://arxiv.org/abs/2502.18828)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.SE in whitelist
