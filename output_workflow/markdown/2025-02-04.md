> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-04

共有765篇相关领域论文, 另有91篇其他

## 地球和行星天体物理学(astro-ph.EP:Earth and Planetary Astrophysics)

### Grid-based exoplanet atmospheric mass loss predictions through neural network 
[[arxiv](https://arxiv.org/abs/2502.01510)] [[cool](https://papers.cool/arxiv/2502.01510)] [[pdf](https://arxiv.org/pdf/2502.01510)]
> **Authors**: Amit Reza,Daria Kubyshkina,Luca Fossati,Christiane Helling
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted for publication on A&A
- **标题**: None
- **领域**: 地球和行星天体物理学,机器学习
- **Abstract**: The fast and accurate estimation of planetary mass-loss rates is critical for planet population and evolution modelling. We use machine learning (ML) for fast interpolation across an existing large grid of hydrodynamic upper atmosphere models, providing mass-loss rates for any planet inside the grid boundaries with superior accuracy compared to previously published interpolation schemes. We consider an already available grid comprising about 11000 hydrodynamic upper atmosphere models for training and generate an additional grid of about 250 models for testing purposes. We develop the ML interpolation scheme (dubbed "atmospheric Mass Loss INquiry frameworK"; MLink) using a Dense Neural Network, further comparing the results with what was obtained employing classical approaches (e.g. linear interpolation and radial basis function-based regression). Finally, we study the impact of the different interpolation schemes on the evolution of a small sample of carefully selected synthetic planets. MLink provides high-quality interpolation across the entire parameter space by significantly reducing both the number of points with large interpolation errors and the maximum interpolation error compared to previously available schemes. For most cases, evolutionary tracks computed employing MLink and classical schemes lead to comparable planetary parameters at Gyr-timescales. However, particularly for planets close to the top edge of the radius gap, the difference between the predicted planetary radii at a given age of tracks obtained employing MLink and classical interpolation schemes can exceed the typical observational uncertainties. Machine learning can be successfully used to estimate atmospheric mass-loss rates from model grids paving the way to explore future larger and more complex grids of models computed accounting for more physical processes.

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### A Poisson Process AutoDecoder for X-ray Sources 
[[arxiv](https://arxiv.org/abs/2502.01627)] [[cool](https://papers.cool/arxiv/2502.01627)] [[pdf](https://arxiv.org/pdf/2502.01627)]
> **Authors**: Yanke Song,Victoria Ashley Villar,Juan Rafael Martinez-Galarza,Steven Dillmann
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 13 pages, 5 figures
- **标题**: None
- **领域**: 天体物理学仪器和方法,高能天体物理现象,机器学习,应用领域
- **Abstract**: X-ray observing facilities, such as the Chandra X-ray Observatory and the eROSITA, have detected millions of astronomical sources associated with high-energy phenomena. The arrival of photons as a function of time follows a Poisson process and can vary by orders-of-magnitude, presenting obstacles for common tasks such as source classification, physical property derivation, and anomaly detection. Previous work has either failed to directly capture the Poisson nature of the data or only focuses on Poisson rate function reconstruction. In this work, we present Poisson Process AutoDecoder (PPAD). PPAD is a neural field decoder that maps fixed-length latent features to continuous Poisson rate functions across energy band and time via unsupervised learning. PPAD reconstructs the rate function and yields a representation at the same time. We demonstrate the efficacy of PPAD via reconstruction, regression, classification and anomaly detection experiments using the Chandra Source Catalog.

### Gamma/hadron separation in the TAIGA experiment with neural network methods 
[[arxiv](https://arxiv.org/abs/2502.01500)] [[cool](https://papers.cool/arxiv/2502.01500)] [[pdf](https://arxiv.org/pdf/2502.01500)]
> **Authors**: E. O. Gres,A. P. Kryukov,P. A. Volchugov,J. J. Dubenskaya,D. P. Zhurov,S. P. Polyakov,E. B. Postnikov,A. A. Vlaskina
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 7 pages, 5 figures, Proceedings of The 8th International Conference onDeepLearningin Computational Physics, June 19-21, 2024, Moscow, Russia
- **标题**: None
- **领域**: 天体物理学仪器和方法,高能天体物理现象,机器学习
- **Abstract**: In this work, the ability of rare VHE gamma ray selection with neural network methods is investigated in the case when cosmic radiation flux strongly prevails (ratio up to {10^4} over the gamma radiation flux from a point source). This ratio is valid for the Crab Nebula in the TeV energy range, since the Crab is a well-studied source for calibration and test of various methods and installations in gamma astronomy. The part of TAIGA experiment which includes three Imaging Atmospheric Cherenkov Telescopes observes this gamma-source too. Cherenkov telescopes obtain images of Extensive Air Showers. Hillas parameters can be used to analyse images in standard processing method, or images can be processed with convolutional neural networks. In this work we would like to describe the main steps and results obtained in the gamma/hadron separation task from the Crab Nebula with neural network methods. The results obtained are compared with standard processing method applied in the TAIGA collaboration and using Hillas parameter cuts. It is demonstrated that a signal was received at the level of higher than 5.5σ in 21 hours of Crab Nebula observations after processing the experimental data with the neural network method.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Deep Neural Network for Phonon-Assisted Optical Spectra in Semiconductors 
[[arxiv](https://arxiv.org/abs/2502.00798)] [[cool](https://papers.cool/arxiv/2502.00798)] [[pdf](https://arxiv.org/pdf/2502.00798)]
> **Authors**: Qiangqiang Gu,Shishir Kumar Pandey
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 5 pages, 5 figures
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Phonon-assisted optical absorption in semiconductors is crucial for understanding and optimizing optoelectronic devices, yet its accurate simulation remains a significant challenge in computational materials science. We present an efficient approach that combines deep learning tight-binding (TB) and potential models to efficiently calculate the phonon-assisted optical absorption in semiconductors with $ab$ $initio$ accuracy. Our strategy enables efficient sampling of atomic configurations through molecular dynamics and rapid computation of electronic structure and optical properties from the TB models. We demonstrate its efficacy by calculating the temperature-dependent optical absorption spectra and band gap renormalization of Si and GaAs due to electron-phonon coupling over a temperature range of 100-400 K. Our results show excellent agreement with experimental data, capturing both indirect and direct absorption processes, including subtle features like the Urbach tail. This approach offers a powerful tool for studying complex materials with high accuracy and efficiency, paving the way for high-throughput screening of optoelectronic materials.

## 强关联电子(cond-mat.str-el:Strongly Correlated Electrons)

### Generalized Lanczos method for systematic optimization of neural-network quantum states 
[[arxiv](https://arxiv.org/abs/2502.01264)] [[cool](https://papers.cool/arxiv/2502.01264)] [[pdf](https://arxiv.org/pdf/2502.01264)]
> **Authors**: Jia-Qi Wang,Rong-Qiang He,Zhong-Yi Lu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 7 figures, 3 tables
- **标题**: None
- **领域**: 强关联电子,机器学习,计算物理
- **Abstract**: Recently, artificial intelligence for science has made significant inroads into various fields of natural science research. In the field of quantum many-body computation, researchers have developed numerous ground state solvers based on neural-network quantum states (NQSs), achieving ground state energies with accuracy comparable to or surpassing traditional methods such as variational Monte Carlo methods, density matrix renormalization group, and quantum Monte Carlo methods. Here, we combine supervised learning, reinforcement learning, and the Lanczos method to develop a systematic approach to improving the NQSs of many-body systems, which we refer to as the NQS Lanczos method. The algorithm mainly consists of two parts: the supervised learning part and the reinforcement learning part. Through supervised learning, the Lanczos states are represented by the NQSs. Through reinforcement learning, the NQSs are further optimized. We analyze the reasons for the underfitting problem and demonstrate how the NQS Lanczos method systematically improves the energy in the highly frustrated regime of the two-dimensional Heisenberg $J_1$-$J_2$ model. Compared to the existing method that combines the Lanczos method with the restricted Boltzmann machine, the primary advantage of the NQS Lanczos method is its linearly increasing computational cost.

## 人工智能(cs.AI:Artificial Intelligence)

### An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data 
[[arxiv](https://arxiv.org/abs/2502.01789)] [[cool](https://papers.cool/arxiv/2502.01789)] [[pdf](https://arxiv.org/pdf/2502.01789)]
> **Authors**: Jiazi Tian,Liqin Wang,Pedram Fard,Valdery Moura Junior,Deborah Blacker,Jennifer S. Haas,Chirag Patel,Shawn N. Murphy,Lidia M. V. R. Moura,Hossein Estiri
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Early identification of cognitive concerns is critical but often hindered by subtle symptom presentation. This study developed and validated a fully automated, multi-agent AI workflow using LLaMA 3 8B to identify cognitive concerns in 3,338 clinical notes from Mass General Brigham. The agentic workflow, leveraging task-specific agents that dynamically collaborate to extract meaningful insights from clinical notes, was compared to an expert-driven benchmark. Both workflows achieved high classification performance, with F1-scores of 0.90 and 0.91, respectively. The agentic workflow demonstrated improved specificity (1.00) and achieved prompt refinement in fewer iterations. Although both workflows showed reduced performance on validation data, the agentic workflow maintained perfect specificity. These findings highlight the potential of fully automated multi-agent AI workflows to achieve expert-level accuracy with greater efficiency, offering a scalable and cost-effective solution for detecting cognitive concerns in clinical settings.

### Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation 
[[arxiv](https://arxiv.org/abs/2502.01694)] [[cool](https://papers.cool/arxiv/2502.01694)] [[pdf](https://arxiv.org/pdf/2502.01694)]
> **Authors**: Juno Kim,Denny Wu,Jason Lee,Taiji Suzuki
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 55 pages, 3 figures
- **标题**: None
- **领域**: 人工智能,机器学习,机器学习
- **Abstract**: A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time compute by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed metastable representation of the reasoning dynamics can be distilled into a smaller, more efficient model.

### TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues 
[[arxiv](https://arxiv.org/abs/2502.01630)] [[cool](https://papers.cool/arxiv/2502.01630)] [[pdf](https://arxiv.org/pdf/2502.01630)]
> **Authors**: Yubin Ge,Salvatore Romeo,Jason Cai,Raphael Shu,Monica Sunkara,Yassine Benajiba,Yi Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs \textit{time-aware memorization} through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate \textit{neuro-symbolic temporal reasoning}, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.

### PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01584)] [[cool](https://papers.cool/arxiv/2502.01584)] [[pdf](https://arxiv.org/pdf/2502.01584)]
> **Authors**: Carolyn Jane Anderson,Joydeep Biswas,Aleksander Boruch-Gruszecki,Federico Cassano,Molly Q Feldman,Arjun Guha,Francesca Lucchetti,Zixuan Wu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot. Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.

### Sea-cret Agents: Maritime Abduction for Region Generation to Expose Dark Vessel Trajectories 
[[arxiv](https://arxiv.org/abs/2502.01503)] [[cool](https://papers.cool/arxiv/2502.01503)] [[pdf](https://arxiv.org/pdf/2502.01503)]
> **Authors**: Divyagna Bavikadi,Nathaniel Lee,Paulo Shakarian,Chad Parvis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted to 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)
- **标题**: None
- **领域**: 人工智能,机器学习,计算机科学中的逻辑,符号计算
- **Abstract**: Bad actors in the maritime industry engage in illegal behaviors after disabling their vessel's automatic identification system (AIS) - which makes finding such vessels difficult for analysts. Machine learning approaches only succeed in identifying the locations of these ``dark vessels'' in the immediate future. This work leverages ideas from the literature on abductive inference applied to locating adversarial agents to solve the problem. Specifically, we combine concepts from abduction, logic programming, and rule learning to create an efficient method that approaches full recall of dark vessels while requiring less search area than machine learning methods. We provide a logic-based paradigm for reasoning about maritime vessels, an abductive inference query method, an automatically extracted rule-based behavior model methodology, and a thorough suite of experiments.

### Develop AI Agents for System Engineering in Factorio 
[[arxiv](https://arxiv.org/abs/2502.01492)] [[cool](https://papers.cool/arxiv/2502.01492)] [[pdf](https://arxiv.org/pdf/2502.01492)]
> **Authors**: Neel Kant
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Continuing advances in frontier model research are paving the way for widespread deployment of AI agents. Meanwhile, global interest in building large, complex systems in software, manufacturing, energy and logistics has never been greater. Although AI driven system engineering holds tremendous promise, the static benchmarks dominating agent evaluations today fail to capture the crucial skills required for implementing dynamic systems, such as managing uncertain trade-offs and ensuring proactive adaptability. This position paper advocates for training and evaluating AI agents' system engineering abilities through automation-oriented sandbox games-particularly Factorio. By directing research efforts in this direction, we can equip AI agents with the specialized reasoning and long-horizon planning necessary to design, maintain, and optimize tomorrow's most demanding engineering projects.

### TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01387)] [[cool](https://papers.cool/arxiv/2502.01387)] [[pdf](https://arxiv.org/pdf/2502.01387)]
> **Authors**: Chengkai Xu,Jiaqi Liu,Shiyu Fang,Yiming Cui,Dong Chen,Peng Hang,Jian Sun
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器人技术
- **Abstract**: Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) each show promise in addressing decision-making challenges in autonomous driving, DRL often suffers from high sample complexity, while LLMs have difficulty ensuring real-time decision making. To address these limitations, we propose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide an attention-based Student DRL policy. By incorporating risk metrics, historical scenario retrieval, and domain heuristics into context-rich prompts, the LLM produces high-level driving strategies through chain-of-thought reasoning. A self-attention mechanism then fuses these strategies with the DRL agent's exploration, accelerating policy convergence and boosting robustness across diverse driving conditions. The experimental results, evaluated across multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline methods, including other LLM-based approaches, in terms of success rates, average returns, and real-time feasibility. Ablation studies underscore the importance of each model component, especially the synergy between the attention mechanism and LLM-driven guidance. Finally, we build a virtual-real fusion experimental platform to verify the real-time performance, robustness, and reliability of the algorithm running on real vehicles through vehicle-in-loop experiments.

### PSSD: Making Large Language Models Self-denial via Human Psyche Structure 
[[arxiv](https://arxiv.org/abs/2502.01344)] [[cool](https://papers.cool/arxiv/2502.01344)] [[pdf](https://arxiv.org/pdf/2502.01344)]
> **Authors**: Jinzhi Liao,Zenghua Liao,Xiang Zhao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: WWW '25
- **标题**: None
- **领域**: 人工智能,计算语言学,信息检索
- **Abstract**: The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource competition demanding significant time and computing expenses. The cause of the situation lies in the failure of identifying the fundamental feature of the solutions in this line, coined as the self-denial of LLMs. In other words, LLMs should confidently determine the potential existence of mistakes and carefully execute the targeted correction. As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged. In response to the challenges, we present PSSD, which refers to and implements the human psyche structure such that three distinct and interconnected roles contribute to human reasoning. Specifically, PSSD leverages the recent multi-agent paradigm, and is further enhanced with three innovatively conceived roles: (1) the intuition-based id role that provides initial attempts based on benign LLMs; (2) the rule-driven superego role that summarizes rules to regulate the above attempts, and returns specific key points as guidance; and (3) the script-centric ego role that absorbs all procedural information to generate executable script for the final answer prediction. Extensive experiments demonstrate that the proposed design not only better enhance reasoning capabilities, but also seamlessly integrate with current models, leading to superior performance.

### Skewed Memorization in Large Language Models: Quantification and Decomposition 
[[arxiv](https://arxiv.org/abs/2502.01187)] [[cool](https://papers.cool/arxiv/2502.01187)] [[pdf](https://arxiv.org/pdf/2502.01187)]
> **Authors**: Hao Li,Di Huang,Ziyu Wang,Amir M. Rahmani
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Memorization in Large Language Models (LLMs) poses privacy and security risks, as models may unintentionally reproduce sensitive or copyrighted data. Existing analyses focus on average-case scenarios, often neglecting the highly skewed distribution of memorization. This paper examines memorization in LLM supervised fine-tuning (SFT), exploring its relationships with training duration, dataset size, and inter-sample similarity. By analyzing memorization probabilities over sequence lengths, we link this skewness to the token generation process, offering insights for estimating memorization and comparing it to established metrics. Through theoretical analysis and empirical evaluation, we provide a comprehensive understanding of memorization behaviors and propose strategies to detect and mitigate risks, contributing to more privacy-preserving LLMs.

### Scalable Precise Computation of Shannon Entropy 
[[arxiv](https://arxiv.org/abs/2502.01160)] [[cool](https://papers.cool/arxiv/2502.01160)] [[pdf](https://arxiv.org/pdf/2502.01160)]
> **Authors**: Yong Lai,Haolong Tong,Zhenghang Xu,Minghao Yin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 3 figures
- **标题**: None
- **领域**: 人工智能,信息论
- **Abstract**: Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs. Shannon entropy is an important method to quantify the amount of leakage in QIF. This paper focuses on the programs modeled in Boolean constraints and optimizes the two stages of the Shannon entropy computation to implement a scalable precise tool PSE. In the first stage, we design a knowledge compilation language called \ADDAND that combines Algebraic Decision Diagrams and conjunctive decomposition. \ADDAND avoids enumerating possible outputs of a program and supports tractable entropy computation. In the second stage, we optimize the model counting queries that are used to compute the probabilities of outputs. We compare PSE with the state-of-the-art probably approximately correct tool EntropyEstimation, which was shown to significantly outperform the existing precise tools. The experimental results demonstrate that PSE solved 55 more benchmarks compared to EntropyEstimation in a total of 441. For 98% of the benchmarks that both PSE and EntropyEstimation solved, PSE is at least $10\times$ as efficient as EntropyEstimation.

### DeepRAG: Thinking to Retrieval Step by Step for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01142)] [[cool](https://papers.cool/arxiv/2502.01142)] [[pdf](https://arxiv.org/pdf/2502.01142)]
> **Authors**: Xinyan Guan,Jiali Zeng,Fandong Meng,Chunlei Xin,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Jie Zhou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,信息检索
- **Abstract**: Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.

### Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment after Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.01116)] [[cool](https://papers.cool/arxiv/2502.01116)] [[pdf](https://arxiv.org/pdf/2502.01116)]
> **Authors**: Guanlin Li,Kangjie Chen,Shangwei Guo,Jie Zhang,Han Qiu,Chao Zhang,Guoyin Wang,Tianwei Zhang,Jiwei Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have emerged as powerful tools for addressing a wide range of general inquiries and tasks. Despite this, fine-tuning aligned LLMs on smaller, domain-specific datasets, critical to adapting them to specialized tasks, can inadvertently degrade their safety alignment, even when the datasets are benign. This phenomenon makes models more susceptible to providing inappropriate responses. In this study, we systematically examine the factors contributing to safety alignment degradation in benign fine-tuning scenarios. Our analysis identifies three critical factors affecting aligned LLMs: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of state-of-the-art reward models (RMs), which are often used to guide alignment processes. Our findings reveal that these RMs frequently fail to accurately reflect human preferences regarding safety, underscoring their limitations in practical applications. By uncovering these challenges, our work highlights the complexities of maintaining safety alignment during fine-tuning and offers guidance to help developers balance utility and safety in LLMs. Datasets and fine-tuning code used in our experiments can be found in https://github.com/GuanlinLee/llm_instruction_tuning.

### ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.01100)] [[cool](https://papers.cool/arxiv/2502.01100)] [[pdf](https://arxiv.org/pdf/2502.01100)]
> **Authors**: Bill Yuchen Lin,Ronan Le Bras,Kyle Richardson,Ashish Sabharwal,Radha Poovendran,Peter Clark,Yejin Choi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Website: https://huggingface.co/spaces/WildEval/ZebraLogic
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.

### Language Models Use Trigonometry to Do Addition 
[[arxiv](https://arxiv.org/abs/2502.00873)] [[cool](https://papers.cool/arxiv/2502.00873)] [[pdf](https://arxiv.org/pdf/2502.00873)]
> **Authors**: Subhash Kantamneni,Max Tegmark
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the "Clock" algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability.

### Learning to Plan with Personalized Preferences 
[[arxiv](https://arxiv.org/abs/2502.00858)] [[cool](https://papers.cool/arxiv/2502.00858)] [[pdf](https://arxiv.org/pdf/2502.00858)]
> **Authors**: Manjie Xu,Xinyi Yang,Wei Liang,Chi Zhang,Yixin Zhu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Effective integration of AI agents into daily life requires them to understand and adapt to individual human preferences, particularly in collaborative roles. Although recent studies on embodied intelligence have advanced significantly, they typically adopt generalized approaches that overlook personal preferences in planning. We address this limitation by developing agents that not only learn preferences from few demonstrations but also learn to adapt their planning strategies based on these preferences. Our research leverages the observation that preferences, though implicitly expressed through minimal demonstrations, can generalize across diverse planning scenarios. To systematically evaluate this hypothesis, we introduce Preference-based Planning (PbP) benchmark, an embodied benchmark featuring hundreds of diverse preferences spanning from atomic actions to complex sequences. Our evaluation of SOTA methods reveals that while symbol-based approaches show promise in scalability, significant challenges remain in learning to generate and execute plans that satisfy personalized preferences. We further demonstrate that incorporating learned preferences as intermediate representations in planning significantly improves the agent's ability to construct personalized plans. These findings establish preferences as a valuable abstraction layer for adaptive planning, opening new directions for research in preference-guided plan generation and execution.

### Psychometric-Based Evaluation for Theorem Proving with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00855)] [[cool](https://papers.cool/arxiv/2502.00855)] [[pdf](https://arxiv.org/pdf/2502.00855)]
> **Authors**: Jianyu Zhang,Yongwang Zhao,Long Zhang,Jilin Hu,Xiaokun Luan,Zhiwei Xu,Feng Yang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) for formal theorem proving have become a prominent research focus. At present, the proving ability of these LLMs is mainly evaluated through proof pass rates on datasets such as miniF2F. However, this evaluation method overlooks the varying importance of theorems. As a result, it fails to highlight the real performance disparities between LLMs and leads to high evaluation costs. This study proposes a psychometric-based evaluation method for theorem proving with LLMs, comprising two main components: Dataset Annotation and Adaptive Evaluation. First, we propose a metric calculation method to annotate the dataset with difficulty and discrimination metrics. Specifically, we annotate each theorem in the miniF2F dataset and grade them into varying difficulty levels according to the performance of LLMs, resulting in an enhanced dataset: miniF2F-Graded. Experimental results show that the difficulty grading in miniF2F-Graded better reflects the theorem difficulty perceived by LLMs. Secondly, we design an adaptive evaluation method to dynamically select the most suitable theorems for testing based on the annotated metrics and the real-time performance of LLMs. We apply this method to evaluate 10 LLMs. The results show that our method finely highlights the performance disparities between LLMs. It also reduces evaluation costs by using only 23% of the theorems in the dataset.

### RTBAgent: A LLM-based Agent System for Real-Time Bidding 
[[arxiv](https://arxiv.org/abs/2502.00792)] [[cool](https://papers.cool/arxiv/2502.00792)] [[pdf](https://arxiv.org/pdf/2502.00792)]
> **Authors**: Leng Cai,Junxuan He,Yikai Li,Junjie Liang,Yuanping Lin,Ziming Quan,Yawen Zeng,Jin Xu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Accepted by WWW 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent.

### Zero-Shot Warning Generation for Misinformative Multimodal Content 
[[arxiv](https://arxiv.org/abs/2502.00752)] [[cool](https://papers.cool/arxiv/2502.00752)] [[pdf](https://arxiv.org/pdf/2502.00752)]
> **Authors**: Giovanni Pio Delvecchio,Huy Hong Nguyen,Isao Echizen
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,信息检索
- **Abstract**: The widespread prevalence of misinformation poses significant societal concerns. Out-of-context misinformation, where authentic images are paired with false text, is particularly deceptive and easily misleads audiences. Most existing detection methods primarily evaluate image-text consistency but often lack sufficient explanations, which are essential for effectively debunking misinformation. We present a model that detects multimodal misinformation through cross-modality consistency checks, requiring minimal training time. Additionally, we propose a lightweight model that achieves competitive performance using only one-third of the parameters. We also introduce a dual-purpose zero-shot learning task for generating contextualized warnings, enabling automated debunking and enhancing user comprehension. Qualitative and human evaluations of the generated warnings highlight both the potential and limitations of our approach.

### Selective Response Strategies for GenAI 
[[arxiv](https://arxiv.org/abs/2502.00729)] [[cool](https://papers.cool/arxiv/2502.00729)] [[pdf](https://arxiv.org/pdf/2502.00729)]
> **Authors**: Boaz Taitler,Omer Ben-Porat
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论,社交和信息网络
- **Abstract**: The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.

### Perspectives for Direct Interpretability in Multi-Agent Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.00726)] [[cool](https://papers.cool/arxiv/2502.00726)] [[pdf](https://arxiv.org/pdf/2502.00726)]
> **Authors**: Yoann Poupart,Aurélie Beynier,Nicolas Maudet
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in solving complex problems in robotics or games, yet most of the trained models are hard to interpret. While learning intrinsically interpretable models remains a prominent approach, its scalability and flexibility are limited in handling complex tasks or multi-agent dynamics. This paper advocates for direct interpretability, generating post hoc explanations directly from trained models, as a versatile and scalable alternative, offering insights into agents' behaviour, emergent phenomena, and biases without altering models' architectures. We explore modern methods, including relevance backpropagation, knowledge edition, model steering, activation patching, sparse autoencoders and circuit discovery, to highlight their applicability to single-agent, multi-agent, and training process challenges. By addressing MADRL interpretability, we propose directions aiming to advance active topics such as team identification, swarm coordination and sample efficiency.

### MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models 
[[arxiv](https://arxiv.org/abs/2502.00698)] [[cool](https://papers.cool/arxiv/2502.00698)] [[pdf](https://arxiv.org/pdf/2502.00698)]
> **Authors**: Huanqia Cai,Yijun Yang,Winston Hu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别
- **Abstract**: IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms. Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.

### Learning Autonomous Code Integration for Math Language Models 
[[arxiv](https://arxiv.org/abs/2502.00691)] [[cool](https://papers.cool/arxiv/2502.00691)] [[pdf](https://arxiv.org/pdf/2502.00691)]
> **Authors**: Haozhe Wang,Long Li,Chao Qu,Fengming Zhu,Weidi Xu,Wei Chu,Fangzhen Lin
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training. While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.

### LLM-based event log analysis techniques: A survey 
[[arxiv](https://arxiv.org/abs/2502.00677)] [[cool](https://papers.cool/arxiv/2502.00677)] [[pdf](https://arxiv.org/pdf/2502.00677)]
> **Authors**: Siraaj Akhtar,Saad Khan,Simon Parkinson
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,密码学和安全
- **Abstract**: Event log analysis is an important task that security professionals undertake. Event logs record key information on activities that occur on computing devices, and due to the substantial number of events generated, they consume a large amount of time and resources to analyse. This demanding and repetitive task is also prone to errors. To address these concerns, researchers have developed automated techniques to improve the event log analysis process. Large Language Models (LLMs) have recently demonstrated the ability to successfully perform a wide range of tasks that individuals would usually partake in, to high standards, and at a pace and degree of complexity that outperform humans. Due to this, researchers are rapidly investigating the use of LLMs for event log analysis. This includes fine-tuning, Retrieval-Augmented Generation (RAG) and in-context learning, which affect performance. These works demonstrate good progress, yet there is a need to understand the developing body of knowledge, identify commonalities between works, and identify key challenges and potential solutions to further developments in this domain. This paper aims to survey LLM-based event log analysis techniques, providing readers with an in-depth overview of the domain, gaps identified in previous research, and concluding with potential avenues to explore in future.

### Agency in the Age of AI 
[[arxiv](https://arxiv.org/abs/2502.00648)] [[cool](https://papers.cool/arxiv/2502.00648)] [[pdf](https://arxiv.org/pdf/2502.00648)]
> **Authors**: Samarth Swarup
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: There is significant concern about the impact of generative AI on society. Modern AI tools are capable of generating ever more realistic text, images, and videos, and functional code, from minimal prompts. Accompanying this rise in ability and usability, there is increasing alarm about the misuses to which these tools can be put, and the intentional and unintentional harms to individuals and society that may result. In this paper, we argue that \emph{agency} is the appropriate lens to study these harms and benefits, but that doing so will require advancement in the theory of agency, and advancement in how this theory is applied in (agent-based) models.

### CollabLLM: From Passive Responders to Active Collaborators 
[[arxiv](https://arxiv.org/abs/2502.00640)] [[cool](https://papers.cool/arxiv/2502.00640)] [[pdf](https://arxiv.org/pdf/2502.00640)]
> **Authors**: Shirley Wu,Michel Galley,Baolin Peng,Hao Cheng,Gavin Li,Yao Dou,Weixin Cai,James Zou,Jure Leskovec,Jianfeng Gao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 23 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.

### Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.00629)] [[cool](https://papers.cool/arxiv/2502.00629)] [[pdf](https://arxiv.org/pdf/2502.00629)]
> **Authors**: Yuxuan Wu,Hideki Nakayama
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: In recent years, neuro-symbolic methods have become a popular and powerful approach that augments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed symbolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this, in this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects.

### Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach 
[[arxiv](https://arxiv.org/abs/2502.00577)] [[cool](https://papers.cool/arxiv/2502.00577)] [[pdf](https://arxiv.org/pdf/2502.00577)]
> **Authors**: Changdae Oh,Zhen Fang,Shawn Im,Xuefeng Du,Yixuan Li
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies. Extensive experiments on real benchmark datasets, spanning 61 shift scenarios empirically validate our theoretical insights.

### Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.00510)] [[cool](https://papers.cool/arxiv/2502.00510)] [[pdf](https://arxiv.org/pdf/2502.00510)]
> **Authors**: Yingxuan Yang,Bo Huang,Siyuan Qi,Chao Feng,Haoyi Hu,Yuxuan Zhu,Jinbo Hu,Haoran Zhao,Ziyi He,Xiao Liu,Zongyu Wang,Lin Qiu,Xuezhi Cao,Xunliang Cai,Yong Yu,Weinan Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory's Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent's architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.

### MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for Automating CFD Simulation and Post-Processing 
[[arxiv](https://arxiv.org/abs/2502.00498)] [[cool](https://papers.cool/arxiv/2502.00498)] [[pdf](https://arxiv.org/pdf/2502.00498)]
> **Authors**: Yuxuan Chen,Xu Zhu,Hua Zhou,Zhuyin Ren
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 16 pages,11 figures
- **标题**: None
- **领域**: 人工智能,计算物理
- **Abstract**: Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and biology to model fluid flow, heat transfer, and chemical reactions. While Large Language Models (LLMs) have transformed various domains, their application in CFD remains limited, particularly for complex tasks like post-processing. To bridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of Thought (COT) decomposition and iterative verification to enhance accessibility for non-expert users through natural language inputs. Tested on a new benchmark covering simulation (fluid flow, heat transfer, combustion) and post-processing (extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score of 6.3/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0 (2.1/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case. An ablation study confirmed that COT-driven decomposition and iterative refinement substantially improved task performance. Furthermore, scaling laws showed that increasing COT steps enhanced accuracy while raising token usage, aligning with LLM post-training scaling trends. These results highlight the transformative potential of LLMs in automating CFD workflows for industrial and research applications. Code is available at https://github.com/Terry-cyx/MetaOpenFOAM

### Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey 
[[arxiv](https://arxiv.org/abs/2502.00409)] [[cool](https://papers.cool/arxiv/2502.00409)] [[pdf](https://arxiv.org/pdf/2502.00409)]
> **Authors**: Clovis Varangot-Reille,Christophe Bouvard,Antoine Gourru,Mathieu Ciancone,Marion Schaeffer,François Jacquenet
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very large multi-topic corpora can perform well in a variety of tasks. They require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research.

### ALU: Agentic LLM Unlearning 
[[arxiv](https://arxiv.org/abs/2502.00406)] [[cool](https://papers.cool/arxiv/2502.00406)] [[pdf](https://arxiv.org/pdf/2502.00406)]
> **Authors**: Debdeep Sanyal,Murari Mandal
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU's superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.

### The role of positional encodings in the ARC benchmark 
[[arxiv](https://arxiv.org/abs/2502.00174)] [[cool](https://papers.cool/arxiv/2502.00174)] [[pdf](https://arxiv.org/pdf/2502.00174)]
> **Authors**: Guilherme H. Bandeira Costa,Miguel Freire,Arlindo L. Oliveira
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The Abstraction and Reasoning Corpus challenges AI systems to perform abstract reasoning with minimal training data, a task intuitive for humans but demanding for machine learning models. Using CodeT5+ as a case study, we demonstrate how limitations in positional encoding hinder reasoning and impact performance. This work further examines the role of positional encoding across transformer architectures, highlighting its critical influence on models of varying sizes and configurations. Comparing several strategies, we find that while 2D positional encoding and Rotary Position Embedding offer competitive performance, 2D encoding excels in data-constrained scenarios, emphasizing its effectiveness for ARC tasks

### Counting and Reasoning with Plans 
[[arxiv](https://arxiv.org/abs/2502.00145)] [[cool](https://papers.cool/arxiv/2502.00145)] [[pdf](https://arxiv.org/pdf/2502.00145)]
> **Authors**: David Speck,Markus Hecher,Daniel Gnad,Johannes K. Fichte,Augusto B. Corrêa
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Classical planning asks for a sequence of operators reaching a given goal. While the most common case is to compute a plan, many scenarios require more than that. However, quantitative reasoning on the plan space remains mostly unexplored. A fundamental problem is to count plans, which relates to the conditional probability on the plan space. Indeed, qualitative and quantitative approaches are well-established in various other areas of automated reasoning. We present the first study to quantitative and qualitative reasoning on the plan space. In particular, we focus on polynomially bounded plans. On the theoretical side, we study its complexity, which gives rise to rich reasoning modes. Since counting is hard in general, we introduce the easier notion of facets, which enables understanding the significance of operators. On the practical side, we implement quantitative reasoning for planning. Thereby, we transform a planning task into a propositional formula and use knowledge compilation to count different plans. This framework scales well to large plan spaces, while enabling rich reasoning capabilities such as learning pruning functions and explainable planning.

### Towards Efficient Multi-Objective Optimisation for Real-World Power Grid Topology Control 
[[arxiv](https://arxiv.org/abs/2502.00034)] [[cool](https://papers.cool/arxiv/2502.00034)] [[pdf](https://arxiv.org/pdf/2502.00034)]
> **Authors**: Yassine El Manyari,Anton R. Fuxjager,Stefan Zahlner,Joost Van Dijk,Alberto Castagna,Davide Barbieri,Jan Viebahn,Marcel Wasserer
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,系统与控制
- **Abstract**: Power grid operators face increasing difficulties in the control room as the increase in energy demand and the shift to renewable energy introduce new complexities in managing congestion and maintaining a stable supply. Effective grid topology control requires advanced tools capable of handling multi-objective trade-offs. While Reinforcement Learning (RL) offers a promising framework for tackling such challenges, existing Multi-Objective Reinforcement Learning (MORL) approaches fail to scale to the large state and action spaces inherent in real-world grid operations. Here we present a two-phase, efficient and scalable Multi-Objective Optimisation (MOO) method designed for grid topology control, combining an efficient RL learning phase with a rapid planning phase to generate day-ahead plans for unseen scenarios. We validate our approach using historical data from TenneT, a European Transmission System Operator (TSO), demonstrating minimal deployment time, generating day-ahead plans within 4-7 minutes with strong performance. These results underline the potential of our scalable method to support real-world power grid management, offering a practical, computationally efficient, and time-effective tool for operational planning. Based on current congestion costs and inefficiencies in grid operations, adopting our approach by TSOs could potentially save millions of euros annually, providing a compelling economic incentive for its integration in the control room.

### A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs 
[[arxiv](https://arxiv.org/abs/2502.00022)] [[cool](https://papers.cool/arxiv/2502.00022)] [[pdf](https://arxiv.org/pdf/2502.00022)]
> **Authors**: Xingyu Xiao,Peng Chen,Qianqian Jia,Jiejuan Tong,Jingang Liang,Haitao Wang
> **First submission**: 2025-01-16
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: HRA (Human Reliability Analysis) data is crucial for advancing HRA methodologies. however, existing data collection methods lack the necessary granularity, and most approaches fail to capture dynamic features. Additionally, many methods require expert knowledge as input, making them time-consuming and labor-intensive. To address these challenges, we propose a new paradigm for the automated collection of HRA data. Our approach focuses on key indicators behind human error, specifically measuring workload in collaborative settings. This study introduces a novel, scenario-driven method for workload estimation, leveraging fine-tuned large language models (LLMs). By training LLMs on real-world operational data from high-temperature gas-cooled reactors (HTGRs), we simulate human behavior and cognitive load in real time across various collaborative scenarios. The method dynamically adapts to changes in operator workload, providing more accurate, flexible, and scalable workload estimates. The results demonstrate that the proposed WELLA (Workload Estimation with LLMs and Agents) outperforms existing commercial LLM-based methods in terms of prediction accuracy.

### Temporal Reasoning in AI systems 
[[arxiv](https://arxiv.org/abs/2502.00020)] [[cool](https://papers.cool/arxiv/2502.00020)] [[pdf](https://arxiv.org/pdf/2502.00020)]
> **Authors**: Abhishek Sharma
> **First submission**: 2025-01-15
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Commonsense temporal reasoning at scale is a core problem for cognitive systems. The correct inference of the duration for which fluents hold is required by many tasks, including natural language understanding and planning. Many AI systems have limited deductive closure because they cannot extrapolate information correctly regarding existing fluents and events. In this study, we discuss the knowledge representation and reasoning schemes required for robust temporal projection in the Cyc Knowledge Base. We discuss how events can start and end risk periods for fluents. We then use discrete survival functions, which represent knowledge of the persistence of facts, to extrapolate a given fluent. The extrapolated intervals can be truncated by temporal constraints and other types of commonsense knowledge. Finally, we present the results of experiments to demonstrate that these methods obtain significant improvements in terms of Q/A performance.

### An Expectation-Maximization Algorithm-based Autoregressive Model for the Fuzzy Job Shop Scheduling Problem 
[[arxiv](https://arxiv.org/abs/2502.00018)] [[cool](https://papers.cool/arxiv/2502.00018)] [[pdf](https://arxiv.org/pdf/2502.00018)]
> **Authors**: Yijian Wang,Tongxian Guo,Zhaoqiang Liu
> **First submission**: 2025-01-11
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The fuzzy job shop scheduling problem (FJSSP) emerges as an innovative extension to the job shop scheduling problem (JSSP), incorporating a layer of uncertainty that aligns the problem more closely with the complexities of real-world manufacturing environments. This improvement increases the computational complexity of deriving the solution while improving its applicability. In the domain of deterministic scheduling, neural combinatorial optimization (NCO) has recently demonstrated remarkable efficacy. However, its application to the realm of fuzzy scheduling has been relatively unexplored. This paper aims to bridge this gap by investigating the feasibility of employing neural networks to assimilate and process fuzzy information for the resolution of FJSSP, thereby leveraging the advancements in NCO to enhance fuzzy scheduling methodologies. To achieve this, we approach the FJSSP as a generative task and introduce an expectation-maximization algorithm-based autoregressive model (EMARM) to address it. During training, our model alternates between generating scheduling schemes from given instances (E-step) and adjusting the autoregressive model weights based on these generated schemes (M-step). This novel methodology effectively navigates around the substantial hurdle of obtaining ground-truth labels, which is a prevalent issue in NCO frameworks. In testing, the experimental results demonstrate the superior capability of EMARM in addressing the FJSSP, showcasing its effectiveness and potential for practical applications in fuzzy scheduling.

## 硬件架构(cs.AR:Hardware Architecture)

### Life-Cycle Emissions of AI Hardware: A Cradle-To-Grave Approach and Generational Trends 
[[arxiv](https://arxiv.org/abs/2502.01671)] [[cool](https://papers.cool/arxiv/2502.01671)] [[pdf](https://arxiv.org/pdf/2502.01671)]
> **Authors**: Ian Schneider,Hui Xu,Stephan Benecke,David Patterson,Keguo Huang,Parthasarathy Ranganathan,Cooper Elsworth
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: Specialized hardware accelerators aid the rapid advancement of artificial intelligence (AI), and their efficiency impacts AI's environmental sustainability. This study presents the first publication of a comprehensive AI accelerator life-cycle assessment (LCA) of greenhouse gas emissions, including the first publication of manufacturing emissions of an AI accelerator. Our analysis of five Tensor Processing Units (TPUs) encompasses all stages of the hardware lifespan - from raw material extraction, manufacturing, and disposal, to energy consumption during development, deployment, and serving of AI models. Using first-party data, it offers the most comprehensive evaluation to date of AI hardware's environmental impact. We include detailed descriptions of our LCA to act as a tutorial, road map, and inspiration for other computer engineers to perform similar LCAs to help us all understand the environmental impacts of our chips and of AI. A byproduct of this study is the new metric compute carbon intensity (CCI) that is helpful in evaluating AI hardware sustainability and in estimating the carbon footprint of training and inference. This study shows that CCI improves 3x from TPU v4i to TPU v6e. Moreover, while this paper's focus is on hardware, software advancements leverage and amplify these gains.

### A Hardware-Efficient Photonic Tensor Core: Accelerating Deep Neural Networks with Structured Compression 
[[arxiv](https://arxiv.org/abs/2502.01670)] [[cool](https://papers.cool/arxiv/2502.01670)] [[pdf](https://arxiv.org/pdf/2502.01670)]
> **Authors**: Shupeng Ning,Hanqing Zhu,Chenghao Feng,Jiaqi Gu,David Z. Pan,Ray T. Chen
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,新兴技术,机器学习
- **Abstract**: Recent advancements in artificial intelligence (AI) and deep neural networks (DNNs) have revolutionized numerous fields, enabling complex tasks by extracting intricate features from large datasets. However, the exponential growth in computational demands has outstripped the capabilities of traditional electrical hardware accelerators. Optical computing offers a promising alternative due to its inherent advantages of parallelism, high computational speed, and low power consumption. Yet, current photonic integrated circuits (PICs) designed for general matrix multiplication (GEMM) are constrained by large footprints, high costs of electro-optical (E-O) interfaces, and high control complexity, limiting their scalability. To overcome these challenges, we introduce a block-circulant photonic tensor core (CirPTC) for a structure-compressed optical neural network (StrC-ONN) architecture. By applying a structured compression strategy to weight matrices, StrC-ONN significantly reduces model parameters and hardware requirements while preserving the universal representability of networks and maintaining comparable expressivity. Additionally, we propose a hardware-aware training framework to compensate for on-chip nonidealities to improve model robustness and accuracy. We experimentally demonstrate image processing and classification tasks, achieving up to a 74.91% reduction in trainable parameters while maintaining competitive accuracies. Performance analysis expects a computational density of 5.84 tera operations per second (TOPS) per mm^2 and a power efficiency of 47.94 TOPS/W, marking a 6.87-times improvement achieved through the hardware-software co-design approach. By reducing both hardware requirements and control complexity across multiple dimensions, this work explores a new pathway to push the limits of optical computing in the pursuit of high efficiency and scalability.

### Analysis of a Memcapacitor-Based for Neural Network Accelerator Framework 
[[arxiv](https://arxiv.org/abs/2502.00027)] [[cool](https://papers.cool/arxiv/2502.00027)] [[pdf](https://arxiv.org/pdf/2502.00027)]
> **Authors**: Ankur Singh,Dowon Kim,Byung-Geun Lee
> **First submission**: 2025-01-21
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 7 figures
- **标题**: None
- **领域**: 硬件架构,人工智能,神经和进化计算
- **Abstract**: Data-intensive computing tasks, such as training neural networks, are crucial for artificial intelligence applications but often come with high energy demands. One promising solution is to develop specialized hardware that directly maps neural networks, utilizing arrays of memristive devices to perform parallel multiply-accumulate operations. In our research, we introduce a novel CMOS-based memcapacitor circuit that is validated using the cadence tool. Additionally, we developed the device in Python to facilitate the design of a memcapacitive-based accelerator. Our proposed framework employs a crossbar array of memcapacitor devices to train a neural network capable of digit classification and CIFAR dataset recognition. We tested the non-ideal characteristics of the constructed memcapacitor-based neural network. The system achieved an impressive 98.4% training accuracy in digit recognition and 94.4% training accuracy in CIFAR recognition, highlighting its effectiveness. This study demonstrates the potential of memcapacitor-based neural network systems in handling classification tasks and sets the stage for further advancements in neuromorphic computing.

### Pushing the Limits of BFP on Narrow Precision LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.00026)] [[cool](https://papers.cool/arxiv/2502.00026)] [[pdf](https://arxiv.org/pdf/2502.00026)]
> **Authors**: Hui Wang,Yuan Cheng,Xiaomeng Han,Zhengpeng Zhao,Dawei Yang,Zhe Jiang
> **First submission**: 2025-01-21
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能
- **Abstract**: The substantial computational and memory demands of Large Language Models (LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective in accelerating linear operations, a cornerstone of LLM workloads. However, as sequence lengths grow, nonlinear operations, such as Attention, increasingly become performance bottlenecks due to their quadratic computational complexity. These nonlinear operations are predominantly executed using inefficient floating-point formats, which renders the system challenging to optimize software efficiency and hardware overhead. In this paper, we delve into the limitations and potential of applying BFP to nonlinear operations. Given our findings, we introduce a hardware-software co-design framework (DB-Attn), including: (i) DBFP, an advanced BFP version, overcomes nonlinear operation challenges with a pivot-focus strategy for diverse data and an adaptive grouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup table algorithm dedicated to accelerating nonlinear operations with DBFP format. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn, applicable to FPGA and ASIC. Results show that DB-Attn provides significant performance improvements with negligible accuracy loss, achieving 74% GPU speedup on Softmax of LLaMA and 10x low overhead performance improvement over SOTA designs.

## 计算复杂度(cs.CC:Computational Complexity)

### Compilation and Fast Model Counting beyond CNF 
[[arxiv](https://arxiv.org/abs/2502.00434)] [[cool](https://papers.cool/arxiv/2502.00434)] [[pdf](https://arxiv.org/pdf/2502.00434)]
> **Authors**: Alexis de Colnet,Stefan Szeider,Tianwei Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算复杂度,人工智能,计算机科学中的逻辑
- **Abstract**: Circuits in deterministic decomposable negation normal form (d-DNNF) are representations of Boolean functions that enable linear-time model counting. This paper strengthens our theoretical knowledge of what classes of functions can be efficiently transformed, or compiled, into d-DNNF. Our main contribution is the fixed-parameter tractable (FPT) compilation of conjunctions of specific constraints parameterized by incidence treewidth. This subsumes the known result for CNF. The constraints in question are all functions representable by constant-width ordered binary decision diagrams (OBDDs) for all variable orderings. For instance, this includes parity constraints and cardinality constraints with constant threshold. The running time of the FPT compilation is singly exponential in the incidence treewidth but hides large constants in the exponent. To balance that, we give a more efficient FPT algorithm for model counting that applies to a sub-family of the constraints and does not require compilation.

## 计算语言学(cs.CL:Computation and Language)

### Reasoning Bias of Next Token Prediction Training 
[[arxiv](https://arxiv.org/abs/2502.02007)] [[cool](https://papers.cool/arxiv/2502.02007)] [[pdf](https://arxiv.org/pdf/2502.02007)]
> **Authors**: Pengxiao Lin,Zhongwang Zhang,Zhi-Qin John Xu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Since the inception of Large Language Models (LLMs), the quest to efficiently train them for superior reasoning capabilities has been a pivotal challenge. The dominant training paradigm for LLMs is based on next token prediction (NTP). Alternative methodologies, called Critical Token Prediction (CTP), focused exclusively on specific critical tokens (such as the answer in Q\&A dataset), aiming to reduce the overfitting of extraneous information and noise. Contrary to initial assumptions, our research reveals that despite NTP's exposure to noise during training, it surpasses CTP in reasoning ability. We attribute this counterintuitive outcome to the regularizing influence of noise on the training dynamics. Our empirical analysis shows that NTP-trained models exhibit enhanced generalization and robustness across various benchmark reasoning datasets, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings illuminate that NTP is instrumental in fostering reasoning abilities during pretraining, whereas CTP is more effective for finetuning, thereby enriching our comprehension of optimal training strategies in LLM development.

### Wavelet-based Positional Representation for Long Context 
[[arxiv](https://arxiv.org/abs/2502.02004)] [[cool](https://papers.cool/arxiv/2502.02004)] [[pdf](https://arxiv.org/pdf/2502.02004)]
> **Authors**: Yui Oka,Taku Hasegawa,Kyosuke Nishida,Kuniko Saito
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICLR 2025. 28 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. This is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences. We analyzed conventional position encoding methods for long contexts and found the following characteristics. (1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using Haar-like wavelets. However, because it uses only a fixed scale parameter, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). This limitation could explain why RoPE performs poorly in extrapolation. (2) Previous research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes. However, it has limitations in capturing deep dependencies because it restricts the receptive field of the model. From these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field. Experimental results show that this new method improves the performance of the model in both short and long contexts. In particular, our method allows extrapolation of position information without limiting the model's attention field.

### Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media 
[[arxiv](https://arxiv.org/abs/2502.01991)] [[cool](https://papers.cool/arxiv/2502.01991)] [[pdf](https://arxiv.org/pdf/2502.01991)]
> **Authors**: Tunazzina Islam,Dan Goldwasser
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at 17th ACM Web Science Conference 2025 (WebSci'25)
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,人机交互,社交和信息网络
- **Abstract**: Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks, such as identifying morality frames, make relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.

### Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis 
[[arxiv](https://arxiv.org/abs/2502.01979)] [[cool](https://papers.cool/arxiv/2502.01979)] [[pdf](https://arxiv.org/pdf/2502.01979)]
> **Authors**: Derek Yotheringhay,Beatrix Nightingale,Maximilian Featherstone,Edmund Worthington,Hugo Ashdown
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating structured textual content requires mechanisms that enforce coherence, stability, and adherence to predefined constraints while maintaining semantic fidelity. Conventional approaches often rely on rule-based heuristics or fine-tuning strategies that lack flexibility and generalizability across diverse tasks. The incorporation of Gradient-Regularized Latent Space Modulation (GRLSM) introduces a novel paradigm for guiding text generation through the application of structured constraints within the latent space. The integration of gradient-based regularization mitigates abrupt variations in latent representations, ensuring a smoother encoding process that enhances structural consistency and logical progression within generated sequences. Comparative evaluations demonstrate that latent space modulation leads to a reduction in perplexity, increased coherence scores, and improved structural alignment across multiple domains. Stability assessments further indicate that the imposition of spectral norm constraints facilitates more controlled variations in generated text, preserving semantic consistency under input perturbations. Empirical results confirm that structured latent space constraints not only refine the organization of generated outputs but also enhance interpretability through more predictable and reliable synthesis patterns. Performance metrics illustrate that the GRLSM framework substantially reduces structural inconsistencies while preserving the generative flexibility inherent in neural models.

### CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing 
[[arxiv](https://arxiv.org/abs/2502.01976)] [[cool](https://papers.cool/arxiv/2502.01976)] [[pdf](https://arxiv.org/pdf/2502.01976)]
> **Authors**: Wenhao Zheng,Yixiao Chen,Weitong Zhang,Souvik Kundu,Yun Li,Zhengzhong Liu,Eric P. Xing,Hongyi Wang,Huaxiu Yao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,表现
- **Abstract**: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (Collaborative Inference with Token-lEvel Routing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.

### Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.01968)] [[cool](https://papers.cool/arxiv/2502.01968)] [[pdf](https://arxiv.org/pdf/2502.01968)]
> **Authors**: Jinlong Pang,Na Di,Zhaowei Zhu,Jiaheng Wei,Hao Cheng,Chen Qian,Yang Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.

### Boundary-Driven Table-Filling with Cross-Granularity Contrastive Learning for Aspect Sentiment Triplet Extraction 
[[arxiv](https://arxiv.org/abs/2502.01942)] [[cool](https://papers.cool/arxiv/2502.01942)] [[pdf](https://arxiv.org/pdf/2502.01942)]
> **Authors**: Qingling Li,Wushao Wen,Jinghui Qin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICASSP 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The Aspect Sentiment Triplet Extraction (ASTE) task aims to extract aspect terms, opinion terms, and their corresponding sentiment polarity from a given sentence. It remains one of the most prominent subtasks in fine-grained sentiment analysis. Most existing approaches frame triplet extraction as a 2D table-filling process in an end-to-end manner, focusing primarily on word-level interactions while often overlooking sentence-level representations. This limitation hampers the model's ability to capture global contextual information, particularly when dealing with multi-word aspect and opinion terms in complex sentences. To address these issues, we propose boundary-driven table-filling with cross-granularity contrastive learning (BTF-CCL) to enhance the semantic consistency between sentence-level representations and word-level representations. By constructing positive and negative sample pairs, the model is forced to learn the associations at both the sentence level and the word level. Additionally, a multi-scale, multi-granularity convolutional method is proposed to capture rich semantic information better. Our approach can capture sentence-level contextual information more effectively while maintaining sensitivity to local details. Experimental results show that the proposed method achieves state-of-the-art performance on public benchmarks according to the F1 score.

### Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 
[[arxiv](https://arxiv.org/abs/2502.01941)] [[cool](https://papers.cool/arxiv/2502.01941)] [[pdf](https://arxiv.org/pdf/2502.01941)]
> **Authors**: Xiang Liu,Zhenheng Tang,Hong Chen,Peijie Dong,Zeyu Li,Xiuze Zhou,Bo Li,Xuming Hu,Xiaowen Chu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\%$-$43.3\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\%$-$25.53\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios.

### PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling 
[[arxiv](https://arxiv.org/abs/2502.01925)] [[cool](https://papers.cool/arxiv/2502.01925)] [[pdf](https://arxiv.org/pdf/2502.01925)]
> **Authors**: Avery Ma,Yangchen Pan,Amir-massoud Farahmand
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,密码学和安全,机器学习
- **Abstract**: Many-shot jailbreaking circumvents the safety alignment of large language models by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational turns between the user and the model. These fabricated exchanges are randomly sampled from a pool of malicious questions and responses, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with positive affirmations, negative demonstrations, and an optimized adaptive sampling method tailored to the target prompt's topic. Extensive experiments on AdvBench and HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS significantly outperforms baseline methods in long-context scenarios. Through an attention analysis, we provide insights on how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.

### Conceptual Metaphor Theory as a Prompting Paradigm for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01901)] [[cool](https://papers.cool/arxiv/2502.01901)] [[pdf](https://arxiv.org/pdf/2502.01901)]
> **Authors**: Oliver Kramer
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing large language models (LLMs) through cognitive prompting in complex reasoning tasks. CMT leverages metaphorical mappings to structure abstract reasoning, improving models' ability to process and explain intricate concepts. By incorporating CMT-based prompts, we guide LLMs toward more structured and human-like reasoning patterns. To evaluate this approach, we compare four native models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented counterparts on benchmark tasks spanning domain-specific reasoning, creative insight, and metaphor interpretation. Responses were automatically evaluated using the Llama3.3 70B model. Experimental results indicate that CMT prompting significantly enhances reasoning accuracy, clarity, and metaphorical coherence, outperforming baseline models across all evaluated tasks.

### Latent Lexical Projection in Large Language Models: A Novel Approach to Implicit Representation Refinement 
[[arxiv](https://arxiv.org/abs/2502.01882)] [[cool](https://papers.cool/arxiv/2502.01882)] [[pdf](https://arxiv.org/pdf/2502.01882)]
> **Authors**: Ziad Shaker,Brendan Ashdown,Hugo Fitzalan,Alistair Heathcote,Jocasta Huntington
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Generating semantically coherent text requires a robust internal representation of linguistic structures, which traditional embedding techniques often fail to capture adequately. A novel approach, Latent Lexical Projection (LLP), is introduced to refine lexical representations through a structured transformation into a latent space, thereby enhancing the alignment between input embeddings and their contextual meanings. The method integrates an optimized projection mechanism within an existing language model architecture, enabling more accurate token selection while maintaining syntactic integrity. Evaluations across multiple benchmarks indicate a reduction in perplexity and an increase in BLEU scores, suggesting improvements in predictive accuracy and fluency. The analysis of lexical diversity reveals a more varied vocabulary in generated text, addressing common issues of redundancy and repetitive phrase structures. Further assessments of entropy distributions demonstrate a decline in uncertainty during decoding, reflecting enhanced confidence in word selection. Additionally, long-range dependency retention exhibits measurable gains, with increased classification accuracy at extended token distances. Computational efficiency remains within manageable constraints, despite the added projection mechanism, highlighting the practicality of LLP for integration into existing architectures.

### SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01812)] [[cool](https://papers.cool/arxiv/2502.01812)] [[pdf](https://arxiv.org/pdf/2502.01812)]
> **Authors**: Diyana Muhammed,Gollam Rabby,Sören Auer
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent's applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.

### On Bob Dylan: A Computational Perspective 
[[arxiv](https://arxiv.org/abs/2502.01772)] [[cool](https://papers.cool/arxiv/2502.01772)] [[pdf](https://arxiv.org/pdf/2502.01772)]
> **Authors**: Prashant Garg
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,社交和信息网络
- **Abstract**: Cass Sunstein's essay 'On Bob Dylan' describes Dylan's 'dishabituating' style -- a constant refusal to conform to expectation and a penchant for reinventing his musical and lyrical identity. In this paper, I extend Sunstein's observations through a large-scale computational analysis of Dylan's lyrics from 1962 to 2012. Using o3-mini-high (a large language model), I extract concept-to-concept relationships from the lyrics and construct directed knowledge graphs that capture Dylan's thematic structure. I then quantify shifts in sentiment, metaphorical expression, thematic diversity, and network complexity over time. The results indicate that Dylan's lyrics increasingly rely on metaphor, display an evolving sentiment profile, and exhibit heightened dishabituation -- measured here as a growing variance in the network centrality of key concepts. I also find that references to movement, protest, and mythic imagery fluctuate in ways that align with well-known phases of Dylan's career, reflecting the dynamic and unpredictable quality of his art. These findings not only deepen our empirical understanding of Sunstein's thesis but also introduce a novel computational method for analyzing an artist's evolution-offering broader applicability to the study of cultural and creative change.

### Evaluation of Large Language Models via Coupled Token Generation 
[[arxiv](https://arxiv.org/abs/2502.01754)] [[cool](https://papers.cool/arxiv/2502.01754)] [[pdf](https://arxiv.org/pdf/2502.01754)]
> **Authors**: Nina Corvelo Benz,Stratis Tsirtsis,Eleni Straitouri,Ivi Chatzi,Ander Artola Velasco,Suhas Thejaswi,Manuel Gomez-Rodriguez
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation.

### Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction 
[[arxiv](https://arxiv.org/abs/2502.01706)] [[cool](https://papers.cool/arxiv/2502.01706)] [[pdf](https://arxiv.org/pdf/2502.01706)]
> **Authors**: Alexei Figueroa,Justus Westerhoff,Golzar Atefi,Dennis Fast,Benjamin Winter,Felix Alexader Gers,Alexander Löser,Wolfang Nejdl
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at NICE2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,神经和进化计算
- **Abstract**: Biologically inspired neural networks offer alternative avenues to model data distributions. FlyVec is a recent example that draws inspiration from the fruit fly's olfactory circuit to tackle the task of learning word embeddings. Surprisingly, this model performs competitively even against deep learning approaches specifically designed to encode text, and it does so with the highest degree of computational efficiency. We pose the question of whether this performance can be improved further. For this, we introduce Comply. By incorporating positional information through complex weights, we enable a single-layer neural network to learn sequence representations. Our experiments show that Comply not only supersedes FlyVec but also performs on par with significantly larger state-of-the-art models. We achieve this without additional parameters. Comply yields sparse contextual representations of sentences that can be interpreted explicitly from the neuron weights.

### BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation 
[[arxiv](https://arxiv.org/abs/2502.01697)] [[cool](https://papers.cool/arxiv/2502.01697)] [[pdf](https://arxiv.org/pdf/2502.01697)]
> **Authors**: Alan Zhu,Parth Asawa,Jared Quincy Davis,Lingjiao Chen,Boris Hanin,Ion Stoica,Joseph E. Gonzalez,Matei Zaharia
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT.

### Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.01691)] [[cool](https://papers.cool/arxiv/2502.01691)] [[pdf](https://arxiv.org/pdf/2502.01691)]
> **Authors**: Hadas Ben-Atya,Naama Gavrielov,Zvi Badash,Gili Focht,Ruth Cytter-Kuint,Talar Hagopian,Dan Turner,Moti Freiman
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen's Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.

### LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient 
[[arxiv](https://arxiv.org/abs/2502.01683)] [[cool](https://papers.cool/arxiv/2502.01683)] [[pdf](https://arxiv.org/pdf/2502.01683)]
> **Authors**: Peiwen Yuan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed. However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization. To fill this gap, we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as BenchMaker. Experiments across multiple LLMs and tasks confirm that BenchMaker achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.

### Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset 
[[arxiv](https://arxiv.org/abs/2502.01676)] [[cool](https://papers.cool/arxiv/2502.01676)] [[pdf](https://arxiv.org/pdf/2502.01676)]
> **Authors**: Man Luo,Bradley Peterson,Rafael Gan,Hari Ramalingame,Navya Gangrade,Ariadne Dimarogona,Imon Banerjee,Phillip Howard
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted to WiML workshop @Neurips 2024
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Peer review is crucial for advancing and improving science through constructive criticism. However, toxic feedback can discourage authors and hinder scientific progress. This work explores an important but underexplored area: detecting toxicity in peer reviews. We first define toxicity in peer reviews across four distinct categories and curate a dataset of peer reviews from the OpenReview platform, annotated by human experts according to these definitions. Leveraging this dataset, we benchmark a variety of models, including a dedicated toxicity detection model, a sentiment analysis model, several open-source large language models (LLMs), and two closed-source LLMs. Our experiments explore the impact of different prompt granularities, from coarse to fine-grained instructions, on model performance. Notably, state-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments under simple prompts but achieve improved alignment with detailed instructions. Moreover, the model's confidence score is a good indicator of better alignment with human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56 with human judgments, which increases to 0.63 when using only predictions with a confidence score higher than 95%. Overall, our dataset and benchmarks underscore the need for continued research to enhance toxicity detection capabilities of LLMs. By addressing this issue, our work aims to contribute to a healthy and responsible environment for constructive academic discourse and scientific collaboration.

### Multilingual State Space Models for Structured Question Answering in Indic Languages 
[[arxiv](https://arxiv.org/abs/2502.01673)] [[cool](https://papers.cool/arxiv/2502.01673)] [[pdf](https://arxiv.org/pdf/2502.01673)]
> **Authors**: Arpita Vats,Rahul Raja,Mrinal Mathur,Vinija Jain,Aman Chadha
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.

### Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using XLNet 
[[arxiv](https://arxiv.org/abs/2502.01663)] [[cool](https://papers.cool/arxiv/2502.01663)] [[pdf](https://arxiv.org/pdf/2502.01663)]
> **Authors**: Md. Shahriar Hossain Apu,Md Saiful Islam,Tanjim Taharat Aurpa
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In 2024, the outbreak of Human Metapneumovirus (HMPV) in China, which later spread to the UK and other countries, raised significant public concern. While HMPV typically causes mild symptoms, its effects on vulnerable individuals prompted health authorities to emphasize preventive measures. This paper explores how sentiment analysis can enhance our understanding of public reactions to HMPV by analyzing social media data. We apply transformer models, particularly XLNet, achieving 93.50% accuracy in sentiment classification. Additionally, we use explainable AI (XAI) through SHAP to improve model transparency.

### Speculative Ensemble: Fast Large Language Model Ensemble via Speculation 
[[arxiv](https://arxiv.org/abs/2502.01662)] [[cool](https://papers.cool/arxiv/2502.01662)] [[pdf](https://arxiv.org/pdf/2502.01662)]
> **Authors**: Jiale Fu,Yuchu Jiang,Junkai Chen,Jiaming Fan,Xin Geng,Xu Yang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce Speculative Ensemble, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding-where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with n models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of 1.11x-2.23x over standard ensemble techniques without compromising generation quality. Our code is available at https://github.com/Kamichanw/Speculative-Ensemble/

### Large Language Models' Accuracy in Emulating Human Experts' Evaluation of Public Sentiments about Heated Tobacco Products on Social Media 
[[arxiv](https://arxiv.org/abs/2502.01658)] [[cool](https://papers.cool/arxiv/2502.01658)] [[pdf](https://arxiv.org/pdf/2502.01658)]
> **Authors**: Kwanho Kim,Soojong Kim
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会,社交和信息网络
- **Abstract**: Sentiment analysis of alternative tobacco products on social media is important for tobacco control research. Large Language Models (LLMs) can help streamline the labor-intensive human sentiment analysis process. This study examined the accuracy of LLMs in replicating human sentiment evaluation of social media messages about heated tobacco products (HTPs). The research used GPT-3.5 and GPT-4 Turbo to classify 500 Facebook and 500 Twitter messages, including anti-HTPs, pro-HTPs, and neutral messages. The models evaluated each message up to 20 times, and their majority label was compared to human evaluators. Results showed that GPT-3.5 accurately replicated human sentiment 61.2% of the time for Facebook messages and 57.0% for Twitter messages. GPT-4 Turbo performed better, with 81.7% accuracy for Facebook and 77.0% for Twitter. Using three response instances, GPT-4 Turbo achieved 99% of the accuracy of twenty instances. GPT-4 Turbo also had higher accuracy for anti- and pro-HTPs messages compared to neutral ones. Misclassifications by GPT-3.5 often involved anti- or pro-HTPs messages being labeled as neutral or irrelevant, while GPT-4 Turbo showed improvements across all categories. In conclusion, LLMs can be used for sentiment analysis of HTP-related social media messages, with GPT-4 Turbo reaching around 80% accuracy compared to human experts. However, there's a risk of misrepresenting overall sentiment due to differences in accuracy across sentiment categories.

### Scaling Embedding Layers in Language Models 
[[arxiv](https://arxiv.org/abs/2502.01637)] [[cool](https://papers.cool/arxiv/2502.01637)] [[pdf](https://arxiv.org/pdf/2502.01637)]
> **Authors**: Da Yu,Edith Cohen,Badih Ghazi,Yangsibo Huang,Pritish Kamath,Ravi Kumar,Daogao Liu,Chiyuan Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We propose SCONE ($\textbf{S}$calable, $\textbf{C}$ontextualized, $\textbf{O}$ffloaded, $\textbf{N}$-gram $\textbf{E}$mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached $n$-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.

### Lifelong Sequential Knowledge Editing without Model Degradation 
[[arxiv](https://arxiv.org/abs/2502.01636)] [[cool](https://papers.cool/arxiv/2502.01636)] [[pdf](https://arxiv.org/pdf/2502.01636)]
> **Authors**: Akshat Gupta,Phudish Prateepamornkul,Maochuan Lu,Ahmed Alaa,Thomas Hartvigsen,Gopala Anumanchipalli
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model's output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.

### LLM-TA: An LLM-Enhanced Thematic Analysis Pipeline for Transcripts from Parents of Children with Congenital Heart Disease 
[[arxiv](https://arxiv.org/abs/2502.01620)] [[cool](https://papers.cool/arxiv/2502.01620)] [[pdf](https://arxiv.org/pdf/2502.01620)]
> **Authors**: Muhammad Zain Raza,Jiawei Xu,Terence Lim,Lily Boddy,Carlos M. Mery,Andrew Well,Ying Ding
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted by GenAI for Health Workshop @ AAAI 2025, Philadelphia
- **标题**: None
- **领域**: 计算语言学,人机交互
- **Abstract**: Thematic Analysis (TA) is a fundamental method in healthcare research for analyzing transcript data, but it is resource-intensive and difficult to scale for large, complex datasets. This study investigates the potential of large language models (LLMs) to augment the inductive TA process in high-stakes healthcare settings. Focusing on interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we propose an LLM-Enhanced Thematic Analysis (LLM-TA) pipeline. Our pipeline integrates an affordable state-of-the-art LLM (GPT-4o mini), LangChain, and prompt engineering with chunking techniques to analyze nine detailed transcripts following the inductive TA framework. We evaluate the LLM-generated themes against human-generated results using thematic similarity metrics, LLM-assisted assessments, and expert reviews. Results demonstrate that our pipeline outperforms existing LLM-assisted TA methods significantly. While the pipeline alone has not yet reached human-level quality in inductive TA, it shows great potential to improve scalability, efficiency, and accuracy while reducing analyst workload when working collaboratively with domain experts. We provide practical recommendations for incorporating LLMs into high-stakes TA workflows and emphasize the importance of close collaboration with domain experts to address challenges related to real-world applicability and dataset complexity. https://github.com/jiaweixu98/LLM-TA

### Large Language Models Are Human-Like Internally 
[[arxiv](https://arxiv.org/abs/2502.01615)] [[cool](https://papers.cool/arxiv/2502.01615)] [[pdf](https://arxiv.org/pdf/2502.01615)]
> **Authors**: Tatsuki Kuribayashi,Yohei Oseki,Souhaib Ben Taieb,Kentaro Inui,Timothy Baldwin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 19 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent cognitive modeling studies have reported that larger language models (LMs) exhibit a poorer fit to human reading behavior, leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of LMs. Our analysis reveals that next-word probabilities derived from internal layers of larger LMs align with human sentence processing data as well as, or better than, those from smaller LMs. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, MAZE task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger LMs has been underestimated. Furthermore, we first identify an intriguing relationship between LM layers and human measures: earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and MAZE processing times. Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.

### Breaking Focus: Contextual Distraction Curse in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01609)] [[cool](https://papers.cool/arxiv/2502.01609)] [[pdf](https://arxiv.org/pdf/2502.01609)]
> **Authors**: Yue Huang,Yanbo Wang,Zixiang Xu,Chujie Gao,Siyuan Wu,Jiayi Ye,Xiuying Chen,Pin-Yu Chen,Xiangliang Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in Large Language Models (LLMs) have revolutionized generative systems, achieving excellent performance across diverse domains. Although these models perform well in controlled environments, their real-world applications frequently encounter inputs containing both essential and irrelevant details. Our investigation has revealed a critical vulnerability in LLMs, which we term Contextual Distraction Vulnerability (CDV). This phenomenon arises when models fail to maintain consistent performance on questions modified with semantically coherent but irrelevant context. To systematically investigate this vulnerability, we propose an efficient tree-based search methodology to automatically generate CDV examples. Our approach successfully generates CDV examples across four datasets, causing an average performance degradation of approximately 45% in state-of-the-art LLMs. To address this critical issue, we explore various mitigation strategies and find that post-targeted training approaches can effectively enhance model robustness against contextual distractions. Our findings highlight the fundamental nature of CDV as an ability-level challenge rather than a knowledge-level issue since models demonstrate the necessary knowledge by answering correctly in the absence of distractions. This calls the community's attention to address CDV during model development to ensure reliability. The code is available at https://github.com/wyf23187/LLM_CDV.

### ReGLA: Refining Gated Linear Attention 
[[arxiv](https://arxiv.org/abs/2502.01578)] [[cool](https://papers.cool/arxiv/2502.01578)] [[pdf](https://arxiv.org/pdf/2502.01578)]
> **Authors**: Peng Lu,Ivan Kobyzev,Mehdi Rezagholizadeh,Boxing Chen,Philippe Langlais
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted by NAACL 2025 (main)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module: feature maps, normalization, and the gating mechanism. We developed a feature mapping function to address some crucial issues that previous suggestions overlooked. Then we offered further rationale for the integration of normalization layers to stabilize the training process. Moreover, we explored the saturation phenomenon of the gating mechanism and augmented it with a refining module. We conducted extensive experiments and showed our architecture outperforms previous Gated Linear Attention mechanisms in extensive tasks including training from scratch and post-linearization with continual pre-training.

### Visual Theory of Mind Enables the Invention of Writing Systems 
[[arxiv](https://arxiv.org/abs/2502.01568)] [[cool](https://papers.cool/arxiv/2502.01568)] [[pdf](https://arxiv.org/pdf/2502.01568)]
> **Authors**: Benjamin A. Spiegel,Lucas Gelfond,George Konidaris
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Currently under submission to a non-archival conference, published here with permission from organizers
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Abstract symbolic writing systems are semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic writing systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes that led to the development of early writing systems.

### Scalable Language Models with Posterior Inference of Latent Thought Vectors 
[[arxiv](https://arxiv.org/abs/2502.01567)] [[cool](https://papers.cool/arxiv/2502.01567)] [[pdf](https://arxiv.org/pdf/2502.01567)]
> **Authors**: Deqian Kong,Minglu Zhao,Dehong Xu,Bo Pang,Shu Wang,Edouardo Honig,Zhangzhang Si,Chuan Li,Jianwen Xie,Sirui Xie,Ying Nian Wu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习,机器学习
- **Abstract**: We propose a novel family of language models, Latent-Thought Language Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive generation of ground tokens through a Transformer decoder. Training employs a dual-rate optimization process within the classical variational Bayes framework: fast learning of local variational parameters for the posterior distribution of latent vectors, and slow learning of global decoder parameters. Empirical studies reveal that LTMs possess additional scaling dimensions beyond traditional LLMs, yielding a structured design space. Higher sample efficiency can be achieved by increasing training compute per token, with further gains possible by trading model size for more inference steps. Designed based on these scaling properties, LTMs demonstrate superior sample and parameter efficiency compared to conventional autoregressive models and discrete diffusion models. They significantly outperform these counterparts in validation perplexity and zero-shot language modeling. Additionally, LTMs exhibit emergent few-shot in-context reasoning capabilities that scale with model and latent size, and achieve competitive performance in conditional and unconditional text generation.

### Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding 
[[arxiv](https://arxiv.org/abs/2502.01563)] [[cool](https://papers.cool/arxiv/2502.01563)] [[pdf](https://arxiv.org/pdf/2502.01563)]
> **Authors**: Mingyu Jin,Kai Mei,Wujiang Xu,Mingjie Sun,Ruixiang Tang,Mengnan Du,Zirui Liu,Yongfeng Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.

### What is a Number, That a Large Language Model May Know It? 
[[arxiv](https://arxiv.org/abs/2502.01540)] [[cool](https://papers.cool/arxiv/2502.01540)] [[pdf](https://arxiv.org/pdf/2502.01540)]
> **Authors**: Raja Marjieh,Veniamin Veselovsky,Thomas L. Griffiths,Ilia Sucholutsky
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 16 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Numbers are a basic part of how humans represent and describe the world around them. As a consequence, learning effective representations of numbers is critical for the success of large language models as they become more integrated into everyday decisions. However, these models face a challenge: depending on context, the same sequence of digit tokens, e.g., 911, can be treated as a number or as a string. What kind of representations arise from this duality, and what are its downstream implications? Using a similarity-based prompting technique from cognitive science, we show that LLMs learn representational spaces that blend string-like and numerical representations. In particular, we show that elicited similarity judgments from these models over integer pairs can be captured by a combination of Levenshtein edit distance and numerical Log-Linear distance, suggesting an entangled representation. In a series of experiments we show how this entanglement is reflected in the latent embeddings, how it can be reduced but not entirely eliminated by context, and how it can propagate into a realistic decision scenario. These results shed light on a representational tension in transformer models that must learn what a number is from text input.

### CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering 
[[arxiv](https://arxiv.org/abs/2502.01523)] [[cool](https://papers.cool/arxiv/2502.01523)] [[pdf](https://arxiv.org/pdf/2502.01523)]
> **Authors**: Zongxi Li,Yang Li,Haoran Xie,S. Joe Qin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are prone to hallucinations in question-answering (QA) tasks when faced with ambiguous questions. Users often assume that LLMs share their cognitive alignment, a mutual understanding of context, intent, and implicit details, leading them to omit critical information in the queries. However, LLMs generate responses based on assumptions that can misalign with user intent, which may be perceived as hallucinations if they misalign with the user's intent. Therefore, identifying those implicit assumptions is crucial to resolve ambiguities in QA. Prior work, such as AmbigQA, reduces ambiguity in queries via human-annotated clarifications, which is not feasible in real application. Meanwhile, ASQA compiles AmbigQA's short answers into long-form responses but inherits human biases and fails capture explicit logical distinctions that differentiates the answers. We introduce Conditional Ambiguous Question-Answering (CondAmbigQA), a benchmark with 200 ambiguous queries and condition-aware evaluation metrics. Our study pioneers the concept of ``conditions'' in ambiguous QA tasks, where conditions stand for contextual constraints or assumptions that resolve ambiguities. The retrieval-based annotation strategy uses retrieved Wikipedia fragments to identify possible interpretations for a given query as its conditions and annotate the answers through those conditions. Such a strategy minimizes human bias introduced by different knowledge levels among annotators. By fixing retrieval results, CondAmbigQA evaluates how RAG systems leverage conditions to resolve ambiguities. Experiments show that models considering conditions before answering improve performance by $20\%$, with an additional $5\%$ gain when conditions are explicitly provided. These results underscore the value of conditional reasoning in QA, offering researchers tools to rigorously evaluate ambiguity resolution.

### Hybrid Machine Learning Model for Detecting Bangla Smishing Text Using BERT and Character-Level CNN 
[[arxiv](https://arxiv.org/abs/2502.01518)] [[cool](https://papers.cool/arxiv/2502.01518)] [[pdf](https://arxiv.org/pdf/2502.01518)]
> **Authors**: Gazi Tanbhir,Md. Farhan Shahriyar,Khandker Shahed,Abdullah Md Raihan Chy,Md Al Adnan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Conference Name: 13th International Conference on Electrical and Computer Engineering (ICECE 2024)
- **标题**: None
- **领域**: 计算语言学,机器学习,社交和信息网络
- **Abstract**: Smishing is a social engineering attack using SMS containing malicious content to deceive individuals into disclosing sensitive information or transferring money to cybercriminals. Smishing attacks have surged by 328%, posing a major threat to mobile users, with losses exceeding \$54.2 million in 2019. Despite its growing prevalence, the issue remains significantly under-addressed. This paper presents a novel hybrid machine learning model for detecting Bangla smishing texts, combining Bidirectional Encoder Representations from Transformers (BERT) with Convolutional Neural Networks (CNNs) for enhanced character-level analysis. Our model addresses multi-class classification by distinguishing between Normal, Promotional, and Smishing SMS. Unlike traditional binary classification methods, our approach integrates BERT's contextual embeddings with CNN's character-level features, improving detection accuracy. Enhanced by an attention mechanism, the model effectively prioritizes crucial text segments. Our model achieves 98.47% accuracy, outperforming traditional classifiers, with high precision and recall in Smishing detection, and strong performance across all categories.

### Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation 
[[arxiv](https://arxiv.org/abs/2502.01491)] [[cool](https://papers.cool/arxiv/2502.01491)] [[pdf](https://arxiv.org/pdf/2502.01491)]
> **Authors**: Verna Dankers,Vikas Raunak
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this work, we explore how instance-level memorization in the teacher Neural Machine Translation (NMT) model gets inherited by the student model in sequence-level knowledge distillation (SeqKD). We find that despite not directly seeing the original training data, students memorize more than baseline models (models of the same size, trained on the original data) -- 3.4% for exact matches and 57% for extractive memorization -- and show increased hallucination rates. Further, under this SeqKD setting, we also characterize how students behave on specific training data subgroups, such as subgroups with low quality and specific counterfactual memorization (CM) scores, and find that students exhibit amplified denoising on low-quality subgroups. Finally, we propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD to reduce memorization and hallucinations. Overall, we recommend caution when applying SeqKD: students inherit both their teachers' superior performance and their fault modes, thereby requiring active monitoring.

### FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.01472)] [[cool](https://papers.cool/arxiv/2502.01472)] [[pdf](https://arxiv.org/pdf/2502.01472)]
> **Authors**: Jinwei Hu,Zhenglin Huang,Xiangyu Yin,Wenjie Ruan,Guangliang Cheng,Yi Dong,Xiaowei Huang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.

### Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of Custom GPTs 
[[arxiv](https://arxiv.org/abs/2502.01436)] [[cool](https://papers.cool/arxiv/2502.01436)] [[pdf](https://arxiv.org/pdf/2502.01436)]
> **Authors**: David Rodriguez,William Seymour,Jose M. Del Alamo,Jose Such
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: :I.2.1; I.2.7
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have gained unprecedented prominence, achieving widespread adoption across diverse domains and integrating deeply into society. The capability to fine-tune general-purpose LLMs, such as Generative Pre-trained Transformers (GPT), for specific tasks has facilitated the emergence of numerous Custom GPTs. These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store. However, their black-box nature introduces significant safety and compliance risks. In this work, we present a scalable framework for the automated evaluation of Custom GPTs against OpenAI's usage policies, which define the permissible behaviors of these systems. Our framework integrates three core components: (1) automated discovery and data collection of models from the GPT store, (2) a red-teaming prompt generator tailored to specific policy categories and the characteristics of each target GPT, and (3) an LLM-as-a-judge technique to analyze each prompt-response pair for potential policy violations. We validate our framework with a manually annotated ground truth, and evaluate it through a large-scale study with 782 Custom GPTs across three categories: Romantic, Cybersecurity, and Academic GPTs. Our manual annotation process achieved an F1 score of 0.975 in identifying policy violations, confirming the reliability of the framework's assessments. The results reveal that 58.7% of the analyzed models exhibit indications of non-compliance, exposing weaknesses in the GPT store's review and approval processes. Furthermore, our findings indicate that a model's popularity does not correlate with compliance, and non-compliance issues largely stem from behaviors inherited from base models rather than user-driven customizations. We believe this approach is extendable to other chatbot platforms and policy domains, improving LLM-based systems safety.

### Emergent Stack Representations in Modeling Counter Languages Using Transformers 
[[arxiv](https://arxiv.org/abs/2502.01432)] [[cool](https://papers.cool/arxiv/2502.01432)] [[pdf](https://arxiv.org/pdf/2502.01432)]
> **Authors**: Utkarsh Tiwari,Aviral Gupta,Michael Hahn
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Transformer architectures are the backbone of most modern language models, but understanding the inner workings of these models still largely remains an open problem. One way that research in the past has tackled this problem is by isolating the learning capabilities of these architectures by training them over well-understood classes of formal languages. We extend this literature by analyzing models trained over counter languages, which can be modeled using counter variables. We train transformer models on 4 counter languages, and equivalently formulate these languages using stacks, whose depths can be understood as the counter values. We then probe their internal representations for stack depths at each input token to show that these models when trained as next token predictors learn stack-like representations. This brings us closer to understanding the algorithmic details of how transformers learn languages and helps in circuit discovery.

### Annotation Tool and Dataset for Fact-Checking Podcasts 
[[arxiv](https://arxiv.org/abs/2502.01402)] [[cool](https://papers.cool/arxiv/2502.01402)] [[pdf](https://arxiv.org/pdf/2502.01402)]
> **Authors**: Vinay Setty,Adam James Becker
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted as resource paper in TheWebConf 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Podcasts are a popular medium on the web, featuring diverse and multilingual content that often includes unverified claims. Fact-checking podcasts is a challenging task, requiring transcription, annotation, and claim verification, all while preserving the contextual details of spoken content. Our tool offers a novel approach to tackle these challenges by enabling real-time annotation of podcasts during playback. This unique capability allows users to listen to the podcast and annotate key elements, such as check-worthy claims, claim spans, and contextual errors, simultaneously. By integrating advanced transcription models like OpenAI's Whisper and leveraging crowdsourced annotations, we create high-quality datasets to fine-tune multilingual transformer models such as XLM-RoBERTa for tasks like claim detection and stance classification. Furthermore, we release the annotated podcast transcripts and sample annotations with preliminary experiments.

### Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models 
[[arxiv](https://arxiv.org/abs/2502.01386)] [[cool](https://papers.cool/arxiv/2502.01386)] [[pdf](https://arxiv.org/pdf/2502.01386)]
> **Authors**: Yuyang Gong,Zhuo Chen,Miaokun Chen,Fengchang Yu,Wei Lu,Xiaofeng Wang,Xiaozhong Liu,Jiawei Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,密码学和安全,信息检索
- **Abstract**: Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become essential for tasks such as question answering and content generation. However, their increasing impact on public opinion and information dissemination has made them a critical focus for security research due to inherent vulnerabilities. Previous studies have predominantly addressed attacks targeting factual or single-query manipulations. In this paper, we address a more practical scenario: topic-oriented adversarial opinion manipulation attacks on RAG models, where LLMs are required to reason and synthesize multiple perspectives, rendering them particularly susceptible to systematic knowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage manipulation attack pipeline that strategically crafts adversarial perturbations to influence opinions across related queries. This approach combines traditional adversarial ranking attack techniques and leverages the extensive internal relevant knowledge and reasoning capabilities of LLMs to execute semantic-level perturbations. Experiments show that the proposed attacks effectively shift the opinion of the model's outputs on specific topics, significantly impacting user information perception. Current mitigation methods cannot effectively defend against such attacks, highlighting the necessity for enhanced safeguards for RAG systems, and offering crucial insights for LLM security research.

### Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations 
[[arxiv](https://arxiv.org/abs/2502.01349)] [[cool](https://papers.cool/arxiv/2502.01349)] [[pdf](https://arxiv.org/pdf/2502.01349)]
> **Authors**: Giorgos Filandrianos,Angeliki Dimitriou,Maria Lymperaiou,Konstantinos Thomas,Giorgos Stamou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The advent of Large Language Models (LLMs) has revolutionized product recommendation systems, yet their susceptibility to adversarial manipulation poses critical challenges, particularly in real-world commercial applications. Our approach is the first one to tap into human psychological principles, seamlessly modifying product descriptions, making these adversarial manipulations hard to detect. In this work, we investigate cognitive biases as black-box adversarial strategies, drawing parallels between their effects on LLMs and human purchasing behavior. Through extensive experiments on LLMs of varying scales, we reveal significant vulnerabilities in their use as recommenders, providing critical insights into safeguarding these systems.

### AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding 
[[arxiv](https://arxiv.org/abs/2502.01341)] [[cool](https://papers.cool/arxiv/2502.01341)] [[pdf](https://arxiv.org/pdf/2502.01341)]
> **Authors**: Ahmed Masry,Juan A. Rodriguez,Tianyu Zhang,Suyuchen Wang,Chao Wang,Aarash Feizi,Akshay Kalkunte Suresh,Abhay Puri,Xiangru Jian,Pierre-André Noël,Sathwik Tejaswi Madhusudhan,Marco Pedersoli,Bang Liu,Nicolas Chapados,Yoshua Bengio,Enamul Hoque,Christopher Pal,Issam H. Laradji,David Vazquez,Perouz Taslakian,Spandana Gella,Sai Rajeswar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.

### Main Predicate and Their Arguments as Explanation Signals For Intent Classification 
[[arxiv](https://arxiv.org/abs/2502.01270)] [[cool](https://papers.cool/arxiv/2502.01270)] [[pdf](https://arxiv.org/pdf/2502.01270)]
> **Authors**: Sameer Pimparkhede,Pushpak Bhattacharyya
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Intent classification is crucial for conversational agents (chatbots), and deep learning models perform well in this area. However, little research has been done on the explainability of intent classification due to the absence of suitable benchmark data. Human annotation of explanation signals in text samples is time-consuming and costly. However, from inspection of data on intent classification, we see that, more often than not, the main verb denotes the action, and the direct object indicates the domain of conversation, serving as explanation signals for intent. This observation enables us to hypothesize that the main predicate in the text utterances, along with the arguments of the main predicate, can serve as explanation signals. Leveraging this, we introduce a new technique to automatically augment text samples from intent classification datasets with word-level explanations. We mark main predicates (primarily verbs) and their arguments (dependency relations) as explanation signals in benchmark intent classification datasets ATIS and SNIPS, creating a unique 21k-instance dataset for explainability. Further, we experiment with deep learning and language models. We observe that models that work well for classification do not perform well in explainability metrics like plausibility and faithfulness. We also observe that guiding models to focus on explanation signals from our dataset during training improves the plausibility Token F1 score by 3-4%, improving the model's reasoning.

### OphthBench: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Ophthalmology 
[[arxiv](https://arxiv.org/abs/2502.01243)] [[cool](https://papers.cool/arxiv/2502.01243)] [[pdf](https://arxiv.org/pdf/2502.01243)]
> **Authors**: Chengfeng Zhou,Ji Wang,Juanjuan Qin,Yining Wang,Ling Sun,Weiwei Dai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have shown significant promise across various medical applications, with ophthalmology being a notable area of focus. Many ophthalmic tasks have shown substantial improvement through the integration of LLMs. However, before these models can be widely adopted in clinical practice, evaluating their capabilities and identifying their limitations is crucial. To address this research gap and support the real-world application of LLMs, we introduce the OphthBench, a specialized benchmark designed to assess LLM performance within the context of Chinese ophthalmic practices. This benchmark systematically divides a typical ophthalmic clinical workflow into five key scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each scenario, we developed multiple tasks featuring diverse question types, resulting in a comprehensive benchmark comprising 9 tasks and 591 questions. This comprehensive framework allows for a thorough assessment of LLMs' capabilities and provides insights into their practical application in Chinese ophthalmology. Using this benchmark, we conducted extensive experiments and analyzed the results from 39 popular LLMs. Our evaluation highlights the current gap between LLM development and its practical utility in clinical settings, providing a clear direction for future advancements. By bridging this gap, we aim to unlock the potential of LLMs and advance their development in ophthalmology.

### Language Models Struggle to Achieve a Consistent Temporal Representation of Facts 
[[arxiv](https://arxiv.org/abs/2502.01220)] [[cool](https://papers.cool/arxiv/2502.01220)] [[pdf](https://arxiv.org/pdf/2502.01220)]
> **Authors**: Hichem Ammar Khodja,Frédéric Béchet,Quentin Brabant,Alexis Nasr,Gwénolé Lecorvé
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: preprint v2
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Language Models (LMs) have shown substantial improvements in handling factual knowledge, yet their capability to consistently represent temporal facts, which are valid only within specific timeframes, remains underexplored. To investigate this, we introduce TimeStress, a novel dataset comprising 521K statements on 2003 of the most popular temporal facts in Wikidata. Each statement contextualizes a fact with correct and incorrect dates across three precisions (Day, Month, Year). This setup allows us to evaluate LMs' ability to discern between correct and incorrect temporal statements based on their probability of being generated. We assess 18 LMs across various architectures using two metrics: the win rate, indicating how often correct dates outperform incorrect ones, and robustness, reflecting consistent performance across all dates. Our findings reveal that while some LMs achieve a win rate exceeding 80\%, robustness remains low, with the best model achieving only 6\%. Furthermore, robust knowledge at one date precision does not reliably transfer to others, highlighting a significant generalization gap. These results underscore the struggle of LMs to maintain a consistent temporal representation, supporting their limitations as reliable sources of temporal knowledge. We provide all data and code for further research.

### Modelling change in neural dynamics during phonetic accommodation 
[[arxiv](https://arxiv.org/abs/2502.01210)] [[cool](https://papers.cool/arxiv/2502.01210)] [[pdf](https://arxiv.org/pdf/2502.01210)]
> **Authors**: Sam Kirkham,Patrycja Strycharczuk,Rob Davies,Danielle Welburn
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Short-term phonetic accommodation is a fundamental driver behind accent change, but how does real-time input from another speaker's voice shape the speech planning representations of an interlocutor? We advance a computational model of change in phonetic representations during phonetic accommodation, grounded in dynamic neural field equations for movement planning and memory dynamics. We test the model's ability to capture empirical patterns from an experimental study where speakers shadowed a model talker with a different accent from their own. The experimental data shows vowel-specific degrees of convergence during shadowing, followed by return to baseline (or minor divergence) post-shadowing. The model can reproduce these phenomena by modulating the magnitude of inhibitory memory dynamics, which may reflect resistance to accommodation due to phonological and/or sociolinguistic pressures. We discuss the implications of these results for the relation between short-term phonetic accommodation and longer-term patterns of sound change.

### OCR Error Post-Correction with LLMs in Historical Documents: No Free Lunches 
[[arxiv](https://arxiv.org/abs/2502.01205)] [[cool](https://papers.cool/arxiv/2502.01205)] [[pdf](https://arxiv.org/pdf/2502.01205)]
> **Authors**: Jenna Kanerva,Cassandra Ledins,Siiri Käpyaho,Filip Ginter
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: To be published in RESOURCEFUL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora.

### COVE: COntext and VEracity prediction for out-of-context images 
[[arxiv](https://arxiv.org/abs/2502.01194)] [[cool](https://papers.cool/arxiv/2502.01194)] [[pdf](https://arxiv.org/pdf/2502.01194)]
> **Authors**: Jonathan Tonglet,Gabriel Thiem,Iryna Gurevych
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Camera-ready version accepted to NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image's caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction model on all context items, often by more than five percentage points. It is competitive with the best veracity prediction models on synthetic data and outperforms them on real-world data, showing that it is beneficial to combine the two tasks sequentially. Finally, we conduct a human study that reveals that the predicted context is a reusable and interpretable artifact to verify new out-of-context captions for the same image. Our code and data are made available.

### A Single Model Ensemble Framework for Neural Machine Translation using Pivot Translation 
[[arxiv](https://arxiv.org/abs/2502.01182)] [[cool](https://papers.cool/arxiv/2502.01182)] [[pdf](https://arxiv.org/pdf/2502.01182)]
> **Authors**: Seokjin Oh,Keonwoong Noh,Woohwan Jung
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Despite the significant advances in neural machine translation, performance remains subpar for low-resource language pairs. Ensembling multiple systems is a widely adopted technique to enhance performance, often accomplished by combining probability distributions. However, the previous approaches face the challenge of high computational costs for training multiple models. Furthermore, for black-box models, averaging token-level probabilities at each decoding step is not feasible. To address the problems of multi-model ensemble methods, we present a pivot-based single model ensemble. The proposed strategy consists of two steps: pivot-based candidate generation and post-hoc aggregation. In the first step, we generate candidates through pivot translation. This can be achieved with only a single model and facilitates knowledge transfer from high-resource pivot languages, resulting in candidates that are not only diverse but also more accurate. Next, in the aggregation step, we select k high-quality candidates from the generated candidates and merge them to generate a final translation that outperforms the existing candidates. Our experimental results show that our method produces translations of superior quality by leveraging candidates from pivot translation to capture the subtle nuances of the source sentence.

### Joint Localization and Activation Editing for Low-Resource Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.01179)] [[cool](https://papers.cool/arxiv/2502.01179)] [[pdf](https://arxiv.org/pdf/2502.01179)]
> **Authors**: Wen Lai,Alexander Fraser,Ivan Titov
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: The code for the method is released at https://github.com/wenlai-lavine/jola
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods.

### Jailbreaking with Universal Multi-Prompts 
[[arxiv](https://arxiv.org/abs/2502.01154)] [[cool](https://papers.cool/arxiv/2502.01154)] [[pdf](https://arxiv.org/pdf/2502.01154)]
> **Authors**: Yu-Ling Hsu,Hsuan Su,Shang-Tse Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted by NAACL Findings 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,密码学和安全,机器学习
- **Abstract**: Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.

### Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences 
[[arxiv](https://arxiv.org/abs/2502.01126)] [[cool](https://papers.cool/arxiv/2502.01126)] [[pdf](https://arxiv.org/pdf/2502.01126)]
> **Authors**: Vaishnavi Shrivastava,Ananya Kumar,Percy Liang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model's preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model's confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets.

### Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian Language 
[[arxiv](https://arxiv.org/abs/2502.01091)] [[cool](https://papers.cool/arxiv/2502.01091)] [[pdf](https://arxiv.org/pdf/2502.01091)]
> **Authors**: Farid Ariai,Maryam Tayefeh Mahmoudi,Ali Moeini
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ef:Journal ofAIand Data Mining, 2024, 12(1): 1-14
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In the era of pervasive internet use and the dominance of social networks, researchers face significant challenges in Persian text mining including the scarcity of adequate datasets in Persian and the inefficiency of existing language models. This paper specifically tackles these challenges, aiming to amplify the efficiency of language models tailored to the Persian language. Focusing on enhancing the effectiveness of sentiment analysis, our approach employs an aspect-based methodology utilizing the ParsBERT model, augmented with a relevant lexicon. The study centers on sentiment analysis of user opinions extracted from the Persian website 'Digikala.' The experimental results not only highlight the proposed method's superior semantic capabilities but also showcase its efficiency gains with an accuracy of 88.2% and an F1 score of 61.7. The importance of enhancing language models in this context lies in their pivotal role in extracting nuanced sentiments from user-generated content, ultimately advancing the field of sentiment analysis in Persian text mining by increasing efficiency and accuracy.

### Classic4Children: Adapting Chinese Literary Classics for Children with Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.01090)] [[cool](https://papers.cool/arxiv/2502.01090)] [[pdf](https://arxiv.org/pdf/2502.01090)]
> **Authors**: Jiali Chen,Xusen Hei,Yuqi Xue,Zihan Wu,Jiayuan Xie,Yi Cai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children's reading preferences (\ie, vivid character portrayals, concise narrative structures, and appropriate readability), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters' personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children's reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves automatic and human evaluation performance.

### Knowledge Synthesis of Photosynthesis Research Using a Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.01059)] [[cool](https://papers.cool/arxiv/2502.01059)] [[pdf](https://arxiv.org/pdf/2502.01059)]
> **Authors**: Seungri Yoon,Woosang Jeon,Sanghyeok Choi,Taehyeong Kim,Tae In Ahn
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The development of biological data analysis tools and large language models (LLMs) has opened up new possibilities for utilizing AI in plant science research, with the potential to contribute significantly to knowledge integration and research gap identification. Nonetheless, current LLMs struggle to handle complex biological data and theoretical models in photosynthesis research and often fail to provide accurate scientific contexts. Therefore, this study proposed a photosynthesis research assistant (PRAG) based on OpenAI's GPT-4o with retrieval-augmented generation (RAG) techniques and prompt optimization. Vector databases and an automated feedback loop were used in the prompt optimization process to enhance the accuracy and relevance of the responses to photosynthesis-related queries. PRAG showed an average improvement of 8.7% across five metrics related to scientific writing, with a 25.4% increase in source transparency. Additionally, its scientific depth and domain coverage were comparable to those of photosynthesis research papers. A knowledge graph was used to structure PRAG's responses with papers within and outside the database, which allowed PRAG to match key entities with 63% and 39.5% of the database and test papers, respectively. PRAG can be applied for photosynthesis research and broader plant science domains, paving the way for more in-depth data analysis and predictive capabilities.

### PARA: Parameter-Efficient Fine-tuning with Prompt Aware Representation Adjustment 
[[arxiv](https://arxiv.org/abs/2502.01033)] [[cool](https://papers.cool/arxiv/2502.01033)] [[pdf](https://arxiv.org/pdf/2502.01033)]
> **Authors**: Zequan Liu,Yi Zhao,Ming Tan,Wei Zhu,Aaron Xuxiang Tian
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: accepted by ACL-2024
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In the realm of parameter-efficient fine-tuning (PEFT) methods, while options like LoRA are available, there is a persistent demand in the industry for a PEFT approach that excels in both efficiency and performance within the context of single-backbone multi-tenant applications. This paper introduces a new and straightforward PEFT technique, termed \underline{P}rompt \underline{A}ware \underline{R}epresentation \underline{A}djustment (PARA). The core of our proposal is to integrate a lightweight vector generator within each Transformer layer. This generator produces vectors that are responsive to input prompts, thereby adjusting the hidden representations accordingly. Our extensive experimentation across diverse tasks has yielded promising results. Firstly, the PARA method has been shown to surpass current PEFT benchmarks in terms of performance, despite having a similar number of adjustable parameters. Secondly, it has proven to be more efficient than LoRA in the single-backbone multi-tenant scenario, highlighting its significant potential for industrial adoption.

### Knowing When to Stop: Dynamic Context Cutoff for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01025)] [[cool](https://papers.cool/arxiv/2502.01025)] [[pdf](https://arxiv.org/pdf/2502.01025)]
> **Authors**: Roy Xie,Junlin Wang,Paul Rosu,Chunyuan Deng,Bolun Sun,Zihao Lin,Bhuwan Dhingra
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Project Website: https://royxie.com/when-to-stop-project/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method enabling LLMs to self-terminate processing upon acquiring sufficient task-relevant information. Through analysis of model internals, we discover that specific attention heads inherently encode "sufficiency signals" - detectable through lightweight classifiers - that predict when critical information has been processed. This reveals a new efficiency paradigm: models' internal understanding naturally dictates processing needs rather than external compression heuristics. Comprehensive experiments across six QA datasets (up to 40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B0-70B) demonstrate 1.33x average token reduction while improving accuracy by 1.3%. Furthermore, our method demonstrates better performance with the same rate of token reduction compared to other context efficiency methods. Additionally, we observe an emergent scaling phenomenon: while smaller models require require probing for sufficiency detection, larger models exhibit intrinsic self-assessment capabilities through prompting.

### MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs 
[[arxiv](https://arxiv.org/abs/2502.00997)] [[cool](https://papers.cool/arxiv/2502.00997)] [[pdf](https://arxiv.org/pdf/2502.00997)]
> **Authors**: Yuhang Zhou,Giannis Karamanolakis,Victor Soto,Anna Rumshisky,Mayank Kulkarni,Furong Huang,Wei Ai,Jianhua Lu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Accepted by NAACL 2025 Main
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.

### Self-supervised Analogical Learning using Language Models 
[[arxiv](https://arxiv.org/abs/2502.00996)] [[cool](https://papers.cool/arxiv/2502.00996)] [[pdf](https://arxiv.org/pdf/2502.00996)]
> **Authors**: Ben Zhou,Sarthak Jain,Yi Zhang,Qiang Ning,Shuai Wang,Yassine Benajiba,Dan Roth
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can successfully solve. Such observations motivate us to propose methods that encourage models to understand the high-level and abstract reasoning processes during training instead of only the final answer. This way, models can transfer the exact solution to similar cases, regardless of their relevance to the pre-training data distribution. In this work, we propose SAL, a self-supervised analogical learning framework. SAL mimics the human analogy process and trains models to explicitly transfer high-quality symbolic solutions from cases that they know how to solve to other rare cases in which they tend to fail more. We show that the resulting models after SAL learning outperform base language models on a wide range of reasoning benchmarks, such as StrategyQA, GSM8K, and HotpotQA, by 2% to 20%. At the same time, we show that our model is more generalizable and controllable through analytical studies.

### ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution 
[[arxiv](https://arxiv.org/abs/2502.00989)] [[cool](https://papers.cool/arxiv/2502.00989)] [[pdf](https://arxiv.org/pdf/2502.00989)]
> **Authors**: Kanika Goswami,Puneet Mathur,Ryan Rossi,Franck Dernoncourt
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.

### PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback 
[[arxiv](https://arxiv.org/abs/2502.00988)] [[cool](https://papers.cool/arxiv/2502.00988)] [[pdf](https://arxiv.org/pdf/2502.00988)]
> **Authors**: Kanika Goswami,Puneet Mathur,Ryan Rossi,Franck Dernoncourt
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.

### RandLoRA: Full-rank parameter-efficient fine-tuning of large models 
[[arxiv](https://arxiv.org/abs/2502.00987)] [[cool](https://papers.cool/arxiv/2502.00987)] [[pdf](https://arxiv.org/pdf/2502.00987)]
> **Authors**: Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Cristian Rodriguez-Opazo,Anton van den Hengel,Ehsan Abbasnejad
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: To appear at the International Conference onLearningRepresentations (ICLR) 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. However, the low-rank nature of the weight update inherently limits the representation power of fine-tuned models, potentially compromising performance on complex tasks. This raises a critical question: when a performance gap between LoRA and standard fine-tuning is observed, is it due to the reduced number of trainable parameters or the rank deficiency? This paper aims to answer this question by introducing RandLoRA, a parameter-efficient method that performs full-rank updates using a learned linear combinations of low-rank, non-trainable random matrices. Our method limits the number of trainable parameters by restricting optimization to diagonal scaling matrices applied to the fixed random matrices. This allows us to effectively overcome the low-rank limitations while maintaining parameter and memory efficiency during training. Through extensive experimentation across vision, language, and vision-language benchmarks, we systematically evaluate the limitations of LoRA and existing random basis methods. Our findings reveal that full-rank updates are beneficial across vision and language tasks individually, and even more so for vision-language tasks, where RandLoRA significantly reduces -- and sometimes eliminates -- the performance gap between standard fine-tuning and LoRA, demonstrating its efficacy.

### Context-Aware Hierarchical Merging for Long Document Summarization 
[[arxiv](https://arxiv.org/abs/2502.00977)] [[cool](https://papers.cool/arxiv/2502.00977)] [[pdf](https://arxiv.org/pdf/2502.00977)]
> **Authors**: Litu Ou,Mirella Lapata
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 30 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Hierarchical Merging is a technique commonly used to summarize very long texts ($>$100K tokens) by breaking down the input into smaller sections, summarizing those sections individually, and then merging or combining those summaries into a final coherent summary. Although it helps address the limitations of large language models (LLMs) with fixed input length constraints, the recursive merging process can amplify LLM hallucinations, increasing the risk of factual inaccuracies. In this paper, we seek to mitigate hallucinations by enriching hierarchical merging with context from the source document. Specifically, we propose different approaches to contextual augmentation ranging from \emph{replacing} intermediate summaries with relevant input context, to \emph{refining} them while using the context as supporting evidence, and \emph{aligning} them implicitly (via citations) to the input. Experimental results on datasets representing legal and narrative domains show that contextual augmentation consistently outperforms zero-shot and hierarchical merging baselines for the Llama 3.1 model family. Our analysis further reveals that refinement methods tend to perform best when paired with extractive summarization for identifying relevant input.

### Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with Decision Tree Branching 
[[arxiv](https://arxiv.org/abs/2502.00969)] [[cool](https://papers.cool/arxiv/2502.00969)] [[pdf](https://arxiv.org/pdf/2502.00969)]
> **Authors**: Xiangci Li,Zhiyu Chen,Jason Ingyu Choi,Nikhita Vedula,Besnik Fetahu,Oleg Rokhlenko,Shervin Malmasi
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Accepted by SIGDIAL 2024 but withdrawn
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The goal of conversational product search (CPS) is to develop an intelligent, chat-based shopping assistant that can directly interact with customers to understand shopping intents, ask clarification questions, and find relevant products. However, training such assistants is hindered mainly due to the lack of reliable and large-scale datasets. Prior human-annotated CPS datasets are extremely small in size and lack integration with real-world product search systems. We propose a novel approach, TRACER, which leverages large language models (LLMs) to generate realistic and natural conversations for different shopping domains. TRACER's novelty lies in grounding the generation to dialogue plans, which are product search trajectories predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions. We also release the first target-oriented CPS dataset Wizard of Shopping (WoS), containing highly natural and coherent conversations (3.6k) from three shopping domains. Finally, we demonstrate the quality and effectiveness of WoS via human evaluations and downstream tasks.

### Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search 
[[arxiv](https://arxiv.org/abs/2502.00955)] [[cool](https://papers.cool/arxiv/2502.00955)] [[pdf](https://arxiv.org/pdf/2502.00955)]
> **Authors**: Wentao Shi,Zichun Yu,Fuli Feng,Xiangnan He,Chenyan Xiong
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training.

### Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale 
[[arxiv](https://arxiv.org/abs/2502.00943)] [[cool](https://papers.cool/arxiv/2502.00943)] [[pdf](https://arxiv.org/pdf/2502.00943)]
> **Authors**: Cliff Wong,Sam Preston,Qianchu Liu,Zelalem Gero,Jass Bagga,Sheng Zhang,Shrey Jain,Theodore Zhao,Yu Gu,Yanbo Xu,Sid Kiblawi,Roshanthi Weerasinghe,Rom Leidner,Kristina Young,Brian Piening,Carlo Bifulco,Tristan Naumann,Mu Wei,Hoifung Poon
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The vast majority of real-world patient information resides in unstructured clinical text, and the process of medical abstraction seeks to extract and normalize structured information from this unstructured input. However, traditional medical abstraction methods can require significant manual efforts that can include crafting rules or annotating training labels, limiting scalability. In this paper, we propose UniMedAbstractor (UMA), a zero-shot medical abstraction framework leveraging Large Language Models (LLMs) through a modular and customizable prompt template. We refer to our approach as universal abstraction as it can quickly scale to new attributes through its universal prompt template without curating attribute-specific training labels or rules. We evaluate UMA for oncology applications, focusing on fifteen key attributes representing the cancer patient journey, from short-context attributes (e.g., performance status, treatment) to complex long-context attributes requiring longitudinal reasoning (e.g., tumor site, histology, TNM staging). Experiments on real-world data show UMA's strong performance and generalizability. Compared to supervised and heuristic baselines, UMA with GPT-4o achieves on average an absolute 2-point F1/accuracy improvement for both short-context and long-context attribute abstraction. For pathologic T staging, UMA even outperforms the supervised model by 20 points in accuracy.

### Attention Sinks and Outlier Features: A 'Catch, Tag, and Release' Mechanism for Embeddings 
[[arxiv](https://arxiv.org/abs/2502.00919)] [[cool](https://papers.cool/arxiv/2502.00919)] [[pdf](https://arxiv.org/pdf/2502.00919)]
> **Authors**: Stephen Zhang,Mustafa Khan,Vardan Papyan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Two prominent features of large language models (LLMs) is the presence of large-norm (outlier) features and the tendency for tokens to attend very strongly to a select few tokens. Despite often having no semantic relevance, these select tokens, called attention sinks, along with the large outlier features, have proven important for model performance, compression, and streaming. Consequently, investigating the roles of these phenomena within models and exploring how they might manifest in the model parameters has become an area of active interest. Through an empirical investigation, we demonstrate that attention sinks utilize outlier features to: catch a sequence of tokens, tag the captured tokens by applying a common perturbation, and then release the tokens back into the residual stream, where the tagged tokens are eventually retrieved. We prove that simple tasks, like averaging, necessitate the 'catch, tag, release' mechanism hence explaining why it would arise organically in modern LLMs. Our experiments also show that the creation of attention sinks can be completely captured in the model parameters using low-rank matrices, which has important implications for model compression and substantiates the success of recent approaches that incorporate a low-rank term to offset performance degradation.

### The Accuracy, Robustness, and Readability of LLM-Generated Sustainability-Related Word Definitions 
[[arxiv](https://arxiv.org/abs/2502.00916)] [[cool](https://papers.cool/arxiv/2502.00916)] [[pdf](https://arxiv.org/pdf/2502.00916)]
> **Authors**: Alice Heiman
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: NLP4Ecology Workshop 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: A common language with standardized definitions is crucial for effective climate discussions. However, concerns exist about LLMs misrepresenting climate terms. We compared 300 official IPCC glossary definitions with those generated by GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness, and readability using SBERT sentence embeddings. The LLMs scored an average adherence of $0.57-0.59 \pm 0.15$, and their definitions proved harder to read than the originals. Model-generated definitions vary mainly among words with multiple or ambiguous definitions, showing the potential to highlight terms that need standardization. The results show how LLMs could support environmental discourse while emphasizing the need to align model outputs with established terminology for clarity and consistency.

### Embracing Dialectic Intersubjectivity: Coordination of Different Perspectives in Content Analysis with LLM Persona Simulation 
[[arxiv](https://arxiv.org/abs/2502.00903)] [[cool](https://papers.cool/arxiv/2502.00903)] [[pdf](https://arxiv.org/pdf/2502.00903)]
> **Authors**: Taewoo Kang,Kjerstin Thorson,Tai-Quan Peng,Dan Hiaeshutter-Rice,Sanguk Lee,Stuart Soroka
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,社交和信息网络
- **Abstract**: This study attempts to advancing content analysis methodology from consensus-oriented to coordination-oriented practices, thereby embracing diverse coding outputs and exploring the dynamics among differential perspectives. As an exploratory investigation of this approach, we evaluate six GPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on Biden and Trump during the 2020 U.S. presidential campaign, examining patterns across these models. By assessing each model's alignment with ideological perspectives, we explore how partisan selective processing could be identified in LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona LLMs exhibit stronger ideological biases when processing politically congruent content. Additionally, intercoder reliability is higher among same-partisan personas compared to cross-partisan pairs. This approach enhances the nuanced understanding of LLM outputs and advances the integrity of AI-driven social science research, enabling simulations of real-world implications.

### MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for Efficient LLM Training Across Morphologies 
[[arxiv](https://arxiv.org/abs/2502.00894)] [[cool](https://papers.cool/arxiv/2502.00894)] [[pdf](https://arxiv.org/pdf/2502.00894)]
> **Authors**: Ehsaneddin Asgari,Yassine El Kheir,Mohammad Ali Sadraei Javaheri
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Tokenization is fundamental to Natural Language Processing (NLP), directly impacting model efficiency and linguistic fidelity. While Byte Pair Encoding (BPE) is widely used in Large Language Models (LLMs), it often disregards morpheme boundaries, leading to suboptimal segmentation, particularly in morphologically rich languages. We introduce MorphBPE, a morphology-aware extension of BPE that integrates linguistic structure into subword tokenization while preserving statistical efficiency. Additionally, we propose two morphology-based evaluation metrics: (i) Morphological Consistency F1-Score, which quantifies the consistency between morpheme sharing and token sharing, contributing to LLM training convergence, and (ii) Morphological Edit Distance, which measures alignment between morphemes and tokens concerning interpretability. Experiments on English, Russian, Hungarian, and Arabic across 300M and 1B parameter LLMs demonstrate that MorphBPE consistently reduces cross-entropy loss, accelerates convergence, and improves morphological alignment scores. Fully compatible with existing LLM pipelines, MorphBPE requires minimal modifications for integration. The MorphBPE codebase and tokenizer playground will be available at: https://github.com/llm-lab-org/MorphBPE and https://tokenizer.llm-lab.org

### Predicting potentially unfair clauses in Chilean terms of services with natural language processing 
[[arxiv](https://arxiv.org/abs/2502.00865)] [[cool](https://papers.cool/arxiv/2502.00865)] [[pdf](https://arxiv.org/pdf/2502.00865)]
> **Authors**: Christoffer Loeffler,Andrea Martínez Freile,Tomás Rey Pizarro
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 37 pages, 2 figures, under review
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习
- **Abstract**: This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole.

### HintEval: A Comprehensive Framework for Hint Generation and Evaluation for Questions 
[[arxiv](https://arxiv.org/abs/2502.00857)] [[cool](https://papers.cool/arxiv/2502.00857)] [[pdf](https://arxiv.org/pdf/2502.00857)]
> **Authors**: Jamshid Mozafari,Bhawna Piryani,Abdelrahman Abdallah,Adam Jatowt
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Submitted to SIGIR 2025
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Large Language Models (LLMs) are transforming how people find information, and many users turn nowadays to chatbots to obtain answers to their questions. Despite the instant access to abundant information that LLMs offer, it is still important to promote critical thinking and problem-solving skills. Automatic hint generation is a new task that aims to support humans in answering questions by themselves by creating hints that guide users toward answers without directly revealing them. In this context, hint evaluation focuses on measuring the quality of hints, helping to improve the hint generation approaches. However, resources for hint research are currently spanning different formats and datasets, while the evaluation tools are missing or incompatible, making it hard for researchers to compare and test their models. To overcome these challenges, we introduce HintEval, a Python library that makes it easy to access diverse datasets and provides multiple approaches to generate and evaluate hints. HintEval aggregates the scattered resources into a single toolkit that supports a range of research goals and enables a clear, multi-faceted, and reliable evaluation. The proposed library also includes detailed online documentation, helping users quickly explore its features and get started. By reducing barriers to entry and encouraging consistent evaluation practices, HintEval offers a major step forward for facilitating hint generation and analysis research within the NLP/IR community.

### Explainability in Practice: A Survey of Explainable NLP Across Various Domains 
[[arxiv](https://arxiv.org/abs/2502.00837)] [[cool](https://papers.cool/arxiv/2502.00837)] [[pdf](https://arxiv.org/pdf/2502.00837)]
> **Authors**: Hadi Mohammadi,Ayoub Bagheri,Anastasia Giachanou,Daniel L. Oberski
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Natural Language Processing (NLP) has become a cornerstone in many critical sectors, including healthcare, finance, and customer relationship management. This is especially true with the development and use of advanced models such as GPT-based architectures and BERT, which are widely used in decision-making processes. However, the black-box nature of these advanced NLP models has created an urgent need for transparency and explainability. This review explores explainable NLP (XNLP) with a focus on its practical deployment and real-world applications, examining its implementation and the challenges faced in domain-specific contexts. The paper underscores the importance of explainability in NLP and provides a comprehensive perspective on how XNLP can be designed to meet the unique demands of various sectors, from healthcare's need for clear insights to finance's emphasis on fraud detection and risk assessment. Additionally, this review aims to bridge the knowledge gap in XNLP literature by offering a domain-specific exploration and discussing underrepresented areas such as real-world applicability, metric evaluation, and the role of human interaction in model assessment. The paper concludes by suggesting future research directions that could enhance the understanding and broader application of XNLP.

### Generalization of Medical Large Language Models through Cross-Domain Weak Supervision 
[[arxiv](https://arxiv.org/abs/2502.00832)] [[cool](https://papers.cool/arxiv/2502.00832)] [[pdf](https://arxiv.org/pdf/2502.00832)]
> **Authors**: Robert Long,Eric Gonzalez,Harrison Fuller
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The advancement of large language models (LLMs) has opened new frontiers in natural language processing, particularly in specialized domains like healthcare. In this paper, we propose the Incremental Curriculum-Based Fine-Tuning (ICFT) framework to enhance the generative capabilities of medical large language models (MLLMs). ICFT combines curriculum-based learning, dual-stage memory coordination, and parameter-efficient fine-tuning to enable a progressive transition from general linguistic knowledge to strong domain-specific expertise. Experimental results across diverse medical NLP tasks, including question answering, preference classification, and response generation, demonstrate that ICFT consistently outperforms state-of-the-art baselines, achieving improvements in both accuracy and efficiency. Further analysis reveals the framework's ability to generalize to unseen data, reduce errors, and deliver diverse, contextually relevant medical responses. These findings establish ICFT as a robust and scalable solution for adapting LLMs to the medical domain, offering practical benefits for real-world healthcare applications.

### Weak Supervision Dynamic KL-Weighted Diffusion Models Guided by Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00826)] [[cool](https://papers.cool/arxiv/2502.00826)] [[pdf](https://arxiv.org/pdf/2502.00826)]
> **Authors**: Julian Perry,Frank Sanders,Carter Scott
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this paper, we presents a novel method for improving text-to-image generation by combining Large Language Models (LLMs) with diffusion models, a hybrid approach aimed at achieving both higher quality and efficiency in image synthesis from text descriptions. Our approach introduces a new dynamic KL-weighting strategy to optimize the diffusion process, along with incorporating semantic understanding from pre-trained LLMs to guide the generation process. The proposed method significantly improves both the visual quality and alignment of generated images with text descriptions, addressing challenges such as computational inefficiency, instability in training, and robustness to textual variability. We evaluate our method on the COCO dataset and demonstrate its superior performance over traditional GAN-based models, both quantitatively and qualitatively. Extensive experiments, including ablation studies and human evaluations, confirm that our method outperforms existing approaches in terms of image realism, relevance to the input text, and overall aesthetic quality. Our approach also shows promise in scalability to other multimodal tasks, making it a versatile solution for a wide range of generative applications.

### Probing Large Language Models in Reasoning and Translating Complex Linguistic Puzzles 
[[arxiv](https://arxiv.org/abs/2502.00817)] [[cool](https://papers.cool/arxiv/2502.00817)] [[pdf](https://arxiv.org/pdf/2502.00817)]
> **Authors**: Zheng-Lin Lin,Yu-Fei Shih,Shu-Kai Hsieh
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper investigates the utilization of Large Language Models (LLMs) for solving complex linguistic puzzles, a domain requiring advanced reasoning and adept translation capabilities akin to human cognitive processes. We explore specific prompting techniques designed to enhance ability of LLMs to reason and elucidate their decision-making pathways, with a focus on Input-Output Prompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance Prompting (SPP). Utilizing datasets from the Puzzling Machine Competition and various Linguistics Olympiads, we employ a comprehensive set of metrics to assess the performance of GPT-4 0603, a prominent LLM, across these prompting methods. Our findings illuminate the potential of LLMs in linguistic reasoning and complex translation tasks, highlighting their capabilities and identifying limitations in the context of linguistic puzzles. This research contributes significantly to the broader field of Natural Language Processing (NLP) by providing insights into the optimization of LLM applications for improved reasoning and translation accuracy, thereby enriching the ongoing dialogue in NLP advancements.

### Vision-centric Token Compression in Large Language Model 
[[arxiv](https://arxiv.org/abs/2502.00791)] [[cool](https://papers.cool/arxiv/2502.00791)] [[pdf](https://arxiv.org/pdf/2502.00791)]
> **Authors**: Ling Xing,Alex Jinpeng Wang,Rui Yan,Jinhui Tang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Large Language Models (LLMs) have revolutionized natural language processing, excelling in handling longer sequences. However, the inefficiency and redundancy in processing extended in-context tokens remain a challenge. Many attempts to address this rely on compressing tokens with smaller text encoders, yet we question whether text encoders are truly indispensable. Our journey leads to an unexpected discovery-a much smaller vision encoder, applied directly to sequences of text tokens, can rival text encoders on text tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small text understanding benchmarks, VIST leads to comparable results with 16% fewer FLOPs and 50% less memory usage. We further uncover significant token redundancy and devise a frequency-based masking strategy to guide the focus of the visual encoder toward the most critical tokens. Interestingly, we observe the trained visual encoder performs like a summarizer, selectively ignoring less important words such as prepositions and conjunctions. This approach delivers remarkable results, outperforming traditional text encoder-based methods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF, SST2, and SST5, setting a new standard for token efficiency in LLMs.

### FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training 
[[arxiv](https://arxiv.org/abs/2502.00761)] [[cool](https://papers.cool/arxiv/2502.00761)] [[pdf](https://arxiv.org/pdf/2502.00761)]
> **Authors**: Liangyu Xu,Xuemiao Zhang,Feiyu Duan,Sirui Wang,Jingang Wang,Xunliang Cai
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Selecting high-quality data can significantly improve the pretraining efficiency of large language models (LLMs). Existing methods generally rely on heuristic techniques and single-quality signals, limiting their ability to evaluate data quality comprehensively. In this work, we propose FIRE, a flexible and scalable framework for integrating multiple data quality raters, which allows for a comprehensive assessment of data quality across various dimensions. FIRE aligns multiple quality signals into a unified space, and integrates diverse data quality raters to provide a comprehensive quality signal for each data point. Further, we introduce a progressive data selection scheme based on FIRE that iteratively refines the selection of high-quality data points. Experiments on the SlimPajama dataset reveal that FIRE outperforms other data selection methods and significantly enhances the pretrained model across a wide range of downstream tasks, with a 2.9% average performance improvement over Random and reducing the FLOPs necessary to achieve a certain performance level by more than half.

### Structural Latency Perturbation in Large Language Models Through Recursive State Induction 
[[arxiv](https://arxiv.org/abs/2502.00758)] [[cool](https://papers.cool/arxiv/2502.00758)] [[pdf](https://arxiv.org/pdf/2502.00758)]
> **Authors**: Michael Mangrum,Jonathan Pemberton,Benedict Wetherby,Philip Montague
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Computational efficiency has remained a critical consideration in scaling high-capacity language models, with inference latency and resource consumption presenting significant constraints on real-time applications. The study has introduced a structured latency perturbation mechanism that modifies computational pathways through recursive state induction, enabling dynamic suppression of redundant activations while preserving generative fidelity. A formal mathematical framework has been established to describe recursive perturbations, ensuring that modifications remain adaptive rather than statically imposed. Experiments have demonstrated that applying recursive state adjustments reduces inference latency across varying sequence lengths, with longer text generations benefiting from cumulative efficiency improvements. Comparative evaluations against structured pruning and quantization have indicated that latency gains can be achieved without compromising token retention or memory utilization. The analysis of computational overhead has suggested that selectively suppressing redundant activations contributes to improved power efficiency, particularly in scenarios requiring extended text generation. An assessment of linguistic stability has shown that token-level consistency remains largely intact under controlled perturbation thresholds, reinforcing the viability of structural latency modifications as an alternative to weight-centric optimization techniques. The results have supported the hypothesis that recursive state induction offers an effective method for reducing computational complexity without requiring architectural modifications or external augmentation.

### Universal Post-Processing Networks for Joint Optimization of Modules in Task-Oriented Dialogue Systems 
[[arxiv](https://arxiv.org/abs/2502.00747)] [[cool](https://papers.cool/arxiv/2502.00747)] [[pdf](https://arxiv.org/pdf/2502.00747)]
> **Authors**: Atsumoto Ohashi,Ryuichiro Higashinaka
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Accepted by AAAI 2025 Main Technical Track
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Post-processing networks (PPNs) are components that modify the outputs of arbitrary modules in task-oriented dialogue systems and are optimized using reinforcement learning (RL) to improve the overall task completion capability of the system. However, previous PPN-based approaches have been limited to handling only a subset of modules within a system, which poses a significant limitation in improving the system performance. In this study, we propose a joint optimization method for post-processing the outputs of all modules using universal post-processing networks (UniPPNs), which are language-model-based networks that can modify the outputs of arbitrary modules in a system as a sequence-transformation task. Moreover, our RL algorithm, which employs a module-level Markov decision process, enables fine-grained value and advantage estimation for each module, thereby stabilizing joint learning for post-processing the outputs of all modules. Through both simulation-based and human evaluation experiments using the MultiWOZ dataset, we demonstrated that UniPPN outperforms conventional PPNs in the task completion capability of task-oriented dialogue systems.

### ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction, and Column Exploration 
[[arxiv](https://arxiv.org/abs/2502.00675)] [[cool](https://papers.cool/arxiv/2502.00675)] [[pdf](https://arxiv.org/pdf/2502.00675)]
> **Authors**: Minghang Deng,Ashwin Ramachandran,Canwen Xu,Lanxiang Hu,Zhewei Yao,Anupam Datta,Hao Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 13 pages, 1 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (> 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.

### Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial? 
[[arxiv](https://arxiv.org/abs/2502.00674)] [[cool](https://papers.cool/arxiv/2502.00674)] [[pdf](https://arxiv.org/pdf/2502.00674)]
> **Authors**: Wenzhe Li,Yong Lin,Mengzhou Xia,Chi Jin
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.

### Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance 
[[arxiv](https://arxiv.org/abs/2502.00641)] [[cool](https://papers.cool/arxiv/2502.00641)] [[pdf](https://arxiv.org/pdf/2502.00641)]
> **Authors**: Borui Xu,Yao Chen,Zeyi Wen,Weiguo Liu,Bingsheng He
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence, factual consistency, and summary length. Our findings reveal significant variations in SLM performance, with top-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results comparable to those of 70B LLMs while generating more concise summaries. Notably, SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in summary quality. Additionally, our analysis indicates that instruction tuning does not consistently enhance the news summarization capabilities of SLMs. This research not only contributes to the understanding of SLMs but also provides practical insights for researchers seeking efficient summarization solutions that balance performance and resource use.

### SimulPL: Aligning Human Preferences in Simultaneous Machine Translation 
[[arxiv](https://arxiv.org/abs/2502.00634)] [[cool](https://papers.cool/arxiv/2502.00634)] [[pdf](https://arxiv.org/pdf/2502.00634)]
> **Authors**: Donglei Yu,Yang Zhao,Jie Zhu,Yangyifan Xu,Yu Zhou,Chengqing Zong
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICLR 2025. 23 pages,13 figures,11 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: \textbf{translation quality preference}, \textbf{monotonicity preference}, \textbf{key point preference}, \textbf{simplicity preference}, and \textbf{latency preference}. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates \textbf{latency preference} into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. Our data and code will be available at https://github.com/EurekaForNLP/SimulPL.

### Efficient Language Modeling for Low-Resource Settings with Hybrid RNN-Transformer Architectures 
[[arxiv](https://arxiv.org/abs/2502.00617)] [[cool](https://papers.cool/arxiv/2502.00617)] [[pdf](https://arxiv.org/pdf/2502.00617)]
> **Authors**: Gabriel Lindenmaier,Sean Papay,Sebastian Padó
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: PDF has 12 pages total, 7 without references and abstract; 10 individual graphics combined to 3 figures; 5 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Transformer-based language models have recently been at the forefront of active research in text generation. However, these models' advances come at the price of prohibitive training costs, with parameter counts in the billions and compute requirements measured in petaflop/s-decades. In this paper, we investigate transformer-based architectures for improving model performance in a low-data regime by selectively replacing attention layers with feed-forward and quasi-recurrent neural network layers. We test these architectures on the standard Enwik8 and Wikitext-103 corpora. Our results show that our reduced architectures outperform existing models with a comparable number of parameters, and obtain comparable performance to larger models while significantly reducing the number of parameters.

### Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing 
[[arxiv](https://arxiv.org/abs/2502.00602)] [[cool](https://papers.cool/arxiv/2502.00602)] [[pdf](https://arxiv.org/pdf/2502.00602)]
> **Authors**: Tianci Liu,Zihan Dong,Linjun Zhang,Haoyu Wang,Jing Gao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates. Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: heterogeneous token overfitting (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.

### RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines 
[[arxiv](https://arxiv.org/abs/2502.00595)] [[cool](https://papers.cool/arxiv/2502.00595)] [[pdf](https://arxiv.org/pdf/2502.00595)]
> **Authors**: Pengfei Yu,Dongming Shen,Silin Meng,Jaewon Lee,Weisu Yin,Andrea Yaoyun Cui,Zhenlin Xu,Yi Zhu,Xingjian Shi,Mu Li,Alex Smola
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Submitted to ICML 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present RPGBench, the first benchmark designed to evaluate large language models (LLMs) as text-based role-playing game (RPG) engines. RPGBench comprises two core tasks: Game Creation (GC) and Game Simulation (GS). In GC, an LLM must craft a valid and playable RPG world using a structured event-state representation, ensuring logical coherence and proper termination conditions. In GS, the LLM simulates interactive gameplay across multiple rounds while consistently updating states and enforcing game rules. To comprehensively assess performance, RPGBench integrates objective and subjective evaluation methodologies. Objective measures verify adherence to event mechanics and check variable updates without requiring human intervention. Subjective measures, such as content interestingness, action quality, and role-playing capability, are evaluated via an LLM-as-a-judge framework, where a strong LLM grades each candidate's outputs. Empirical results demonstrate that state-of-the-art LLMs can produce engaging stories but often struggle to implement consistent, verifiable game mechanics, particularly in long or complex scenarios. By combining structured, rule-based assessments with LLM-based judgments, RPGBench provides a new standard for evaluating how well LLMs can balance creativity, coherence, and complexity in text-based RPGs, opening avenues for more immersive and controllable interactive storytelling.

### M+: Extending MemoryLLM with Scalable Long-Term Memory 
[[arxiv](https://arxiv.org/abs/2502.00592)] [[cool](https://papers.cool/arxiv/2502.00592)] [[pdf](https://arxiv.org/pdf/2502.00592)]
> **Authors**: Yu Wang,Dmitry Krotov,Yuanzhe Hu,Yifan Gao,Wangchunshu Zhou,Julian McAuley,Dan Gutfreund,Rogerio Feris,Zexue He
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Equipping large language models (LLMs) with latent-space memory has attracted increasing attention as they can extend the context window of existing language models. However, retaining information from the distant past remains a challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative work with latent-space memory, compresses past information into hidden states across all layers, forming a memory pool of 1B parameters. While effective for sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k tokens. In this work, we address this limitation by introducing M+, a memory-augmented model based on MemoryLLM that significantly enhances long-term information retention. M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation. We evaluate M+ on diverse benchmarks, including long-context understanding and knowledge retention tasks. Experimental results show that M+ significantly outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens with similar GPU memory overhead.

### Data-Driven Mispronunciation Pattern Discovery for Robust Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.00583)] [[cool](https://papers.cool/arxiv/2502.00583)] [[pdf](https://arxiv.org/pdf/2502.00583)]
> **Authors**: Anna Seo Gyeong Choi,Jonghyeon Park,Myungwoo Oh
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICASSP 2025
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: Recent advancements in machine learning have significantly improved speech recognition, but recognizing speech from non-fluent or accented speakers remains a challenge. Previous efforts, relying on rule-based pronunciation patterns, have struggled to fully capture non-native errors. We propose two data-driven approaches using speech corpora to automatically detect mispronunciation patterns. By aligning non-native phones with their native counterparts using attention maps, we achieved a 5.7% improvement in speech recognition on native English datasets and a 12.8% improvement for non-native English speakers, particularly Korean speakers. Our method offers practical advancements for robust Automatic Speech Recognition (ASR) systems particularly for situations where prior linguistic knowledge is not applicable.

### Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in Enterprise AI Assistants 
[[arxiv](https://arxiv.org/abs/2502.00537)] [[cool](https://papers.cool/arxiv/2502.00537)] [[pdf](https://arxiv.org/pdf/2502.00537)]
> **Authors**: Md Mehrab Tanjim,Xiang Chen,Victor S. Bursztyn,Uttaran Bhattacharya,Tung Mai,Vaishnavi Muppala,Akash Maharaj,Saayan Mitra,Eunyee Koh,Yunyao Li,Ken Russell
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multi-turn conversations with an Enterprise AI Assistant can be challenging due to conversational dependencies in questions, leading to ambiguities and errors. To address this, we propose an NLU-NLG framework for ambiguity detection and resolution through reformulating query automatically and introduce a new task called "Ambiguity-guided Query Rewrite." To detect ambiguities, we develop a taxonomy based on real user conversational logs and draw insights from it to design rules and extract features for a classifier which yields superior performance in detecting ambiguous queries, outperforming LLM-based baselines. Furthermore, coupling the query rewrite module with our ambiguity detecting classifier shows that this end-to-end framework can effectively mitigate ambiguities without risking unnecessary insertions of unwanted phrases for clear queries, leading to an improvement in the overall performance of the AI Assistant. Due to its significance, this has been deployed in the real world application, namely Adobe Experience Platform AI Assistant.

### A statistically consistent measure of Semantic Variability using Language Models 
[[arxiv](https://arxiv.org/abs/2502.00507)] [[cool](https://papers.cool/arxiv/2502.00507)] [[pdf](https://arxiv.org/pdf/2502.00507)]
> **Authors**: Yi Liu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: To address the issue of variability in the output generated by a language model, we present a measure of semantic variability that is statistically consistent under mild assumptions. This measure, denoted as semantic spectral entropy, is a easy to implement algorithm that requires just off the shelf language models. We put very few restrictions on the language models and we have shown in a clear simulation studies that such method can generate accurate metric despite randomness that arise from the language models.

### Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.00451)] [[cool](https://papers.cool/arxiv/2502.00451)] [[pdf](https://arxiv.org/pdf/2502.00451)]
> **Authors**: Aishik Mandal,Tanmoy Chakraborty,Iryna Gurevych
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 2 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Mental illness is a widespread and debilitating condition with substantial societal and personal costs. Traditional diagnostic and treatment approaches, such as self-reported questionnaires and psychotherapy sessions, often impose significant burdens on both patients and clinicians, limiting accessibility and efficiency. Recent advances in Artificial Intelligence (AI), particularly in Natural Language Processing and multimodal techniques, hold great potential for recognizing and addressing conditions such as depression, anxiety, bipolar disorder, schizophrenia, and post-traumatic stress disorder. However, privacy concerns, including the risk of sensitive data leakage from datasets and trained models, remain a critical barrier to deploying these AI systems in real-world clinical settings. These challenges are amplified in multimodal methods, where personal identifiers such as voice and facial data can be misused. This paper presents a critical and comprehensive study of the privacy challenges associated with developing and deploying AI models for mental health. We further prescribe potential solutions, including data anonymization, synthetic data generation, and privacy-preserving model training, to strengthen privacy safeguards in practical applications. Additionally, we discuss evaluation frameworks to assess the privacy-utility trade-offs in these approaches. By addressing these challenges, our work aims to advance the development of reliable, privacy-aware AI tools to support clinical decision-making and improve mental health outcomes.

### HERA: Improving Long Document Summarization using Large Language Models with Context Packaging and Reordering 
[[arxiv](https://arxiv.org/abs/2502.00448)] [[cool](https://papers.cool/arxiv/2502.00448)] [[pdf](https://arxiv.org/pdf/2502.00448)]
> **Authors**: Taiji Li,Hao Chen,Fei Yu,Yin Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 7 pages, 1 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the rapid growth of context length of large language models (LLMs) , LLMs still perform poorly in long document summarization. An important reason for this is that relevant information about an event is scattered throughout long documents, and the messy narrative order impairs the accurate understanding and utilization of LLMs for long documents. To address these issues, we propose a novel summary generation framework, called HERA. Specifically, we first segment a long document by its semantic structure and retrieve text segments about the same event, and finally reorder them to form the input context. We evaluate our approach on two long document summarization datasets. The experimental results show that HERA outperforms foundation models in ROUGE, BERTScore and faithfulness metrics, while HERA does not require additional fine-tuning and resources.

### UniAttn: Reducing Inference Costs via Softmax Unification for Post-Training LLMs 
[[arxiv](https://arxiv.org/abs/2502.00439)] [[cool](https://papers.cool/arxiv/2502.00439)] [[pdf](https://arxiv.org/pdf/2502.00439)]
> **Authors**: Yizhe Xiong,Wei Huang,Xin Ye,Hui Chen,Zijia Lin,Haoran Lian,Zhenpeng Su,Jungong Han,Guiguang Ding
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 4 figures. Preprint, under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Post-training is essential for adapting Large Language Models (LLMs) to real-world applications. Deploying post-trained models faces significant challenges due to substantial memory overhead and noticeable inference latency. Existing work has identified significant redundancies in LLMs and proposed efficient architectures, namely intra-layer KV sharing and cross-layer KV sharing. However, intra-layer KV sharing still results in high inference costs, while cross-layer KV sharing leads to significant performance degradation. As a result, both methods remain suboptimal for post-training pre-trained LLMs. In this paper, we identify that the \texttt{Softmax} operation is a primary bottleneck for LLM inference and discover that it is actually highly redundant during post-training. We propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}), a novel post-training method that unifies Softmax activations across transformer blocks to reduce LLM inference costs. Additionally, UniAttn adopts a linear projection to compensate for the errors induced by Softmax unification. Experiments show that UniAttn matches the performance of standard post-training while significantly reducing inference costs, outperforming existing efficient architectures during post-training. Our code will be available at \url{https://github.com/Bostoncake/UniAttn}.

### Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language 
[[arxiv](https://arxiv.org/abs/2502.00421)] [[cool](https://papers.cool/arxiv/2502.00421)] [[pdf](https://arxiv.org/pdf/2502.00421)]
> **Authors**: Turi Abu,Ying Shi,Thomas Fang Zheng,Dong Wang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted for ICASSP2025 (2025 IEEE International Conference on Acoustics, Speech, and Signal Processing)
- **标题**: None
- **领域**: 计算语言学,声音,音频和语音处理
- **Abstract**: We present a novel Automatic Speech Recognition (ASR) dataset for the Oromo language, a widely spoken language in Ethiopia and neighboring regions. The dataset was collected through a crowd-sourcing initiative, encompassing a diverse range of speakers and phonetic variations. It consists of 100 hours of real-world audio recordings paired with transcriptions, covering read speech in both clean and noisy environments. This dataset addresses the critical need for ASR resources for the Oromo language which is underrepresented. To show its applicability for the ASR task, we conducted experiments using the Conformer model, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss and WER of 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper model resulted in a significantly improved WER of 10.82%. These results establish baselines for Oromo ASR, highlighting both the challenges and the potential for improving ASR performance in Oromo. The dataset is publicly available at https://github.com/turinaf/sagalee and we encourage its use for further research and development in Oromo speech processing.

### Social media polarization during conflict: Insights from an ideological stance dataset on Israel-Palestine Reddit comments 
[[arxiv](https://arxiv.org/abs/2502.00414)] [[cool](https://papers.cool/arxiv/2502.00414)] [[pdf](https://arxiv.org/pdf/2502.00414)]
> **Authors**: Hasin Jawad Ali,Ajwad Abrar,S. M. Hozaifa Hossain,M. Firoz Mridha
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In politically sensitive scenarios like wars, social media serves as a platform for polarized discourse and expressions of strong ideological stances. While prior studies have explored ideological stance detection in general contexts, limited attention has been given to conflict-specific settings. This study addresses this gap by analyzing 9,969 Reddit comments related to the Israel-Palestine conflict, collected between October 2023 and August 2024. The comments were categorized into three stance classes: Pro-Israel, Pro-Palestine, and Neutral. Various approaches, including machine learning, pre-trained language models, neural networks, and prompt engineering strategies for open source large language models (LLMs), were employed to classify these stances. Performance was assessed using metrics such as accuracy, precision, recall, and F1-score. Among the tested methods, the Scoring and Reflective Re-read prompt in Mixtral 8x7B demonstrated the highest performance across all metrics. This study provides comparative insights into the effectiveness of different models for detecting ideological stances in highly polarized social media contexts. The dataset used in this research is publicly available for further exploration and validation.

### The Impact of Persona-based Political Perspectives on Hateful Content Detection 
[[arxiv](https://arxiv.org/abs/2502.00385)] [[cool](https://papers.cool/arxiv/2502.00385)] [[pdf](https://arxiv.org/pdf/2502.00385)]
> **Authors**: Stefano Civelli,Pietro Bernardelle,Gianluca Demartini
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Companion Proceedings of the ACM Web Conference 2025 (WWW Companion'25)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While pretraining language models with politically diverse content has been shown to improve downstream task fairness, such approaches require significant computational resources often inaccessible to many researchers and organizations. Recent work has established that persona-based prompting can introduce political diversity in model outputs without additional training. However, it remains unclear whether such prompting strategies can achieve results comparable to political pretraining for downstream tasks. We investigate this question using persona-based prompting strategies in multimodal hate-speech detection tasks, specifically focusing on hate speech in memes. Our analysis reveals that when mapping personas onto a political compass and measuring persona agreement, inherent political positioning has surprisingly little correlation with classification decisions. Notably, this lack of correlation persists even when personas are explicitly injected with stronger ideological descriptors. Our findings suggest that while LLMs can exhibit political biases in their responses to direct political questions, these biases may have less impact on practical classification tasks than previously assumed. This raises important questions about the necessity of computationally expensive political pretraining for achieving fair performance in downstream tasks.

### When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text Translation 
[[arxiv](https://arxiv.org/abs/2502.00377)] [[cool](https://papers.cool/arxiv/2502.00377)] [[pdf](https://arxiv.org/pdf/2502.00377)]
> **Authors**: Anna Min,Chenxu Hu,Yi Ren,Hang Zhao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,多媒体,声音,音频和语音处理
- **Abstract**: Though end-to-end speech-to-text translation has been a great success, we argue that the cascaded speech-to-text translation model still has its place, which is usually criticized for the error propagation between automatic speech recognition (ASR) and machine translation (MT) models. In this paper, we explore the benefits of incorporating multiple candidates from ASR and self-supervised speech features into MT. Our analysis reveals that the primary cause of cascading errors stems from the increased divergence between similar samples in the speech domain when mapped to the text domain. By including multiple candidates and self-supervised speech features, our approach allows the machine translation model to choose the right words and ensure precise translation using various speech samples. This strategy minimizes error spread and takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT models, while addressing associated issues.

### A Unit-based System and Dataset for Expressive Direct Speech-to-Speech Translation 
[[arxiv](https://arxiv.org/abs/2502.00374)] [[cool](https://papers.cool/arxiv/2502.00374)] [[pdf](https://arxiv.org/pdf/2502.00374)]
> **Authors**: Anna Min,Chenxu Hu,Yi Ren,Hang Zhao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别,多媒体,声音,音频和语音处理
- **Abstract**: Current research in speech-to-speech translation (S2ST) primarily concentrates on translation accuracy and speech naturalness, often overlooking key elements like paralinguistic information, which is essential for conveying emotions and attitudes in communication. To address this, our research introduces a novel, carefully curated multilingual dataset from various movie audio tracks. Each dataset pair is precisely matched for paralinguistic information and duration. We enhance this by integrating multiple prosody transfer techniques, aiming for translations that are accurate, natural-sounding, and rich in paralinguistic details. Our experimental results confirm that our model retains more paralinguistic information from the source speech while maintaining high standards of translation accuracy and naturalness.

### FinchGPT: a Transformer based language model for birdsong analysis 
[[arxiv](https://arxiv.org/abs/2502.00344)] [[cool](https://papers.cool/arxiv/2502.00344)] [[pdf](https://arxiv.org/pdf/2502.00344)]
> **Authors**: Kosei Kobayashi,Kosuke Matsuzaki,Masaya Taniguchi,Keisuke Sakaguchi,Kentaro Inui,Kentaro Abe
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 4 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The long-range dependencies among the tokens, which originate from hierarchical structures, are a defining hallmark of human language. However, whether similar dependencies exist within the sequential vocalization of non-human animals remains a topic of investigation. Transformer architectures, known for their ability to model long-range dependencies among tokens, provide a powerful tool for investigating this phenomenon. In this study, we employed the Transformer architecture to analyze the songs of Bengalese finch (Lonchura striata domestica), which are characterized by their highly variable and complex syllable sequences. To this end, we developed FinchGPT, a Transformer-based model trained on a textualized corpus of birdsongs, which outperformed other architecture models in this domain. Attention weight analysis revealed that FinchGPT effectively captures long-range dependencies within syllables sequences. Furthermore, reverse engineering approaches demonstrated the impact of computational and biological manipulations on its performance: restricting FinchGPT's attention span and disrupting birdsong syntax through the ablation of specific brain nuclei markedly influenced the model's outputs. Our study highlights the transformative potential of large language models (LLMs) in deciphering the complexities of animal vocalizations, offering a novel framework for exploring the structural properties of non-human communication systems while shedding light on the computational distinctions between biological brains and artificial neural networks.

### Challenges and Innovations in LLM-Powered Fake News Detection: A Synthesis of Approaches and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.00339)] [[cool](https://papers.cool/arxiv/2502.00339)] [[pdf](https://arxiv.org/pdf/2502.00339)]
> **Authors**: Jingyuan Yi,Zeqiu Xu,Tianyi Huang,Peiyang Yu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: The pervasiveness of the dissemination of fake news through social media platforms poses critical risks to the trust of the general public, societal stability, and democratic institutions. This challenge calls for novel methodologies in detection, which can keep pace with the dynamic and multi-modal nature of misinformation. Recent works include powering the detection using large language model advances in multimodal frameworks, methodologies using graphs, and adversarial training in the literature of fake news. Based on the different approaches which can bring success, some key highlights will be underlined: enhanced LLM-improves accuracy through more advanced semantics and cross-modality fusion for robust detections. The review further identifies critical gaps in adaptability to dynamic social media trends, real-time, and cross-platform detection capabilities, as well as the ethical challenges thrown up by the misuse of LLMs. Future directions underline the development of style-agnostic models, cross-lingual detection frameworks, and robust policies with a view to mitigating LLM-driven misinformation. This synthesis thus lays a concrete foundation for those researchers and practitioners committed to reinforcing fake news detection systems with complications that keep on growing in the digital landscape.

### UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00334)] [[cool](https://papers.cool/arxiv/2502.00334)] [[pdf](https://arxiv.org/pdf/2502.00334)]
> **Authors**: Xin Xu,Qiyun Xu,Tong Xiao,Tianhao Chen,Yuchen Yan,Jiaxin Zhang,Shizhe Diao,Can Yang,Yang Wang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics .

### MODS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections 
[[arxiv](https://arxiv.org/abs/2502.00322)] [[cool](https://papers.cool/arxiv/2502.00322)] [[pdf](https://arxiv.org/pdf/2502.00322)]
> **Authors**: Nishant Balepur,Alexa Siu,Nedim Lipka,Franck Dernoncourt,Tong Sun,Jordan Boyd-Graber,Puneet Mathur
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted at NAACL 2025(main)
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: Query-focused summarization (QFS) gives a summary of documents to answer a query. Past QFS work assumes queries have one answer, ignoring debatable ones (Is law school worth it?). We introduce Debatable QFS (DQFS), a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must comprehensively cover all sources and balance perspectives, favoring no side. These goals elude LLM QFS systems, which: 1) lack structured content plans, failing to guide LLMs to write balanced summaries, and 2) use the same query to retrieve contexts across documents, failing to cover all perspectives specific to each document's content. To overcome this, we design MODS, a multi-LLM framework mirroring human panel discussions. MODS treats documents as individual Speaker LLMs and has a Moderator LLM that picks speakers to respond to tailored queries for planned topics. Speakers use tailored queries to retrieve relevant contexts from their documents and supply perspectives, which are tracked in a rich outline, yielding a content plan to guide the final summary. Experiments on ConflictingQA with controversial web queries and DebateQFS, our new dataset of debate queries from Debatepedia, show MODS beats SOTA by 38-59% in topic paragraph coverage and balance, based on new citation metrics. Users also find MODS's summaries to be readable and more balanced.

### DEUCE: Dual-diversity Enhancement and Uncertainty-awareness for Cold-start Active Learning 
[[arxiv](https://arxiv.org/abs/2502.00305)] [[cool](https://papers.cool/arxiv/2502.00305)] [[pdf](https://arxiv.org/pdf/2502.00305)]
> **Authors**: Jiaxin Guo,C. L. Philip Chen,Shuzhen Li,Tong Zhang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 3 figures, 12 tables. Accepted manuscript by TACL. For published version by MIT Press, see https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00731/125950
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Cold-start active learning (CSAL) selects valuable instances from an unlabeled dataset for manual annotation. It provides high-quality data at a low annotation cost for label-scarce text classification. However, existing CSAL methods overlook weak classes and hard representative examples, resulting in biased learning. To address these issues, this paper proposes a novel dual-diversity enhancing and uncertainty-aware (DEUCE) framework for CSAL. Specifically, DEUCE leverages a pretrained language model (PLM) to efficiently extract textual representations, class predictions, and predictive uncertainty. Then, it constructs a Dual-Neighbor Graph (DNG) to combine information on both textual diversity and class diversity, ensuring a balanced data distribution. It further propagates uncertainty information via density-based clustering to select hard representative instances. DEUCE performs well in selecting class-balanced and hard representative data by dual-diversity and informativeness. Experiments on six NLP datasets demonstrate the superiority and efficiency of DEUCE.

### Contextual Morphogenesis in Large Language Models: A Novel Approach to Self-Organizing Token Representations 
[[arxiv](https://arxiv.org/abs/2502.00301)] [[cool](https://papers.cool/arxiv/2502.00301)] [[pdf](https://arxiv.org/pdf/2502.00301)]
> **Authors**: Alistair Dombrowski,Beatrix Engelhardt,Dimitri Fairbrother,Henry Evidail
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Token representations influence the efficiency and adaptability of language models, yet conventional tokenization strategies impose rigid segmentation boundaries that do not adjust dynamically to evolving contextual relationships. The introduction of contextual morphogenesis establishes a self-organizing mechanism that restructures token boundaries based on learned contextual dependencies, allowing embeddings to evolve progressively across iterative processing steps. Empirical evaluations demonstrate that dynamically adjusted tokenization contributes to reductions in perplexity while maintaining representational stability, particularly in linguistically complex domains where static segmentation fails to capture nuanced dependencies. Computational trade-offs associated with self-organizing token structures indicate that additional processing overhead remains within feasible limits, provided that optimization strategies account for segmentation update efficiency. Comparative assessments across different linguistic corpora suggest that adaptive tokenization preserves interpretability while improving alignment with contextual cues, reinforcing the potential of morphogenetic segmentation mechanisms to refine predictive accuracy. Stability analyses confirm that evolving token structures maintain consistent segmentation behaviors across varied text distributions, ensuring that representational adaptations remain linguistically coherent. The effectiveness of contextual morphogenesis in refining structural stability and predictive performance highlights its viability as an alternative to traditional tokenization methods. Further analysis of computational efficiency considerations suggests that hybrid strategies integrating both static and dynamic segmentation techniques may offer a balanced approach to optimizing representational flexibility while maintaining inference efficiency.

### ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.00299)] [[cool](https://papers.cool/arxiv/2502.00299)] [[pdf](https://arxiv.org/pdf/2502.00299)]
> **Authors**: Xiang Liu,Zhenheng Tang,Peijie Dong,Zeyu Li,Bo Li,Xuming Hu,Xiaowen Chu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 35 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\% performance improvement under aggressive compression ratios compared to existing methods.

### Estimating LLM Uncertainty with Logits 
[[arxiv](https://arxiv.org/abs/2502.00290)] [[cool](https://papers.cool/arxiv/2502.00290)] [[pdf](https://arxiv.org/pdf/2502.00290)]
> **Authors**: Huan Ma,Jingdong Chen,Guangyu Wang,Changqing Zhang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Fixed some data errors in Table 1
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In recent years, Large Language Models (LLMs) have seen remarkable advancements and have been extensively integrated across various fields. Despite their progress, LLMs are prone to hallucinations, producing responses that may not be dependable if the models lack sufficient grounding knowledge. To mitigate this issue, methods for estimating uncertainty have been adopted, with a focus on critical tokens as indicators of reliability. Nevertheless, probability-based approaches have shown limitations in assessing token-level reliability due to the erosion of evidence strength information acquired during training. In this paper, we introduce Logits-induced Token Uncertainty (LogU), a novel framework designed to estimate token-specific uncertainty in LLMs in real time, without the need for multiple sampling rounds. By leveraging evidence modeling for the implementation of LogU, we utilize the derived uncertainty measures to steer downstream tasks. Our experimental findings highlight the substantial effectiveness and potential of LogU, marking a significant advancement in addressing the challenge of model hallucinations.

### Scaling Flaws of Verifier-Guided Search in Mathematical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.00271)] [[cool](https://papers.cool/arxiv/2502.00271)] [[pdf](https://arxiv.org/pdf/2502.00271)]
> **Authors**: Fei Yu,Yingru Li,Benyou Wang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) struggle with multi-step reasoning, where inference-time scaling has emerged as a promising strategy for performance improvement. Verifier-guided search outperforms repeated sampling when sample size is limited by selecting and prioritizing valid reasoning paths. However, we identify a critical limitation: scaling flaws, prevalent across different models (Mistral 7B and DeepSeekMath 7B), benchmarks (GSM8K and MATH), and verifiers (outcome value models and process reward models). As sample size increases, verifier-guided search exhibits diminishing advantages and eventually underperforms repeated sampling. Our analysis attributes this to verifier failures, where imperfect verifiers misrank candidates and erroneously prune all valid paths. These issues are further exacerbated in challenging and out-of-distribution problems, restricting search effectiveness. To mitigate verifier failures, we explore reducing reliance on verifiers and conduct preliminary investigations using two simple methods. Our findings reveal fundamental limitations in verifier-guided search and suggest future directions.

### Context-Preserving Tensorial Reconfiguration in Large Language Model Training 
[[arxiv](https://arxiv.org/abs/2502.00246)] [[cool](https://papers.cool/arxiv/2502.00246)] [[pdf](https://arxiv.org/pdf/2502.00246)]
> **Authors**: Larin Tonix,Morgana Baskerville,Nathaniel Stourton,Ophelia Tattershall
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Handling long-range dependencies in neural architectures has remained a persistent challenge due to computational limitations and inefficient contextual retention mechanisms. Tensorial operations have provided a foundation for restructuring model representations, yet conventional architectures have struggled to incorporate such techniques without introducing excessive complexity. A novel approach, Context-Preserving Tensorial Reconfiguration (CPTR), enables dynamic reorganization of weight tensors through structured factorization and adaptive contraction, allowing for enhanced contextual integration without substantial computational overhead. Empirical evaluations demonstrate that CPTR improves coherence retention across extended sequences, leading to measurable reductions in perplexity and improved recall accuracy for long-context tasks. Performance comparisons reveal that CPTR-enhanced models exhibit greater computational efficiency and reduced memory consumption while maintaining competitive language generation fluency and accuracy. Gradient stability metrics further validate the improved training efficiency, revealing more controlled variance in weight updates. Comparative studies across baseline and CPTR-enhanced models confirm that tensorial reconfiguration contributes to more stable and computationally efficient language modeling. The findings support the potential of CPTR in refining contemporary neural architectures for tasks requiring long-range contextual understanding and efficient memory utilization.

### Resolving Editing-Unlearning Conflicts: A Knowledge Codebook Framework for Large Language Model Updating 
[[arxiv](https://arxiv.org/abs/2502.00158)] [[cool](https://papers.cool/arxiv/2502.00158)] [[pdf](https://arxiv.org/pdf/2502.00158)]
> **Authors**: Binchi Zhang,Zhengzhang Chen,Zaiyi Zheng,Jundong Li,Haifeng Chen
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) excel in natural language processing by encoding extensive human knowledge, but their utility relies on timely updates as knowledge evolves. Updating LLMs involves two key tasks simultaneously: unlearning to remove unwanted knowledge and editing to incorporate new information. Existing methods face two major challenges: ineffective knowledge storage (either too sparse or too dense) and task conflicts between editing and unlearning, as validated through our theoretical and experimental results. To address these issues, we propose LOKA, a conflict-free framework for LLM updating based on a knowledge codebook. During training, updated knowledge is stored in multiple codebook memories. To optimize knowledge storage, a similarity-aware knowledge mapping ensures that related knowledge pieces are clustered and allocated to the same memory. Additionally, LOKA resolves task conflicts by employing task-specific and multi-task memories guided by a conflict score. In the inference stage, LOKA retrieves the most relevant memory from the codebook and plugs it into the original LLM to apply the updated knowledge. A learning-based router controls codebook activation to further improve knowledge utilization. Extensive experiments demonstrate the effectiveness of LOKA in LLM knowledge updating tasks.

### A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical Alignment of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00136)] [[cool](https://papers.cool/arxiv/2502.00136)] [[pdf](https://arxiv.org/pdf/2502.00136)]
> **Authors**: Edward Y. Chang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 6 tables, 6 figures. arXiv admin note: substantial text overlap with arXiv:2405.07076
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper introduces a three-branch checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by governmental systems. It implements three independent yet interacting components: LLMs as the executive branch for knowledge generation, DIKE as the legislative branch establishing ethical guardrails, and ERIS as the judicial branch for contextual interpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse cultural contexts while upholding consistent ethical principles. This architecture addresses limitations of reinforcement learning with human feedback (RLHF) by providing interpretable, adaptable, and culturally-aware ethical reasoning. Through self-supervised learning and adversarial testing, our framework demonstrates how emotional modeling can guide linguistic behaviors toward ethical outcomes while preserving independence across knowledge generation, ethical oversight, and contextual interpretation.

### Sparse Autoencoder Insights on Voice Embeddings 
[[arxiv](https://arxiv.org/abs/2502.00127)] [[cool](https://papers.cool/arxiv/2502.00127)] [[pdf](https://arxiv.org/pdf/2502.00127)]
> **Authors**: Daniel Pluth,Yu Zhou,Vijay K. Gurbani
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in explainable machine learning have highlighted the potential of sparse autoencoders in uncovering mono-semantic features in densely encoded embeddings. While most research has focused on Large Language Model (LLM) embeddings, the applicability of this technique to other domains remains largely unexplored. This study applies sparse autoencoders to speaker embeddings generated from a Titanet model, demonstrating the effectiveness of this technique in extracting mono-semantic features from non-textual embedded data. The results show that the extracted features exhibit characteristics similar to those found in LLM embeddings, including feature splitting and steering. The analysis reveals that the autoencoder can identify and manipulate features such as language and music, which are not evident in the original embedding. The findings suggest that sparse autoencoders can be a valuable tool for understanding and interpreting embedded data in many domains, including audio-based speaker recognition.

### Ensembles of Low-Rank Expert Adapters 
[[arxiv](https://arxiv.org/abs/2502.00089)] [[cool](https://papers.cool/arxiv/2502.00089)] [[pdf](https://arxiv.org/pdf/2502.00089)]
> **Authors**: Yinghao Li,Vianne Gao,Chao Zhang,MohamadAli Torkamani
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 29 pages, 5 figures, 5 tables; proceedings in ICLR 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks, resulting in reduced downstream performance. Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset. Building on these insights, we propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve the model's capability to handle diverse tasks. ELREA clusters the training instructions based on their gradient directions, representing different areas of expertise and thereby reducing conflicts during optimization. Expert adapters are then trained on these clusters, utilizing the low-rank adaptation (LoRA) technique to ensure training efficiency and model scalability. During inference, ELREA combines predictions from the most relevant expert adapters based on the input data's gradient similarity to the training clusters, ensuring optimal adapter selection for each task. Experiments show that our method outperforms baseline LoRA adapters trained on the full dataset and other ensemble approaches with similar training and inference complexity across a range of domain-specific tasks.

### Efficient Beam Search for Large Language Models Using Trie-Based Decoding 
[[arxiv](https://arxiv.org/abs/2502.00085)] [[cool](https://papers.cool/arxiv/2502.00085)] [[pdf](https://arxiv.org/pdf/2502.00085)]
> **Authors**: Brian J Chan,Jui-Hung Cheng,Mao Xun Huang,Chao-Ting Chen,Hen-Hsen Huang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In Transformer-based sequence-to-sequence generation, beam search has proven effective in enhancing the quality of generated sequences compared to greedy decoding. Conventional beam search methods typically adopt either a sequential or batch-based approach. The sequential approach, while memory-efficient, requires multiple decoding passes to construct a complete search tree, leading to significantly slower inference. On the other hand, the batch-based approach enables parallel computation across beams, but at the expense of high memory consumption due to the need to maintain separate key-value (KV) caches for each beam. In this study, we introduce a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache among all beams that share the same prefix, the proposed method not only reduces memory consumption dramatically but also enables parallel decoding across all branches. This innovative use of a prefix tree offers an efficient alternative for beam search, achieving significant memory savings while preserving inference speed, making it particularly well-suited for memory-constrained environments or large-scale model deployments.

### BTS: Harmonizing Specialized Experts into a Generalist LLM 
[[arxiv](https://arxiv.org/abs/2502.00075)] [[cool](https://papers.cool/arxiv/2502.00075)] [[pdf](https://arxiv.org/pdf/2502.00075)]
> **Authors**: Qizhen Zhang,Prajjwal Bhargava,Chloe Bi,Chris X. Cai,Jakob Foerster,Jeremy Fu,Punit Singh Koura,Ruan Silva,Sheng Shen,Emily Dinan,Suchin Gururangan,Mike Lewis
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We present Branch-Train-Stitch (BTS), an efficient and flexible training algorithm for combining independently trained large language model (LLM) experts into a single, capable generalist model. Following Li et al., we start with a single seed language model which is branched into domain-specific (e.g., coding or math) experts with continual pretraining. BTS combines experts into a generalist model using lightweight stitch layers, which are inserted between frozen experts and the seed LLM, and trained on a small datamix of the expert domains. Stitch layers enable the seed LLM to integrate representations from any number of experts during the forward pass, allowing it to generalize to new domains, despite remaining frozen. Because BTS does not alter the constituent LLMs, BTS provides a modular and flexible approach: experts can be easily removed and new experts can be added with only a small amount of training. Compared to alternative model merging approaches, BTS yields the best generalist performance on a variety of downstream tasks, retaining the specialized capabilities of each of the experts.

### A Multi-Layered Large Language Model Framework for Disease Prediction 
[[arxiv](https://arxiv.org/abs/2502.00063)] [[cool](https://papers.cool/arxiv/2502.00063)] [[pdf](https://arxiv.org/pdf/2502.00063)]
> **Authors**: Malak Mohamed,Rokaia Emad,Ali Hamdi
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Social telehealth has revolutionized healthcare by enabling patients to share symptoms and receive medical consultations remotely. Users frequently post symptoms on social media and online health platforms, generating a vast repository of medical data that can be leveraged for disease classification and symptom severity assessment. Large language models (LLMs), such as LLAMA3, GPT-3.5 Turbo, and BERT, process complex medical data to enhance disease classification. This study explores three Arabic medical text preprocessing techniques: text summarization, text refinement, and Named Entity Recognition (NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best performance was achieved using CAMeL-BERT with NER-augmented text (83% type classification, 69% severity assessment). Non-fine-tuned models performed poorly (13%-20% type classification, 40%-49% severity assessment). Integrating LLMs into social telehealth systems enhances diagnostic accuracy and treatment outcomes.

### MALT: Mechanistic Ablation of Lossy Translation in LLMs for a Low-Resource Language: Urdu 
[[arxiv](https://arxiv.org/abs/2502.00041)] [[cool](https://papers.cool/arxiv/2502.00041)] [[pdf](https://arxiv.org/pdf/2502.00041)]
> **Authors**: Taaha Saleem Bajwa
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-04
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: LLMs are predominantly trained on English data, which leads to a significant drop in performance on low-resource languages. Understanding how LLMs handle these languages is crucial for improving their effectiveness. This study focuses on Urdu as a use case for exploring the challenges faced by LLMs in processing low-resource languages. LLMs primarily reason in English when prompted in another language, with the final layers acting as translators to convert the English response into the target language. This study finds that even for low-resource languages, the internal latent response of LLMs in English is quite coherent; however, the translation features are lossy and result in poor translations, leading to reduced performance. By mechanistically removing these translation features and using a separate translation model to translate the internal latent response of LLM, the performance of LLMs improves significantly while also preserving the cultural nuances of the input in low-resource languages.

## 密码学和安全(cs.CR:Cryptography and Security)

### Security and Quality in LLM-Generated Code: A Multi-Language, Multi-Model Analysis 
[[arxiv](https://arxiv.org/abs/2502.01853)] [[cool](https://papers.cool/arxiv/2502.01853)] [[pdf](https://arxiv.org/pdf/2502.01853)]
> **Authors**: Mohammed Kharma,Soohyeon Choi,Mohammed AlKhanafseh,David Mohaisen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 10 tables. In submission to IEEE Transactions on Dependable and Secure Computing
- **标题**: None
- **领域**: 密码学和安全,机器学习,软件工程
- **Abstract**: Artificial Intelligence (AI)-driven code generation tools are increasingly used throughout the software development lifecycle to accelerate coding tasks. However, the security of AI-generated code using Large Language Models (LLMs) remains underexplored, with studies revealing various risks and weaknesses. This paper analyzes the security of code generated by LLMs across different programming languages. We introduce a dataset of 200 tasks grouped into six categories to evaluate the performance of LLMs in generating secure and maintainable code. Our research shows that while LLMs can automate code creation, their security effectiveness varies by language. Many models fail to utilize modern security features in recent compiler and toolkit updates, such as Java 17. Moreover, outdated methods are still commonly used, particularly in C++. This highlights the need for advancing LLMs to enhance security and quality while incorporating emerging best practices in programming languages.

### The dark deep side of DeepSeek: Fine-tuning attacks against the safety alignment of CoT-enabled models 
[[arxiv](https://arxiv.org/abs/2502.01225)] [[cool](https://papers.cool/arxiv/2502.01225)] [[pdf](https://arxiv.org/pdf/2502.01225)]
> **Authors**: Zhiyuan Xu,Joseph Gardiner,Sana Belguith
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 12 Pages
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large language models are typically trained on vast amounts of data during the pre-training phase, which may include some potentially harmful information. Fine-tuning attacks can exploit this by prompting the model to reveal such behaviours, leading to the generation of harmful content. In this paper, we focus on investigating the performance of the Chain of Thought based reasoning model, DeepSeek, when subjected to fine-tuning attacks. Specifically, we explore how fine-tuning manipulates the model's output, exacerbating the harmfulness of its responses while examining the interaction between the Chain of Thought reasoning and adversarial inputs. Through this study, we aim to shed light on the vulnerability of Chain of Thought enabled models to fine-tuning attacks and the implications for their safety and ethical deployment.

### Encrypted Large Model Inference: The Equivariant Encryption Paradigm 
[[arxiv](https://arxiv.org/abs/2502.01013)] [[cool](https://papers.cool/arxiv/2502.01013)] [[pdf](https://arxiv.org/pdf/2502.01013)]
> **Authors**: James Buban,Hongyang Zhang,Claudio Angione,Harry Yang,Ahmad Farhan,Seyfal Sultanov,Michael Du,Xuran Ma,Zihao Wang,Yue Zhao,Arria Owlia,Fielding Johnston,Patrick Colangelo
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large scale deep learning model, such as modern language models and diffusion architectures, have revolutionized applications ranging from natural language processing to computer vision. However, their deployment in distributed or decentralized environments raises significant privacy concerns, as sensitive data may be exposed during inference. Traditional techniques like secure multi-party computation, homomorphic encryption, and differential privacy offer partial remedies but often incur substantial computational overhead, latency penalties, or limited compatibility with non-linear network operations. In this work, we introduce Equivariant Encryption (EE), a novel paradigm designed to enable secure, "blind" inference on encrypted data with near zero performance overhead. Unlike fully homomorphic approaches that encrypt the entire computational graph, EE selectively obfuscates critical internal representations within neural network layers while preserving the exact functionality of both linear and a prescribed set of non-linear operations. This targeted encryption ensures that raw inputs, intermediate activations, and outputs remain confidential, even when processed on untrusted infrastructure. We detail the theoretical foundations of EE, compare its performance and integration complexity against conventional privacy preserving techniques, and demonstrate its applicability across a range of architectures, from convolutional networks to large language models. Furthermore, our work provides a comprehensive threat analysis, outlining potential attack vectors and baseline strategies, and benchmarks EE against standard inference pipelines in decentralized settings. The results confirm that EE maintains high fidelity and throughput, effectively bridging the gap between robust data confidentiality and the stringent efficiency requirements of modern, large scale model inference.

### SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00847)] [[cool](https://papers.cool/arxiv/2502.00847)] [[pdf](https://arxiv.org/pdf/2502.00847)]
> **Authors**: Jiawen Zhang,Kejia Chen,Zunlei Feng,Jian Lou,Mingli Song,Jian Liu,Xiaohu Yang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: With the growing popularity of LLMs among the general public users, privacy-preserving and adversarial robustness have become two pressing demands for LLM-based services, which have largely been pursued separately but rarely jointly. In this paper, to the best of our knowledge, we are among the first attempts towards robust and private LLM inference by tightly integrating two disconnected fields: private inference and prompt ensembling. The former protects users' privacy by encrypting inference data transmitted and processed by LLMs, while the latter enhances adversarial robustness by yielding an aggregated output from multiple prompted LLM responses. Although widely recognized as effective individually, private inference for prompt ensembling together entails new challenges that render the naive combination of existing techniques inefficient. To overcome the hurdles, we propose SecPE, which designs efficient fully homomorphic encryption (FHE) counterparts for the core algorithmic building blocks of prompt ensembling. We conduct extensive experiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of SecPE. The results show that SecPE maintains high clean accuracy and offers better robustness at the expense of merely $2.5\%$ efficiency overhead compared to baseline private inference methods, indicating a satisfactory ``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the encrypted Argmax operation that incurs major slowdown for prompt ensembling, SecPE is 35.4x faster than the state-of-the-art peers, which can be of independent interest beyond this work.

### Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense 
[[arxiv](https://arxiv.org/abs/2502.00840)] [[cool](https://papers.cool/arxiv/2502.00840)] [[pdf](https://arxiv.org/pdf/2502.00840)]
> **Authors**: Jiawen Zhang,Kejia Chen,Lipeng He,Jian Lou,Dan Li,Zunlei Feng,Mingli Song,Jian Liu,Kui Ren,Xiaohu Yang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 19 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, and Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven sota techniques across three popular categories, revealing consistent safety degradation across ten safety-aligned LLMs.

### AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds 
[[arxiv](https://arxiv.org/abs/2502.00757)] [[cool](https://papers.cool/arxiv/2502.00757)] [[pdf](https://arxiv.org/pdf/2502.00757)]
> **Authors**: J Rosser,Jakob Nicolaus Foerster
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: :68T42; 68T50ACM Class:I.2.11
- **标题**: None
- **领域**: 密码学和安全,人工智能,神经和进化计算
- **Abstract**: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been as thoroughly explored. In this paper, we introduce AGENTBREEDER a framework for multi-objective evolutionary search over scaffolds. Our REDAGENTBREEDER evolves scaffolds towards jailbreaking the base LLM while achieving high task success, while BLUEAGENTBREEDER instead aims to combine safety with task reward. We evaluate the systems discovered by the different instances of AGENTBREEDER and popular baselines using widely recognized reasoning, mathematics, and safety benchmarks. Our work highlights and mitigates the safety risks due to multi-agent scaffolding.

### `Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.00735)] [[cool](https://papers.cool/arxiv/2502.00735)] [[pdf](https://arxiv.org/pdf/2502.00735)]
> **Authors**: Chun Wai Chiu,Linghan Huang,Bo Li,Huaming Chen
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,软件工程
- **Abstract**: Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flanking Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios.

### Model Provenance Testing for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00706)] [[cool](https://papers.cool/arxiv/2502.00706)] [[pdf](https://arxiv.org/pdf/2502.00706)]
> **Authors**: Ivica Nikolic,Teodora Baluta,Prateek Saxena
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算语言学,机器学习
- **Abstract**: Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available.

### TrojanTime: Backdoor Attacks on Time Series Classification 
[[arxiv](https://arxiv.org/abs/2502.00646)] [[cool](https://papers.cool/arxiv/2502.00646)] [[pdf](https://arxiv.org/pdf/2502.00646)]
> **Authors**: Chang Dong,Zechao Sun,Guangdong Bai,Shuying Piao,Weitong Chen,Wei Emma Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 13 pages, 3 figures, 3 tables
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Time Series Classification (TSC) is highly vulnerable to backdoor attacks, posing significant security threats. Existing methods primarily focus on data poisoning during the training phase, designing sophisticated triggers to improve stealthiness and attack success rate (ASR). However, in practical scenarios, attackers often face restrictions in accessing training data. Moreover, it is a challenge for the model to maintain generalization ability on clean test data while remaining vulnerable to poisoned inputs when data is inaccessible. To address these challenges, we propose TrojanTime, a novel two-step training algorithm. In the first stage, we generate a pseudo-dataset using an external arbitrary dataset through target adversarial attacks. The clean model is then continually trained on this pseudo-dataset and its poisoned version. To ensure generalization ability, the second stage employs a carefully designed training strategy, combining logits alignment and batch norm freezing. We evaluate TrojanTime using five types of triggers across four TSC architectures in UCR benchmark datasets from diverse domains. The results demonstrate the effectiveness of TrojanTime in executing backdoor attacks while maintaining clean accuracy. Finally, to mitigate this threat, we propose a defensive unlearning strategy that effectively reduces the ASR while preserving clean accuracy.

### Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation 
[[arxiv](https://arxiv.org/abs/2502.00580)] [[cool](https://papers.cool/arxiv/2502.00580)] [[pdf](https://arxiv.org/pdf/2502.00580)]
> **Authors**: Stuart Armstrong,Matija Franklin,Connor Stevens,Rebecca Gorman
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: :I.2.0
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,计算机与社会
- **Abstract**: Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random augmentations (such as capitalization, punctuation, etc) is effective against all major large language models (LLMs). We have found that $100\%$ of the BoN paper's successful jailbreaks (confidence interval $[99.65\%, 100.00\%]$) and $99.8\%$ of successful jailbreaks in our replication (confidence interval $[99.28\%, 99.98\%]$) were blocked with our Defense Against The Dark Prompts (DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation LLM to evaluate a prompt for dangerous or manipulative behaviors--unlike some other approaches, DATDP also explicitly looks for jailbreaking attempts--until a robust safety rating is generated. This success persisted even when utilizing smaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved almost equally capable). These results show that, though language models are sensitive to seemingly innocuous changes to inputs, they seem also capable of successfully evaluating the dangers of these inputs. Versions of DATDP can therefore be added cheaply to generative AI systems to produce an immediate significant increase in safety.

### Data Overvaluation Attack and Truthful Data Valuation 
[[arxiv](https://arxiv.org/abs/2502.00494)] [[cool](https://papers.cool/arxiv/2502.00494)] [[pdf](https://arxiv.org/pdf/2502.00494)]
> **Authors**: Shuyuan Zheng,Sudong Cai,Chuan Xiao,Yang Cao,Jianbin Qin,Masatoshi Yoshikawa,Makoto Onizuka
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: In collaborative machine learning, data valuation, i.e., evaluating the contribution of each client' data to the machine learning model, has become a critical task for incentivizing and selecting positive data contributions. However, existing studies often assume that clients engage in data valuation truthfully, overlooking the practical motivation for clients to exaggerate their contributions. To unlock this threat, this paper introduces the first data overvaluation attack, enabling strategic clients to have their data significantly overvalued. Furthermore, we propose a truthful data valuation metric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees some promising axioms for data valuation while ensuring that clients' optimal strategy is to perform truthful data valuation. Our experiments demonstrate the vulnerability of existing data valuation metrics to the data overvaluation attack and validate the robustness and effectiveness of Truth-Shapley.

### It's Not Just a Phase: On Investigating Phase Transitions in Deep Learning-based Side-channel Analysis 
[[arxiv](https://arxiv.org/abs/2502.00384)] [[cool](https://papers.cool/arxiv/2502.00384)] [[pdf](https://arxiv.org/pdf/2502.00384)]
> **Authors**: Sengim Karayalçin,Marina Krček,Stjepan Picek
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 13 figures, 1 table
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Side-channel analysis (SCA) represents a realistic threat where the attacker can observe unintentional information to obtain secret data. Evaluation labs also use the same SCA techniques in the security certification process. The results in the last decade have shown that machine learning, especially deep learning, is an extremely powerful SCA approach, allowing the breaking of protected devices while achieving optimal attack performance. Unfortunately, deep learning operates as a black-box, making it less useful for security evaluators who must understand how attacks work to prevent them in the future. This work demonstrates that mechanistic interpretability can effectively scale to realistic scenarios where relevant information is sparse and well-defined interchange interventions to the input are impossible due to side-channel protections. Concretely, we reverse engineer the features the network learns during phase transitions, eventually retrieving secret masks, allowing us to move from black-box to white-box evaluation.

### Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.00306)] [[cool](https://papers.cool/arxiv/2502.00306)] [[pdf](https://arxiv.org/pdf/2502.00306)]
> **Authors**: Ali Naseh,Yuefeng Peng,Anshuman Suri,Harsh Chaudhari,Alina Oprea,Amir Houmansadr
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,信息检索,机器学习
- **Abstract**: Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.

### LLM Cyber Evaluations Don't Capture Real-World Risk 
[[arxiv](https://arxiv.org/abs/2502.00072)] [[cool](https://papers.cool/arxiv/2502.00072)] [[pdf](https://arxiv.org/pdf/2502.00072)]
> **Authors**: Kamilė Lukošiūtė,Adam Swanda
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 11 pages
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学,机器学习
- **Abstract**: Large language models (LLMs) are demonstrating increasing prowess in cybersecurity applications, creating creating inherent risks alongside their potential for strengthening defenses. In this position paper, we argue that current efforts to evaluate risks posed by these capabilities are misaligned with the goal of understanding real-world impact. Evaluating LLM cybersecurity risk requires more than just measuring model capabilities -- it demands a comprehensive risk assessment that incorporates analysis of threat actor adoption behavior and potential for impact. We propose a risk assessment framework for LLM cyber capabilities and apply it to a case study of language models used as cybersecurity assistants. Our evaluation of frontier models reveals high compliance rates but moderate accuracy on realistic cyber assistance tasks. However, our framework suggests that this particular use case presents only moderate risk due to limited operational advantages and impact potential. Based on these findings, we recommend several improvements to align research priorities with real-world impact assessment, including closer academia-industry collaboration, more realistic modeling of attacker behavior, and inclusion of economic metrics in evaluations. This work represents an important step toward more effective assessment and mitigation of LLM-enabled cybersecurity risks.

### Privacy Preserving Charge Location Prediction for Electric Vehicles 
[[arxiv](https://arxiv.org/abs/2502.00068)] [[cool](https://papers.cool/arxiv/2502.00068)] [[pdf](https://arxiv.org/pdf/2502.00068)]
> **Authors**: Robert Marlin,Raja Jurdak,Alsharif Abuadbba,Dimity Miller
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 7 figures, IEEE Journal paper
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: By 2050, electric vehicles (EVs) are projected to account for 70% of global vehicle sales. While EVs provide environmental benefits, they also pose challenges for energy generation, grid infrastructure, and data privacy. Current research on EV routing and charge management often overlooks privacy when predicting energy demands, leaving sensitive mobility data vulnerable. To address this, we developed a Federated Learning Transformer Network (FLTN) to predict EVs' next charge location with enhanced privacy measures. Each EV operates as a client, training an onboard FLTN model that shares only model weights, not raw data with a community-based Distributed Energy Resource Management System (DERMS), which aggregates them into a community global model. To further enhance privacy, non-transitory EVs use peer-to-peer weight sharing and augmentation within their community, obfuscating individual contributions and improving model accuracy. Community DERMS global model weights are then redistributed to EVs for continuous training. Our FLTN approach achieved up to 92% accuracy while preserving data privacy, compared to our baseline centralised model, which achieved 98% accuracy with no data privacy. Simulations conducted across diverse charge levels confirm the FLTN's ability to forecast energy demands over extended periods. We present a privacy-focused solution for forecasting EV charge location prediction, effectively mitigating data leakage risks.

### Evaluating Large Language Models in Vulnerability Detection Under Variable Context Windows 
[[arxiv](https://arxiv.org/abs/2502.00064)] [[cool](https://papers.cool/arxiv/2502.00064)] [[pdf](https://arxiv.org/pdf/2502.00064)]
> **Authors**: Jie Lin,David Mohaisen
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 5 pages, 2 tables. Appeared in ICMLA 2024
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: This study examines the impact of tokenized Java code length on the accuracy and explicitness of ten major LLMs in vulnerability detection. Using chi-square tests and known ground truth, we found inconsistencies across models: some, like GPT-4, Mistral, and Mixtral, showed robustness, while others exhibited a significant link between tokenized length and performance. We recommend future LLM development focus on minimizing the influence of input length for better vulnerability detection. Additionally, preprocessing techniques that reduce token count while preserving code structure could enhance LLM accuracy and explicitness in these tasks.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### One Diffusion Step to Real-World Super-Resolution via Flow Trajectory Distillation 
[[arxiv](https://arxiv.org/abs/2502.01993)] [[cool](https://papers.cool/arxiv/2502.01993)] [[pdf](https://arxiv.org/pdf/2502.01993)]
> **Authors**: Jianze Li,Jiezhang Cao,Yong Guo,Wenbo Li,Yulun Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Diffusion models (DMs) have significantly advanced the development of real-world image super-resolution (Real-ISR), but the computational cost of multi-step diffusion models limits their application. One-step diffusion models generate high-quality images in a one sampling step, greatly reducing computational overhead and inference latency. However, most existing one-step diffusion methods are constrained by the performance of the teacher model, where poor teacher performance results in image artifacts. To address this limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique based on flow matching models. We use the state-of-the-art diffusion model FLUX.1-dev as both the teacher model and the base model. First, we introduce Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model into a one-step Real-ISR. Second, to improve image realism and address high-frequency artifact issues in generated images, we propose TV-LPIPS as a perceptual loss and introduce Attention Diversification Loss (ADL) as a regularization term to reduce token similarity in transformer, thereby eliminating high-frequency artifacts. Comprehensive experiments demonstrate that our method outperforms existing one-step diffusion-based Real-ISR methods. The code and model will be released at https://github.com/JianzeLi-114/FluxSR.

### DCT-Mamba3D: Spectral Decorrelation and Spatial-Spectral Feature Extraction for Hyperspectral Image Classification 
[[arxiv](https://arxiv.org/abs/2502.01986)] [[cool](https://papers.cool/arxiv/2502.01986)] [[pdf](https://arxiv.org/pdf/2502.01986)]
> **Authors**: Weijia Cao,Xiaofei Yang,Yicong Zhou,Zheng Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Hyperspectral image classification presents challenges due to spectral redundancy and complex spatial-spectral dependencies. This paper proposes a novel framework, DCT-Mamba3D, for hyperspectral image classification. DCT-Mamba3D incorporates: (1) a 3D spectral-spatial decorrelation module that applies 3D discrete cosine transform basis functions to reduce both spectral and spatial redundancy, enhancing feature clarity across dimensions; (2) a 3D-Mamba module that leverages a bidirectional state-space model to capture intricate spatial-spectral dependencies; and (3) a global residual enhancement module that stabilizes feature representation, improving robustness and convergence. Extensive experiments on benchmark datasets show that our DCT-Mamba3D outperforms the state-of-the-art methods in challenging scenarios such as the same object in different spectra and different objects in the same spectra.

### AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs 
[[arxiv](https://arxiv.org/abs/2502.01977)] [[cool](https://papers.cool/arxiv/2502.01977)] [[pdf](https://arxiv.org/pdf/2502.01977)]
> **Authors**: Hongxin Li,Jingfan Chen,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Technical Report
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation. However, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale. In this work, we propose the \methodname{} pipeline for automatically annotating UI elements with detailed functionality descriptions at scale. Specifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor. We construct an \methodname{}-704k dataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets. Human evaluation shows that the AutoGUI pipeline achieves annotation correctness comparable to trained human annotators. Extensive experimental results show that our \methodname{}-704k dataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types. We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs. AutoGUI dataset can be viewed at this anonymous URL: https://autogui-project.github.io/.

### Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration 
[[arxiv](https://arxiv.org/abs/2502.01969)] [[cool](https://papers.cool/arxiv/2502.01969)] [[pdf](https://arxiv.org/pdf/2502.01969)]
> **Authors**: Younan Zhu,Linwei Tao,Minjing Dong,Chang Xu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent works attribute this issue to an inherent bias of LVLMs where vision token attention map has a fixed correlation with spatial position, and propose to mitigate this issue by reordering visual tokens. However, we find that different LVLMs exhibit different correlations between attention and spatial position, which makes the existing solution difficult to generalize to other LVLMs. To address this issue, we first introduce a training-free solution, Uniform Attention Calibration (UAC), that estimates the bias from single meaningless input image and applies a calibration matrix to rectify attention imbalances. To further alleviate the bias, we relax the assumption of single meaningless input in UAC and introduce a fine-tuning solution, Dynamic Attention Calibration (DAC), that enforces the consistent outputs wherever the object locates in the image via a plug-and-plays module. Comprehensive experiments across multiple benchmarks demonstrate that UAC and DAC significantly reduce object hallucination while improving general multimodal alignment. Our methods achieve state-of-the-art performance across diverse LVLM architectures on various metrics.

### Memory Efficient Transformer Adapter for Dense Predictions 
[[arxiv](https://arxiv.org/abs/2502.01962)] [[cool](https://papers.cool/arxiv/2502.01962)] [[pdf](https://arxiv.org/pdf/2502.01962)]
> **Authors**: Dong Zhang,Rui Yan,Pingcheng Dong,Kwang-Ting Cheng
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: This paper is accepted by ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While current Vision Transformer (ViT) adapter methods have shown promising accuracy, their inference speed is implicitly hindered by inefficient memory access operations, e.g., standard normalization and frequent reshaping. In this work, we propose META, a simple and fast ViT adapter that can improve the model's memory efficiency and decrease memory time consumption by reducing the inefficient memory access operations. Our method features a memory-efficient adapter block that enables the common sharing of layer normalization between the self-attention and feed-forward network layers, thereby reducing the model's reliance on normalization operations. Within the proposed block, the cross-shaped self-attention is employed to reduce the model's frequent reshaping operations. Moreover, we augment the adapter block with a lightweight convolutional branch that can enhance local inductive biases, particularly beneficial for the dense prediction tasks, e.g., object detection, instance segmentation, and semantic segmentation. The adapter block is finally formulated in a cascaded manner to compute diverse head features, thereby enriching the variety of feature representations. Empirically, extensive evaluations on multiple representative datasets validate that META substantially enhances the predicted quality, while achieving a new state-of-the-art accuracy-efficiency trade-off. Theoretically, we demonstrate that META exhibits superior generalization capability and stronger adaptability.

### Hierarchical Consensus Network for Multiview Feature Learning 
[[arxiv](https://arxiv.org/abs/2502.01961)] [[cool](https://papers.cool/arxiv/2502.01961)] [[pdf](https://arxiv.org/pdf/2502.01961)]
> **Authors**: Chengwei Xia,Chaoxi Niu,Kun Zhan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: AAAI 2025 accepted paper
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Multiview feature learning aims to learn discriminative features by integrating the distinct information in each view. However, most existing methods still face significant challenges in learning view-consistency features, which are crucial for effective multiview learning. Motivated by the theories of CCA and contrastive learning in multiview feature learning, we propose the hierarchical consensus network (HCN) in this paper. The HCN derives three consensus indices for capturing the hierarchical consensus across views, which are classifying consensus, coding consensus, and global consensus, respectively. Specifically, classifying consensus reinforces class-level correspondence between views from a CCA perspective, while coding consensus closely resembles contrastive learning and reflects contrastive comparison of individual instances. Global consensus aims to extract consensus information from two perspectives simultaneously. By enforcing the hierarchical consensus, the information within each view is better integrated to obtain more comprehensive and discriminative features. The extensive experimental results obtained on four multiview datasets demonstrate that the proposed method significantly outperforms several state-of-the-art methods.

### MATCNN: Infrared and Visible Image Fusion Method Based on Multi-scale CNN with Attention Transformer 
[[arxiv](https://arxiv.org/abs/2502.01959)] [[cool](https://papers.cool/arxiv/2502.01959)] [[pdf](https://arxiv.org/pdf/2502.01959)]
> **Authors**: Jingjing Liu,Li Zhang,Xiaoyang Zeng,Wanquan Liu,Jianhua Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While attention-based approaches have shown considerable progress in enhancing image fusion and addressing the challenges posed by long-range feature dependencies, their efficacy in capturing local features is compromised by the lack of diverse receptive field extraction techniques. To overcome the shortcomings of existing fusion methods in extracting multi-scale local features and preserving global features, this paper proposes a novel cross-modal image fusion approach based on a multi-scale convolutional neural network with attention Transformer (MATCNN). MATCNN utilizes the multi-scale fusion module (MSFM) to extract local features at different scales and employs the global feature extraction module (GFEM) to extract global features. Combining the two reduces the loss of detail features and improves the ability of global feature representation. Simultaneously, an information mask is used to label pertinent details within the images, aiming to enhance the proportion of preserving significant information in infrared images and background textures in visible images in fused images. Subsequently, a novel optimization algorithm is developed, leveraging the mask to guide feature extraction through the integration of content, structural similarity index measurement, and global feature loss. Quantitative and qualitative evaluations are conducted across various datasets, revealing that MATCNN effectively highlights infrared salient targets, preserves additional details in visible images, and achieves better fusion results for cross-modal images. The code of MATCNN will be available at https://github.com/zhang3849/MATCNN.git.

### DAMA: Data- and Model-aware Alignment of Multi-modal LLMs 
[[arxiv](https://arxiv.org/abs/2502.01943)] [[cool](https://papers.cool/arxiv/2502.01943)] [[pdf](https://arxiv.org/pdf/2502.01943)]
> **Authors**: Jinda Lu,Junkang Wu,Jinghan Li,Xiaojun Jia,Shuo Wang,YiFan Zhang,Junfeng Fang,Xiang Wang,Xiangnan He
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences. However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data. In this paper, we propose Data- and Model-aware DPO (DAMA) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that incorporates data hardness, and (2) a model-aware strategy that integrates real-time model responses. By combining the two strategies, DAMA enables the model to effectively adapt to data with varying levels of hardness. Extensive experiments on five benchmarks demonstrate that DAMA not only significantly enhances the trustworthiness, but also improves the effectiveness over general tasks. For instance, on the Object-HalBench, our DAMA-7B reduces response-level and mentioned-level hallucination by 90.0% and 95.3%, respectively, surpassing the performance of GPT-4V.

### Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.01940)] [[cool](https://papers.cool/arxiv/2502.01940)] [[pdf](https://arxiv.org/pdf/2502.01940)]
> **Authors**: Mohammed Alsakabi,Aidan Erickson,John M. Dolan,Ozan K. Tonguz
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: We present a cost-effective new approach for generating denser depth maps for Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images obtained from deep neural network (DNN) 4D radar detectors with conventional camera RGB images. Our approach introduces a novel pixel positional encoding algorithm inspired by Bartlett's spatial spectrum estimation technique. This algorithm transforms both radar depth maps and RGB images into a unified pixel image subspace called the Spatial Spectrum, facilitating effective learning based on their similarities and differences. Our method effectively leverages high-resolution camera images to train radar depth map generative models, addressing the limitations of conventional radar detectors in complex vehicular environments, thus sharpening the radar output. We develop spectrum estimation algorithms tailored for radar depth maps and RGB images, a comprehensive training framework for data-driven generative models, and a camera-radar deployment scheme for AV operation. Our results demonstrate that our approach also outperforms the state-of-the-art (SOTA) by 27.95% in terms of Unidirectional Chamfer Distance (UCD).

### PATCH: a deep learning method to assess heterogeneity of artistic practice in historical paintings 
[[arxiv](https://arxiv.org/abs/2502.01912)] [[cool](https://papers.cool/arxiv/2502.01912)] [[pdf](https://arxiv.org/pdf/2502.01912)]
> **Authors**: Andrew Van Horn,Lauryn Smith,Mahamad Mahmoud,Michael McMaster,Clara Pinchbeck,Ina Martin,Andrew Lininger,Anthony Ingrisano,Adam Lowe,Carlos Bayod,Elizabeth Bolman,Kenneth Singer,Michael Hinczewski
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: main text: 16 pages, 6 figures; SI: 7 pages, 3 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The history of art has seen significant shifts in the manner in which artworks are created, making understanding of creative processes a central question in technical art history. In the Renaissance and Early Modern period, paintings were largely produced by master painters directing workshops of apprentices who often contributed to projects. The masters varied significantly in artistic and managerial styles, meaning different combinations of artists and implements might be seen both between masters and within workshops or even individual canvases. Information on how different workshops were managed and the processes by which artworks were created remains elusive. Machine learning methods have potential to unearth new information about artists' creative processes by extending the analysis of brushwork to a microscopic scale. Analysis of workshop paintings, however, presents a challenge in that documentation of the artists and materials involved is sparse, meaning external examples are not available to train networks to recognize their contributions. Here we present a novel machine learning approach we call pairwise assignment training for classifying heterogeneity (PATCH) that is capable of identifying individual artistic practice regimes with no external training data, or "ground truth." The method achieves unsupervised results by supervised means, and outperforms both simple statistical procedures and unsupervised machine learning methods. We apply this method to two historical paintings by the Spanish Renaissance master, El Greco: The Baptism of Christ and Christ on the Cross with Landscape, and our findings regarding the former potentially challenge previous work that has assigned the painting to workshop members. Further, the results of our analyses create a measure of heterogeneity of artistic practice that can be used to characterize artworks across time and space.

### Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models 
[[arxiv](https://arxiv.org/abs/2502.01906)] [[cool](https://papers.cool/arxiv/2502.01906)] [[pdf](https://arxiv.org/pdf/2502.01906)]
> **Authors**: Chia-Wen Kuo,Sijie Zhu,Fan Chen,Xiaohui Shen,Longyin Wen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure. In this paper, we propose Decomposed Attention (D-Attn), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs. After the attention decomposition, D-Attn diagonalizes visual-to-visual self-attention, reducing computation from $\mathcal{O}(|V|^2)$ to $\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance. Moreover, D-Attn debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding. Finally, we introduce an $α$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM's capabilities with minimal modifications. Extensive experiments and rigorous analyses validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs. Code, data, and models will be publicly available.

### INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy 
[[arxiv](https://arxiv.org/abs/2502.01896)] [[cool](https://papers.cool/arxiv/2502.01896)] [[pdf](https://arxiv.org/pdf/2502.01896)]
> **Authors**: Nastaran Darabi,Divake Kumar,Sina Tayebati,Amit Ranjan Trivedi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks. INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds. The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions. The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience. INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40. Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods. This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems. INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications. INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios.

### SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset 
[[arxiv](https://arxiv.org/abs/2502.01894)] [[cool](https://papers.cool/arxiv/2502.01894)] [[pdf](https://arxiv.org/pdf/2502.01894)]
> **Authors**: Goodarz Mehr,Azim Eskandarian
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器人技术
- **Abstract**: Bird's-eye view (BEV) perception for autonomous driving has garnered significant attention in recent years, in part because BEV representation facilitates the fusion of multi-sensor data. This enables a variety of perception tasks including BEV segmentation, a concise view of the environment that can be used to plan a vehicle's trajectory. However, this representation is not fully supported by existing datasets, and creation of new datasets can be a time-consuming endeavor. To address this problem, in this paper we introduce SimBEV, an extensively configurable and scalable randomized synthetic data generation tool that incorporates information from multiple sources to capture accurate BEV ground truth data, supports a comprehensive array of sensors, and enables a variety of perception tasks including BEV segmentation and 3D object detection. We use SimBEV to create the SimBEV dataset, a large collection of annotated perception data from diverse driving scenarios.

### Geometric Framework for 3D Cell Segmentation Correction 
[[arxiv](https://arxiv.org/abs/2502.01890)] [[cool](https://papers.cool/arxiv/2502.01890)] [[pdf](https://arxiv.org/pdf/2502.01890)]
> **Authors**: Peter Chen,Bryan Chang,Olivia Annette Creasey,Julie Beth Sneddon,Yining Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 16 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: 3D cellular image segmentation methods are commonly divided into non-2D-based and 2D-based approaches, the latter reconstructing 3D shapes from the segmentation results of 2D layers. However, errors in 2D results often propagate, leading to oversegmentations in the final 3D results. To tackle this issue, we introduce an interpretable geometric framework that addresses the oversegmentations by correcting the 2D segmentation results based on geometric information from adjacent layers. Leveraging both geometric (layer-to-layer, 2D) and topological (3D shape) features, we use binary classification to determine whether neighboring cells should be stitched. We develop a pre-trained classifier on public plant cell datasets and validate its performance on animal cell datasets, confirming its effectiveness in correcting oversegmentations under the transfer learning setting. Furthermore, we demonstrate that our framework can be extended to correcting oversegmentation on non-2D-based methods. A clear pipeline is provided for end-users to build the pre-trained model to any labeled dataset.

### Explaining Automatic Image Assessment 
[[arxiv](https://arxiv.org/abs/2502.01873)] [[cool](https://papers.cool/arxiv/2502.01873)] [[pdf](https://arxiv.org/pdf/2502.01873)]
> **Authors**: Max Lisaius,Scott Wehrwein
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Previous work in aesthetic categorization and explainability utilizes manual labeling and classification to explain aesthetic scores. These methods require a complex labeling process and are limited in size. Our proposed approach attempts to explain aesthetic assessment models through visualizing dataset trends and automatic categorization of visual aesthetic features through training neural networks on different versions of the same dataset. By evaluating the models adapted to each specific modality using existing and novel metrics, we can capture and visualize aesthetic features and trends.

### Reliability-Driven LiDAR-Camera Fusion for Robust 3D Object Detection 
[[arxiv](https://arxiv.org/abs/2502.01856)] [[cool](https://papers.cool/arxiv/2502.01856)] [[pdf](https://arxiv.org/pdf/2502.01856)]
> **Authors**: Reza Sadeghian,Niloofar Hooshyaripour,Chris Joslin,WonSook Lee
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Accurate and robust 3D object detection is essential for autonomous driving, where fusing data from sensors like LiDAR and camera enhances detection accuracy. However, sensor malfunctions such as corruption or disconnection can degrade performance, and existing fusion models often struggle to maintain reliability when one modality fails. To address this, we propose ReliFusion, a novel LiDAR-camera fusion framework operating in the bird's-eye view (BEV) space. ReliFusion integrates three key components: the Spatio-Temporal Feature Aggregation (STFA) module, which captures dependencies across frames to stabilize predictions over time; the Reliability module, which assigns confidence scores to quantify the dependability of each modality under challenging conditions; and the Confidence-Weighted Mutual Cross-Attention (CW-MCA) module, which dynamically balances information from LiDAR and camera modalities based on these confidence scores. Experiments on the nuScenes dataset show that ReliFusion significantly outperforms state-of-the-art methods, achieving superior robustness and accuracy in scenarios with limited LiDAR fields of view and severe sensor malfunctions.

### Foundation Model-Based Apple Ripeness and Size Estimation for Selective Harvesting 
[[arxiv](https://arxiv.org/abs/2502.01850)] [[cool](https://papers.cool/arxiv/2502.01850)] [[pdf](https://arxiv.org/pdf/2502.01850)]
> **Authors**: Keyi Zhu,Jiajia Li,Kaixiang Zhang,Chaaran Arunachalam,Siddhartha Bhattacharya,Renfu Lu,Zhaojian Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Harvesting is a critical task in the tree fruit industry, demanding extensive manual labor and substantial costs, and exposing workers to potential hazards. Recent advances in automated harvesting offer a promising solution by enabling efficient, cost-effective, and ergonomic fruit picking within tight harvesting windows. However, existing harvesting technologies often indiscriminately harvest all visible and accessible fruits, including those that are unripe or undersized. This study introduces a novel foundation model-based framework for efficient apple ripeness and size estimation. Specifically, we curated two public RGBD-based Fuji apple image datasets, integrating expanded annotations for ripeness ("Ripe" vs. "Unripe") based on fruit color and image capture dates. The resulting comprehensive dataset, Fuji-Ripeness-Size Dataset, includes 4,027 images and 16,257 annotated apples with ripeness and size labels. Using Grounding-DINO, a language-model-based object detector, we achieved robust apple detection and ripeness classification, outperforming other state-of-the-art models. Additionally, we developed and evaluated six size estimation algorithms, selecting the one with the lowest error and variation for optimal performance. The Fuji-Ripeness-Size Dataset and the apple detection and size estimation algorithms are made publicly available, which provides valuable benchmarks for future studies in automated and selective harvesting.

### UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping 
[[arxiv](https://arxiv.org/abs/2502.01846)] [[cool](https://papers.cool/arxiv/2502.01846)] [[pdf](https://arxiv.org/pdf/2502.01846)]
> **Authors**: Aashish Rai,Dilin Wang,Mihir Jain,Nikolaos Sarafianos,Kefan Chen,Srinath Sridhar,Aayush Prakash
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: https://aashishrai3799.github.io/uvgs
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D objects and scenes. However, generating 3DGS remains challenging due to their discrete, unstructured, and permutation-invariant nature. In this work, we present a simple yet effective method to overcome these challenges. We utilize spherical mapping to transform 3DGS into a structured 2D representation, termed UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a concatenation of Gaussian attributes such as position, scale, color, opacity, and rotation. We further find that these heterogeneous features can be compressed into a lower-dimensional (e.g., 3-channel) shared feature space using a carefully designed multi-branch network. The compressed UVGS can be treated as typical RGB images. Remarkably, we discover that typical VAEs trained with latent diffusion models can directly generalize to this new representation without additional training. Our novel representation makes it effortless to leverage foundational 2D models, such as diffusion models, to directly model 3DGS. Additionally, one can simply increase the 2D UV resolution to accommodate more Gaussians, making UVGS a scalable solution compared to typical 3D backbones. This approach immediately unlocks various novel generation applications of 3DGS by inherently utilizing the already developed superior 2D generation capabilities. In our experiments, we demonstrate various unconditional, conditional generation, and inpainting applications of 3DGS based on diffusion models, which were previously non-trivial.

### Texture Image Synthesis Using Spatial GAN Based on Vision Transformers 
[[arxiv](https://arxiv.org/abs/2502.01842)] [[cool](https://papers.cool/arxiv/2502.01842)] [[pdf](https://arxiv.org/pdf/2502.01842)]
> **Authors**: Elahe Salari,Zohreh Azimifar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Published at the 2nd International Conference on Artificial Intelligence and Software Engineering (AI-SOFT), Shiraz University, Shiraz, Iran, 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. While traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. In this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to address the limitations of previous methods. By incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of ViTs, our model achieves superior texture synthesis. This approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS demonstrate the substantial improvement of ViT-SGAN, which underlines its efficiency in generating diverse realistic textures.

### Low Resource Video Super-resolution using Memory and Residual Deformable Convolutions 
[[arxiv](https://arxiv.org/abs/2502.01816)] [[cool](https://papers.cool/arxiv/2502.01816)] [[pdf](https://arxiv.org/pdf/2502.01816)]
> **Authors**: Kavitha Viswanathan,Shashwat Pathak,Piyush Bharambe,Harsh Choudhary,Amit Sethi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Transformer-based video super-resolution (VSR) models have set new benchmarks in recent years, but their substantial computational demands make most of them unsuitable for deployment on resource-constrained devices. Achieving a balance between model complexity and output quality remains a formidable challenge in VSR. Although lightweight models have been introduced to address this issue, they often struggle to deliver state-of-the-art performance. We propose a novel lightweight, parameter-efficient deep residual deformable convolution network for VSR. Unlike prior methods, our model enhances feature utilization through residual connections and employs deformable convolution for precise frame alignment, addressing motion dynamics effectively. Furthermore, we introduce a single memory tensor to capture information accrued from the past frames and improve motion estimation across frames. This design enables an efficient balance between computational cost and reconstruction quality. With just 2.3 million parameters, our model achieves state-of-the-art SSIM of 0.9175 on the REDS4 dataset, surpassing existing lightweight and many heavy models in both accuracy and resource efficiency. Architectural insights from our model pave the way for real-time VSR on streaming data.

### PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph 
[[arxiv](https://arxiv.org/abs/2502.01814)] [[cool](https://papers.cool/arxiv/2502.01814)] [[pdf](https://arxiv.org/pdf/2502.01814)]
> **Authors**: Dazhou Yu,Genpei Zhang,Liang Zhao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification, clustering, and generation. Recent years have witnessed significant strides in this domain, yet most efforts focus on the vertex sequence of a polyhedron, neglecting the complex surface modeling crucial in real-world polyhedral objects. This study proposes \textbf{PolyhedronNet}, a general framework tailored for learning representations of 3D polyhedral objects. We propose the concept of the surface-attributed graph to seamlessly model the vertices, edges, faces, and their geometric interrelationships within a polyhedron. To effectively learn the representation of the entire surface-attributed graph, we first propose to break it down into local rigid representations to effectively learn each local region's relative positions against the remaining regions without geometric information loss. Subsequently, we propose PolyhedronGNN to hierarchically aggregate the local rigid representation via intra-face and inter-face geometric message passing modules, to obtain a global representation that minimizes information loss while maintaining rotation and translation invariance. Our experimental evaluations on four distinct datasets, encompassing both classification and retrieval tasks, substantiate PolyhedronNet's efficacy in capturing comprehensive and informative representations of 3D polyhedral objects. Code and data are available at {https://github.com/dyu62/3D_polyhedron}.

### AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis 
[[arxiv](https://arxiv.org/abs/2502.01785)] [[cool](https://papers.cool/arxiv/2502.01785)] [[pdf](https://arxiv.org/pdf/2502.01785)]
> **Authors**: Basit Alawode,Iyyakutti Iyappan Ganapathi,Sajid Javed,Naoufel Werghi,Mohammed Bennamoun,Arif Mahmood
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this paper, we introduce AquaticCLIP, a novel contrastive language-image pre-training model tailored for aquatic scene understanding. AquaticCLIP presents a new unsupervised learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth annotations, our model enriches existing vision-language models in the aquatic domain. For this purpose, we construct a 2 million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, NatGeo, etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both robustness and interpretability. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at xxx.

### Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity 
[[arxiv](https://arxiv.org/abs/2502.01776)] [[cool](https://papers.cool/arxiv/2502.01776)] [[pdf](https://arxiv.org/pdf/2502.01776)]
> **Authors**: Haocheng Xi,Shuo Yang,Yilong Zhao,Chenfeng Xu,Muyang Li,Xiuyu Li,Yujun Lin,Han Cai,Jintao Zhang,Dacheng Li,Jianfei Chen,Ion Stoica,Kurt Keutzer,Song Han
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 13 pages, 8 figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality.

### Generating Multi-Image Synthetic Data for Text-to-Image Customization 
[[arxiv](https://arxiv.org/abs/2502.01720)] [[cool](https://papers.cool/arxiv/2502.01720)] [[pdf](https://arxiv.org/pdf/2502.01720)]
> **Authors**: Nupur Kumari,Xi Yin,Jun-Yan Zhu,Ishan Misra,Samaneh Azadi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Project webpage: https://www.cs.cmu.edu/~syncd-project/
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **Abstract**: Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks.

### A Multi-Scale Feature Fusion Framework Integrating Frequency Domain and Cross-View Attention for Dual-View X-ray Security Inspections 
[[arxiv](https://arxiv.org/abs/2502.01710)] [[cool](https://papers.cool/arxiv/2502.01710)] [[pdf](https://arxiv.org/pdf/2502.01710)]
> **Authors**: Shilong Hong,Yanzhou Zhou,Weichao Xu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray equipment is widely deployed, it struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose an innovative multi-scale interactive feature fusion framework tailored for dual-view X-ray security inspection image classification. The framework comprises three core modules: the Frequency Domain Interaction Module (FDIM) enhances frequency-domain features through Fourier transform; the Multi-Scale Cross-View Feature Enhancement (MSCFE) leverages cross-view attention mechanisms to strengthen feature interactions; and the Convolutional Attention Fusion Module (CAFM) efficiently fuses features by integrating channel attention with depthwise-separable convolutions. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches across multiple backbone architectures, particularly excelling in complex scenarios with occlusions and object stacking.

### Semantic Communication based on Generative AI: A New Approach to Image Compression and Edge Optimization 
[[arxiv](https://arxiv.org/abs/2502.01675)] [[cool](https://papers.cool/arxiv/2502.01675)] [[pdf](https://arxiv.org/pdf/2502.01675)]
> **Authors**: Francesco Pezone
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: PhD thesis
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: As digital technologies advance, communication networks face challenges in handling the vast data generated by intelligent devices. Autonomous vehicles, smart sensors, and IoT systems necessitate new paradigms. This thesis addresses these challenges by integrating semantic communication and generative models for optimized image compression and edge network resource allocation. Unlike bit-centric systems, semantic communication prioritizes transmitting meaningful data specifically selected to convey the meaning rather than obtain a faithful representation of the original data. The communication infrastructure can benefit to significant improvements in bandwidth efficiency and latency reduction. Central to this work is the design of semantic-preserving image compression using Generative Adversarial Networks and Denoising Diffusion Probabilistic Models. These models compress images by encoding only semantically relevant features, allowing for high-quality reconstruction with minimal transmission. Additionally, a Goal-Oriented edge network optimization framework is introduced, leveraging the Information Bottleneck principle and stochastic optimization to dynamically allocate resources and enhance efficiency. By integrating semantic communication into edge networks, this approach balances computational efficiency and communication effectiveness, making it suitable for real-time applications. The thesis compares semantic-aware models with conventional image compression techniques using classical and semantic evaluation metrics. Results demonstrate the potential of combining generative AI and semantic communication to create more efficient semantic-goal-oriented communication networks that meet the demands of modern data-driven applications.

### Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding 
[[arxiv](https://arxiv.org/abs/2502.01666)] [[cool](https://papers.cool/arxiv/2502.01666)] [[pdf](https://arxiv.org/pdf/2502.01666)]
> **Authors**: Jingming Xia,Guanqun Cao,Guang Ma,Yiben Luo,Qinzhao Li,John Oyekan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Monocular depth estimation involves predicting depth from a single RGB image and plays a crucial role in applications such as autonomous driving, robotic navigation, 3D reconstruction, etc. Recent advancements in learning-based methods have significantly improved depth estimation performance. Generative models, particularly Stable Diffusion, have shown remarkable potential in recovering fine details and reconstructing missing regions through large-scale training on diverse datasets. However, models like CLIP, which rely on textual embeddings, face limitations in complex outdoor environments where rich context information is needed. These limitations reduce their effectiveness in such challenging scenarios. Here, we propose a novel image-based semantic embedding that extracts contextual information directly from visual features, significantly improving depth prediction in complex environments. Evaluated on the KITTI and Waymo datasets, our method achieves performance comparable to state-of-the-art models while addressing the shortcomings of CLIP embeddings in handling outdoor scenes. By leveraging visual semantics directly, our method demonstrates enhanced robustness and adaptability in depth estimation tasks, showcasing its potential for application to other visual perception tasks.

### SliderSpace: Decomposing the Visual Capabilities of Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.01639)] [[cool](https://papers.cool/arxiv/2502.01639)] [[pdf](https://arxiv.org/pdf/2502.01639)]
> **Authors**: Rohit Gandikota,Zongze Wu,Richard Zhang,David Bau,Eli Shechtman,Nick Kolkin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Project Website: https://sliderspace.baulab.info
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **Abstract**: We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info

### MFP-VTON: Enhancing Mask-Free Person-to-Person Virtual Try-On via Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2502.01626)] [[cool](https://papers.cool/arxiv/2502.01626)] [[pdf](https://arxiv.org/pdf/2502.01626)]
> **Authors**: Le Shen,Yanting Kang,Rong Huang,Zhijie Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The garment-to-person virtual try-on (VTON) task, which aims to generate fitting images of a person wearing a reference garment, has made significant strides. However, obtaining a standard garment is often more challenging than using the garment already worn by the person. To improve ease of use, we propose MFP-VTON, a Mask-Free framework for Person-to-Person VTON. Recognizing the scarcity of person-to-person data, we adapt a garment-to-person model and dataset to construct a specialized dataset for this task. Our approach builds upon a pretrained diffusion transformer, leveraging its strong generative capabilities. During mask-free model fine-tuning, we introduce a Focus Attention loss to emphasize the garment of the reference person and the details outside the garment of the target person. Experimental results demonstrate that our model excels in both person-to-person and garment-to-person VTON tasks, generating high-fidelity fitting images.

### Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders for Multi-modal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01576)] [[cool](https://papers.cool/arxiv/2502.01576)] [[pdf](https://arxiv.org/pdf/2502.01576)]
> **Authors**: Hashmat Shadab Malik,Fahad Shamshad,Muzammal Naseer,Karthik Nandakumar,Fahad Khan,Salman Khan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under Review
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multi-modal Large Language Models (MLLMs) excel in vision-language tasks but remain vulnerable to visual adversarial perturbations that can induce hallucinations, manipulate responses, or bypass safety mechanisms. Existing methods seek to mitigate these risks by applying constrained adversarial fine-tuning to CLIP vision encoders on ImageNet-scale data, ensuring their generalization ability is preserved. However, this limited adversarial training restricts robustness and broader generalization. In this work, we explore an alternative approach of leveraging existing vision classification models that have been adversarially pre-trained on large-scale data. Our analysis reveals two principal contributions: (1) the extensive scale and diversity of adversarial pre-training enables these models to demonstrate superior robustness against diverse adversarial threats, ranging from imperceptible perturbations to advanced jailbreaking attempts, without requiring additional adversarial training, and (2) end-to-end MLLM integration with these robust models facilitates enhanced adaptation of language components to robust visual features, outperforming existing plug-and-play methodologies on complex reasoning tasks. Through systematic evaluation across visual question-answering, image captioning, and jail-break attacks, we demonstrate that MLLMs trained with these robust models achieve superior adversarial robustness while maintaining favorable clean performance. Our framework achieves 2x and 1.5x average robustness gains in captioning and VQA tasks, respectively, and delivers over 10% improvement against jailbreak attacks. Code and pretrained models will be available at https://github.com/HashmatShadab/Robust-LLaVA.

### MakeAnything: Harnessing Diffusion Transformers for Multi-Domain Procedural Sequence Generation 
[[arxiv](https://arxiv.org/abs/2502.01572)] [[cool](https://papers.cool/arxiv/2502.01572)] [[pdf](https://arxiv.org/pdf/2502.01572)]
> **Authors**: Yiren Song,Cheng Liu,Mike Zheng Shou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: A hallmark of human intelligence is the ability to create complex artifacts through structured multi-step processes. Generating procedural tutorials with AI is a longstanding but challenging goal, facing three key obstacles: (1) scarcity of multi-task procedural datasets, (2) maintaining logical continuity and visual consistency between steps, and (3) generalizing across multiple domains. To address these challenges, we propose a multi-domain dataset covering 21 tasks with over 24,000 procedural sequences. Building upon this foundation, we introduce MakeAnything, a framework based on the diffusion transformer (DIT), which leverages fine-tuning to activate the in-context capabilities of DIT for generating consistent procedural sequences. We introduce asymmetric low-rank adaptation (LoRA) for image generation, which balances generalization capabilities and task-specific performance by freezing encoder parameters while adaptively tuning decoder layers. Additionally, our ReCraft model enables image-to-process generation through spatiotemporal consistency constraints, allowing static images to be decomposed into plausible creation sequences. Extensive experiments demonstrate that MakeAnything surpasses existing methods, setting new performance benchmarks for procedural generation tasks.

### FireCastNet: Earth-as-a-Graph for Seasonal Fire Prediction 
[[arxiv](https://arxiv.org/abs/2502.01550)] [[cool](https://papers.cool/arxiv/2502.01550)] [[pdf](https://arxiv.org/pdf/2502.01550)]
> **Authors**: Dimitrios Michail,Charalampos Davalas,Lefki-Ioanna Panagiotou,Ioannis Prapas,Spyros Kondylatos,Nikolaos Ioannis Bountos,Ioannis Papoutsis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: With climate change expected to exacerbate fire weather conditions, the accurate and timely anticipation of wildfires becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we present FireCastNet, a novel architecture which combines a 3D convolutional encoder with GraphCast, originally developed for global short-term weather forecasting using graph neural networks. FireCastNet is trained to capture the context leading to wildfires, at different spatial and temporal scales. Our investigation focuses on assessing the effectiveness of our model in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance. Our findings demonstrate the potential of deep learning models in seasonal fire forecasting; longer input time-series leads to more robust predictions, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.

### VisTA: Vision-Text Alignment Model with Contrastive Learning using Multimodal Data for Evidence-Driven, Reliable, and Explainable Alzheimer's Disease Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.01535)] [[cool](https://papers.cool/arxiv/2502.01535)] [[pdf](https://arxiv.org/pdf/2502.01535)]
> **Authors**: Duy-Cat Can,Linh D. Dang,Quang-Huy Tang,Dang Minh Ly,Huong Ha,Guillaume Blanc,Oliver Y. Chén,Binh T. Nguyen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,定量方法
- **Abstract**: Objective: Assessing Alzheimer's disease (AD) using high-dimensional radiology images is clinically important but challenging. Although Artificial Intelligence (AI) has advanced AD diagnosis, it remains unclear how to design AI models embracing predictability and explainability. Here, we propose VisTA, a multimodal language-vision model assisted by contrastive learning, to optimize disease prediction and evidence-based, interpretable explanations for clinical decision-making. Methods: We developed VisTA (Vision-Text Alignment Model) for AD diagnosis. Architecturally, we built VisTA from BiomedCLIP and fine-tuned it using contrastive learning to align images with verified abnormalities and their descriptions. To train VisTA, we used a constructed reference dataset containing images, abnormality types, and descriptions verified by medical experts. VisTA produces four outputs: predicted abnormality type, similarity to reference cases, evidence-driven explanation, and final AD diagnoses. To illustrate VisTA's efficacy, we reported accuracy metrics for abnormality retrieval and dementia prediction. To demonstrate VisTA's explainability, we compared its explanations with human experts' explanations. Results: Compared to 15 million images used for baseline pretraining, VisTA only used 170 samples for fine-tuning and obtained significant improvement in abnormality retrieval and dementia prediction. For abnormality retrieval, VisTA reached 74% accuracy and an AUC of 0.87 (26% and 0.74, respectively, from baseline models). For dementia prediction, VisTA achieved 88% accuracy and an AUC of 0.82 (30% and 0.57, respectively, from baseline models). The generated explanations agreed strongly with human experts' and provided insights into the diagnostic process. Taken together, VisTA optimize prediction, clinical reasoning, and explanation.

### The in-context inductive biases of vision-language models differ across modalities 
[[arxiv](https://arxiv.org/abs/2502.01530)] [[cool](https://papers.cool/arxiv/2502.01530)] [[pdf](https://arxiv.org/pdf/2502.01530)]
> **Authors**: Kelsey Allen,Ishita Dasgupta,Eliza Kosoy,Andrew K. Lampinen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 10 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Inductive biases are what allow learners to make guesses in the absence of conclusive evidence. These biases have often been studied in cognitive science using concepts or categories -- e.g. by testing how humans generalize a new category from a few examples that leave the category boundary ambiguous. We use these approaches to study generalization in foundation models during in-context learning. Modern foundation models can condition on both vision and text, and differences in how they interpret and learn from these different modalities is an emerging area of study. Here, we study how their generalizations vary by the modality in which stimuli are presented, and the way the stimuli are described in text. We study these biases with three different experimental paradigms, across three different vision-language models. We find that the models generally show some bias towards generalizing according to shape over color. This shape bias tends to be amplified when the examples are presented visually. By contrast, when examples are presented in text, the ordering of adjectives affects generalization. However, the extent of these effects vary across models and paradigms. These results help to reveal how vision-language models represent different types of inputs in context, and may have practical implications for the use of vision-language models.

### Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective 
[[arxiv](https://arxiv.org/abs/2502.01524)] [[cool](https://papers.cool/arxiv/2502.01524)] [[pdf](https://arxiv.org/pdf/2502.01524)]
> **Authors**: Xiaorui Ma,Haoran Xie,S. Joe Qin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 28 pages, 3 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习
- **Abstract**: The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs.

### MoireDB: Formula-generated Interference-fringe Image Dataset 
[[arxiv](https://arxiv.org/abs/2502.01490)] [[cool](https://papers.cool/arxiv/2502.01490)] [[pdf](https://arxiv.org/pdf/2502.01490)]
> **Authors**: Yuto Matsuo,Ryo Hayamizu,Hirokatsu Kataoka,Akio Nakamura
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Image recognition models have struggled to treat recognition robustness to real-world degradations. In this context, data augmentation methods like PixMix improve robustness but rely on generative arts and feature visualizations (FVis), which have copyright, drawing cost, and scalability issues. We propose MoireDB, a formula-generated interference-fringe image dataset for image augmentation enhancing robustness. MoireDB eliminates copyright concerns, reduces dataset assembly costs, and enhances robustness by leveraging illusory patterns. Experiments show that MoireDB augmented images outperforms traditional Fractal arts and FVis-based augmentations, making it a scalable and effective solution for improving model robustness against real-world degradations.

### Simultaneous Automatic Picking and Manual Picking Refinement for First-Break 
[[arxiv](https://arxiv.org/abs/2502.01474)] [[cool](https://papers.cool/arxiv/2502.01474)] [[pdf](https://arxiv.org/pdf/2502.01474)]
> **Authors**: Haowen Bai,Zixiang Zhao,Jiangshe Zhang,Yukun Cui,Chunxia Zhang,Zhenbo Guo,Yongjun Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ef:IEEE Transactions on Geoscience and Remote Sensing (TGRS) (Volume: 62), May 14, 2024, Article Sequence Number: 5916112
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: First-break picking is a pivotal procedure in processing microseismic data for geophysics and resource exploration. Recent advancements in deep learning have catalyzed the evolution of automated methods for identifying first-break. Nevertheless, the complexity of seismic data acquisition and the requirement for detailed, expert-driven labeling often result in outliers and potential mislabeling within manually labeled datasets. These issues can negatively affect the training of neural networks, necessitating algorithms that handle outliers or mislabeled data effectively. We introduce the Simultaneous Picking and Refinement (SPR) algorithm, designed to handle datasets plagued by outlier samples or even noisy labels. Unlike conventional approaches that regard manual picks as ground truth, our method treats the true first-break as a latent variable within a probabilistic model that includes a first-break labeling prior. SPR aims to uncover this variable, enabling dynamic adjustments and improved accuracy across the dataset. This strategy mitigates the impact of outliers or inaccuracies in manual labels. Intra-site picking experiments and cross-site generalization experiments on publicly available data confirm our method's performance in identifying first-break and its generalization across different sites. Additionally, our investigations into noisy signals and labels underscore SPR's resilience to both types of noise and its capability to refine misaligned manual annotations. Moreover, the flexibility of SPR, not being limited to any single network architecture, enhances its adaptability across various deep learning-based picking methods. Focusing on learning from data that may contain outliers or partial inaccuracies, SPR provides a robust solution to some of the principal obstacles in automatic first-break picking.

### Temporal-consistent CAMs for Weakly Supervised Video Segmentation in Waste Sorting 
[[arxiv](https://arxiv.org/abs/2502.01455)] [[cool](https://papers.cool/arxiv/2502.01455)] [[pdf](https://arxiv.org/pdf/2502.01455)]
> **Authors**: Andrea Marelli,Luca Magri,Federica Arrigoni,Giacomo Boracchi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 14 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In industrial settings, weakly supervised (WS) methods are usually preferred over their fully supervised (FS) counterparts as they do not require costly manual annotations. Unfortunately, the segmentation masks obtained in the WS regime are typically poor in terms of accuracy. In this work, we present a WS method capable of producing accurate masks for semantic segmentation in the case of video streams. More specifically, we build saliency maps that exploit the temporal coherence between consecutive frames in a video, promoting consistency when objects appear in different frames. We apply our method in a waste-sorting scenario, where we perform weakly supervised video segmentation (WSVS) by training an auxiliary classifier that distinguishes between videos recorded before and after a human operator, who manually removes specific wastes from a conveyor belt. The saliency maps of this classifier identify materials to be removed, and we modify the classifier training to minimize differences between the saliency map of a central frame and those in adjacent frames, after having compensated object displacement. Experiments on a real-world dataset demonstrate the benefits of integrating temporal coherence directly during the training phase of the classifier. Code and dataset are available upon request.

### Improved Training Technique for Latent Consistency Models 
[[arxiv](https://arxiv.org/abs/2502.01441)] [[cool](https://papers.cool/arxiv/2502.01441)] [[pdf](https://arxiv.org/pdf/2502.01441)]
> **Authors**: Quan Dao,Khanh Doan,Di Liu,Trung Le,Dimitris Metaxas
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at ICLR2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-$c$ scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/

### Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01419)] [[cool](https://papers.cool/arxiv/2502.01419)] [[pdf](https://arxiv.org/pdf/2502.01419)]
> **Authors**: Mingi Jung,Saehuyng Lee,Eunji Kim,Sungroh Yoon
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.

### AdaSVD: Adaptive Singular Value Decomposition for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.01403)] [[cool](https://papers.cool/arxiv/2502.01403)] [[pdf](https://arxiv.org/pdf/2502.01403)]
> **Authors**: Zhiteng Li,Mingyuan Xia,Jingyuan Zhang,Zheng Hui,Linghe Kong,Yulun Zhang,Xiaokang Yang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: The code and models will be available at https://github.com/ZHITENGLI/AdaSVD
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Large language models (LLMs) have achieved remarkable success in natural language processing (NLP) tasks, yet their substantial memory requirements present significant challenges for deployment on resource-constrained devices. Singular Value Decomposition (SVD) has emerged as a promising compression technique for LLMs, offering considerable reductions in memory overhead. However, existing SVD-based methods often struggle to effectively mitigate the errors introduced by SVD truncation, leading to a noticeable performance gap when compared to the original models. Furthermore, applying a uniform compression ratio across all transformer layers fails to account for the varying importance of different layers. To address these challenges, we propose AdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD introduces adaComp, which adaptively compensates for SVD truncation errors by alternately updating the singular matrices U and V^T. Additionally, AdaSVD introduces adaCR, which adaptively assigns layer-specific compression ratios based on the relative importance of each layer. Extensive experiments across multiple LLM families and evaluation metrics demonstrate that AdaSVD consistently outperforms state-of-the-art (SOTA) SVD-based methods, achieving superior performance with significantly reduced memory requirements. The code and models will be available at https://github.com/ZHITENGLI/AdaSVD.

### Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection 
[[arxiv](https://arxiv.org/abs/2502.01401)] [[cool](https://papers.cool/arxiv/2502.01401)] [[pdf](https://arxiv.org/pdf/2502.01401)]
> **Authors**: Boyu Mi,Hanqing Wang,Tai Wang,Yilun Chen,Jiangmiao Pang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D visual grounding (3DVG) is challenging because of the requirement of understanding on visual information, language and spatial relationships. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high cost of 3D vision-language datasets. On the other hand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for training data. However, these methods incur prohibitive time and token costs during inference. To address the challenges, we introduce a novel training-free symbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual Grounder, that offers significantly reduced inference costs compared to previous agent-based methods while maintaining comparable performance. EaSe uses LLM generated codes to compute on spatial relationships. EaSe also implements an automatic pipeline to evaluate and optimize the quality of these codes and integrate VLMs to assist in the grounding process. Experimental results demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2% Acc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover, it substantially reduces the inference time and cost, offering a balanced trade-off between performance and efficiency. Codes are available at https://github.com/OpenRobotLab/EaSe.

### Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar 
[[arxiv](https://arxiv.org/abs/2502.01357)] [[cool](https://papers.cool/arxiv/2502.01357)] [[pdf](https://arxiv.org/pdf/2502.01357)]
> **Authors**: Dong-In Kim,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 6pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.

### Quasi-Conformal Convolution : A Learnable Convolution for Deep Learning on Riemann Surfaces 
[[arxiv](https://arxiv.org/abs/2502.01356)] [[cool](https://papers.cool/arxiv/2502.01356)] [[pdf](https://arxiv.org/pdf/2502.01356)]
> **Authors**: Han Zhang,Tsz Lok Ip,Lok Ming Lui
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning on non-Euclidean domains is important for analyzing complex geometric data that lacks common coordinate systems and familiar Euclidean properties. A central challenge in this field is to define convolution on domains, which inherently possess irregular and non-Euclidean structures. In this work, we introduce Quasi-conformal Convolution (QCC), a novel framework for defining convolution on Riemann surfaces using quasi-conformal theories. Each QCC operator is linked to a specific quasi-conformal mapping, enabling the adjustment of the convolution operation through manipulation of this mapping. By utilizing trainable estimator modules that produce Quasi-conformal mappings, QCC facilitates adaptive and learnable convolution operators that can be dynamically adjusted according to the underlying data structured on Riemann surfaces. QCC unifies a broad range of spatially defined convolutions, facilitating the learning of tailored convolution operators on each underlying surface optimized for specific tasks. Building on this foundation, we develop the Quasi-Conformal Convolutional Neural Network (QCCNN) to address a variety of tasks related to geometric data. We validate the efficacy of QCCNN through the classification of images defined on curvilinear Riemann surfaces, demonstrating superior performance in this context. Additionally, we explore its potential in medical applications, including craniofacial analysis using 3D facial data and lesion segmentation on 3D human faces, achieving enhanced accuracy and reliability.

### CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation 
[[arxiv](https://arxiv.org/abs/2502.01312)] [[cool](https://papers.cool/arxiv/2502.01312)] [[pdf](https://arxiv.org/pdf/2502.01312)]
> **Authors**: Xiao Lin,Yun Peng,Liuyi Wang,Xianyou Zhong,Minghao Zhu,Jingwei Yang,Chengju Liu,Qijun Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Category-level object pose estimation aims to recover the rotation, translation and size of unseen instances within predefined categories. In this task, deep neural network-based methods have demonstrated remarkable performance. However, previous studies show they suffer from spurious correlations raised by "unclean" confounders in models, hindering their performance on novel instances with significant variations. To address this issue, we propose CleanPose, a novel approach integrating causal learning and knowledge distillation to enhance category-level pose estimation. To mitigate the negative effect of unobserved confounders, we develop a causal inference module based on front-door adjustment, which promotes unbiased estimation by reducing potential spurious correlations. Additionally, to further improve generalization ability, we devise a residual-based knowledge distillation method that has proven effective in providing comprehensive category information guidance. Extensive experiments across multiple benchmarks (REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed CleanPose over state-of-the-art methods. Code will be released.

### Template Matching in Images using Segmented Normalized Cross-Correlation 
[[arxiv](https://arxiv.org/abs/2502.01286)] [[cool](https://papers.cool/arxiv/2502.01286)] [[pdf](https://arxiv.org/pdf/2502.01286)]
> **Authors**: Davor Marušić,Siniša Popović,Zoran Kalafatić
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 14 pages, 2 tables, 3 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, a new variant of an algorithm for normalized cross-correlation (NCC) is proposed in the context of template matching in images. The proposed algorithm is based on the precomputation of a template image approximation, enabling more efficient calculation of approximate NCC with the source image than using the original template for exact NCC calculation. The approximate template is precomputed from the template image by a split-and-merge approach, resulting in a decomposition to axis-aligned rectangular segments, whose sizes depend on per-segment pixel intensity variance. In the approximate template, each segment is assigned the mean grayscale value of the corresponding pixels from the original template. The proposed algorithm achieves superior computational performance with negligible NCC approximation errors compared to the well-known Fast Fourier Transform (FFT)-based NCC algorithm, when applied on less visually complex and/or smaller template images. In other cases, the proposed algorithm can maintain either computational performance or NCC approximation error within the range of the FFT-based algorithm, but not both.

### Label Correction for Road Segmentation Using Road-side Cameras 
[[arxiv](https://arxiv.org/abs/2502.01281)] [[cool](https://papers.cool/arxiv/2502.01281)] [[pdf](https://arxiv.org/pdf/2502.01281)]
> **Authors**: Henrik Toikka,Eerik Alamikkotervo,Risto Ojala
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Reliable road segmentation in all weather conditions is critical for intelligent transportation applications, autonomous vehicles and advanced driver's assistance systems. For robust performance, all weather conditions should be included in the training data of deep learning-based perception models. However, collecting and annotating such a dataset requires extensive resources. In this paper, existing roadside camera infrastructure is utilized for collecting road data in varying weather conditions automatically. Additionally, a novel semi-automatic annotation method for roadside cameras is proposed. For each camera, only one frame is labeled manually and then the label is transferred to other frames of that camera feed. The small camera movements between frames are compensated using frequency domain image registration. The proposed method is validated with roadside camera data collected from 927 cameras across Finland over 4 month time period during winter. Training on the semi-automatically labeled data boosted the segmentation performance of several deep learning segmentation models. Testing was carried out on two different datasets to evaluate the robustness of the resulting models. These datasets were an in-domain roadside camera dataset and out-of-domain dataset captured with a vehicle on-board camera.

### One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.01201)] [[cool](https://papers.cool/arxiv/2502.01201)] [[pdf](https://arxiv.org/pdf/2502.01201)]
> **Authors**: Yiyue Li,Shaoting Zhang,Kang Li,Qicheng Lao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: In The Thirty-eighth Annual Conference onNeuralInformation Processing Systems (NeurIPS2024)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query image's features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domains--an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods.

### Nearly Lossless Adaptive Bit Switching 
[[arxiv](https://arxiv.org/abs/2502.01199)] [[cool](https://papers.cool/arxiv/2502.01199)] [[pdf](https://arxiv.org/pdf/2502.01199)]
> **Authors**: Haiduo Huang,Zhenhua Liu,Tian Xia,Wenzhe zhao,Pengju Ren
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Model quantization is widely applied for compressing and accelerating deep neural networks (DNNs). However, conventional Quantization-Aware Training (QAT) focuses on training DNNs with uniform bit-width. The bit-width settings vary across different hardware and transmission demands, which induces considerable training and storage costs. Hence, the scheme of one-shot joint training multiple precisions is proposed to address this issue. Previous works either store a larger FP32 model to switch between different precision models for higher accuracy or store a smaller INT8 model but compromise accuracy due to using shared quantization parameters. In this paper, we introduce the Double Rounding quantization method, which fully utilizes the quantized representation range to accomplish nearly lossless bit-switching while reducing storage by using the highest integer precision instead of full precision. Furthermore, we observe a competitive interference among different precisions during one-shot joint training, primarily due to inconsistent gradients of quantization scales during backward propagation. To tackle this problem, we propose an Adaptive Learning Rate Scaling (ALRS) technique that dynamically adapts learning rates for various precisions to optimize the training process. Additionally, we extend our Double Rounding to one-shot mixed precision training and develop a Hessian-Aware Stochastic Bit-switching (HASB) strategy. Experimental results on the ImageNet-1K classification demonstrate that our methods have enough advantages to state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision. We also validate the feasibility of our method on detection and segmentation tasks, as well as on LLMs task. Our codes are available at https://github.com/haiduo/Double-Rounding.

### LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2502.01105)] [[cool](https://papers.cool/arxiv/2502.01105)] [[pdf](https://arxiv.org/pdf/2502.01105)]
> **Authors**: Yiren Song,Danze Chen,Mike Zheng Shou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.

### SatFlow: Generative model based framework for producing High Resolution Gap Free Remote Sensing Imagery 
[[arxiv](https://arxiv.org/abs/2502.01098)] [[cool](https://papers.cool/arxiv/2502.01098)] [[pdf](https://arxiv.org/pdf/2502.01098)]
> **Authors**: Bharath Irigireddy,Varaprasad Bandaru
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: :J.2.5, I.4.5
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Frequent, high-resolution remote sensing imagery is crucial for agricultural and environmental monitoring. Satellites from the Landsat collection offer detailed imagery at 30m resolution but with lower temporal frequency, whereas missions like MODIS and VIIRS provide daily coverage at coarser resolutions. Clouds and cloud shadows contaminate about 55\% of the optical remote sensing observations, posing additional challenges. To address these challenges, we present SatFlow, a generative model-based framework that fuses low-resolution MODIS imagery and Landsat observations to produce frequent, high-resolution, gap-free surface reflectance imagery. Our model, trained via Conditional Flow Matching, demonstrates better performance in generating imagery with preserved structural and spectral integrity. Cloud imputation is treated as an image inpainting task, where the model reconstructs cloud-contaminated pixels and fills gaps caused by scan lines during inference by leveraging the learned generative processes. Experimental results demonstrate the capability of our approach in reliably imputing cloud-covered regions. This capability is crucial for downstream applications such as crop phenology tracking, environmental change detection etc.,

### The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles 
[[arxiv](https://arxiv.org/abs/2502.01081)] [[cool](https://papers.cool/arxiv/2502.01081)] [[pdf](https://arxiv.org/pdf/2502.01081)]
> **Authors**: Vernon Y. H. Toh,Yew Ken Chia,Deepanway Ghosal,Soujanya Poria
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.

### OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models 
[[arxiv](https://arxiv.org/abs/2502.01061)] [[cool](https://papers.cool/arxiv/2502.01061)] [[pdf](https://arxiv.org/pdf/2502.01061)]
> **Authors**: Gaojie Lin,Jianwen Jiang,Jiaqi Yang,Zerong Zheng,Chao Liang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: https://omnihuman-lab.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)

### Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding 
[[arxiv](https://arxiv.org/abs/2502.01056)] [[cool](https://papers.cool/arxiv/2502.01056)] [[pdf](https://arxiv.org/pdf/2502.01056)]
> **Authors**: Chao Wang,Xuancheng Zhou,Weiwei Fu,Yang Zhou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Large Visual Language Models (LVLMs) integrate visual and linguistic modalities, exhibiting exceptional performance across various multimodal tasks. Nevertheless, LVLMs remain vulnerable to the issue of object hallucinations. Previous efforts to mitigate this issue focus on supervised fine-tuning (SFT) or incorporating external knowledge, both of which entail significant costs related to training and the acquisition of external data. To address these challenges, we propose a novel model-agnostic approach termed Internal Fact-based Contrastive Decoding (IFCD), designed to mitigate and suppress hallucinations during the inference process of LVLMs by exploiting the LVLMs' own hallucinations. IFCD is grounded in experimental observations that alterations to the LVLMs' internal representations tend to amplify hallucinations caused by language bias. By contrasting disturbed distribution, IFCD calibrates the LVLMs' output and effectively removes the hallucinatory logits from the final predictions. Experimental results validate that IFCD significantly alleviates both object-level and attribute-level hallucinations while achieving an average 9% accuracy improvement on POPE and 8% accuracy improvement on MME object hallucinations subset compared with direct decoding, respectively.

### Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.01051)] [[cool](https://papers.cool/arxiv/2502.01051)] [[pdf](https://arxiv.org/pdf/2502.01051)]
> **Authors**: Tao Zhang,Cheng Da,Kun Ding,Kun Jin,Yan Li,Tingting Gao,Di Zhang,Shiming Xiang,Chunhong Pan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 14 tables, 15 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code will be available at https://github.com/casiatao/LPO.

### Sparks of Explainability: Recent Advancements in Explaining Large Vision Models 
[[arxiv](https://arxiv.org/abs/2502.01048)] [[cool](https://papers.cool/arxiv/2502.01048)] [[pdf](https://arxiv.org/pdf/2502.01048)]
> **Authors**: Thomas Fel
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Doctoral thesis
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: This thesis explores advanced approaches to improve explainability in computer vision by analyzing and modeling the features exploited by deep neural networks. Initially, it evaluates attribution methods, notably saliency maps, by introducing a metric based on algorithmic stability and an approach utilizing Sobol indices, which, through quasi-Monte Carlo sequences, allows a significant reduction in computation time. In addition, the EVA method offers a first formulation of attribution with formal guarantees via verified perturbation analysis. Experimental results indicate that in complex scenarios these methods do not provide sufficient understanding, particularly because they identify only "where" the model focuses without clarifying "what" it perceives. Two hypotheses are therefore examined: aligning models with human reasoning -- through the introduction of a training routine that integrates the imitation of human explanations and optimization within the space of 1-Lipschitz functions -- and adopting a conceptual explainability approach. The CRAFT method is proposed to automate the extraction of the concepts used by the model and to assess their importance, complemented by MACO, which enables their visualization. These works converge towards a unified framework, illustrated by an interactive demonstration applied to the 1000 ImageNet classes in a ResNet model.

### Vessel segmentation for X-separation 
[[arxiv](https://arxiv.org/abs/2502.01023)] [[cool](https://papers.cool/arxiv/2502.01023)] [[pdf](https://arxiv.org/pdf/2502.01023)]
> **Authors**: Taechang Kim,Sooyeon Ji,Kyeongseon Min,Minjun Kim,Jonghyo Youn,Chungseok Oh,Jiye Kim,Jongho Lee
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,定量方法
- **Abstract**: $χ$-separation is an advanced quantitative susceptibility mapping (QSM) method that is designed to generate paramagnetic ($χ_{para}$) and diamagnetic ($|χ_{dia}|$) susceptibility maps, reflecting the distribution of iron and myelin in the brain. However, vessels have shown artifacts, interfering with the accurate quantification of iron and myelin in applications. To address this challenge, a new vessel segmentation method for $χ$-separation is developed. The method comprises three steps: 1) Seed generation from $\textit{R}_2^*$ and the product of $χ_{para}$ and $|χ_{dia}|$ maps; 2) Region growing, guided by vessel geometry, creating a vessel mask; 3) Refinement of the vessel mask by excluding non-vessel structures. The performance of the method was compared to conventional vessel segmentation methods both qualitatively and quantitatively. To demonstrate the utility of the method, it was tested in two applications: quantitative evaluation of a neural network-based $χ$-separation reconstruction method ($χ$-sepnet-$\textit{R}_2^*$) and population-averaged region of interest (ROI) analysis. The proposed method demonstrates superior performance to the conventional vessel segmentation methods, effectively excluding the non-vessel structures, achieving the highest Dice score coefficient. For the applications, applying vessel masks report notable improvements for the quantitative evaluation of $χ$-sepnet-$\textit{R}_2^*$ and statistically significant differences in population-averaged ROI analysis. These applications suggest excluding vessels when analyzing the $χ$-separation maps provide more accurate evaluations. The proposed method has the potential to facilitate various applications, offering reliable analysis through the generation of a high-quality vessel mask.

### Multi-Resolution SAR and Optical Remote Sensing Image Registration Methods: A Review, Datasets, and Future Perspectives 
[[arxiv](https://arxiv.org/abs/2502.01002)] [[cool](https://papers.cool/arxiv/2502.01002)] [[pdf](https://arxiv.org/pdf/2502.01002)]
> **Authors**: Wenfei Zhang,Ruipeng Zhao,Yongxiang Yao,Yi Wan,Peihao Wu,Jiayuan Li,Yansheng Li,Yongjun Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 48 pages, 10 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Synthetic Aperture Radar (SAR) and optical image registration is essential for remote sensing data fusion, with applications in military reconnaissance, environmental monitoring, and disaster management. However, challenges arise from differences in imaging mechanisms, geometric distortions, and radiometric properties between SAR and optical images. As image resolution increases, fine SAR textures become more significant, leading to alignment issues and 3D spatial discrepancies. Two major gaps exist: the lack of a publicly available multi-resolution, multi-scene registration dataset and the absence of systematic analysis of current methods. To address this, the MultiResSAR dataset was created, containing over 10k pairs of multi-source, multi-resolution, and multi-scene SAR and optical images. Sixteen state-of-the-art algorithms were tested. Results show no algorithm achieves 100% success, and performance decreases as resolution increases, with most failing on sub-meter data. XoFTR performs best among deep learning methods (40.58%), while RIFT performs best among traditional methods (66.51%). Future research should focus on noise suppression, 3D geometric fusion, cross-view transformation modeling, and deep learning optimization for robust registration of high-resolution SAR and optical images. The dataset is available at https://github.com/betterlll/Multi-Resolution-SAR-dataset-.

### Pushing the Boundaries of State Space Models for Image and Video Generation 
[[arxiv](https://arxiv.org/abs/2502.00972)] [[cool](https://papers.cool/arxiv/2502.00972)] [[pdf](https://arxiv.org/pdf/2502.00972)]
> **Authors**: Yicong Hong,Long Mai,Yuan Yao,Feng Liu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 21 pages, paper under review
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: While Transformers have become the dominant architecture for visual generation, linear attention models, such as the state-space models (SSM), are increasingly recognized for their efficiency in processing long visual sequences. However, the essential efficiency of these models comes from formulating a limited recurrent state, enforcing causality among tokens that are prone to inconsistent modeling of N-dimensional visual data, leaving questions on their capacity to generate long non-causal sequences. In this paper, we explore the boundary of SSM on image and video generation by building the largest-scale diffusion SSM-Transformer hybrid model to date (5B parameters) based on the sub-quadratic bi-directional Hydra and self-attention, and generate up to 2K images and 360p 8 seconds (16 FPS) videos. Our results demonstrate that the model can produce faithful results aligned with complex text prompts and temporal consistent videos with high dynamics, suggesting the great potential of using SSMs for visual generation tasks.

### CoDe: Blockwise Control for Denoising Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.00968)] [[cool](https://papers.cool/arxiv/2502.00968)] [[pdf](https://arxiv.org/pdf/2502.00968)]
> **Authors**: Anuj Singh,Sayak Mukherjee,Ahmad Beirami,Hadi Jamali-Rad
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. Our code is available at: https://github.com/anujinho/code.

### CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling 
[[arxiv](https://arxiv.org/abs/2502.00965)] [[cool](https://papers.cool/arxiv/2502.00965)] [[pdf](https://arxiv.org/pdf/2502.00965)]
> **Authors**: Xinze Wang,Chen Chen,Yinfei Yang,Hong-You Chen,Bowen Zhang,Aditya Pal,Xiangxin Zhu,Xianzhi Du
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Mixture-of-Experts (MoE) models are crucial for scaling model capacity while controlling inference costs. While integrating MoE into multimodal models like CLIP improves performance, training these models is notoriously challenging and expensive. We propose CLIP-Upcycling (CLIP-UP), an efficient alternative training strategy that converts a pre-trained dense CLIP model into a sparse MoE architecture. Through extensive experimentation with various settings and auxiliary losses, we demonstrate that CLIP-UP significantly reduces training complexity and cost. Remarkably, our sparse CLIP B/16 model, trained with CLIP-UP, outperforms its dense counterpart by 7.2% and 6.6% on COCO and Flickr30k text-to-image Recall@1 benchmarks respectively. It even surpasses the larger CLIP L/14 model on this task while using only 30% of the inference FLOPs. We further demonstrate the generalizability of our training recipe across different scales, establishing sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models.

### Hypo3D: Exploring Hypothetical Reasoning in 3D 
[[arxiv](https://arxiv.org/abs/2502.00954)] [[cool](https://papers.cool/arxiv/2502.00954)] [[pdf](https://arxiv.org/pdf/2502.00954)]
> **Authors**: Ye Mao,Weixun Luo,Junpeng Jing,Anlan Qiu,Krystian Mikolajczyk
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 15 figures, 9 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce Hypothetical 3D Reasoning, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the context change is irrelevant to the question, models often incorrectly adjust their answers.

### Fruit Fly Classification (Diptera: Tephritidae) in Images, Applying Transfer Learning 
[[arxiv](https://arxiv.org/abs/2502.00939)] [[cool](https://papers.cool/arxiv/2502.00939)] [[pdf](https://arxiv.org/pdf/2502.00939)]
> **Authors**: Erick Andrew Bustamante Flores,Harley Vera Olivera,Ivan Cesar Medrano Valencia,Carlos Fernando Montoya Cubas
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 15 pages and 19 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: This study develops a transfer learning model for the automated classification of two species of fruit flies, Anastrepha fraterculus and Ceratitis capitata, in a controlled laboratory environment. The research addresses the need to optimize identification and classification, which are currently performed manually by experts, being affected by human factors and facing time challenges. The methodological process of this study includes the capture of high-quality images using a mobile phone camera and a stereo microscope, followed by segmentation to reduce size and focus on relevant morphological areas. The images were carefully labeled and preprocessed to ensure the quality and consistency of the dataset used to train the pre-trained convolutional neural network models VGG16, VGG19, and Inception-v3. The results were evaluated using the F1-score, achieving 82% for VGG16 and VGG19, while Inception-v3 reached an F1-score of 93%. Inception-v3's reliability was verified through model testing in uncontrolled environments, with positive results, complemented by the Grad-CAM technique, demonstrating its ability to capture essential morphological features. These findings indicate that Inception-v3 is an effective and replicable approach for classifying Anastrepha fraterculus and Ceratitis capitata, with potential for implementation in automated monitoring systems.

### STAF: Sinusoidal Trainable Activation Functions for Implicit Neural Representation 
[[arxiv](https://arxiv.org/abs/2502.00869)] [[cool](https://papers.cool/arxiv/2502.00869)] [[pdf](https://arxiv.org/pdf/2502.00869)]
> **Authors**: Alireza Morsali,MohammadJavad Vaez,Hossein Soltani,Amirhossein Kazerouni,Babak Taati,Morteza Mohammad-Noori
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Implicit Neural Representations (INRs) have emerged as a powerful framework for modeling continuous signals. The spectral bias of ReLU-based networks is a well-established limitation, restricting their ability to capture fine-grained details in target signals. While previous works have attempted to mitigate this issue through frequency-based encodings or architectural modifications, these approaches often introduce additional complexity and do not fully address the underlying challenge of learning high-frequency components efficiently. We introduce Sinusoidal Trainable Activation Functions (STAF), designed to directly tackle this limitation by enabling networks to adaptively learn and represent complex signals with higher precision and efficiency. STAF inherently modulates its frequency components, allowing for self-adaptive spectral learning. This capability significantly improves convergence speed and expressivity, making STAF highly effective for both signal representations and inverse problems. Through extensive evaluations, we demonstrate that STAF outperforms state-of-the-art (SOTA) methods in accuracy and reconstruction fidelity with superior Peak Signal-to-Noise Ratio (PSNR). These results establish STAF as a robust solution for overcoming spectral bias and the capacity-convergence gap, making it valuable for computer graphics and related fields. Our codebase is publicly accessible on the https://github.com/AlirezaMorsali/STAF.

### VLM-Assisted Continual learning for Visual Question Answering in Self-Driving 
[[arxiv](https://arxiv.org/abs/2502.00843)] [[cool](https://papers.cool/arxiv/2502.00843)] [[pdf](https://arxiv.org/pdf/2502.00843)]
> **Authors**: Yuxin Lin,Mengshi Qi,Liang Liu,Huadong Ma
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 21.40% to 32.28% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.

### Cross multiscale vision transformer for deep fake detection 
[[arxiv](https://arxiv.org/abs/2502.00833)] [[cool](https://papers.cool/arxiv/2502.00833)] [[pdf](https://arxiv.org/pdf/2502.00833)]
> **Authors**: Akhshan P,Taneti Sanjay,Chandrakala S
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The proliferation of deep fake technology poses significant challenges to digital media authenticity, necessitating robust detection mechanisms. This project evaluates deep fake detection using the SP Cup's 2025 deep fake detection challenge dataset. We focused on exploring various deep learning models for detecting deep fake content, utilizing traditional deep learning techniques alongside newer architectures. Our approach involved training a series of models and rigorously assessing their performance using metrics such as accuracy.

### Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data 
[[arxiv](https://arxiv.org/abs/2502.00800)] [[cool](https://papers.cool/arxiv/2502.00800)] [[pdf](https://arxiv.org/pdf/2502.00800)]
> **Authors**: Mengping Yang,Zhe Wang,Ziqiu Chi,Dongdong Li,Wenli Du
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: This work was completed in 2022 and submitted to an IEEE journal for potential publication
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Generative adversarial networks (GANs) have made remarkable achievements in synthesizing images in recent years. Typically, training GANs requires massive data, and the performance of GANs deteriorates significantly when training data is limited. To improve the synthesis performance of GANs in low-data regimes, existing approaches use various data augmentation techniques to enlarge the training sets. However, it is identified that these augmentation techniques may leak or even alter the data distribution. To remedy this, we propose an adversarial semantic augmentation (ASA) technique to enlarge the training data at the semantic level instead of the image level. Concretely, considering semantic features usually encode informative information of images, we estimate the covariance matrices of semantic features for both real and generated images to find meaningful transformation directions. Such directions translate original features to another semantic representation, e.g., changing the backgrounds or expressions of the human face dataset. Moreover, we derive an upper bound of the expected adversarial loss. By optimizing the upper bound, our semantic augmentation is implicitly achieved. Such design avoids redundant sampling of the augmented features and introduces negligible computation overhead, making our approach computation efficient. Extensive experiments on both few-shot and large-scale datasets demonstrate that our method consistently improve the synthesis quality under various data regimes, and further visualized and analytic results suggesting satisfactory versatility of our proposed method.

### Estimating forest carbon stocks from high-resolution remote sensing imagery by reducing domain shift with style transfer 
[[arxiv](https://arxiv.org/abs/2502.00784)] [[cool](https://papers.cool/arxiv/2502.00784)] [[pdf](https://arxiv.org/pdf/2502.00784)]
> **Authors**: Zhenyu Yu,Jinnian Wang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Forests function as crucial carbon reservoirs on land, and their carbon sinks can efficiently reduce atmospheric CO2 concentrations and mitigate climate change. Currently, the overall trend for monitoring and assessing forest carbon stocks is to integrate ground monitoring sample data with satellite remote sensing imagery. This style of analysis facilitates large-scale observation. However, these techniques require improvement in accuracy. We used GF-1 WFV and Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in China. Using the style transfer method, we introduced Swin Transformer to extract global features through attention mechanisms, converting the carbon stock estimation into an image translation.

### Vision and Language Reference Prompt into SAM for Few-shot Segmentation 
[[arxiv](https://arxiv.org/abs/2502.00719)] [[cool](https://papers.cool/arxiv/2502.00719)] [[pdf](https://arxiv.org/pdf/2502.00719)]
> **Authors**: Kosuke Sakurai,Ryotaro Shimizu,Masayuki Goto
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 2 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Segment Anything Model (SAM) represents a large-scale segmentation model that enables powerful zero-shot capabilities with flexible prompts. While SAM can segment any object in zero-shot, it requires user-provided prompts for each target image and does not attach any label information to masks. Few-shot segmentation models addressed these issues by inputting annotated reference images as prompts to SAM and can segment specific objects in target images without user-provided prompts. Previous SAM-based few-shot segmentation models only use annotated reference images as prompts, resulting in limited accuracy due to a lack of reference information. In this paper, we propose a novel few-shot segmentation model, Vision and Language reference Prompt into SAM (VLP-SAM), that utilizes the visual information of the reference images and the semantic information of the text labels by inputting not only images but also language as reference information. In particular, VLP-SAM is a simple and scalable structure with minimal learnable parameters, which inputs prompt embeddings with vision-language information into SAM using a multimodal vision-language model. To demonstrate the effectiveness of VLP-SAM, we conducted experiments on the PASCAL-5i and COCO-20i datasets, and achieved high performance in the few-shot segmentation task, outperforming the previous state-of-the-art model by a large margin (6.3% and 9.5% in mIoU, respectively). Furthermore, VLP-SAM demonstrates its generality in unseen objects that are not included in the training data. Our code is available at https://github.com/kosukesakurai1/VLP-SAM.

### MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction 
[[arxiv](https://arxiv.org/abs/2502.00717)] [[cool](https://papers.cool/arxiv/2502.00717)] [[pdf](https://arxiv.org/pdf/2502.00717)]
> **Authors**: Chao Wang,Jianming Yang,Yang Zhou
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 5 figures, 4 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Hallucination has been a long-standing and inevitable problem that hinders the application of Large Vision-Language Models (LVLMs) in domains that require high reliability. Various methods focus on improvement depending on data annotations or training strategies, yet place less emphasis on LLM's inherent problems. To fill this gap, we delve into the attention mechanism of the decoding process in the LVLM. Intriguingly, our investigation uncovers the prevalent attention redundancy within the hierarchical architecture of the LVLM, manifesting as overextended image processing in deep layers and an overabundance of non-essential image tokens. Stemming from the observation, we thus propose MINT, a novel training-free decoding strategy, MItigating hallucinations via tokeN reducTion. Specifically, we dynamically intensify the LVLM's local perception capability by masking its attention to irrelevant image tokens. In addition, we use contrastive decoding that pushes the model to focus more on those key image regions. Our full method aims to guide the model in concentrating more on key visual elements during generation. Extensive experimental results on several popular public benchmarks show that our approach achieves a 4% improvement in mitigating hallucinations caused by distracted perception compared to original models. Meanwhile, our approach is demonstrated to make the model perceive 5% more visual points even though we reduce a suite of image tokens.

### VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework 
[[arxiv](https://arxiv.org/abs/2502.00711)] [[cool](https://papers.cool/arxiv/2502.00711)] [[pdf](https://arxiv.org/pdf/2502.00711)]
> **Authors**: Chunbai Zhang,Chao Wang,Yang Zhou,Yan Peng
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 17 pages,12 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Visual reasoning refers to the task of solving questions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text. Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks. To address these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the question with underspecification. Additionally, we design a novel prompting method called Chain-of-Evidence (CoE), which leverages the power of ``evidence for reasoning'' to endow VIKSER with interpretable reasoning capabilities. Meanwhile, the integration of self-reflection technology empowers VIKSER with the ability to learn and improve from its mistakes. Experiments conducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks.

### PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation 
[[arxiv](https://arxiv.org/abs/2502.00708)] [[cool](https://papers.cool/arxiv/2502.00708)] [[pdf](https://arxiv.org/pdf/2502.00708)]
> **Authors**: Qixuan Li,Chao Wang,Zongjin He,Yan Peng
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 13 pages.8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-3D asset generation has achieved significant optimization under the supervision of 2D diffusion priors. However, when dealing with compositional scenes, existing methods encounter several challenges: 1). failure to ensure that composite scene layouts comply with physical laws; 2). difficulty in accurately capturing the assets and relationships described in complex scene descriptions; 3). limited autonomous asset generation capabilities among layout approaches leveraging large language models (LLMs). To avoid these compromises, we propose a novel framework for compositional scene generation, PhiP-G, which seamlessly integrates generation techniques with layout guidance based on a world model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene description to generate a scene graph, and integrating a multimodal 2D generation agent and a 3D Gaussian generation method for targeted assets creation. For the stage of layout, PhiP-G employs a physical pool with adhesion capabilities and a visual supervision agent, forming a world model for layout prediction and planning. Extensive experiments demonstrate that PhiP-G significantly enhances the generation quality and physical rationality of the compositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA) performance in CLIP scores, achieves parity with the leading methods in generation quality as measured by the T$^3$Bench, and improves efficiency by 24x.

### S2CFormer: Reorienting Learned Image Compression from Spatial Interaction to Channel Aggregation 
[[arxiv](https://arxiv.org/abs/2502.00700)] [[cool](https://papers.cool/arxiv/2502.00700)] [[pdf](https://arxiv.org/pdf/2502.00700)]
> **Authors**: Yunuo Chen,Qian Li,Bing He,Donghui Feng,Ronghua Wu,Qi Wang,Li Song,Guo Lu,Wenjun Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Transformers have achieved significant success in learned image compression (LIC), with Swin Transformers emerging as the mainstream choice for nonlinear transforms. A common belief is that their sophisticated spatial operations contribute most to their efficacy. However, the crucial role of the feed-forward network (FFN) based Channel Aggregation module within the transformer architecture has been largely overlooked, and the over-design of spatial operations leads to a suboptimal trade-off between decoding latency and R-D performance. In this paper, we reevaluate the key factors behind the competence of transformers in LIC. By replacing spatial operations with identity mapping, we are surprised to find that channel operations alone can approach the R-D performance of the leading methods. This solid lower bound of performance emphasizes that the presence of channel aggregation is more essential for the LIC model to achieve competitive performance, while the previously complex spatial interactions are partly redundant. Based on this insight, we initiate the "S2CFormer" paradigm, a general architecture that reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each one incorporates a simple operator for spatial interaction and serves as nonlinear transform blocks for our LIC models. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. These results also motivate further exploration of advanced FFN structures to enhance the R-D performance while maintaining model efficiency. With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that combines the strengths of different S2CFormer instantiations. This model outperforms all the existing methods on several datasets, setting a new benchmark for efficient and high-performance LIC.

### High-Order Matching for One-Step Shortcut Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.00688)] [[cool](https://papers.cool/arxiv/2502.00688)] [[pdf](https://arxiv.org/pdf/2502.00688)]
> **Authors**: Bo Chen,Chengyue Gong,Xiaoyu Li,Yingyu Liang,Zhizhou Sha,Zhenmei Shi,Zhao Song,Mingda Wan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: One-step shortcut diffusion models [Frans, Hafner, Levine and Abbeel, ICLR 2025] have shown potential in vision generation, but their reliance on first-order trajectory supervision is fundamentally limited. The Shortcut model's simplistic velocity-only approach fails to capture intrinsic manifold geometry, leading to erratic trajectories, poor geometric alignment, and instability-especially in high-curvature regions. These shortcomings stem from its inability to model mid-horizon dependencies or complex distributional features, leaving it ill-equipped for robust generative modeling. In this work, we introduce HOMO (High-Order Matching for One-Step Shortcut Diffusion), a game-changing framework that leverages high-order supervision to revolutionize distribution transportation. By incorporating acceleration, jerk, and beyond, HOMO not only fixes the flaws of the Shortcut model but also achieves unprecedented smoothness, stability, and geometric precision. Theoretically, we prove that HOMO's high-order supervision ensures superior approximation accuracy, outperforming first-order methods. Empirically, HOMO dominates in complex settings, particularly in high-curvature regions where the Shortcut model struggles. Our experiments show that HOMO delivers smoother trajectories and better distributional alignment, setting a new standard for one-step generative models.

### Cross-Modal Synergies: Unveiling the Potential of Motion-Aware Fusion Networks in Handling Dynamic and Static ReID Scenarios 
[[arxiv](https://arxiv.org/abs/2502.00665)] [[cool](https://papers.cool/arxiv/2502.00665)] [[pdf](https://arxiv.org/pdf/2502.00665)]
> **Authors**: Fuxi Ling,Hongye Liu,Guoqiang Huang,Jing Li,Hong Wu,Zhihao Tang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Navigating the complexities of person re-identification (ReID) in varied surveillance scenarios, particularly when occlusions occur, poses significant challenges. We introduce an innovative Motion-Aware Fusion (MOTAR-FUSE) network that utilizes motion cues derived from static imagery to significantly enhance ReID capabilities. This network incorporates a dual-input visual adapter capable of processing both images and videos, thereby facilitating more effective feature extraction. A unique aspect of our approach is the integration of a motion consistency task, which empowers the motion-aware transformer to adeptly capture the dynamics of human motion. This technique substantially improves the recognition of features in scenarios where occlusions are prevalent, thereby advancing the ReID process. Our comprehensive evaluations across multiple ReID benchmarks, including holistic, occluded, and video-based scenarios, demonstrate that our MOTAR-FUSE network achieves superior performance compared to existing approaches.

### Enhanced Convolutional Neural Networks for Improved Image Classification 
[[arxiv](https://arxiv.org/abs/2502.00663)] [[cool](https://papers.cool/arxiv/2502.00663)] [[pdf](https://arxiv.org/pdf/2502.00663)]
> **Authors**: Xiaoran Yang,Shuhan Yu,Wenxi Xu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Image classification is a fundamental task in computer vision with diverse applications, ranging from autonomous systems to medical imaging. The CIFAR-10 dataset is a widely used benchmark to evaluate the performance of classification models on small-scale, multi-class datasets. Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art results; however, they often suffer from overfitting and suboptimal feature representation when applied to challenging datasets like CIFAR-10. In this paper, we propose an enhanced CNN architecture that integrates deeper convolutional blocks, batch normalization, and dropout regularization to achieve superior performance. The proposed model achieves a test accuracy of 84.95%, outperforming baseline CNN architectures. Through detailed ablation studies, we demonstrate the effectiveness of the enhancements and analyze the hierarchical feature representations. This work highlights the potential of refined CNN architectures for tackling small-scale image classification problems effectively.

### Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation 
[[arxiv](https://arxiv.org/abs/2502.00662)] [[cool](https://papers.cool/arxiv/2502.00662)] [[pdf](https://arxiv.org/pdf/2502.00662)]
> **Authors**: Yimu Wang,Evelien Riddell,Adrian Chow,Sean Sedwards,Krzysztof Czarnecki
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods.

### Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer 
[[arxiv](https://arxiv.org/abs/2502.00639)] [[cool](https://papers.cool/arxiv/2502.00639)] [[pdf](https://arxiv.org/pdf/2502.00639)]
> **Authors**: Tao Ren,Zishi Zhang,Zehao Li,Jingyang Jiang,Shentao Qin,Guanghao Li,Yan Li,Yi Zheng,Xinping Li,Min Zhan,Yijie Peng
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,机器学习
- **Abstract**: The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator an unbiased one with the lower variance than other methods. We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect.

### MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density Prediction 
[[arxiv](https://arxiv.org/abs/2502.00631)] [[cool](https://papers.cool/arxiv/2502.00631)] [[pdf](https://arxiv.org/pdf/2502.00631)]
> **Authors**: Xuyin Qi,Zeyu Zhang,Huazhan Zheng,Mingxi Chen,Numan Kutaiba,Ruth Lim,Cherie Chiang,Zi En Tham,Xuan Ren,Wenxin Zhang,Lei Zhang,Hao Zhang,Wenbing Lv,Guangzhen Yao,Renda Han,Kangsheng Wang,Mingyuan Li,Hongtao Mao,Yu Li,Zhibin Liao,Yang Zhao,Minh-Son To
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Bone density prediction via CT scans to estimate T-scores is crucial, providing a more precise assessment of bone health compared to traditional methods like X-ray bone density tests, which lack spatial resolution and the ability to detect localized changes. However, CT-based prediction faces two major challenges: the high computational complexity of transformer-based architectures, which limits their deployment in portable and clinical settings, and the imbalanced, long-tailed distribution of real-world hospital data that skews predictions. To address these issues, we introduce MedConv, a convolutional model for bone density prediction that outperforms transformer models with lower computational demands. We also adapt Bal-CE loss and post-hoc logit adjustment to improve class balance. Extensive experiments on our AustinSpine dataset shows that our approach achieves up to 21% improvement in accuracy and 20% in ROC AUC over previous state-of-the-art methods.

### Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM Adaptation 
[[arxiv](https://arxiv.org/abs/2502.00630)] [[cool](https://papers.cool/arxiv/2502.00630)] [[pdf](https://arxiv.org/pdf/2502.00630)]
> **Authors**: Bin Xie,Hao Tang,Dawen Cai,Yan Yan,Gady Agam
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Segment Anything Model (SAM) has demonstrated impressive zero-shot performance and brought a range of unexplored capabilities to natural image segmentation tasks. However, as a very important branch of image segmentation, the performance of SAM remains uncertain when applied to medical image segmentation due to the significant differences between natural images and medical images. Meanwhile, it is harsh to meet the SAM's requirements of extra prompts provided, such as points or boxes to specify medical regions. In this paper, we propose a novel self-prompt SAM adaptation framework for medical image segmentation, named Self-Prompt-SAM. We design a multi-scale prompt generator combined with the image encoder in SAM to generate auxiliary masks. Then, we use the auxiliary masks to generate bounding boxes as box prompts and use Distance Transform to select the most central points as point prompts. Meanwhile, we design a 3D depth-fused adapter (DfusedAdapter) and inject the DFusedAdapter into each transformer in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Extensive experiments demonstrate that our method achieves state-of-the-art performance and outperforms nnUNet by 2.3% on AMOS2022, 1.6% on ACDCand 0.5% on Synapse datasets.

### DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.00618)] [[cool](https://papers.cool/arxiv/2502.00618)] [[pdf](https://arxiv.org/pdf/2502.00618)]
> **Authors**: Chiyuan He,Zihuan Qiu,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Continual adaptation of vision-language models (VLMs) focuses on leveraging cross-modal pretrained knowledge to incrementally adapt for expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research often focuses on connecting visual features with specific class text in downstream tasks, overlooking the latent relationships between general and specialized knowledge. Our findings reveal that forcing models to optimize inappropriate visual-text matches exacerbates forgetting of VLMs. To tackle this issue, we propose DesCLIP, which leverages general attribute (GA) descriptions to guide the understanding of specific class objects, enabling VLMs to establish robust \textit{vision-GA-class} trilateral associations rather than relying solely on \textit{vision-class} connections. Specifically, we introduce a language assistant to generate concrete GA description candidates via proper request prompts. Then, an anchor-based embedding filter is designed to obtain highly relevant GA description embeddings, which are leveraged as the paired text embeddings for visual-textual instance matching, thereby tuning the visual encoder. Correspondingly, the class text embeddings are gradually calibrated to align with these shared GA description embeddings. Extensive experiments demonstrate the advancements and efficacy of our proposed method, with comprehensive empirical evaluations highlighting its superior performance compared to existing pretrained and VLM-based continual learning methods.

### Fast Vision Mamba: Pooling Spatial Dimensions for Accelerated Processing 
[[arxiv](https://arxiv.org/abs/2502.00594)] [[cool](https://papers.cool/arxiv/2502.00594)] [[pdf](https://arxiv.org/pdf/2502.00594)]
> **Authors**: Saarthak Kapse,Robin Betz,Srinivasan Sivanandan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 15 figures, https://github.com/insitro/FastVim
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: State Space Models (SSMs) with selective scan (Mamba) have been adapted into efficient vision models. Mamba, unlike Vision Transformers, achieves linear complexity for token interactions through a recurrent hidden state process. This sequential processing is enhanced by a parallel scan algorithm, which reduces the computational time of recurrent steps from $L$ sequential steps to $log(L)$ parallel steps with respect to the number of input tokens ($L$). In this work, we propose Fast Vision Mamba (FastVim), that further reduces the computational time of the SSM block by reducing the number of recurrent steps in Vision Mamba models while still retaining model performance. By alternately pooling tokens along image dimensions across Mamba blocks, we obtain a 2$\times$ reduction in the number of parallel steps in SSM block. Our model offers up to $72.5\%$ speedup in inference speed compared to baseline Vision Mamba models on high resolution (2048$\times$2048) images. Our experiments demonstrate state-of-the-art performance with dramatically improved throughput in a range of tasks such as image classification, cell perturbation prediction, segmentation, and object detection. Code is made available at https://github.com/insitro/FastVim

### Contrastive Forward-Forward: A Training Algorithm of Vision Transformer 
[[arxiv](https://arxiv.org/abs/2502.00571)] [[cool](https://papers.cool/arxiv/2502.00571)] [[pdf](https://arxiv.org/pdf/2502.00571)]
> **Authors**: Hossein Aghagolzadeh,Mehdi Ezoji
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 22 pages, 8 figures, under review
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Although backpropagation is widely accepted as a training algorithm for artificial neural networks, researchers are always looking for inspiration from the brain to find ways with potentially better performance. Forward-Forward is a new training algorithm that is more similar to what occurs in the brain, although there is a significant performance gap compared to backpropagation. In the Forward-Forward algorithm, the loss functions are placed after each layer, and the updating of a layer is done using two local forward passes and one local backward pass. Forward-Forward is in its early stages and has been designed and evaluated on simple multi-layer perceptron networks to solve image classification tasks. In this work, we have extended the use of this algorithm to a more complex and modern network, namely the Vision Transformer. Inspired by insights from contrastive learning, we have attempted to revise this algorithm, leading to the introduction of Contrastive Forward-Forward. Experimental results show that our proposed algorithm performs significantly better than the baseline Forward-Forward leading to an increase of up to 10% in accuracy and boosting the convergence speed by 5 to 20 times on Vision Transformer. Furthermore, if we take Cross Entropy as the baseline loss function in backpropagation, it will be demonstrated that the proposed modifications to the baseline Forward-Forward reduce its performance gap compared to backpropagation on Vision Transformer, and even outperforms it in certain conditions, such as inaccurate supervision.

### Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions 
[[arxiv](https://arxiv.org/abs/2502.00568)] [[cool](https://papers.cool/arxiv/2502.00568)] [[pdf](https://arxiv.org/pdf/2502.00568)]
> **Authors**: Samiran Dey,Christopher R. S. Banerji,Partha Basuchowdhuri,Sanjoy K. Saha,Deepak Parashar,Tapabrata Chakraborti
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathGen code is available for open use by the research community through GitHub at https://github.com/Samiran-Dey/PathGen.

### Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation 
[[arxiv](https://arxiv.org/abs/2502.00563)] [[cool](https://papers.cool/arxiv/2502.00563)] [[pdf](https://arxiv.org/pdf/2502.00563)]
> **Authors**: Renhao Lu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited for capturing high-dimensional directional features and exhibits greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. The code is available at https://anonymous.4open.science/r/CWMI-83B7/

### Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2502.00547)] [[cool](https://papers.cool/arxiv/2502.00547)] [[pdf](https://arxiv.org/pdf/2502.00547)]
> **Authors**: Zaitian Wang,Jian He,Yu Liang,Xiyuan Hu,Tianhao Peng,Kaixin Wang,Jiakai Wang,Chenlong Zhang,Weili Zhang,Shuang Niu,Xiaoyang Xie
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互
- **Abstract**: Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a cross-modal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies in enhancing emotion recognition performance. Our code are available at https://github.com/liangyubuaa/Milmer.

### CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.00536)] [[cool](https://papers.cool/arxiv/2502.00536)] [[pdf](https://arxiv.org/pdf/2502.00536)]
> **Authors**: Wenbo Xiao,Zhihao Xu,Guiping Liang,Yangjun Deng,Yi Xiao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 3 figures, 4 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Semi-supervised medical image segmentation aims to leverage minimal expert annotations, yet remains confronted by challenges in maintaining high-quality consistency learning. Excessive perturbations can degrade alignment and hinder precise decision boundaries, especially in regions with uncertain predictions. In this paper, we introduce Confidence-Aware Adaptive Displacement (CAD), a framework that selectively identifies and replaces the largest low-confidence regions with high-confidence patches. By dynamically adjusting both the maximum allowable replacement size and the confidence threshold throughout training, CAD progressively refines the segmentation quality without overwhelming the learning process. Experimental results on public medical datasets demonstrate that CAD effectively enhances segmentation quality, establishing new state-of-the-art accuracy in this field. The source code will be released after the paper is published.

### Work-Efficient Parallel Non-Maximum Suppression Kernels 
[[arxiv](https://arxiv.org/abs/2502.00535)] [[cool](https://papers.cool/arxiv/2502.00535)] [[pdf](https://arxiv.org/pdf/2502.00535)]
> **Authors**: David Oro,Carles Fernández,Xavier Martorell,Javier Hernando
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Code: https://github.com/hertasecurity/gpu-nms
- **标题**: None
- **领域**: 计算机视觉和模式识别,分布式、并行和集群计算
- **Abstract**: In the context of object detection, sliding-window classifiers and single-shot Convolutional Neural Network (CNN) meta-architectures typically yield multiple overlapping candidate windows with similar high scores around the true location of a particular object. Non-Maximum Suppression (NMS) is the process of selecting a single representative candidate within this cluster of detections, so as to obtain a unique detection per object appearing on a given picture. In this paper, we present a highly scalable NMS algorithm for embedded GPU architectures that is designed from scratch to handle workloads featuring thousands of simultaneous detections on a given picture. Our kernels are directly applicable to other sequential NMS algorithms such as FeatureNMS, Soft-NMS or AdaptiveNMS that share the inner workings of the classic greedy NMS method. The obtained performance results show that our parallel NMS algorithm is capable of clustering 1024 simultaneous detected objects per frame in roughly 1 ms on both NVIDIA Tegra X1 and NVIDIA Tegra X2 on-die GPUs, while taking 2 ms on NVIDIA Tegra K1. Furthermore, our proposed parallel greedy NMS algorithm yields a 14x-40x speed up when compared to state-of-the-art NMS methods that require learning a CNN from annotated data.

### Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings 
[[arxiv](https://arxiv.org/abs/2502.00528)] [[cool](https://papers.cool/arxiv/2502.00528)] [[pdf](https://arxiv.org/pdf/2502.00528)]
> **Authors**: Zachary Huemann,Samuel Church,Joshua D. Warner,Daniel Tran,Xin Tie,Alan B McMillan,Junjie Hu,Steve Y. Cho,Meghan Lubner,Tyler J. Bradshaw
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Vision-language models can connect the text description of an object to its specific location in an image through visual grounding. This has potential applications in enhanced radiology reporting. However, these models require large annotated image-text datasets, which are lacking for PET/CT. We developed an automated pipeline to generate weak labels linking PET/CT report descriptions to their image locations and used it to train a 3D vision-language visual grounding model. Our pipeline finds positive findings in PET/CT reports by identifying mentions of SUVmax and axial slice numbers. From 25,578 PET/CT exams, we extracted 11,356 sentence-label pairs. Using this data, we trained ConTEXTual Net 3D, which integrates text embeddings from a large language model with a 3D nnU-Net via token-level cross-attention. The model's performance was compared against LLMSeg, a 2.5D version of ConTEXTual Net, and two nuclear medicine physicians. The weak-labeling pipeline accurately identified lesion locations in 98% of cases (246/251), with 7.5% requiring boundary adjustments. ConTEXTual Net 3D achieved an F1 score of 0.80, outperforming LLMSeg (F1=0.22) and the 2.5D model (F1=0.53), though it underperformed both physicians (F1=0.94 and 0.91). The model achieved better performance on FDG (F1=0.78) and DCFPyL (F1=0.75) exams, while performance dropped on DOTATE (F1=0.58) and Fluciclovine (F1=0.66). The model performed consistently across lesion sizes but showed reduced accuracy on lesions with low uptake. Our novel weak labeling pipeline accurately produced an annotated dataset of PET/CT image-text pairs, facilitating the development of 3D visual grounding models. ConTEXTual Net 3D significantly outperformed other models but fell short of the performance of nuclear medicine physicians. Our study suggests that even larger datasets may be needed to close this performance gap.

### Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation 
[[arxiv](https://arxiv.org/abs/2502.00500)] [[cool](https://papers.cool/arxiv/2502.00500)] [[pdf](https://arxiv.org/pdf/2502.00500)]
> **Authors**: Yang Cao,Zhao Song,Chiwun Yang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 39 pages, 6 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: This paper considers an efficient video modeling process called Video Latent Flow Matching (VLFM). Unlike prior works, which randomly sampled latent patches for video generation, our method relies on current strong pre-trained image generation models, modeling a certain caption-guided flow of latent patches that can be decoded to time-dependent video frames. We first speculate multiple images of a video are differentiable with respect to time in some latent space. Based on this conjecture, we introduce the HiPPO framework to approximate the optimal projection for polynomials to generate the probability path. Our approach gains the theoretical benefits of the bounded universal approximation error and timescale robustness. Moreover, VLFM processes the interpolation and extrapolation abilities for video generation with arbitrary frame rates. We conduct experiments on several text-to-video datasets to showcase the effectiveness of our method.

### A framework for river connectivity classification using temporal image processing and attention based neural networks 
[[arxiv](https://arxiv.org/abs/2502.00474)] [[cool](https://papers.cool/arxiv/2502.00474)] [[pdf](https://arxiv.org/pdf/2502.00474)]
> **Authors**: Timothy James Becker,Derin Gezgin,Jun Yi He Wu,Mary Becker
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,图像和视频处理
- **Abstract**: Measuring the connectivity of water in rivers and streams is essential for effective water resource management. Increased extreme weather events associated with climate change can result in alterations to river and stream connectivity. While traditional stream flow gauges are costly to deploy and limited to large river bodies, trail camera methods are a low-cost and easily deployed alternative to collect hourly data. Image capturing, however requires stream ecologists to manually curate (select and label) tens of thousands of images per year. To improve this workflow, we developed an automated instream trail camera image classification system consisting of three parts: (1) image processing, (2) image augmentation and (3) machine learning. The image preprocessing consists of seven image quality filters, foliage-based luma variance reduction, resizing and bottom-center cropping. Images are balanced using variable amount of generative augmentation using diffusion models and then passed to a machine learning classification model in labeled form. By using the vision transformer architecture and temporal image enhancement in our framework, we are able to increase the 75% base accuracy to 90% for a new unseen site image. We make use of a dataset captured and labeled by staff from the Connecticut Department of Energy and Environmental Protection between 2018-2020. Our results indicate that a combination of temporal image processing and attention-based models are effective at classifying unseen river connectivity images.

### Evaluation of End-to-End Continuous Spanish Lipreading in Different Data Conditions 
[[arxiv](https://arxiv.org/abs/2502.00464)] [[cool](https://papers.cool/arxiv/2502.00464)] [[pdf](https://arxiv.org/pdf/2502.00464)]
> **Authors**: David Gimeno-Gómez,Carlos-D. Martínez-Hinarejos
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted in the "LanguageResources and Evaluation" journal, Springer Nature
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual speech recognition remains an open research problem where different challenges must be considered by dispensing with the auditory sense, such as visual ambiguities, the inter-personal variability among speakers, and the complex modeling of silence. Nonetheless, recent remarkable results have been achieved in the field thanks to the availability of large-scale databases and the use of powerful attention mechanisms. Besides, multiple languages apart from English are nowadays a focus of interest. This paper presents noticeable advances in automatic continuous lipreading for Spanish. First, an end-to-end system based on the hybrid CTC/Attention architecture is presented. Experiments are conducted on two corpora of disparate nature, reaching state-of-the-art results that significantly improve the best performance obtained to date for both databases. In addition, a thorough ablation study is carried out, where it is studied how the different components that form the architecture influence the quality of speech recognition. Then, a rigorous error analysis is carried out to investigate the different factors that could affect the learning of the automatic system. Finally, a new Spanish lipreading benchmark is consolidated. Code and trained models are available at https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading.

### MambaGlue: Fast and Robust Local Feature Matching With Mamba 
[[arxiv](https://arxiv.org/abs/2502.00462)] [[cool](https://papers.cool/arxiv/2502.00462)] [[pdf](https://arxiv.org/pdf/2502.00462)]
> **Authors**: Kihwan Ryoo,Hyungtae Lim,Hyun Myung
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Proc. IEEE Int'l Conf. Robotics and Automation (ICRA) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: In recent years, robust matching methods using deep learning-based approaches have been actively studied and improved in computer vision tasks. However, there remains a persistent demand for both robust and fast matching techniques. To address this, we propose a novel Mamba-based local feature matching approach, called MambaGlue, where Mamba is an emerging state-of-the-art architecture rapidly gaining recognition for its superior speed in both training and inference, and promising performance compared with Transformer architectures. In particular, we propose two modules: a) MambaAttention mixer to simultaneously and selectively understand the local and global context through the Mamba-based self-attention structure and b) deep confidence score regressor, which is a multi-layer perceptron (MLP)-based architecture that evaluates a score indicating how confidently matching predictions correspond to the ground-truth correspondences. Consequently, our MambaGlue achieves a balance between robustness and efficiency in real-world applications. As verified on various public datasets, we demonstrate that our MambaGlue yields a substantial performance improvement over baseline approaches while maintaining fast inference speed. Our code will be available on https://github.com/url-kaist/MambaGlue

### SatMamba: Development of Foundation Models for Remote Sensing Imagery Using State Space Models 
[[arxiv](https://arxiv.org/abs/2502.00435)] [[cool](https://papers.cool/arxiv/2502.00435)] [[pdf](https://arxiv.org/pdf/2502.00435)]
> **Authors**: Chuc Man Duc,Hiromichi Fukui
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Foundation models refer to deep learning models pretrained on large unlabeled datasets through self-supervised algorithms. In the Earth science and remote sensing communities, there is growing interest in transforming the use of Earth observation data, including satellite and aerial imagery, through foundation models. Various foundation models have been developed for remote sensing, such as those for multispectral, high-resolution, and hyperspectral images, and have demonstrated superior performance on various downstream tasks compared to traditional supervised models. These models are evolving rapidly, with capabilities to handle multispectral, multitemporal, and multisensor data. Most studies use masked autoencoders in combination with Vision Transformers (ViTs) as the backbone for pretraining. While the models showed promising performance, ViTs face challenges, such as quadratic computational scaling with input length, which may limit performance on multiband and multitemporal data with long sequences. This research aims to address these challenges by proposing SatMamba, a new pretraining framework that combines masked autoencoders with State Space Model, offering linear computational scaling. Experiments on high-resolution imagery across various downstream tasks show promising results, paving the way for more efficient foundation models and unlocking the full potential of Earth observation data. The source code is available in https://github.com/mdchuc/HRSFM.

### TEST-V: TEst-time Support-set Tuning for Zero-shot Video Classification 
[[arxiv](https://arxiv.org/abs/2502.00426)] [[cool](https://papers.cool/arxiv/2502.00426)] [[pdf](https://arxiv.org/pdf/2502.00426)]
> **Authors**: Rui Yan,Jin Wang,Hongyu Qu,Xiaoyu Du,Dong Zhang,Jinhui Tang,Tieniu Tan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recently, adapting Vision Language Models (VLMs) to zero-shot visual classification by tuning class embedding with a few prompts (Test-time Prompt Tuning, TPT) or replacing class names with generated visual samples (support-set) has shown promising results. However, TPT cannot avoid the semantic gap between modalities while the support-set cannot be tuned. To this end, we draw on each other's strengths and propose a novel framework namely TEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It first dilates the support-set with multiple prompts (Multi-prompting Support-set Dilation, MSD) and then erodes the support-set via learnable weights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE). Specifically, i) MSD expands the support samples for each class based on multiple prompts enquired from LLMs to enrich the diversity of the support-set. ii) TSE tunes the support-set with factorized learnable weights according to the temporal prediction consistency in a self-supervised manner to dig pivotal supporting cues for each class. $\textbf{TEST-V}$ achieves state-of-the-art results across four benchmarks and has good interpretability for the support-set dilation and erosion.

### MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization 
[[arxiv](https://arxiv.org/abs/2502.00425)] [[cool](https://papers.cool/arxiv/2502.00425)] [[pdf](https://arxiv.org/pdf/2502.00425)]
> **Authors**: JiangYong Yu,Sifan Zhou,Dawei Yang,Shuo Wang,Shuoyu Li,Xing Hu,Chen Xu,Zukang Xu,Changyong Shu,Zhihang Yuan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: First quantization solution for Multimodal largelanguagemodels applicable to 5 mainstream MLLMs
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles with MLLMs because of (a) high inference latency from large visual token counts, (b) distributional disparities between visual and textual tokens, and (c) extreme outliers introduced by Hadamard-based transformations. To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs. textual tokens; Attention-Invariant Flexible Switching (AIFS), reordering tokens to preserve casual attention while eliminating expensive token-wise scale computations; Rotation Magnitude Suppression (RMS), mitigating weight outliers arising from online Hadamard rotations. On five mainstream MLLMs (including Qwen-VL, MiniCPM-V, CogVLM2), MQuant under W4A8 achieves near-floating-point accuracy (<1% degradation) while reducing inference latency by up to 30%, significantly outperforming existing PTQ baselines. Our MQuant effectively bridges the gap for efficient and accurate MLLMs inference in resource-constrained devices. Code will be released.

### Parameter Efficient Fine-Tuning of Segment Anything Model 
[[arxiv](https://arxiv.org/abs/2502.00418)] [[cool](https://papers.cool/arxiv/2502.00418)] [[pdf](https://arxiv.org/pdf/2502.00418)]
> **Authors**: Carolin Teuber,Anwai Archit,Constantin Pape
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation. Vision foundation models, such as Segment Anything Model (SAM), address this issue through broad segmentation capabilities. However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions. As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant for their application. We contribute the first comprehensive study of PEFT for SAM applied to biomedical segmentation by evaluating 9 PEFT methods on diverse datasets. We also provide an implementation of QLoRA for vision transformers and a new approach for resource-efficient finetuning of SAM. Our code is publicly available at https://github.com/computational-cell-analytics/peft-sam.

### Exploring Linear Attention Alternative for Single Image Super-Resolution 
[[arxiv](https://arxiv.org/abs/2502.00404)] [[cool](https://papers.cool/arxiv/2502.00404)] [[pdf](https://arxiv.org/pdf/2502.00404)]
> **Authors**: Rongchang Lu,Changyu Li,Donghang Li,Guojing Zhang,Jianqiang Huang,Xilai Li
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: This paper has been published to IEEE International Joint Conference onNeuralNetworks. Feel free to contact on nomodeset@qq.com
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM.

### Minimalistic Video Saliency Prediction via Efficient Decoder & Spatio Temporal Action Cues 
[[arxiv](https://arxiv.org/abs/2502.00397)] [[cool](https://papers.cool/arxiv/2502.00397)] [[pdf](https://arxiv.org/pdf/2502.00397)]
> **Authors**: Rohit Girmaji,Siddharth Jain,Bhav Beri,Sarthak Bansal,Vineet Gandhi
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted at 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper introduces ViNet-S, a 36MB model based on the ViNet architecture with a U-Net design, featuring a lightweight decoder that significantly reduces model size and parameters without compromising performance. Additionally, ViNet-A (148MB) incorporates spatio-temporal action localization (STAL) features, differing from traditional video saliency models that use action classification backbones. Our studies show that an ensemble of ViNet-S and ViNet-A, by averaging predicted saliency maps, achieves state-of-the-art performance on three visual-only and six audio-visual saliency datasets, outperforming transformer-based models in both parameter efficiency and real-time performance, with ViNet-S reaching over 1000fps.

### RefDrone: A Challenging Benchmark for Referring Expression Comprehension in Drone Scenes 
[[arxiv](https://arxiv.org/abs/2502.00392)] [[cool](https://papers.cool/arxiv/2502.00392)] [[pdf](https://arxiv.org/pdf/2502.00392)]
> **Authors**: Zhichao Sun,Yepeng Liu,Huachao Zhu,Yuliang Gu,Yuda Zou,Zelong Liu,Gui-Song Xia,Bo Du,Yongchao Xu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code will be publicly at https://github.com/sunzc-sunny/refdrone.

### Efficient Adaptive Label Refinement for Label Noise Learning 
[[arxiv](https://arxiv.org/abs/2502.00386)] [[cool](https://papers.cool/arxiv/2502.00386)] [[pdf](https://arxiv.org/pdf/2502.00386)]
> **Authors**: Wenzhen Zhang,Debo Cheng,Guangquan Lu,Bo Zhou,Jiaye Li,Shichao Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep neural networks are highly susceptible to overfitting noisy labels, which leads to degraded performance. Existing methods address this issue by employing manually defined criteria, aiming to achieve optimal partitioning in each iteration to avoid fitting noisy labels while thoroughly learning clean samples. However, this often results in overly complex and difficult-to-train models. To address this issue, we decouple the tasks of avoiding fitting incorrect labels and thoroughly learning clean samples and propose a simple yet highly applicable method called Adaptive Label Refinement (ALR). First, inspired by label refurbishment techniques, we update the original hard labels to soft labels using the model's predictions to reduce the risk of fitting incorrect labels. Then, by introducing the entropy loss, we gradually `harden' the high-confidence soft labels, guiding the model to better learn from clean samples. This approach is simple and efficient, requiring no prior knowledge of noise or auxiliary datasets, making it more accessible compared to existing methods. We validate ALR's effectiveness through experiments on benchmark datasets with artificial label noise (CIFAR-10/100) and real-world datasets with inherent noise (ANIMAL-10N, Clothing1M, WebVision). The results show that ALR outperforms state-of-the-art methods.

### Masked Generative Nested Transformers with Decode Time Scaling 
[[arxiv](https://arxiv.org/abs/2502.00382)] [[cool](https://papers.cool/arxiv/2502.00382)] [[pdf](https://arxiv.org/pdf/2502.00382)]
> **Authors**: Sahil Goyal,Debapriya Tula,Gagan Jain,Pradeep Shenoy,Prateek Jain,Sujoy Paul
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem - a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas - (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256$\times$256 , UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost $3\times$ less compute than baseline, our model obtains competitive performance.

### Latent Action Learning Requires Supervision in the Presence of Distractors 
[[arxiv](https://arxiv.org/abs/2502.00379)] [[cool](https://papers.cool/arxiv/2502.00379)] [[pdf](https://arxiv.org/pdf/2502.00379)]
> **Authors**: Alexander Nikulin,Ilya Zisman,Denis Tarasov,Nikita Lyubaykin,Andrei Polubarov,Igor Kiselev,Vladislav Kurenkov
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Preprint. In review
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.

### Scalable Framework for Classifying AI-Generated Content Across Modalities 
[[arxiv](https://arxiv.org/abs/2502.00375)] [[cool](https://papers.cool/arxiv/2502.00375)] [[pdf](https://arxiv.org/pdf/2502.00375)]
> **Authors**: Anh-Kiet Duong,Petra Gomez-Krämer
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Defactify4 @ AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The rapid growth of generative AI technologies has heightened the importance of effectively distinguishing between human and AI-generated content, as well as classifying outputs from diverse generative models. This paper presents a scalable framework that integrates perceptual hashing, similarity measurement, and pseudo-labeling to address these challenges. Our method enables the incorporation of new generative models without retraining, ensuring adaptability and robustness in dynamic scenarios. Comprehensive evaluations on the Defactify4 dataset demonstrate competitive performance in text and image classification tasks, achieving high accuracy across both distinguishing human and AI-generated content and classifying among generative methods. These results highlight the framework's potential for real-world applications as generative AI continues to evolve. Source codes are publicly available at https://github.com/ffyyytt/defactify4.

### NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning 
[[arxiv](https://arxiv.org/abs/2502.00372)] [[cool](https://papers.cool/arxiv/2502.00372)] [[pdf](https://arxiv.org/pdf/2502.00372)]
> **Authors**: Zhixi Cai,Fucai Ke,Simindokht Jahangard,Maria Garcia de la Banda,Reza Haffari,Peter J. Stuckey,Hamid Rezatofighi
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual Grounding (VG) tasks, such as referring expression detection and segmentation tasks are important for linking visual entities to context, especially in complex reasoning tasks that require detailed query interpretation. This paper explores VG beyond basic perception, highlighting challenges for methods that require reasoning like human cognition. Recent advances in large language methods (LLMs) and Vision-Language methods (VLMs) have improved abilities for visual comprehension, contextual understanding, and reasoning. These methods are mainly split into end-to-end and compositional methods, with the latter offering more flexibility. Compositional approaches that integrate LLMs and foundation models show promising performance but still struggle with complex reasoning with language-based logical representations. To address these limitations, we propose NAVER, a compositional visual grounding method that integrates explicit probabilistic logic reasoning within a finite-state automaton, equipped with a self-correcting mechanism. This design improves robustness and interpretability in inference through explicit logic reasoning. Our results show that NAVER achieves SoTA performance comparing to recent end-to-end and compositional baselines. The code is available at https://github.com/ControlNet/NAVER .

### Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering 
[[arxiv](https://arxiv.org/abs/2502.00342)] [[cool](https://papers.cool/arxiv/2502.00342)] [[pdf](https://arxiv.org/pdf/2502.00342)]
> **Authors**: Zechuan Li,Hongshan Yu,Yihao Ding,Yan Li,Yong He,Naveed Akhtar
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: 3D Scene Question Answering (3D SQA) represents an interdisciplinary task that integrates 3D visual perception and natural language processing, empowering intelligent agents to comprehend and interact with complex 3D environments. Recent advances in large multimodal modelling have driven the creation of diverse datasets and spurred the development of instruction-tuning and zero-shot methods for 3D SQA. However, this rapid progress introduces challenges, particularly in achieving unified analysis and comparison across datasets and baselines. This paper presents the first comprehensive survey of 3D SQA, systematically reviewing datasets, methodologies, and evaluation metrics while highlighting critical challenges and future opportunities in dataset standardization, multimodal fusion, and task design.

### MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.00315)] [[cool](https://papers.cool/arxiv/2502.00315)] [[pdf](https://arxiv.org/pdf/2502.00315)]
> **Authors**: Jihyeok Kim,Seongwoo Moon,Sungwon Nah,David Hyunchul Shim
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper proposes novel methods to enhance the performance of monocular 3D object detection models by leveraging the generalized feature extraction capabilities of a vision foundation model. Unlike traditional CNN-based approaches, which often suffer from inaccurate depth estimation and rely on multi-stage object detection pipelines, this study employs a Vision Transformer (ViT)-based foundation model as the backbone, which excels at capturing global features for depth estimation. It integrates a detection transformer (DETR) architecture to improve both depth estimation and object detection performance in a one-stage manner. Specifically, a hierarchical feature fusion block is introduced to extract richer visual features from the foundation model, further enhancing feature extraction capabilities. Depth estimation accuracy is further improved by incorporating a relative depth estimation model trained on large-scale data and fine-tuning it through transfer learning. Additionally, the use of queries in the transformer's decoder, which consider reference points and the dimensions of 2D bounding boxes, enhances recognition performance. The proposed model outperforms recent state-of-the-art methods, as demonstrated through quantitative and qualitative evaluations on the KITTI 3D benchmark and a custom dataset collected from high-elevation racing environments. Code is available at https://github.com/JihyeokKim/MonoDINO-DETR.

### MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked Images 
[[arxiv](https://arxiv.org/abs/2502.00266)] [[cool](https://papers.cool/arxiv/2502.00266)] [[pdf](https://arxiv.org/pdf/2502.00266)]
> **Authors**: Yuwei Sun,Lu Mi,Ippei Fujisawa,Ryota Kanai
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies perceptual inputs, potentially offering significant advantages in concept learning with large-scale Transformer models. To this end, we propose Multi-layer Concept Map (MCM), the first work to devise an efficient concept learning method based on masked images. In particular, we introduce an asymmetric concept learning architecture by establishing correlations between different encoder and decoder layers, updating concept tokens using backward gradients from reconstruction tasks. The learned concept tokens at various levels of granularity help either reconstruct the masked image patches by filling in gaps or guide the reconstruction results in a direction that reflects specific concepts. Moreover, we present both quantitative and qualitative results across a wide range of metrics, demonstrating that MCM significantly reduces computational costs by training on fewer than 75% of the total image patches while enhancing concept prediction performance. Additionally, editing specific concept tokens in the latent space enables targeted image generation from masked images, aligning both the visible contextual patches and the provided concepts. By further adjusting the testing time mask ratio, we could produce a range of reconstructions that blend the visible patches with the provided concepts, proportional to the chosen ratios.

### INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation 
[[arxiv](https://arxiv.org/abs/2502.00262)] [[cool](https://papers.cool/arxiv/2502.00262)] [[pdf](https://arxiv.org/pdf/2502.00262)]
> **Authors**: Dianwei Chen,Zifan Zhang,Yuchen Liu,Xianfeng Terry Yang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Autonomous driving systems face significant challenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to limitations in traditional detection and prediction approaches. To address this, we propose INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking), a hierarchical vision-language model (VLM) framework designed to enhance hazard detection and edge-case evaluation. By using multimodal data fusion, our approach integrates semantic and visual representations, enabling precise interpretation of driving scenarios and accurate forecasting of potential dangers. Through supervised fine-tuning of VLMs, we optimize spatial hazard localization using attention-based mechanisms and coordinate regression techniques. Experimental results on the BDD100K dataset demonstrate a substantial improvement in hazard prediction straightforwardness and accuracy over existing models, achieving a notable increase in generalization performance. This advancement enhances the robustness and safety of autonomous driving systems, ensuring improved situational awareness and potential decision-making in complex real-world scenarios.

### Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript 
[[arxiv](https://arxiv.org/abs/2502.00250)] [[cool](https://papers.cool/arxiv/2502.00250)] [[pdf](https://arxiv.org/pdf/2502.00250)]
> **Authors**: Takumu Fujioka,Gouhei Tanaka
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 8 figures, 4 tables, Submitted to IJCNN 2025. Code available at https://github.com/fjktkm/truetype-vs-postscript-transformer
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.

### A Hybrid Random Forest and CNN Framework for Tile-Wise Oil-Water Classification in Hyperspectral Images 
[[arxiv](https://arxiv.org/abs/2502.00232)] [[cool](https://papers.cool/arxiv/2502.00232)] [[pdf](https://arxiv.org/pdf/2502.00232)]
> **Authors**: Mehdi Nickzamir,Seyed Mohammad Sheikh Ahamdi Gandab
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: A novel hybrid Random Forest and Convolutional Neural Network (CNN) framework is presented for oil-water classification in hyperspectral images (HSI). To address the challenge of preserving spatial context, the images were divided into smaller, non-overlapping tiles, which served as the basis for training, validation, and testing. Random Forest demonstrated strong performance in pixel-wise classification, outperforming models such as XGBoost, Attention-Based U-Net, and HybridSN. However, Random Forest loses spatial context, limiting its ability to fully exploit the spatial relationships in hyperspectral data. To improve performance, a CNN was trained on the probability maps generated by the Random Forest, leveraging the CNN's capacity to incorporate spatial context. The hybrid approach achieved 7.6% improvement in recall (to 0.85), 2.4% improvement in F1 score (to 0.84), and 0.54% improvement in AUC (to 0.99) compared to the baseline. These results highlight the effectiveness of combining probabilistic outputs with spatial feature learning for context-aware analysis of hyperspectral images.

### DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets 
[[arxiv](https://arxiv.org/abs/2502.00196)] [[cool](https://papers.cool/arxiv/2502.00196)] [[pdf](https://arxiv.org/pdf/2502.00196)]
> **Authors**: Abdurrahim Yilmaz,Furkan Yuceyalcin,Ece Gokyayla,Donghee Choi,Ozan Erdem Ali Anil Demircali,Rahmetullah Varol,Ufuk Gorkem Kirabali,Gulsum Gencoglan,Joram M. Posma,Burak Temelkuran
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: A major barrier to developing vision large language models (LLMs) in dermatology is the lack of large image--text pairs dataset. We introduce DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using Gemini 2.0, we used clinically related prompts and self-instruct method to generate diverse and rich synthetic texts. Metadata of the datasets were incorporated into the input prompts by targeting to reduce potential hallucinations. The resulting dataset builds upon open access dermatological image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We anticipate this dataset to support and accelerate AI research in dermatology. Data and code underlying this work are accessible at https://github.com/abdurrahimyilmaz/DermaSynth.

### ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition 
[[arxiv](https://arxiv.org/abs/2502.00156)] [[cool](https://papers.cool/arxiv/2502.00156)] [[pdf](https://arxiv.org/pdf/2502.00156)]
> **Authors**: Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全
- **Abstract**: Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases. We propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed entropy maximization loss. Additionally, we introduce a gradient penalty loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over 12% on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches. Project Page: https://joefioresi718.github.io/ALBAR_webpage/

### Exploring Transfer Learning for Deep Learning Polyp Detection in Colonoscopy Images Using YOLOv8 
[[arxiv](https://arxiv.org/abs/2502.00133)] [[cool](https://papers.cool/arxiv/2502.00133)] [[pdf](https://arxiv.org/pdf/2502.00133)]
> **Authors**: Fabian Vazquez,Jose Angel Nuñez,Xiaoyan Fu,Pengfei Gu,Bin Fu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 10 pages, 3 figures, 6 tables, SPIE conference
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Deep learning methods have demonstrated strong performance in objection tasks; however, their ability to learn domain-specific applications with limited training data remains a significant challenge. Transfer learning techniques address this issue by leveraging knowledge from pre-training on related datasets, enabling faster and more efficient learning for new tasks. Finding the right dataset for pre-training can play a critical role in determining the success of transfer learning and overall model performance. In this paper, we investigate the impact of pre-training a YOLOv8n model on seven distinct datasets, evaluating their effectiveness when transferred to the task of polyp detection. We compare whether large, general-purpose datasets with diverse objects outperform niche datasets with characteristics similar to polyps. In addition, we assess the influence of the size of the dataset on the efficacy of transfer learning. Experiments on the polyp datasets show that models pre-trained on relevant datasets consistently outperform those trained from scratch, highlighting the benefit of pre-training on datasets with shared domain-specific features.

### ProtoSnap: Prototype Alignment for Cuneiform Signs 
[[arxiv](https://arxiv.org/abs/2502.00129)] [[cool](https://papers.cool/arxiv/2502.00129)] [[pdf](https://arxiv.org/pdf/2502.00129)]
> **Authors**: Rachel Mikulinsky,Morris Alper,Shai Gordin,Enrique Jiménez,Yoram Cohen,Hadar Averbuch-Elor
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Accepted to ICLR 2025. Project page: https://tau-vailab.github.io/ProtoSnap/
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The cuneiform writing system served as the medium for transmitting knowledge in the ancient Near East for a period of over three thousand years. Cuneiform signs have a complex internal structure which is the subject of expert paleographic analysis, as variations in sign shapes bear witness to historical developments and transmission of writing and culture over time. However, prior automated techniques mostly treat sign types as categorical and do not explicitly model their highly varied internal configurations. In this work, we present an unsupervised approach for recovering the fine-grained internal configuration of cuneiform signs by leveraging powerful generative models and the appearance and structure of prototype font images as priors. Our approach, ProtoSnap, enforces structural consistency on matches found with deep image features to estimate the diverse configurations of cuneiform characters, snapping a skeleton-based template to photographed cuneiform signs. We provide a new benchmark of expert annotations and evaluate our method on this task. Our evaluation shows that our approach succeeds in aligning prototype skeletons to a wide variety of cuneiform signs. Moreover, we show that conditioning on structures produced by our method allows for generating synthetic data with correct structural configurations, significantly boosting the performance of cuneiform sign recognition beyond existing techniques, in particular over rare signs. Our code, data, and trained models are available at the project page: https://tau-vailab.github.io/ProtoSnap/

### AIN: The Arabic INclusive Large Multimodal Model 
[[arxiv](https://arxiv.org/abs/2502.00094)] [[cool](https://papers.cool/arxiv/2502.00094)] [[pdf](https://arxiv.org/pdf/2502.00094)]
> **Authors**: Ahmed Heakl,Sara Ghaboura,Omkar Thawkar,Fahad Shahbaz Khan,Hisham Cholakkal,Rao Muhammad Anwer,Salman Khan
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 16 figures, ACL
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,人机交互,机器学习
- **Abstract**: Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.

### CerraData-4MM: A multimodal benchmark dataset on Cerrado for land use and land cover classification 
[[arxiv](https://arxiv.org/abs/2502.00083)] [[cool](https://papers.cool/arxiv/2502.00083)] [[pdf](https://arxiv.org/pdf/2502.00083)]
> **Authors**: Mateus de Souza Miranda,Ronny Hänsch,Valdivino Alexandre de Santiago Júnior,Thales Sehn Körting,Erison Carlos dos Santos Monteiro
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 13 Figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: The Cerrado faces increasing environmental pressures, necessitating accurate land use and land cover (LULC) mapping despite challenges such as class imbalance and visually similar categories. To address this, we present CerraData-4MM, a multimodal dataset combining Sentinel-1 Synthetic Aperture Radar (SAR) and Sentinel-2 MultiSpectral Imagery (MSI) with 10m spatial resolution. The dataset includes two hierarchical classification levels with 7 and 14 classes, respectively, focusing on the diverse Bico do Papagaio ecoregion. We highlight CerraData-4MM's capacity to benchmark advanced semantic segmentation techniques by evaluating a standard U-Net and a more sophisticated Vision Transformer (ViT) model. The ViT achieves superior performance in multimodal scenarios, with the highest macro F1-score of 57.60% and a mean Intersection over Union (mIoU) of 49.05% at the first hierarchical level. Both models struggle with minority classes, particularly at the second hierarchical level, where U-Net's performance drops to an F1-score of 18.16%. Class balancing improves representation for underrepresented classes but reduces overall accuracy, underscoring the trade-off in weighted training. CerraData-4MM offers a challenging benchmark for advancing deep learning models to handle class imbalance and multimodal data fusion. Code, trained models, and data are publicly available at https://github.com/ai4luc/CerraData-4MM.

### Influence of color correction on pathology detection in Capsule Endoscopy 
[[arxiv](https://arxiv.org/abs/2502.00076)] [[cool](https://papers.cool/arxiv/2502.00076)] [[pdf](https://arxiv.org/pdf/2502.00076)]
> **Authors**: Bidossessi Emmanuel Agossou,Marius Pedersen,Kiran Raja,Anuja Vats,Pål Anders Floor
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Pathology detection in Wireless Capsule Endoscopy (WCE) using deep learning has been explored in the recent past. However, deep learning models can be influenced by the color quality of the dataset used to train them, impacting detection, segmentation and classification tasks. In this work, we evaluate the impact of color correction on pathology detection using two prominent object detection models: Retinanet and YOLOv5. We first generate two color corrected versions of a popular WCE dataset (i.e., SEE-AI dataset) using two different color correction functions. We then evaluate the performance of the Retinanet and YOLOv5 on the original and color corrected versions of the dataset. The results reveal that color correction makes the models generate larger bounding boxes and larger intersection areas with the ground truth annotations. Furthermore, color correction leads to an increased number of false positives for certain pathologies. However, these effects do not translate into a consistent improvement in performance metrics such as F1-scores, IoU, and AP50. The code is available at https://github.com/agossouema2011/WCE2024. Keywords: Wireless Capsule Endoscopy, Color correction, Retinanet, YOLOv5, Detection

### SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection 
[[arxiv](https://arxiv.org/abs/2502.00074)] [[cool](https://papers.cool/arxiv/2502.00074)] [[pdf](https://arxiv.org/pdf/2502.00074)]
> **Authors**: Dong-Hee Paek,Seung-Hyun Kong
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: arxiv preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,神经和进化计算
- **Abstract**: Recently, 4D Radar has emerged as a crucial sensor for 3D object detection in autonomous vehicles, offering both stable perception in adverse weather and high-density point clouds for object shape recognition. However, processing such high-density data demands substantial computational resources and energy consumption. We propose SpikingRTNH, the first spiking neural network (SNN) for 3D object detection using 4D Radar data. By replacing conventional ReLU activation functions with leaky integrate-and-fire (LIF) spiking neurons, SpikingRTNH achieves significant energy efficiency gains. Furthermore, inspired by human cognitive processes, we introduce biological top-down inference (BTI), which processes point clouds sequentially from higher to lower densities. This approach effectively utilizes points with lower noise and higher importance for detection. Experiments on K-Radar dataset demonstrate that SpikingRTNH with BTI significantly reduces energy consumption by 78% while achieving comparable detection performance to its ANN counterpart (51.1% AP 3D, 57.0% AP BEV). These results establish the viability of SNNs for energy-efficient 4D Radar-based object detection in autonomous driving systems. All codes are available at https://github.com/kaist-avelab/k-radar.

### A two-stage dual-task learning strategy for early prediction of pathological complete response to neoadjuvant chemotherapy for breast cancer using dynamic contrast-enhanced magnetic resonance images 
[[arxiv](https://arxiv.org/abs/2502.00051)] [[cool](https://papers.cool/arxiv/2502.00051)] [[pdf](https://arxiv.org/pdf/2502.00051)]
> **Authors**: Bowen Jing,Jing Wang
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,医学物理
- **Abstract**: Rationale and Objectives: Early prediction of pathological complete response (pCR) can facilitate personalized treatment for breast cancer patients. To improve prediction accuracy at the early time point of neoadjuvant chemotherapy, we proposed a two-stage dual-task learning strategy to train a deep neural network for early prediction of pCR using early-treatment magnetic resonance images. Methods: We developed and validated the two-stage dual-task learning strategy using the dataset from the national-wide, multi-institutional I-SPY2 clinical trial, which included dynamic contrast-enhanced magnetic resonance images acquired at three time points: pretreatment (T0), after 3 weeks (T1), and after 12 weeks of treatment (T2). First, we trained a convolutional long short-term memory network to predict pCR and extract the latent space image features at T2. At the second stage, we trained a dual-task network to simultaneously predict pCR and the image features at T2 using images from T0 and T1. This allowed us to predict pCR earlier without using images from T2. Results: The conventional single-stage single-task strategy gave an area under the receiver operating characteristic curve (AUROC) of 0.799 for pCR prediction using all the data at time points T0 and T1. By using the proposed two-stage dual-task learning strategy, the AUROC was improved to 0.820. Conclusions: The proposed two-stage dual-task learning strategy can improve model performance significantly (p=0.0025) for predicting pCR at the early stage (3rd week) of neoadjuvant chemotherapy. The early prediction model can potentially help physicians to intervene early and develop personalized plans at the early stage of chemotherapy.

## 计算机与社会(cs.CY:Computers and Society)

### Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs 
[[arxiv](https://arxiv.org/abs/2502.01926)] [[cool](https://papers.cool/arxiv/2502.01926)] [[pdf](https://arxiv.org/pdf/2502.01926)]
> **Authors**: Angelina Wang,Michelle Phan,Daniel E. Ho,Sanmi Koyejo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: Algorithmic fairness has conventionally adopted a perspective of racial color-blindness (i.e., difference unaware treatment). We contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., calling a girl a terrorist may be less harmful than calling a Muslim person one). In our work we first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires distinct interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension of fairness where existing bias mitigation strategies may backfire.

### Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool 
[[arxiv](https://arxiv.org/abs/2502.01713)] [[cool](https://papers.cool/arxiv/2502.01713)] [[pdf](https://arxiv.org/pdf/2502.01713)]
> **Authors**: Floris Holstege,Mackenzie Jorgensen,Kirtan Padh,Jurriaan Parie,Joel Persson,Krsto Prorokovic,Lukas Snoek
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: Algorithms are increasingly used to automate or aid human decisions, yet recent research shows that these algorithms may exhibit bias across legally protected demographic groups. However, data on these groups may be unavailable to organizations or external auditors due to privacy legislation. This paper studies bias detection using an unsupervised clustering tool when data on demographic groups are unavailable. We collaborate with the Dutch Executive Agency for Education to audit an algorithm that was used to assign risk scores to college students at the national level in the Netherlands between 2012-2023. Our audit covers more than 250,000 students from the whole country. The unsupervised clustering tool highlights known disparities between students with a non-European migration background and Dutch origin. Our contributions are three-fold: (1) we assess bias in a real-world, large-scale and high-stakes decision-making process by a governmental organization; (2) we use simulation studies to highlight potential pitfalls of using the unsupervised clustering tool to detect true bias when demographic group data are unavailable and provide recommendations for valid inferences; (3) we provide the unsupervised clustering tool in an open-source library. Our work serves as a starting point for a deliberative assessment by human experts to evaluate potential discrimination in algorithmic-supported decision-making processes.

### LIBRA: Measuring Bias of Large Language Model from a Local Context 
[[arxiv](https://arxiv.org/abs/2502.01679)] [[cool](https://papers.cool/arxiv/2502.01679)] [[pdf](https://arxiv.org/pdf/2502.01679)]
> **Authors**: Bo Pang,Tingrui Qiao,Caroline Walker,Chris Cunningham,Yun Sing Koh
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Paper accepted by ECIR 2025
- **标题**: None
- **领域**: 计算机与社会,计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) have significantly advanced natural language processing applications, yet their widespread use raises concerns regarding inherent biases that may reduce utility or harm for particular social groups. Despite the advancement in addressing LLM bias, existing research has two major limitations. First, existing LLM bias evaluation focuses on the U.S. cultural context, making it challenging to reveal stereotypical biases of LLMs toward other cultures, leading to unfair development and use of LLMs. Second, current bias evaluation often assumes models are familiar with the target social groups. When LLMs encounter words beyond their knowledge boundaries that are unfamiliar in their training data, they produce irrelevant results in the local context due to hallucinations and overconfidence, which are not necessarily indicative of inherent bias. This research addresses these limitations with a Local Integrated Bias Recognition and Assessment Framework (LIBRA) for measuring bias using datasets sourced from local corpora without crowdsourcing. Implementing this framework, we develop a dataset comprising over 360,000 test cases in the New Zealand context. Furthermore, we propose the Enhanced Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge boundary score (bbs) and a distribution divergence-based bias measurement to tackle the challenge of LLMs encountering words beyond knowledge boundaries. Our results show that the BERT family, GPT-2, and Llama-3 models seldom understand local words in different contexts. While Llama-3 exhibits larger bias, it responds better to different cultural contexts. The code and dataset are available at: https://github.com/ipangbo/LIBRA.

### Meursault as a Data Point 
[[arxiv](https://arxiv.org/abs/2502.01364)] [[cool](https://papers.cool/arxiv/2502.01364)] [[pdf](https://arxiv.org/pdf/2502.01364)]
> **Authors**: Abhinav Pratap,Amit Pathak
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 7 pages, 9 figures, 4 tables
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学,数字图书馆,机器学习
- **Abstract**: In an era dominated by datafication, the reduction of human experiences to quantifiable metrics raises profound philosophical and ethical questions. This paper explores these issues through the lens of Meursault, the protagonist of Albert Camus' The Stranger, whose emotionally detached existence epitomizes the existential concept of absurdity. Using natural language processing (NLP) techniques including emotion detection (BERT), sentiment analysis (VADER), and named entity recognition (spaCy)-this study quantifies key events and behaviors in Meursault's life. Our analysis reveals the inherent limitations of applying algorithmic models to complex human experiences, particularly those rooted in existential alienation and moral ambiguity. By examining how modern AI tools misinterpret Meursault's actions and emotions, this research underscores the broader ethical dilemmas of reducing nuanced human narratives to data points, challenging the foundational assumptions of our data-driven society. The findings presented in this paper serve as a critique of the increasing reliance on data-driven narratives and advocate for incorporating humanistic values in artificial intelligence.

### Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services? 
[[arxiv](https://arxiv.org/abs/2502.00495)] [[cool](https://papers.cool/arxiv/2502.00495)] [[pdf](https://arxiv.org/pdf/2502.00495)]
> **Authors**: Mohammad Saleh Torkestani,Robert Davis,Abdolhossein Sarrafzadeh
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 6 pages, 2 figures, 3rd International Conference onMachineLearningand Computing (ICMLC 2011): February 26-28, 2011, Singapore
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Time constraints on doctor patient interaction and restricted access to specialists under the managed care system led to increasingly referring to computers as a medical information source and a self-health-care management tool. However, research show that less than 40% of information seekers indicated that online information helped them to make a decision about their health. Searching multiple web sites that need basic computer skills, lack of interaction and no face to face interaction in most search engines and some social issues, led us to develop a specialized life-like agent that would overcome mentioned problems.

### Can AI Solve the Peer Review Crisis? A Large Scale Experiment on LLM's Performance and Biases in Evaluating Economics Papers 
[[arxiv](https://arxiv.org/abs/2502.00070)] [[cool](https://papers.cool/arxiv/2502.00070)] [[pdf](https://arxiv.org/pdf/2502.00070)]
> **Authors**: Pat Pataranutaporn,Nattavudh Powdthavee,Pattie Maes
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 72 pages
- **标题**: None
- **领域**: 计算机与社会,人工智能,普通经济学
- **Abstract**: We investigate whether artificial intelligence can address the peer review crisis in economics by analyzing 27,090 evaluations of 9,030 unique submissions using a large language model (LLM). The experiment systematically varies author characteristics (e.g., affiliation, reputation, gender) and publication quality (e.g., top-tier, mid-tier, low-tier, AI generated papers). The results indicate that LLMs effectively distinguish paper quality but exhibit biases favoring prominent institutions, male authors, and renowned economists. Additionally, LLMs struggle to differentiate high-quality AI-generated papers from genuine top-tier submissions. While LLMs offer efficiency gains, their susceptibility to bias necessitates cautious integration and hybrid peer review models to balance equity and accuracy.

### A Frugal Model for Accurate Early Student Failure Prediction 
[[arxiv](https://arxiv.org/abs/2502.00017)] [[cool](https://papers.cool/arxiv/2502.00017)] [[pdf](https://arxiv.org/pdf/2502.00017)]
> **Authors**: Gagaoua Ikram,Armelle Brun,Anne Boyer
> **First submission**: 2025-01-10
> **First announcement**: 2025-02-04
> **comment**: LICE - London International Conference on Education, London International Conference on Education, Nov 2024, London, United Kingdom
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: Predicting student success or failure is vital for timely interventions and personalized support. Early failure prediction is particularly crucial, yet limited data availability in the early stages poses challenges, one of the possible solutions is to make use of additional data from other contexts, however, this might lead to overconsumption with no guarantee of better results. To address this, we propose the Frugal Early Prediction (FEP) model, a new hybrid model that selectively incorporates additional data, promoting data frugality and efficient resource utilization. Experiments conducted on a public dataset from a VLE demonstrate FEP's effectiveness in reducing data usage, a primary goal of this research.Experiments showcase a remarkable 27% reduction in data consumption, compared to a systematic use of additional data, aligning with our commitment to data frugality and offering substantial benefits to educational institutions seeking efficient data consumption. Additionally, FEP also excels in enhancing prediction accuracy. Compared to traditional approaches, FEP achieves an average accuracy gain of 7.3%. This not only highlights the practicality and efficiency of FEP but also its superiority in performance, while respecting resource constraints, providing beneficial findings for educational institutions seeking data frugality.

### Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study 
[[arxiv](https://arxiv.org/abs/2502.00015)] [[cool](https://papers.cool/arxiv/2502.00015)] [[pdf](https://arxiv.org/pdf/2502.00015)]
> **Authors**: Yutan Huang,Chetan Arora,Wen Cheng Houng,Tanjila Kanij,Anuradha Madulgalla,John Grundy
> **First submission**: 2025-01-08
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: [Context] Generative AI technologies, particularly Large Language Models (LLMs), have transformed numerous domains by enhancing convenience and efficiency in information retrieval, content generation, and decision-making processes. However, deploying LLMs also presents diverse ethical challenges, and their mitigation strategies remain complex and domain-dependent. [Objective] This paper aims to identify and categorize the key ethical concerns associated with using LLMs, examine existing mitigation strategies, and assess the outstanding challenges in implementing these strategies across various domains. [Method] We conducted a systematic mapping study, reviewing 39 studies that discuss ethical concerns and mitigation strategies related to LLMs. We analyzed these ethical concerns using five ethical dimensions that we extracted based on various existing guidelines, frameworks, and an analysis of the mitigation strategies and implementation challenges. [Results] Our findings reveal that ethical concerns in LLMs are multi-dimensional and context-dependent. While proposed mitigation strategies address some of these concerns, significant challenges still remain. [Conclusion] Our results highlight that ethical issues often hinder the practical implementation of the mitigation strategies, particularly in high-stake areas like healthcare and public governance; existing frameworks often lack adaptability, failing to accommodate evolving societal expectations and diverse contexts.

### Behavioural Analytics: Mathematics of the Mind 
[[arxiv](https://arxiv.org/abs/2502.00013)] [[cool](https://papers.cool/arxiv/2502.00013)] [[pdf](https://arxiv.org/pdf/2502.00013)]
> **Authors**: Richard Lane,Hannah State-Davey,Claire Taylor,Wendy Holmes,Rachel Boon,Mark Round
> **First submission**: 2025-01-07
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 14 figures, presented at 7th IMA Conference on Mathematics in Defence and Security, London, UK, 7 September 2023 (conference page at https://ima.org.uk/20850/7th-ima-defence/)
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: Behavioural analytics provides insights into individual and crowd behaviour, enabling analysis of what previously happened and predictions for how people may be likely to act in the future. In defence and security, this analysis allows organisations to achieve tactical and strategic advantage through influence campaigns, a key counterpart to physical activities. Before action can be taken, online and real-world behaviour must be analysed to determine the level of threat. Huge data volumes mean that automated processes are required to attain an accurate understanding of risk. We describe the mathematical basis of technologies to analyse quotes in multiple languages. These include a Bayesian network to understand behavioural factors, state estimation algorithms for time series analysis, and machine learning algorithms for classification. We present results from studies of quotes in English, French, and Arabic, from anti-violence campaigners, politicians, extremists, and terrorists. The algorithms correctly identify extreme statements; and analysis at individual, group, and population levels detects both trends over time and sharp changes attributed to major geopolitical events. Group analysis shows that additional population characteristics can be determined, such as polarisation over particular issues and large-scale shifts in attitude. Finally, MP voting behaviour and statements from publicly-available records are analysed to determine the level of correlation between what people say and what they do.

### TOAST Framework: A Multidimensional Approach to Ethical and Sustainable AI Integration in Organizations 
[[arxiv](https://arxiv.org/abs/2502.00011)] [[cool](https://papers.cool/arxiv/2502.00011)] [[pdf](https://arxiv.org/pdf/2502.00011)]
> **Authors**: Dian Tjondronegoro
> **First submission**: 2025-01-07
> **First announcement**: 2025-02-04
> **comment**: 25 pages, 1 figure
- **标题**: None
- **领域**: 计算机与社会,人工智能,人机交互
- **Abstract**: Artificial Intelligence (AI) has emerged as a transformative technology with the potential to revolutionize various sectors, from healthcare to finance, education, and beyond. However, successfully implementing AI systems remains a complex challenge, requiring a comprehensive and methodologically sound framework. This paper contributes to this challenge by introducing the Trustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST) framework. It draws on insights from various disciplines to align technical strategy with ethical values, societal responsibilities, and innovation aspirations. The TOAST framework is a novel approach designed to guide the implementation of AI systems, focusing on reliability, accountability, technical advancement, adaptability, and socio-technical harmony. By grounding the TOAST framework in healthcare case studies, this paper provides a robust evaluation of its practicality and theoretical soundness in addressing operational, ethical, and regulatory challenges in high-stakes environments, demonstrating how adaptable AI systems can enhance institutional efficiency, mitigate risks like bias and data privacy, and offer a replicable model for other sectors requiring ethically aligned and efficient AI integration.

### Zoning in American Cities: Are Reforms Making a Difference? An AI-based Analysis 
[[arxiv](https://arxiv.org/abs/2502.00008)] [[cool](https://papers.cool/arxiv/2502.00008)] [[pdf](https://arxiv.org/pdf/2502.00008)]
> **Authors**: Arianna Salazar-Miranda,Emily Talen
> **First submission**: 2025-01-06
> **First announcement**: 2025-02-04
> **comment**: 31 pages, 6 figures, 1 table
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: Cities are at the forefront of addressing global sustainability challenges, particularly those exacerbated by climate change. Traditional zoning codes, which often segregate land uses, have been linked to increased vehicular dependence, urban sprawl, and social disconnection, undermining broader social and environmental sustainability objectives. This study investigates the adoption and impact of form-based codes (FBCs), which aim to promote sustainable, compact, and mixed-use urban forms as a solution to these issues. Using Natural Language Processing (NLP) techniques, we analyzed zoning documents from over 2000 U.S. census-designated places to identify linguistic patterns indicative of FBC principles. Our findings reveal widespread adoption of FBCs across the country, with notable variations within regions. FBCs are associated with higher floor-to-area ratios, narrower and more consistent street setbacks, and smaller plots. We also find that places with FBCs have improved walkability, shorter commutes, and a higher share of multi-family housing. Our findings highlight the utility of NLP for evaluating zoning codes and underscore the potential benefits of form-based zoning reforms for enhancing urban sustainability.

### Defending Compute Thresholds Against Legal Loopholes 
[[arxiv](https://arxiv.org/abs/2502.00003)] [[cool](https://papers.cool/arxiv/2502.00003)] [[pdf](https://arxiv.org/pdf/2502.00003)]
> **Authors**: Matteo Pistillo,Pablo Villalobos
> **First submission**: 2025-01-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: Existing legal frameworks on AI rely on training compute thresholds as a proxy to identify potentially-dangerous AI models and trigger increased regulatory attention. In the United States, Section 4.2(a) of Executive Order 14110 instructs the Secretary of Commerce to require extensive reporting from developers of AI models above a certain training compute threshold. In the European Union, Article 51 of the AI Act establishes a presumption that AI models above a certain compute threshold have high impact capabilities and hence pose systemic risk, thus subjecting their developers to several obligations including capability evaluations, reporting, and incident monitoring. In this paper, we examine some enhancement techniques that are capable of decreasing training compute usage while preserving, or even increasing, model capabilities. Since training compute thresholds rely on training compute as a metric and trigger for increased regulatory attention, these capability-enhancing and compute-saving techniques could constitute a legal loophole to existing training compute thresholds. In particular, we concentrate on four illustrative techniques (fine-tuning, model reuse, model expansion, and above compute-optimal inference compute) with the goal of furthering the conversation about their implications on training compute thresholds as a legal mechanism and advancing policy recommendations that could address the relevant legal loopholes.

## 数据库(cs.DB:Databases)

### Common Foundations for SHACL, ShEx, and PG-Schema 
[[arxiv](https://arxiv.org/abs/2502.01295)] [[cool](https://papers.cool/arxiv/2502.01295)] [[pdf](https://arxiv.org/pdf/2502.01295)]
> **Authors**: S. Ahmetaj,I. Boneva,J. Hidders,K. Hose,M. Jakubowski,J. E. Labra-Gayo,W. Martens,F. Mogavero,F. Murlak,C. Okulmus,A. Polleres,O. Savkovic,M. Simkus,D. Tomaszuk
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: To be published at WWW 2025
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Graphs have emerged as an important foundation for a variety of applications, including capturing and reasoning over factual knowledge, semantic data integration, social networks, and providing factual knowledge for machine learning algorithms. To formalise certain properties of the data and to ensure data quality, there is a need to describe the schema of such graphs. Because of the breadth of applications and availability of different data models, such as RDF and property graphs, both the Semantic Web and the database community have independently developed graph schema languages: SHACL, ShEx, and PG-Schema. Each language has its unique approach to defining constraints and validating graph data, leaving potential users in the dark about their commonalities and differences. In this paper, we provide formal, concise definitions of the core components of each of these schema languages. We employ a uniform framework to facilitate a comprehensive comparison between the languages and identify a common set of functionalities, shedding light on both overlapping and distinctive features of the three languages.

### CoddLLM: Empowering Large Language Models for Data Analytics 
[[arxiv](https://arxiv.org/abs/2502.00329)] [[cool](https://papers.cool/arxiv/2502.00329)] [[pdf](https://arxiv.org/pdf/2502.00329)]
> **Authors**: Jiani Zhang,Hengrui Zhang,Rishav Chakravarti,Yiqun Hu,Patrick Ng,Asterios Katsifodimos,Huzefa Rangwala,George Karypis,Alon Halevy
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Large Language Models (LLMs) have the potential to revolutionize data analytics by simplifying tasks such as data discovery and SQL query synthesis through natural language interactions. This work serves as a pivotal first step toward the development of foundation models explicitly designed for data analytics applications. To propel this vision forward, we unveil a new data recipe for post-training LLMs, enhancing their comprehension of data management and empowering them to tackle complex real-world analytics tasks. Specifically, our innovative approach includes a scalable synthetic data generation method that enables the creation of a broad spectrum of topics centered on data representation and manipulation. Furthermore, we introduce two new tasks that seamlessly bridge tables and text. We show that such tasks can enhance models' understanding of schema creation and the nuanced translation between natural language and tabular data. Leveraging this data recipe, we post-train a new foundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the language understanding and reasoning capabilities of LLMs in the realm of data analytics, we contribute AnalyticsMMLU, a benchmark containing thousands of multiple-choice questions on databases, data analysis, and machine learning. Our focus on data discovery, has resulted in the contribution of three comprehensive benchmarks that address both database and data lake scenarios. CoddLLM not only excels in performance but also sets a new standard, achieving the highest average accuracy across eight datasets. It outperforms GPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection and showing an average improvement of 24.9% in Text-to-SQL compared to the base model.

### Querying Databases with Function Calling 
[[arxiv](https://arxiv.org/abs/2502.00032)] [[cool](https://papers.cool/arxiv/2502.00032)] [[pdf](https://arxiv.org/pdf/2502.00032)]
> **Authors**: Connor Shorten,Charles Pierse,Thomas Benjamin Smith,Karel D'Oosterlinck,Tuana Celik,Erika Cardenas,Leonie Monigatti,Mohd Shukri Hasan,Edward Schmuhl,Daniel Williams,Aravind Kesiraju,Bob van Luijt
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-04
> **comment**: Preprint. 23 pages, 7 figures
- **标题**: None
- **领域**: 数据库,人工智能,信息检索
- **Abstract**: The capabilities of Large Language Models (LLMs) are rapidly accelerating largely thanks to their integration with external tools. Querying databases is among the most effective of these integrations, enabling LLMs to access private or continually updating data. While Function Calling is the most common method for interfacing external tools to LLMs, its application to database querying as a tool has been underexplored. We propose a tool definition for database querying that unifies accessing data with search queries, filters, or a combination both, as well as transforming results with aggregation and groupby operators. To evaluate its effectiveness, we conduct a study with 8 LLMs spanning 5 model families. We present a novel pipeline adapting the Gorilla LLM framework to create synthetic database schemas and queries. We primarily evaluate the models with the Exact Match of predicted and ground truth query APIs. Among the models tested, Claude 3.5 Sonnet achieves the highest performance with an Exact Match score of 74.3%, followed by GPT-4o mini at 73.7%, and GPT-4o at 71.8%. We further breakdown these results per API component utilized and across synthetic use cases. We find that LLMs are highly effective at utilizing operators on boolean properties, but struggle with text property filters. Across use cases we find robust results with the higher performing models such as GPT-4o, but significant performance variance across use cases from lower performing models. We additionally conduct ablation studies exploring the impact of parallel tool calling, adding a rationale as an argument of the tool call, using a separate tool per database collection, and tool calling with structured outputs. Our findings demonstrate the effectiveness of enabling LLMs to query databases with Function Calling. We have open-sourced our experimental code and results at github.com/weaviate/gorilla.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### Self-Organizing Interaction Spaces: A Framework for Engineering Pervasive Applications in Mobile and Distributed Environments 
[[arxiv](https://arxiv.org/abs/2502.01137)] [[cool](https://papers.cool/arxiv/2502.01137)] [[pdf](https://arxiv.org/pdf/2502.01137)]
> **Authors**: Shubham Malhotra
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 3 listings
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,机器学习,软件工程
- **Abstract**: The rapid adoption of pervasive and mobile computing has led to an unprecedented rate of data production and consumption by mobile applications at the network edge. These applications often require interactions such as data exchange, behavior coordination, and collaboration, which are typically mediated by cloud servers. While cloud computing has been effective for distributed systems, challenges like latency, cost, and intermittent connectivity persist. With the advent of 5G technology, features like location-awareness and device-to-device (D2D) communication enable a more distributed and adaptive architecture. This paper introduces Self-Organizing Interaction Spaces (SOIS), a novel framework for engineering pervasive applications. SOIS leverages the dynamic and heterogeneous nature of mobile nodes, allowing them to form adaptive organizational structures based on their individual and social contexts. The framework provides two key abstractions for modeling and programming pervasive applications using an organizational mindset and mechanisms for adapting dynamic organizational structures. Case examples and performance evaluations of a simulated mobile crowd-sensing application demonstrate the feasibility and benefits of SOIS. Results highlight its potential to enhance efficiency and reduce reliance on traditional cloud models, paving the way for innovative solutions in mobile and distributed environments.

### Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless Networks 
[[arxiv](https://arxiv.org/abs/2502.01129)] [[cool](https://papers.cool/arxiv/2502.01129)] [[pdf](https://arxiv.org/pdf/2502.01129)]
> **Authors**: Shubham Malhotra
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 6 pages, 8 figures
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能,新兴技术,机器学习
- **Abstract**: This report investigates the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. An environment that includes a base station, multiple antennas, and user equipment is created. Using the RLlib library, various DRL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) are then applied. These algorithms are compared based on their ability to optimize resource allocation, focusing on the impact of different learning rates and scheduling policies. The findings demonstrate that the choice of algorithm and learning rate significantly influences system performance, with DRL providing more efficient resource allocation compared to traditional methods.

### Towards Efficient Large Multimodal Model Serving 
[[arxiv](https://arxiv.org/abs/2502.00937)] [[cool](https://papers.cool/arxiv/2502.00937)] [[pdf](https://arxiv.org/pdf/2502.00937)]
> **Authors**: Haoran Qiu,Anish Biswas,Zihan Zhao,Jayashree Mohan,Alind Khare,Esha Choukse,Íñigo Goiri,Zeyu Zhang,Haiying Shen,Chetan Bansal,Ramachandran Ramjee,Rodrigo Fonseca
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,人工智能
- **Abstract**: Recent advances in generative AI have led to large multi-modal models (LMMs) capable of simultaneously processing inputs of various modalities such as text, images, video, and audio. While these models demonstrate impressive capabilities, efficiently serving them in production environments poses significant challenges due to their complex architectures and heterogeneous resource requirements. We present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, on six representative open-source models. We investigate their multi-stage inference pipelines and resource utilization patterns that lead to unique systems design implications. We also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions, diverse modal combinations, and bursty traffic patterns. Our key findings reveal that different LMM inference stages exhibit highly heterogeneous performance characteristics and resource demands, while concurrent requests across modalities lead to significant performance interference. To address these challenges, we propose a decoupled serving architecture that enables independent resource allocation and adaptive scaling for each stage. We further propose optimizations such as stage colocation to maximize throughput and resource utilization while meeting the latency objectives.

### General Coded Computing in a Probabilistic Straggler Regime 
[[arxiv](https://arxiv.org/abs/2502.00645)] [[cool](https://papers.cool/arxiv/2502.00645)] [[pdf](https://arxiv.org/pdf/2502.00645)]
> **Authors**: Parsa Moradi,Mohammad Ali Maddah-Ali
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 1 figure
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Coded computing has demonstrated promising results in addressing straggler resiliency in distributed computing systems. However, most coded computing schemes are designed for exact computation, requiring the number of responding servers to exceed a certain recovery threshold. Additionally, these schemes are tailored for highly structured functions. Recently, new coded computing schemes for general computing functions, where exact computation is replaced with approximate computation, have emerged. In these schemes, the availability of additional results corresponds to more accurate estimation of computational tasks. This flexibility introduces new questions that need to be addressed. This paper addresses the practically important scenario in the context of general coded computing, where each server may become a straggler with a probability $p$, independently from others. We theoretically analyze the approximation error of two existing general coded computing schemes: Berrut Approximate Coded Computing (BACC) and Learning Theoretic Coded Computing (LeTCC). Under the probabilistic straggler configuration, we demonstrate that the average approximation error for BACC and LeTCC converge to zero with the rate of at least $\mathcal{O}(\log^3_{\frac{1}{p}}(N)\cdot{N^{-3}})$ and $\mathcal{O}(\log^4_{\frac{1}{p}}(N)\cdot{N^{-2}})$, respectively. This is perhaps surprising, as earlier results does not indicate a convergence when the number of stragglers scales with the total number of servers $N$. However, in this case, despite the average number of stragglers being $Np$, the independence of servers in becoming stragglers allows the approximation error to converge to zero. These theoretical results are validated through experiments on various computing functions, including deep neural networks.

## 数字图书馆(cs.DL:Digital Libraries)

### Originality in scientific titles and abstracts can predict citation count 
[[arxiv](https://arxiv.org/abs/2502.01417)] [[cool](https://papers.cool/arxiv/2502.01417)] [[pdf](https://arxiv.org/pdf/2502.01417)]
> **Authors**: Jack H. Culbert,Yoed N. Kenett,Philipp Mayr
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 6 pages, 3 figures, submitted to ISSI 2025, research in progress paper
- **标题**: None
- **领域**: 数字图书馆,计算语言学
- **Abstract**: In this research-in-progress paper, we apply a computational measure correlating with originality from creativity science: Divergent Semantic Integration (DSI), to a selection of 99,557 scientific abstracts and titles selected from the Web of Science. We observe statistically significant differences in DSI between subject and field of research, and a slight rise in DSI over time. We model the base 10 logarithm of the citation count after 5 years with DSI and find a statistically significant positive correlation in all fields of research with an adjusted $R^2$ of 0.13.

### Paper Copilot: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process 
[[arxiv](https://arxiv.org/abs/2502.00874)] [[cool](https://papers.cool/arxiv/2502.00874)] [[pdf](https://arxiv.org/pdf/2502.00874)]
> **Authors**: Jing Yang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 数字图书馆,人工智能,计算机视觉和模式识别,计算机与社会
- **Abstract**: The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field.

## 图形(cs.GR:Graphics)

### Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries 
[[arxiv](https://arxiv.org/abs/2502.01517)] [[cool](https://papers.cool/arxiv/2502.01517)] [[pdf](https://arxiv.org/pdf/2502.01517)]
> **Authors**: Christos Margadji,Andi Kuswoyo,Sebastian W. Pattinson
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 图形,人工智能
- **Abstract**: The ability to accurately produce geometries with specified properties is perhaps the most important characteristic of a manufacturing process. 3D printing is marked by exceptional design freedom and complexity but is also prone to geometric and other defects that must be resolved for it to reach its full potential. Ultimately, this will require both astute design decisions and timely parameter adjustments to maintain stability that is challenging even with expert human operators. While machine learning is widely investigated in 3D printing, existing methods typically overlook spatial features that vary across prints and thus find it difficult to produce desired geometries. Here, we encode volumetric representations of printed parts into neural fields and apply a new regularization strategy, based on minimizing the partial derivative of the field's output with respect to a single, non-learnable parameter. By thus encouraging small input changes to yield only small output variations, we encourage smooth interpolation between observed volumes and hence realistic geometry predictions. This framework therefore allows the extraction of 'imagined' 3D shapes, revealing how a part would look if manufactured under previously unseen parameters. The resulting continuous field is used for data-driven optimization to maximize geometric fidelity between expected and produced geometries, reducing post-processing, material waste, and production costs. By optimizing process parameters dynamically, our approach enables advanced planning strategies, potentially allowing manufacturers to better realize complex and feature-rich designs.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Policy Design for Two-sided Platforms with Participation Dynamics 
[[arxiv](https://arxiv.org/abs/2502.01792)] [[cool](https://papers.cool/arxiv/2502.01792)] [[pdf](https://arxiv.org/pdf/2502.01792)]
> **Authors**: Haruka Kiyohara,Fan Yao,Sarah Dean
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: preprint, under review
- **标题**: None
- **领域**: 计算机科学与博弈论,信息检索,机器学习,系统与控制
- **Abstract**: In two-sided platforms (e.g., video streaming or e-commerce), viewers and providers engage in interactive dynamics, where an increased provider population results in higher viewer utility and the increase of viewer population results in higher provider utility. Despite the importance of such "population effects" on long-term platform health, recommendation policies do not generally take the participation dynamics into account. This paper thus studies the dynamics and policy design on two-sided platforms under the population effects for the first time. Our control- and game-theoretic findings warn against the use of myopic-greedy policy and shed light on the importance of provider-side considerations (i.e., effectively distributing exposure among provider groups) to improve social welfare via population growth. We also present a simple algorithm to optimize long-term objectives by considering the population effects, and demonstrate its effectiveness in synthetic and real-data experiments.

### Verbalized Bayesian Persuasion 
[[arxiv](https://arxiv.org/abs/2502.01587)] [[cool](https://papers.cool/arxiv/2502.01587)] [[pdf](https://arxiv.org/pdf/2502.01587)]
> **Authors**: Wenhao Li,Yue Lin,Xiangfeng Wang,Bo Jin,Hongyuan Zha,Baoxiang Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 63 pages, 21 figures
- **标题**: None
- **领域**: 计算机科学与博弈论,人工智能,机器学习
- **Abstract**: Information design (ID) explores how a sender influence the optimal behavior of receivers to achieve specific objectives. While ID originates from everyday human communication, existing game-theoretic and machine learning methods often model information structures as numbers, which limits many applications to toy games. This work leverages LLMs and proposes a verbalized framework in Bayesian persuasion (BP), which extends classic BP to real-world games involving human dialogues for the first time. Specifically, we map the BP to a verbalized mediator-augmented extensive-form game, where LLMs instantiate the sender and receiver. To efficiently solve the verbalized game, we propose a generalized equilibrium-finding algorithm combining LLM and game solver. The algorithm is reinforced with techniques including verbalized commitment assumptions, verbalized obedience constraints, and information obfuscation. Numerical experiments in dialogue scenarios, such as recommendation letters, courtroom interactions, and law enforcement, validate that our framework can both reproduce theoretical results in classic BP and discover effective persuasion strategies in more complex natural language and multi-stage scenarios.

### Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values 
[[arxiv](https://arxiv.org/abs/2502.00313)] [[cool](https://papers.cool/arxiv/2502.00313)] [[pdf](https://arxiv.org/pdf/2502.00313)]
> **Authors**: Hadi Hosseini,Samarth Khanna
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,人工智能,计算语言学,多代理系统
- **Abstract**: The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts.

### Fairshare Data Pricing for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00198)] [[cool](https://papers.cool/arxiv/2502.00198)] [[pdf](https://arxiv.org/pdf/2502.00198)]
> **Authors**: Luyang Zhang,Cathy Jiao,Beibei Li,Chenyan Xiong
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,计算语言学
- **Abstract**: Training data is a pivotal resource for building large language models (LLMs), but unfair pricing in data markets poses a serious challenge for both data buyers (e.g., LLM builders) and sellers (e.g., human annotators), which discourages market participation, reducing data quantity and quality. In this paper, we propose a fairshare pricing framework that sets training data prices using data valuation methods to quantify their contribution to LLMs. In our framework, buyers make purchasing decisions using data valuation and sellers set prices to maximize their profits based on the anticipated buyer purchases. We theoretically show that pricing derived from our framework is tightly linked to data valuation and buyers' budget, optimal for both buyers and sellers. Through market simulations using current LLMs and datasets (math problems, medical diagnosis, and physical reasoning), we show that our framework is fairshare for buyers by ensuring their purchased data is reflective of model training value, leading to higher LLM task performances per-dollar spent on data, and fairshare for sellers by ensuring they sell their data at optimal prices. Our framework lays the foundation for future research on equitable and sustainable data markets for large-scale AI.

## 人机交互(cs.HC:Human-Computer Interaction)

### MeetMap: Real-Time Collaborative Dialogue Mapping with LLMs in Online Meetings 
[[arxiv](https://arxiv.org/abs/2502.01564)] [[cool](https://papers.cool/arxiv/2502.01564)] [[pdf](https://arxiv.org/pdf/2502.01564)]
> **Authors**: Xinyue Chen,Nathan Yap,Xinyi Lu,Aylin Gunal,Xu Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: CSCW2025 Accepted
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Video meeting platforms display conversations linearly through transcripts or summaries. However, ideas during a meeting do not emerge linearly. We leverage LLMs to create dialogue maps in real time to help people visually structure and connect ideas. Balancing the need to reduce the cognitive load on users during the conversation while giving them sufficient control when using AI, we explore two system variants that encompass different levels of AI assistance. In Human-Map, AI generates summaries of conversations as nodes, and users create dialogue maps with the nodes. In AI-Map, AI produces dialogue maps where users can make edits. We ran a within-subject experiment with ten pairs of users, comparing the two MeetMap variants and a baseline. Users preferred MeetMap over traditional methods for taking notes, which aligned better with their mental models of conversations. Users liked the ease of use for AI-Map due to the low effort demands and appreciated the hands-on opportunity in Human-Map for sense-making.

### Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant 
[[arxiv](https://arxiv.org/abs/2502.01390)] [[cool](https://papers.cool/arxiv/2502.01390)] [[pdf](https://arxiv.org/pdf/2502.01390)]
> **Authors**: Gaole He,Gianluca Demartini,Ujwal Gadiraju
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: conditionally accepted to CHI 2025
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.

### Guidance Source Matters: How Guidance from AI, Expert, or a Group of Analysts Impacts Visual Data Preparation and Analysis 
[[arxiv](https://arxiv.org/abs/2502.00682)] [[cool](https://papers.cool/arxiv/2502.00682)] [[pdf](https://arxiv.org/pdf/2502.00682)]
> **Authors**: Arpit Narechania,Alex Endert,Atanu R Sinha
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 21 pages, 10 figures, 6 figures, to appear in proceedings of ACM IUI 2025
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: The progress in generative AI has fueled AI-powered tools like co-pilots and assistants to provision better guidance, particularly during data analysis. However, research on guidance has not yet examined the perceived efficacy of the source from which guidance is offered and the impact of this source on the user's perception and usage of guidance. We ask whether users perceive all guidance sources as equal, with particular interest in three sources: (i) AI, (ii) human expert, and (iii) a group of human analysts. As a benchmark, we consider a fourth source, (iv) unattributed guidance, where guidance is provided without attribution to any source, enabling isolation of and comparison with the effects of source-specific guidance. We design a five-condition between-subjects study, with one condition for each of the four guidance sources and an additional (v) no-guidance condition, which serves as a baseline to evaluate the influence of any kind of guidance. We situate our study in a custom data preparation and analysis tool wherein we task users to select relevant attributes from an unfamiliar dataset to inform a business report. Depending on the assigned condition, users can request guidance, which the system then provides in the form of attribute suggestions. To ensure internal validity, we control for the quality of guidance across source-conditions. Through several metrics of usage and perception, we statistically test five preregistered hypotheses and report on additional analysis. We find that the source of guidance matters to users, but not in a manner that matches received wisdom. For instance, users utilize guidance differently at various stages of analysis, including expressing varying levels of regret, despite receiving guidance of similar quality. Notably, users in the AI condition reported both higher post-task benefit and regret.

### A Study about Distribution and Acceptance of Conversational Agents for Mental Health in Germany: Keep the Human in the Loop? 
[[arxiv](https://arxiv.org/abs/2502.00005)] [[cool](https://papers.cool/arxiv/2502.00005)] [[pdf](https://arxiv.org/pdf/2502.00005)]
> **Authors**: Christina Lukas
> **First submission**: 2025-01-05
> **First announcement**: 2025-02-04
> **comment**: Master's thesis
- **标题**: None
- **领域**: 人机交互,人工智能,计算机与社会
- **Abstract**: Good mental health enables individuals to cope with the normal stresses of life. In Germany, approximately one-quarter of the adult population is affected by mental illnesses. Teletherapy and digital health applications are available to bridge gaps in care and relieve healthcare professionals. The acceptance of these tools is a strongly influencing factor for their effectiveness, which also needs to be evaluated for AI-based conversational agents (CAs) (e. g. ChatGPT, Siri) to assess the risks and potential for integration into therapeutic practice. This study investigates the perspectives of both the general population and healthcare professionals with the following questions: 1. How frequently are CAs used for mental health? 2. How high is the acceptance of CAs in the field of mental health? 3. To what extent is the use of CAs in counselling, diagnosis, and treatment acceptable? To address these questions, two quantitative online surveys were conducted with 444 participants from the general population and 351 healthcare professionals. Statistical analyses show that 27 % of the surveyed population already confide their concerns to CAs. Not only experience with this technology but also experience with telemedicine shows a higher acceptance among both groups for using CAs for mental health. Additionally, participants from the general population were more likely to support CAs as companions controlled by healthcare professionals rather than as additional experts for the professionals. CAs have the potential to support mental health, particularly in counselling. Future research should examine the influence of different communication media and further possibilities of augmented intelligence. With the right balance between technology and human care, integration into patient-professional interaction can be achieved.

## 信息检索(cs.IR:Information Retrieval)

### Query Brand Entity Linking in E-Commerce Search 
[[arxiv](https://arxiv.org/abs/2502.01555)] [[cool](https://papers.cool/arxiv/2502.01555)] [[pdf](https://arxiv.org/pdf/2502.01555)]
> **Authors**: Dong Liu,Sreyashi Nag
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: In this work, we address the brand entity linking problem for e-commerce search queries. The entity linking task is done by either i)a two-stage process consisting of entity mention detection followed by entity disambiguation or ii) an end-to-end linking approaches that directly fetch the target entity given the input text. The task presents unique challenges: queries are extremely short (averaging 2.4 words), lack natural language structure, and must handle a massive space of unique brands. We present a two-stage approach combining named-entity recognition with matching, and a novel end-to-end solution using extreme multi-class classification. We validate our solutions by both offline benchmarks and the impact of online A/B test.

### VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos 
[[arxiv](https://arxiv.org/abs/2502.01549)] [[cool](https://papers.cool/arxiv/2502.01549)] [[pdf](https://arxiv.org/pdf/2502.01549)]
> **Authors**: Xubin Ren,Lingrui Xu,Long Xia,Shuaiqiang Wang,Dawei Yin,Chao Huang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,计算机视觉和模式识别
- **Abstract**: Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in enhancing Large Language Models (LLMs) through external knowledge integration, yet its application has primarily focused on textual content, leaving the rich domain of multi-modal video knowledge predominantly unexplored. This paper introduces VideoRAG, the first retrieval-augmented generation framework specifically designed for processing and understanding extremely long-context videos. Our core innovation lies in its dual-channel architecture that seamlessly integrates (i) graph-based textual knowledge grounding for capturing cross-video semantic relationships, and (ii) multi-modal context encoding for efficiently preserving visual features. This novel design empowers VideoRAG to process unlimited-length videos by constructing precise knowledge graphs that span multiple videos while maintaining semantic dependencies through specialized multi-modal retrieval paradigms. Through comprehensive empirical evaluation on our proposed LongerVideos benchmark-comprising over 160 videos totaling 134+ hours across lecture, documentary, and entertainment categories-VideoRAG demonstrates substantial performance compared to existing RAG alternatives and long video understanding methods. The source code of VideoRAG implementation and the benchmark dataset are openly available at: https://github.com/HKUDS/VideoRAG.

### GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.01113)] [[cool](https://papers.cool/arxiv/2502.01113)] [[pdf](https://arxiv.org/pdf/2502.01113)]
> **Authors**: Linhao Luo,Zicheng Zhao,Gholamreza Haffari,Dinh Phung,Chen Gong,Shirui Pan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 信息检索,人工智能,计算语言学
- **Abstract**: Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement.

## 信息论(cs.IT:Information Theory)

### The Query/Hit Model for Sequential Hypothesis Testing 
[[arxiv](https://arxiv.org/abs/2502.00605)] [[cool](https://papers.cool/arxiv/2502.00605)] [[pdf](https://arxiv.org/pdf/2502.00605)]
> **Authors**: Mahshad Shariatnasab,Stefano Rini,Farhad Shirani,S. Sitharama Iyengar
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习,信号处理
- **Abstract**: This work introduces the Query/Hit (Q/H) learning model. The setup consists of two agents. One agent, Alice, has access to a streaming source, while the other, Bob, does not have direct access to the source. Communication occurs through sequential Q/H pairs: Bob sends a sequence of source symbols (queries), and Alice responds with the waiting time until each query appears in the source stream (hits). This model is motivated by scenarios with communication, computation, and privacy constraints that limit real-time access to the source. The error exponent for sequential hypothesis testing under the Q/H model is characterized, and a querying strategy, the Dynamic Scout-Sentinel Algorithm (DSSA), is proposed. The strategy employs a mutual information neural estimator to compute the error exponent associated with each query and to select the query with the highest efficiency. Extensive empirical evaluations on both synthetic and real-world datasets -- including mouse movement trajectories, typesetting patterns, and touch-based user interactions -- are provided to evaluate the performance of the proposed strategy in comparison with baselines, in terms of probability of error, query choice, and time-to-detection.

## 机器学习(cs.LG:Machine Learning)

### Rethinking Timesteps Samplers and Prediction Types 
[[arxiv](https://arxiv.org/abs/2502.01990)] [[cool](https://papers.cool/arxiv/2502.01990)] [[pdf](https://arxiv.org/pdf/2502.01990)]
> **Authors**: Bin Xie,Gady Agam
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Diffusion models suffer from the huge consumption of time and resources to train. For example, diffusion models need hundreds of GPUs to train for several weeks for a high-resolution generative task to meet the requirements of an extremely large number of iterations and a large batch size. Training diffusion models become a millionaire's game. With limited resources that only fit a small batch size, training a diffusion model always fails. In this paper, we investigate the key reasons behind the difficulties of training diffusion models with limited resources. Through numerous experiments and demonstrations, we identified a major factor: the significant variation in the training losses across different timesteps, which can easily disrupt the progress made in previous iterations. Moreover, different prediction types of $x_0$ exhibit varying effectiveness depending on the task and timestep. We hypothesize that using a mixed-prediction approach to identify the most accurate $x_0$ prediction type could potentially serve as a breakthrough in addressing this issue. In this paper, we outline several challenges and insights, with the hope of inspiring further research aimed at tackling the limitations of training diffusion models with constrained resources, particularly for high-resolution tasks.

### T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model 
[[arxiv](https://arxiv.org/abs/2502.01989)] [[cool](https://papers.cool/arxiv/2502.01989)] [[pdf](https://arxiv.org/pdf/2502.01989)]
> **Authors**: Tao Zhang,Jia-Shu Pan,Ruiqi Feng,Tailin Wu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 12 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a novel framework that significantly improves diffusion model's reasoning capabilities with better energy-based training and scaling up test-time computation. We first show that naïvely scaling up inference budget for diffusion models yields marginal gain. To address this, the training of T-SCEND consists of a novel linear-regression negative contrastive learning objective to improve the performance-energy consistency of the energy landscape, and a KL regularization to reduce adversarial sampling. During inference, T-SCEND integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS), which sequentially performs best-of-N random search and MCTS as denoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of T-SCEND's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our T-SCEND solves $88\%$ of Maze problems with much larger sizes of $15\times15$, while standard diffusion completely fails. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/t_scend.

### Ilargi: a GPU Compatible Factorized ML Model Training Framework 
[[arxiv](https://arxiv.org/abs/2502.01985)] [[cool](https://papers.cool/arxiv/2502.01985)] [[pdf](https://arxiv.org/pdf/2502.01985)]
> **Authors**: Wenbo Sun,Rihan Hai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: The machine learning (ML) training over disparate data sources traditionally involves materialization, which can impose substantial time and space overhead due to data movement and replication. Factorized learning, which leverages direct computation on disparate sources through linear algebra (LA) rewriting, has emerged as a viable alternative to improve computational efficiency. However, the adaptation of factorized learning to leverage the full capabilities of modern LA-friendly hardware like GPUs has been limited, often requiring manual intervention for algorithm compatibility. This paper introduces Ilargi, a novel factorized learning framework that utilizes matrix-represented data integration (DI) metadata to facilitate automatic factorization across CPU and GPU environments without the need for costly relational joins. Ilargi incorporates an ML-based cost estimator to intelligently selects between factorization and materialization based on data properties, algorithm complexity, hardware environments, and their interactions. This strategy ensures up to 8.9x speedups on GPUs and achieves over 20% acceleration in batch ML training workloads, thereby enhancing the practicability of ML training across diverse data integration scenarios and hardware platforms. To our knowledge, this work is the very first effort in GPU-compatible factorized learning.

### Generative Data Mining with Longtail-Guided Diffusion 
[[arxiv](https://arxiv.org/abs/2502.01980)] [[cool](https://papers.cool/arxiv/2502.01980)] [[pdf](https://arxiv.org/pdf/2502.01980)]
> **Authors**: David S. Hayden,Mao Ye,Timur Garipov,Gregory P. Meyer,Carl Vondrick,Zhao Chen,Yuning Chai,Eric Wolff,Siddhartha S. Srinivasa
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 20 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: It is difficult to anticipate the myriad challenges that a predictive model will encounter once deployed. Common practice entails a reactive, cyclical approach: model deployment, data mining, and retraining. We instead develop a proactive longtail discovery process by imagining additional data during training. In particular, we develop general model-based longtail signals, including a differentiable, single forward pass formulation of epistemic uncertainty that does not impact model parameters or predictive performance but can flag rare or hard inputs. We leverage these signals as guidance to generate additional training data from a latent diffusion model in a process we call Longtail Guidance (LTG). Crucially, we can perform LTG without retraining the diffusion model or the predictive model, and we do not need to expose the predictive model to intermediate diffusion states. Data generated by LTG exhibit semantically meaningful variation, yield significant generalization improvements on image classification benchmarks, and can be analyzed to proactively discover, explain, and address conceptual gaps in a predictive model.

### MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving 
[[arxiv](https://arxiv.org/abs/2502.01960)] [[cool](https://papers.cool/arxiv/2502.01960)] [[pdf](https://arxiv.org/pdf/2502.01960)]
> **Authors**: Shiju Zhao,Junhao Hu,Rongxiao Huang,Jiaqi Zheng,Guihai Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 14 pages, 11 figures, the first version
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local or remote disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated integrated reuse and recompute mechanisms within the system. The experimental results demonstrate that MPIC can achieve up to 54% reduction in response time compared to existing context caching systems, while maintaining negligible or no accuracy loss.

### Constrained belief updates explain geometric structures in transformer representations 
[[arxiv](https://arxiv.org/abs/2502.01954)] [[cool](https://papers.cool/arxiv/2502.01954)] [[pdf](https://arxiv.org/pdf/2502.01954)]
> **Authors**: Mateusz Piotrowski,Paul M. Riechers,Daniel Filan,Adam S. Shai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: What computational structures emerge in transformers trained on next-token prediction? In this work, we provide evidence that transformers implement constrained Bayesian belief updating -- a parallelized version of partial Bayesian inference shaped by architectural constraints. To do this, we integrate the model-agnostic theory of optimal prediction with mechanistic interpretability to analyze transformers trained on a tractable family of hidden Markov models that generate rich geometric patterns in neural activations. We find that attention heads carry out an algorithm with a natural interpretation in the probability simplex, and create representations with distinctive geometric structure. We show how both the algorithmic behavior and the underlying geometry of these representations can be theoretically predicted in detail -- including the attention pattern, OV-vectors, and embedding vectors -- by modifying the equations for optimal future token predictions to account for the architectural constraints of attention. Our approach provides a principled lens on how gradient descent resolves the tension between optimal prediction and architectural design.

### On the Emergence of Position Bias in Transformers 
[[arxiv](https://arxiv.org/abs/2502.01951)] [[cool](https://papers.cool/arxiv/2502.01951)] [[pdf](https://arxiv.org/pdf/2502.01951)]
> **Authors**: Xinyi Wu,Yifei Wang,Stefanie Jegelka,Ali Jadbabaie
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent studies have revealed various manifestations of position bias in transformer architectures, from the "lost-in-the-middle" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper introduces a novel graph-theoretic framework to analyze position bias in multi-layer attention. Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers -- coupled with the causal mask -- leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.

### Query-Based and Unnoticeable Graph Injection Attack from Neighborhood Perspective 
[[arxiv](https://arxiv.org/abs/2502.01936)] [[cool](https://papers.cool/arxiv/2502.01936)] [[pdf](https://arxiv.org/pdf/2502.01936)]
> **Authors**: Chang Liu,Hai Huang,Yujie Xing,Xingquan Zuo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: The robustness of Graph Neural Networks (GNNs) has become an increasingly important topic due to their expanding range of applications. Various attack methods have been proposed to explore the vulnerabilities of GNNs, ranging from Graph Modification Attacks (GMA) to the more practical and flexible Graph Injection Attacks (GIA). However, existing methods face two key challenges: (i) their reliance on surrogate models, which often leads to reduced attack effectiveness due to structural differences and prior biases, and (ii) existing GIA methods often sacrifice attack success rates in undefended settings to bypass certain defense models, thereby limiting their overall effectiveness. To overcome these limitations, we propose QUGIA, a Query-based and Unnoticeable Graph Injection Attack. QUGIA injects nodes by first selecting edges based on victim node connections and then generating node features using a Bayesian framework. This ensures that the injected nodes are similar to the original graph nodes, implicitly preserving homophily and making the attack more unnoticeable. Unlike previous methods, QUGIA does not rely on surrogate models, thereby avoiding performance degradation and achieving better generalization. Extensive experiments on six real-world datasets with diverse characteristics demonstrate that QUGIA achieves unnoticeable attacks and outperforms state-of-the-art attackers. The code will be released upon acceptance.

### Distributionally Robust Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.01930)] [[cool](https://papers.cool/arxiv/2502.01930)] [[pdf](https://arxiv.org/pdf/2502.01930)]
> **Authors**: Zaiyan Xu,Sushil Vemuri,Kishan Panaganti,Dileep Kalathil,Rahul Jain,Deepak Ramachandran
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A major challenge in aligning large language models (LLMs) with human preferences is the issue of distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they accurately represent real-world user preferences. However, user preferences vary significantly across geographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference distribution shift leads to catastrophic alignment failures in many real-world applications. We address this problem using the principled framework of distributionally robust optimization, and develop two novel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We characterize the sample complexity of learning the optimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style learning algorithms by developing suitable approximations for the challenging minimax loss functions of WDPO and KLDPO. Our empirical experiments demonstrate the superior performance of WDPO and KLDPO in substantially improving the alignment when there is a preference distribution shift.

### LAST SToP For Modeling Asynchronous Time Series 
[[arxiv](https://arxiv.org/abs/2502.01922)] [[cool](https://papers.cool/arxiv/2502.01922)] [[pdf](https://arxiv.org/pdf/2502.01922)]
> **Authors**: Shubham Gupta,Thibaut Durand,Graham Taylor,Lilian W. Białokozowicz
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present a novel prompt design for Large Language Models (LLMs) tailored to Asynchronous Time Series. Unlike regular time series, which assume values at evenly spaced time points, asynchronous time series consist of timestamped events occurring at irregular intervals, each described in natural language. Our approach effectively utilizes the rich natural language of event descriptions, allowing LLMs to benefit from their broad world knowledge for reasoning across different domains and tasks. This allows us to extend the scope of asynchronous time series analysis beyond forecasting to include tasks like anomaly detection and data imputation. We further introduce Stochastic Soft Prompting, a novel prompt-tuning mechanism that significantly improves model performance, outperforming existing fine-tuning methods such as QLoRA. Through extensive experiments on real world datasets, we demonstrate that our approach achieves state-of-the-art performance across different tasks and datasets.

### Anomaly Detection via Autoencoder Composite Features and NCE 
[[arxiv](https://arxiv.org/abs/2502.01920)] [[cool](https://papers.cool/arxiv/2502.01920)] [[pdf](https://arxiv.org/pdf/2502.01920)]
> **Authors**: Yalin Liao,Austin J. Brockmeier
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Unsupervised anomaly detection is a challenging task. Autoencoders (AEs) or generative models are often employed to model the data distribution of normal inputs and subsequently identify anomalous, out-of-distribution inputs by high reconstruction error or low likelihood, respectively. However, AEs may generalize and achieve small reconstruction errors on abnormal inputs. We propose a decoupled training approach for anomaly detection that both an AE and a likelihood model trained with noise contrastive estimation (NCE). After training the AE, NCE estimates a probability density function, to serve as the anomaly score, on the joint space of the AE's latent representation combined with features of the reconstruction quality. To further reduce the false negative rate in NCE we systematically varying the reconstruction features to augment the training and optimize the contrastive Gaussian noise distribution. Experimental assessments on multiple benchmark datasets demonstrate that the proposed approach matches the performance of prevalent state-of-the-art anomaly detection algorithms.

### Unlocking Efficient Large Inference Models: One-Bit Unrolling Tips the Scales 
[[arxiv](https://arxiv.org/abs/2502.01908)] [[cool](https://papers.cool/arxiv/2502.01908)] [[pdf](https://arxiv.org/pdf/2502.01908)]
> **Authors**: Arian Eamaz,Farhang Yeganegi,Mojtaba Soltanalian
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advancements in Large Language Model (LLM) compression, such as BitNet and BitNet b1.58, have marked significant strides in reducing the computational demands of LLMs through innovative one-bit quantization techniques. We extend this frontier by looking at Large Inference Models (LIMs) that have become indispensable across various applications. However, their scale and complexity often come at a significant computational cost. We introduce a novel approach that leverages one-bit algorithm unrolling, effectively integrating information from the physical world in the model architecture. Our method achieves a bit-per-link rate significantly lower than the 1.58 bits reported in prior work, thanks to the natural sparsity that emerges in our network architectures. We numerically demonstrate that the proposed one-bit algorithm unrolling scheme can improve both training and test outcomes by effortlessly increasing the number of layers while substantially compressing the network. Additionally, we provide theoretical results on the generalization gap, convergence rate, stability, and sensitivity of our proposed one-bit algorithm unrolling.

### Training and Evaluating with Human Label Variation: An Empirical Study 
[[arxiv](https://arxiv.org/abs/2502.01891)] [[cool](https://papers.cool/arxiv/2502.01891)] [[pdf](https://arxiv.org/pdf/2502.01891)]
> **Authors**: Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Human label variation (HLV) challenges the standard assumption that an example has a single ground truth, instead embracing the natural variation in human labelling to train and evaluate models. While various training methods and metrics for HLV have been proposed, there has been no systematic meta-evaluation of HLV evaluation metrics, contributing to the lack of clarity in the best HLV training method. We propose new evaluation metrics and training methods and empirically meta-evaluate HLV evaluation metrics. We find that training on either disaggregated annotations or soft labels often performs best across metrics, and that our proposed soft metric correlates best with human preference.

### Displacement-Sparse Neural Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.01889)] [[cool](https://papers.cool/arxiv/2502.01889)] [[pdf](https://arxiv.org/pdf/2502.01889)]
> **Authors**: Peter Chen,Yue Xie,Qingpeng Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 6 Figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Optimal Transport (OT) theory seeks to determine the map $T:X \to Y$ that transports a source measure $P$ to a target measure $Q$, minimizing the cost $c(\mathbf{x}, T(\mathbf{x}))$ between $\mathbf{x}$ and its image $T(\mathbf{x})$. Building upon the Input Convex Neural Network OT solver and incorporating the concept of displacement-sparse maps, we introduce a sparsity penalty into the minimax Wasserstein formulation, promote sparsity in displacement vectors $Δ(\mathbf{x}) := T(\mathbf{x}) - \mathbf{x}$, and enhance the interpretability of the resulting map. However, increasing sparsity often reduces feasibility, causing $T_{\#}(P)$ to deviate more significantly from the target measure. In low-dimensional settings, we propose a heuristic framework to balance the trade-off between sparsity and feasibility by dynamically adjusting the sparsity intensity parameter during training. For high-dimensional settings, we directly constrain the dimensionality of displacement vectors by enforcing $\dim(Δ(\mathbf{x})) \leq l$, where $l < d$ for $X \subseteq \mathbb{R}^d$. Among maps satisfying this constraint, we aim to identify the most feasible one. This goal can be effectively achieved by adapting our low-dimensional heuristic framework without resorting to dimensionality reduction. We validate our method on both synthesized sc-RNA and real 4i cell perturbation datasets, demonstrating improvements over existing methods.

### A Privacy-Preserving Domain Adversarial Federated learning for multi-site brain functional connectivity analysis 
[[arxiv](https://arxiv.org/abs/2502.01885)] [[cool](https://papers.cool/arxiv/2502.01885)] [[pdf](https://arxiv.org/pdf/2502.01885)]
> **Authors**: Yipu Zhang,Likai Wang,Kuan-Jui Su,Aiying Zhang,Hao Zhu,Xiaowen Liu,Hui Shen,Vince D. Calhoun,Yuping Wang,Hongwen Deng
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 34pages, 13 figures
- **标题**: None
- **领域**: 机器学习,人工智能,图像和视频处理
- **Abstract**: Resting-state functional magnetic resonance imaging (rs-fMRI) and its derived functional connectivity networks (FCNs) have become critical for understanding neurological disorders. However, collaborative analyses and the generalizability of models still face significant challenges due to privacy regulations and the non-IID (non-independent and identically distributed) property of multiple data sources. To mitigate these difficulties, we propose Domain Adversarial Federated Learning (DAFed), a novel federated deep learning framework specifically designed for non-IID fMRI data analysis in multi-site settings. DAFed addresses these challenges through feature disentanglement, decomposing the latent feature space into domain-invariant and domain-specific components, to ensure robust global learning while preserving local data specificity. Furthermore, adversarial training facilitates effective knowledge transfer between labeled and unlabeled datasets, while a contrastive learning module enhances the global representation of domain-invariant features. We evaluated DAFed on the diagnosis of ASD and further validated its generalizability in the classification of AD, demonstrating its superior classification accuracy compared to state-of-the-art methods. Additionally, an enhanced Score-CAM module identifies key brain regions and functional connectivity significantly associated with ASD and MCI, respectively, uncovering shared neurobiological patterns across sites. These findings highlight the potential of DAFed to advance multi-site collaborative research in neuroimaging while protecting data confidentiality.

### Reinforcement Learning with Segment Feedback 
[[arxiv](https://arxiv.org/abs/2502.01876)] [[cool](https://papers.cool/arxiv/2502.01876)] [[pdf](https://arxiv.org/pdf/2502.01876)]
> **Authors**: Yihan Du,Anna Winnicki,Gal Dalal,Shie Mannor,R. Srikant
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Standard reinforcement learning (RL) assumes that an agent can observe a reward for each state-action pair. However, in practical applications, it is often difficult and costly to collect a reward for each state-action pair. While there have been several works considering RL with trajectory feedback, it is unclear if trajectory feedback is inefficient for learning when trajectories are long. In this work, we consider a model named RL with segment feedback, which offers a general paradigm filling the gap between per-state-action feedback and trajectory feedback. In this model, we consider an episodic Markov decision process (MDP), where each episode is divided into $m$ segments, and the agent observes reward feedback only at the end of each segment. Under this model, we study two popular feedback settings: binary feedback and sum feedback, where the agent observes a binary outcome and a reward sum according to the underlying reward function, respectively. To investigate the impact of the number of segments $m$ on learning performance, we design efficient algorithms and establish regret upper and lower bounds for both feedback settings. Our theoretical and experimental results show that: under binary feedback, increasing the number of segments $m$ decreases the regret at an exponential rate; in contrast, surprisingly, under sum feedback, increasing $m$ does not reduce the regret significantly.

### Optimizing Online Advertising with Multi-Armed Bandits: Mitigating the Cold Start Problem under Auction Dynamics 
[[arxiv](https://arxiv.org/abs/2502.01867)] [[cool](https://papers.cool/arxiv/2502.01867)] [[pdf](https://arxiv.org/pdf/2502.01867)]
> **Authors**: Anastasiia Soboleva,Andrey Pudovikov,Roman Snetkov,Alina Babenko,Egor Samosvat,Yuriy Dorn
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Online advertising platforms often face a common challenge: the cold start problem. Insufficient behavioral data (clicks) makes accurate click-through rate (CTR) forecasting of new ads challenging. CTR for "old" items can also be significantly underestimated due to their early performance influencing their long-term behavior on the platform. The cold start problem has far-reaching implications for businesses, including missed long-term revenue opportunities. To mitigate this issue, we developed a UCB-like algorithm under multi-armed bandit (MAB) setting for positional-based model (PBM), specifically tailored to auction pay-per-click systems. Our proposed algorithm successfully combines theory and practice: we obtain theoretical upper estimates of budget regret, and conduct a series of experiments on synthetic and real-world data that confirm the applicability of the method on the real platform. In addition to increasing the platform's long-term profitability, we also propose a mechanism for maintaining short-term profits through controlled exploration and exploitation of items.

### Online Curvature-Aware Replay: Leveraging $\mathbf{2^{nd}}$ Order Information for Online Continual Learning 
[[arxiv](https://arxiv.org/abs/2502.01866)] [[cool](https://papers.cool/arxiv/2502.01866)] [[pdf](https://arxiv.org/pdf/2502.01866)]
> **Authors**: Edoardo Urettini,Antonio Carta
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Online Continual Learning (OCL) models continuously adapt to nonstationary data streams, usually without task information. These settings are complex and many traditional CL methods fail, while online methods (mainly replay-based) suffer from instabilities after the task shift. To address this issue, we formalize replay-based OCL as a second-order online joint optimization with explicit KL-divergence constraints on replay data. We propose Online Curvature-Aware Replay (OCAR) to solve the problem: a method that leverages second-order information of the loss using a K-FAC approximation of the Fisher Information Matrix (FIM) to precondition the gradient. The FIM acts as a stabilizer to prevent forgetting while also accelerating the optimization in non-interfering directions. We show how to adapt the estimation of the FIM to a continual setting stabilizing second-order optimization for non-iid data, uncovering the role of the Tikhonov regularization in the stability-plasticity tradeoff. Empirical results show that OCAR outperforms state-of-the-art methods in continual metrics achieving higher average accuracy throughout the training process in three different benchmarks.

### Enhancing Generalization via Sharpness-Aware Trajectory Matching for Dataset Condensation 
[[arxiv](https://arxiv.org/abs/2502.01865)] [[cool](https://papers.cool/arxiv/2502.01865)] [[pdf](https://arxiv.org/pdf/2502.01865)]
> **Authors**: Boyan Gao,Bo Zhao,Shreyank N Gowda,Xingrun Xing,Yibo Yang,Timothy Hospedales,David A. Clifton
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Dataset condensation aims to synthesize datasets with a few representative samples that can effectively represent the original datasets. This enables efficient training and produces models with performance close to those trained on the original sets. Most existing dataset condensation methods conduct dataset learning under the bilevel (inner- and outer-loop) based optimization. However, the preceding methods perform with limited dataset generalization due to the notoriously complicated loss landscape and expensive time-space complexity of the inner-loop unrolling of bilevel optimization. These issues deteriorate when the datasets are learned via matching the trajectories of networks trained on the real and synthetic datasets with a long horizon inner-loop. To address these issues, we introduce Sharpness-Aware Trajectory Matching (SATM), which enhances the generalization capability of learned synthetic datasets by optimising the sharpness of the loss landscape and objective simultaneously. Moreover, our approach is coupled with an efficient hypergradient approximation that is mathematically well-supported and straightforward to implement along with controllable computational overhead. Empirical evaluations of SATM demonstrate its effectiveness across various applications, including in-domain benchmarks and out-of-domain settings. Moreover, its easy-to-implement properties afford flexibility, allowing it to integrate with other advanced sharpness-aware minimizers. Our code will be released.

### Learning Hyperparameters via a Data-Emphasized Variational Objective 
[[arxiv](https://arxiv.org/abs/2502.01861)] [[cool](https://papers.cool/arxiv/2502.01861)] [[pdf](https://arxiv.org/pdf/2502.01861)]
> **Authors**: Ethan Harvey,Mikhail Petrov,Michael C. Hughes
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: arXiv admin note: text overlap with arXiv:2410.19675
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: When training large flexible models, practitioners often rely on grid search to select hyperparameters that control over-fitting. This grid search has several disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the available data for training, and requires users to specify candidate values. In this paper, we propose an alternative: directly learning regularization hyperparameters on the full training set via the evidence lower bound ("ELBo") objective from variational methods. For deep neural networks with millions of parameters, we recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior. Our proposed technique overcomes all three disadvantages of grid search. In a case study on transfer learning of image classifiers, we show how our method reduces the 88+ hour grid search of past work to under 3 hours while delivering comparable accuracy. We further demonstrate how our approach enables efficient yet accurate approximations of Gaussian processes with learnable length-scale kernels.

### How to warm-start your unfolding network 
[[arxiv](https://arxiv.org/abs/2502.01854)] [[cool](https://papers.cool/arxiv/2502.01854)] [[pdf](https://arxiv.org/pdf/2502.01854)]
> **Authors**: Vicky Kouni
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,图像和视频处理,信号处理
- **Abstract**: We present a new ensemble framework for boosting the performance of overparameterized unfolding networks solving the compressed sensing problem. We combine a state-of-the-art overparameterized unfolding network with a continuation technique, to warm-start a crucial quantity of the said network's architecture; we coin the resulting continued network C-DEC. Moreover, for training and evaluating C-DEC, we incorporate the log-cosh loss function, which enjoys both linear and quadratic behavior. Finally, we numerically assess C-DEC's performance on real-world images. Results showcase that the combination of continuation with the overparameterized unfolded architecture, trained and evaluated with the chosen loss function, yields smoother loss landscapes and improved reconstruction and generalization performance of C-DEC, consistently for all datasets.

### Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification 
[[arxiv](https://arxiv.org/abs/2502.01839)] [[cool](https://papers.cool/arxiv/2502.01839)] [[pdf](https://arxiv.org/pdf/2502.01839)]
> **Authors**: Eric Zhao,Pranjal Awasthi,Sreenivas Gollapudi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by having models self-verify each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation of sampling-based search, using only random sampling and direct self-verification, provides a practical inference method that, for example, elevates the reasoning capabilities of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves self-verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.

### Score as Action: Fine-Tuning Diffusion Generative Models by Continuous-time Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01819)] [[cool](https://papers.cool/arxiv/2502.01819)] [[pdf](https://arxiv.org/pdf/2502.01819)]
> **Authors**: Hanyang Zhao,Haoxian Chen,Ji Zhang,David D. Yao,Wenpin Tang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: arXiv admin note: text overlap with arXiv:2409.08400
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Reinforcement learning from human feedback (RLHF), which aligns a diffusion model with input prompt, has become a crucial step in building reliable generative AI models. Most works in this area use a discrete-time formulation, which is prone to induced errors, and often not applicable to models with higher-order/black-box solvers. The objective of this study is to develop a disciplined approach to fine-tune diffusion models using continuous-time RL, formulated as a stochastic control problem with a reward function that aligns the end result (terminal state) with input prompt. The key idea is to treat score matching as controls or actions, and thereby making connections to policy optimization and regularization in continuous-time RL. To carry out this idea, we lay out a new policy optimization framework for continuous-time RL, and illustrate its potential in enhancing the value networks design space via leveraging the structural property of diffusion models. We validate the advantages of our method by experiments in downstream tasks of fine-tuning large-scale Text2Image models of Stable Diffusion v1.5.

### Self-supervised Subgraph Neural Network With Deep Reinforcement Walk Exploration 
[[arxiv](https://arxiv.org/abs/2502.01809)] [[cool](https://papers.cool/arxiv/2502.01809)] [[pdf](https://arxiv.org/pdf/2502.01809)]
> **Authors**: Jianming Huang,Hiroyuki Kasai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph data, with its structurally variable nature, represents complex real-world phenomena like chemical compounds, protein structures, and social networks. Traditional Graph Neural Networks (GNNs) primarily utilize the message-passing mechanism, but their expressive power is limited and their prediction lacks explainability. To address these limitations, researchers have focused on graph substructures. Subgraph neural networks (SGNNs) and GNN explainers have emerged as potential solutions, but each has its limitations. SGNNs computes graph representations based on the bags of subgraphs to enhance the expressive power. However, they often rely on predefined algorithm-based sampling strategies, which is inefficient. GNN explainers adopt data-driven approaches to generate important subgraphs to provide explanation. Nevertheless, their explanation is difficult to be translated into practical improvements on GNNs. To overcome these issues, we propose a novel self-supervised framework that integrates SGNNs with the generation approach of GNN explainers, named the Reinforcement Walk Exploration SGNN (RWE-SGNN). Our approach features a sampling model trained in an explainer fashion, optimizing subgraphs to enhance model performance. To achieve a data-driven sampling approach, unlike traditional subgraph generation approaches, we propose a novel walk exploration process, which efficiently extracts important substructures, simplifying the embedding process and avoiding isomorphism problems. Moreover, we prove that our proposed walk exploration process has equivalent generation capability to the traditional subgraph generation process. Experimental results on various graph datasets validate the effectiveness of our proposed method, demonstrating significant improvements in performance and precision.

### Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging 
[[arxiv](https://arxiv.org/abs/2502.01804)] [[cool](https://papers.cool/arxiv/2502.01804)] [[pdf](https://arxiv.org/pdf/2502.01804)]
> **Authors**: Pierre Ablin,Angelos Katharopoulos,Skyler Seto,David Grangier
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Machine learning models are routinely trained on a mixture of different data domains. Different domain weights yield very different downstream performances. We propose the Soup-of-Experts, a novel architecture that can instantiate a model at test time for any domain weights with minimal computational cost and without re-training the model. Our architecture consists of a bank of expert parameters, which are linearly combined to instantiate one model. We learn the linear combination coefficients as a function of the input domain weights. To train this architecture, we sample random domain weights, instantiate the corresponding model, and backprop through one batch of data sampled with these domain weights. We demonstrate how our approach obtains small specialized models on several language modeling tasks quickly. Soup-of-Experts are particularly appealing when one needs to ship many different specialist models quickly under a model size constraint.

### Discovering Chunks in Neural Embeddings for Interpretability 
[[arxiv](https://arxiv.org/abs/2502.01803)] [[cool](https://papers.cool/arxiv/2502.01803)] [[pdf](https://arxiv.org/pdf/2502.01803)]
> **Authors**: Shuchen Wu,Stephan Alaniz,Eric Schulz,Zeynep Akata
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Understanding neural networks is challenging due to their high-dimensional, interacting components. Inspired by human cognition, which processes complex sensory data by chunking it into recurring entities, we propose leveraging this principle to interpret artificial neural population activities. Biological and artificial intelligence share the challenge of learning from structured, naturalistic data, and we hypothesize that the cognitive mechanism of chunking can provide insights into artificial systems. We first demonstrate this concept in recurrent neural networks (RNNs) trained on artificial sequences with imposed regularities, observing that their hidden states reflect these patterns, which can be extracted as a dictionary of chunks that influence network responses. Extending this to large language models (LLMs) like LLaMA, we identify similar recurring embedding states corresponding to concepts in the input, with perturbations to these states activating or inhibiting the associated concepts. By exploring methods to extract dictionaries of identifiable chunks across neural embeddings of varying complexity, our findings introduce a new framework for interpreting neural networks, framing their population activity as structured reflections of the data they process.

### GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments 
[[arxiv](https://arxiv.org/abs/2502.01778)] [[cool](https://papers.cool/arxiv/2502.01778)] [[pdf](https://arxiv.org/pdf/2502.01778)]
> **Authors**: Stavros Orfanoudakis,Nanda Kishor Panda,Peter Palensky,Pedro P. Vergara
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT reduces dependence on accurate simulators and tackles the sparse rewards limitations of online RL algorithms. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior DT-based approaches

### CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.01777)] [[cool](https://papers.cool/arxiv/2502.01777)] [[pdf](https://arxiv.org/pdf/2502.01777)]
> **Authors**: Martijn Bartelds,Ananjan Nandi,Moussa Koulako Bala Doumbouya,Dan Jurafsky,Tatsunori Hashimoto,Karen Livescu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,音频和语音处理
- **Abstract**: Modern deep learning models often achieve high overall performance, but consistently fail on specific subgroups. Group distributionally robust optimization (group DRO) addresses this problem by minimizing the worst-group loss, but it fails when group losses misrepresent performance differences between groups. This is common in domains like speech, where the widely used connectionist temporal classification (CTC) loss scales with input length and varies with linguistic and acoustic properties, leading to spurious differences between group losses. We present CTC-DRO, which addresses the shortcomings of the group DRO objective by smoothing the group weight update to prevent overemphasis on consistently high-loss groups, while using input length-matched batching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of multilingual automatic speech recognition (ASR) across five language sets from the ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and CTC-based baseline models, reducing the worst-language error by up to 65.9% and the average error by up to 47.7%. CTC-DRO can be applied to ASR with minimal computational costs, and offers the potential for reducing group disparities in other domains with similar challenges.

### Grokking Explained: A Statistical Phenomenon 
[[arxiv](https://arxiv.org/abs/2502.01774)] [[cool](https://papers.cool/arxiv/2502.01774)] [[pdf](https://arxiv.org/pdf/2502.01774)]
> **Authors**: Breno W. Carvalho,Artur S. d'Avila Garcez,Luís C. Lamb,Emílio Vital Brazil
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Grokking, or delayed generalization, is an intriguing learning phenomenon where test set loss decreases sharply only after a model's training set loss has converged. This challenges conventional understanding of the training dynamics in deep learning networks. In this paper, we formalize and investigate grokking, highlighting that a key factor in its emergence is a distribution shift between training and test data. We introduce two synthetic datasets specifically designed to analyze grokking. One dataset examines the impact of limited sampling, and the other investigates transfer learning's role in grokking. By inducing distribution shifts through controlled imbalanced sampling of sub-categories, we systematically reproduce the phenomenon, demonstrating that while small-sampling is strongly associated with grokking, it is not its cause. Instead, small-sampling serves as a convenient mechanism for achieving the necessary distribution shift. We also show that when classes form an equivariant map, grokking can be explained by the model's ability to learn from similar classes or sub-categories. Unlike earlier work suggesting that grokking primarily arises from high regularization and sparse data, we demonstrate that it can also occur with dense data and minimal hyper-parameter tuning. Our findings deepen the understanding of grokking and pave the way for developing better stopping criteria in future training processes.

### Hamming Attention Distillation: Binarizing Keys and Queries for Efficient Long-Context Transformers 
[[arxiv](https://arxiv.org/abs/2502.01770)] [[cool](https://papers.cool/arxiv/2502.01770)] [[pdf](https://arxiv.org/pdf/2502.01770)]
> **Authors**: Mark Horton,Tergel Molom-Ochir,Peter Liu,Bhavna Gopal,Chiyue Wei,Cong Guo,Brady Taylor,Deliang Fan,Shan X. Wang,Hai Li,Yiran Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,图像和视频处理
- **Abstract**: Pre-trained transformer models with extended context windows are notoriously expensive to run at scale, often limiting real-world deployment due to their high computational and memory requirements. In this paper, we introduce Hamming Attention Distillation (HAD), a novel framework that binarizes keys and queries in the attention mechanism to achieve significant efficiency gains. By converting keys and queries into {-1, +1} vectors and replacing dot-product operations with efficient Hamming distance computations, our method drastically reduces computational overhead. Additionally, we incorporate attention matrix sparsification to prune low-impact activations, which further reduces the cost of processing long-context sequences. \par Despite these aggressive compression strategies, our distilled approach preserves a high degree of representational power, leading to substantially improved accuracy compared to prior transformer binarization methods. We evaluate HAD on a range of tasks and models, including the GLUE benchmark, ImageNet, and QuALITY, demonstrating state-of-the-art performance among binarized Transformers while drastically reducing the computational costs of long-context inference. \par We implement HAD in custom hardware simulations, demonstrating superior performance characteristics compared to a custom hardware implementation of standard attention. HAD achieves just $\mathbf{1.78}\%$ performance losses on GLUE compared to $9.08\%$ in state-of-the-art binarization work, and $\mathbf{2.5}\%$ performance losses on ImageNet compared to $12.14\%$, all while targeting custom hardware with a $\mathbf{79}\%$ area reduction and $\mathbf{87}\%$ power reduction compared to its standard attention counterpart.

### On The Concurrence of Layer-wise Preconditioning Methods and Provable Feature Learning 
[[arxiv](https://arxiv.org/abs/2502.01763)] [[cool](https://papers.cool/arxiv/2502.01763)] [[pdf](https://arxiv.org/pdf/2502.01763)]
> **Authors**: Thomas T. Zhang,Behrad Moniri,Ansh Nagwekar,Faraz Rahman,Anton Xue,Hamed Hassani,Nikolai Matni
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Layer-wise preconditioning methods are a family of memory-efficient optimization algorithms that introduce preconditioners per axis of each layer's weight tensors. These methods have seen a recent resurgence, demonstrating impressive performance relative to entry-wise ("diagonal") preconditioning methods such as Adam(W) on a wide range of neural network optimization tasks. Complementary to their practical performance, we demonstrate that layer-wise preconditioning methods are provably necessary from a statistical perspective. To showcase this, we consider two prototypical models, linear representation learning and single-index learning, which are widely used to study how typical algorithms efficiently learn useful features to enable generalization. In these problems, we show SGD is a suboptimal feature learner when extending beyond ideal isotropic inputs $\mathbf{x} \sim \mathsf{N}(\mathbf{0}, \mathbf{I})$ and well-conditioned settings typically assumed in prior work. We demonstrate theoretically and numerically that this suboptimality is fundamental, and that layer-wise preconditioning emerges naturally as the solution. We further show that standard tools like Adam preconditioning and batch-norm only mildly mitigate these issues, supporting the unique benefits of layer-wise preconditioning.

### Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA 
[[arxiv](https://arxiv.org/abs/2502.01755)] [[cool](https://papers.cool/arxiv/2502.01755)] [[pdf](https://arxiv.org/pdf/2502.01755)]
> **Authors**: Shuangyi Chen,Yuanxin Guo,Yue Ju,Harik Dalal,Ashish Khisti
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: A preliminary version was in ICML24 workshop, arXiv:2409.02346
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs. We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We present theoretical analysis on a simplified linear model to demonstrate the importance of learning both down-projection and up-projection matrices in LoRA. We provide extensive experimental evaluations on a toy neural network on MNIST as well as large language models including RoBERTa-Large, Llama-2-7B on diverse tasks to demonstrate the advantages of RoLoRA over other methods.

### Grokking vs. Learning: Same Features, Different Encodings 
[[arxiv](https://arxiv.org/abs/2502.01739)] [[cool](https://papers.cool/arxiv/2502.01739)] [[pdf](https://arxiv.org/pdf/2502.01739)]
> **Authors**: Dmitry Manning-Coe,Jacopo Gliozzi,Alexander G. Stapleton,Edward Hirst,Giuseppe De Tomasi,Barry Bradlyn,David S. Berman
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Code available at: https://github.com/xand-stapleton/grokking_vs_learning
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,人工智能
- **Abstract**: Grokking typically achieves similar loss to ordinary, "steady", learning. We ask whether these different learning paths - grokking versus ordinary training - lead to fundamental differences in the learned models. To do so we compare the features, compressibility, and learning dynamics of models trained via each path in two tasks. We find that grokked and steadily trained models learn the same features, but there can be large differences in the efficiency with which these features are encoded. In particular, we find a novel "compressive regime" of steady training in which there emerges a linear trade-off between model loss and compressibility, and which is absent in grokking. In this regime, we can achieve compression factors 25x times the base model, and 5x times the compression achieved in grokking. We then track how model features and compressibility develop through training. We show that model development in grokking is task-dependent, and that peak compressibility is achieved immediately after the grokking plateau. Finally, novel information-geometric measures are introduced which demonstrate that models undergoing grokking follow a straight path in information space.

### Choose Your Model Size: Any Compression by a Single Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.01717)] [[cool](https://papers.cool/arxiv/2502.01717)] [[pdf](https://arxiv.org/pdf/2502.01717)]
> **Authors**: Martin Genzel,Patrick Putzky,Pengfei Zhao,Sebastian Schulze,Mattes Mollenhauer,Robert Seidel,Stefan Dietzel,Thomas Wollmann
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The adoption of Foundation Models in resource-constrained environments remains challenging due to their large size and inference costs. A promising way to overcome these limitations is post-training compression, which aims to balance reduced model size against performance degradation. This work presents Any Compression via Iterative Pruning (ACIP), a novel algorithmic approach to determine a compression-performance trade-off from a single stochastic gradient descent run. To ensure parameter efficiency, we use an SVD-reparametrization of linear layers and iteratively prune their singular values with a sparsity-inducing penalty. The resulting pruning order gives rise to a global parameter ranking that allows us to materialize models of any target size. Importantly, the compressed models exhibit strong predictive downstream performance without the need for costly fine-tuning. We evaluate ACIP on a large selection of open-weight LLMs and tasks, and demonstrate state-of-the-art results compared to existing factorisation-based compression methods. We also show that ACIP seamlessly complements common quantization-based compression techniques.

### Aspects of Artificial Intelligence: Transforming Machine Learning Systems Naturally 
[[arxiv](https://arxiv.org/abs/2502.01708)] [[cool](https://papers.cool/arxiv/2502.01708)] [[pdf](https://arxiv.org/pdf/2502.01708)]
> **Authors**: Xiuzhan Guo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,数据库,离散数学
- **Abstract**: In this paper, we study the machine learning elements which we are interested in together as a machine learning system, consisting of a collection of machine learning elements and a collection of relations between the elements. The relations we concern are algebraic operations, binary relations, and binary relations with composition that can be reasoned categorically. A machine learning system transformation between two systems is a map between the systems, which preserves the relations we concern. The system transformations given by quotient or clustering, representable functor, and Yoneda embedding are highlighted and discussed by machine learning examples. An adjunction between machine learning systems, a special machine learning system transformation loop, provides the optimal way of solving problems. Machine learning system transformations are linked and compared by their maps at 2-cell, natural transformations. New insights and structures can be obtained from universal properties and algebraic structures given by monads, which are generated from adjunctions.

### Progressive Binarization with Semi-Structured Pruning for LLMs 
[[arxiv](https://arxiv.org/abs/2502.01705)] [[cool](https://papers.cool/arxiv/2502.01705)] [[pdf](https://arxiv.org/pdf/2502.01705)]
> **Authors**: Xianglong Yan,Tianao Zhang,Zhiteng Li,Yulun Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) have achieved remarkable success in natural language processing tasks, but their high computational and memory demands pose challenges for deployment on resource-constrained devices. Binarization, as an efficient compression method that reduces model weights to just 1 bit, significantly lowers both computational and memory requirements. Despite this, the binarized LLM still contains redundancy, which can be further compressed. Semi-structured pruning provides a promising approach to achieve this, which offers a better trade-off between model performance and hardware efficiency. However, simply combining binarization with semi-structured pruning can lead to a significant performance drop. To address this issue, we propose a Progressive Binarization with Semi-Structured Pruning (PBS$^2$P) method for LLM compression. We first propose a Stepwise semi-structured Pruning with Binarization Optimization (SPBO). Our optimization strategy significantly reduces the total error caused by pruning and binarization, even below that of the no-pruning scenario. Furthermore, we design a Coarse-to-Fine Search (CFS) method to select pruning elements more effectively. Extensive experiments demonstrate that PBS$^2$P achieves superior accuracy across various LLM families and evaluation metrics, noticeably outperforming state-of-the-art (SOTA) binary PTQ methods. The code and models will be available at https://github.com/XIANGLONGYAN/PBS2P.

### QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.01703)] [[cool](https://papers.cool/arxiv/2502.01703)] [[pdf](https://arxiv.org/pdf/2502.01703)]
> **Authors**: Moses Ananta,Muhammad Farid Adilazuarda,Zayd Muhammad Kawakibi Zuhri,Ayu Purwarianti,Alham Fikri Aji
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Fine-tuning large language models (LLMs) is often constrained by the computational costs of processing massive datasets. We propose \textbf{QLESS} (Quantized Low-rank Gradient Similarity Search), which integrates gradient quantization with the LESS framework to enable memory-efficient data valuation and selection. QLESS employs a two-step compression process: first, it obtains low-dimensional gradient representations through LoRA-based random projection; then, it quantizes these gradients to low-bitwidth representations. Experiments on multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH, TyDiQA) show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Even 1-bit gradient quantization preserves data valuation quality. These findings underscore QLESS as a practical, scalable approach to identifying informative examples within strict memory constraints.

### Al-Khwarizmi: Discovering Physical Laws with Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.01702)] [[cool](https://papers.cool/arxiv/2502.01702)] [[pdf](https://arxiv.org/pdf/2502.01702)]
> **Authors**: Christopher E. Mower,Haitham Bou-Ammar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Inferring physical laws from data is a central challenge in science and engineering, including but not limited to healthcare, physical sciences, biosciences, social sciences, sustainability, climate, and robotics. Deep networks offer high-accuracy results but lack interpretability, prompting interest in models built from simple components. The Sparse Identification of Nonlinear Dynamics (SINDy) method has become the go-to approach for building such modular and interpretable models. SINDy leverages sparse regression with L1 regularization to identify key terms from a library of candidate functions. However, SINDy's choice of candidate library and optimization method requires significant technical expertise, limiting its widespread applicability. This work introduces Al-Khwarizmi, a novel agentic framework for physical law discovery from data, which integrates foundational models with SINDy. Leveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach automates physical law discovery, incorporating prior knowledge and iteratively refining candidate solutions via reflection. Al-Khwarizmi operates in two steps: it summarizes system observations-comprising textual descriptions, raw data, and plots-followed by a secondary step that generates candidate feature libraries and optimizer configurations to identify hidden physics laws correctly. Evaluating our algorithm on over 198 models, we demonstrate state-of-the-art performance compared to alternatives, reaching a 20 percent increase against the best-performing alternative.

### Learning with Differentially Private (Sliced) Wasserstein Gradients 
[[arxiv](https://arxiv.org/abs/2502.01701)] [[cool](https://papers.cool/arxiv/2502.01701)] [[pdf](https://arxiv.org/pdf/2502.01701)]
> **Authors**: David Rodríguez-Vítores,Clément Lalanne,Jean-Michel Loubes
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论
- **Abstract**: In this work, we introduce a novel framework for privately optimizing objectives that rely on Wasserstein distances between data-dependent empirical measures. Our main theoretical contribution is, based on an explicit formulation of the Wasserstein gradient in a fully discrete setting, a control on the sensitivity of this gradient to individual data points, allowing strong privacy guarantees at minimal utility cost. Building on these insights, we develop a deep learning approach that incorporates gradient and activations clipping, originally designed for DP training of problems with a finite-sum structure. We further demonstrate that privacy accounting methods extend to Wasserstein-based objectives, facilitating large-scale private training. Empirical results confirm that our framework effectively balances accuracy and privacy, offering a theoretically sound solution for privacy-preserving machine learning tasks relying on optimal transport distances such as Wasserstein distance or sliced-Wasserstein distance.

### EdgeMark: An Automation and Benchmarking System for Embedded Artificial Intelligence Tools 
[[arxiv](https://arxiv.org/abs/2502.01700)] [[cool](https://papers.cool/arxiv/2502.01700)] [[pdf](https://arxiv.org/pdf/2502.01700)]
> **Authors**: Mohammad Amin Hasanpour,Mikkel Kirkegaard,Xenofon Fafoutis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: :68T99ACM Class:I.2.1
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The integration of artificial intelligence (AI) into embedded devices, a paradigm known as embedded artificial intelligence (eAI) or tiny machine learning (TinyML), is transforming industries by enabling intelligent data processing at the edge. However, the many tools available in this domain leave researchers and developers wondering which one is best suited to their needs. This paper provides a review of existing eAI tools, highlighting their features, trade-offs, and limitations. Additionally, we introduce EdgeMark, an open-source automation system designed to streamline the workflow for deploying and benchmarking machine learning (ML) models on embedded platforms. EdgeMark simplifies model generation, optimization, conversion, and deployment while promoting modularity, reproducibility, and scalability. Experimental benchmarking results showcase the performance of widely used eAI tools, including TensorFlow Lite Micro (TFLM), Edge Impulse, Ekkono, and Renesas eAI Translator, across a wide range of models, revealing insights into their relative strengths and weaknesses. The findings provide guidance for researchers and developers in selecting the most suitable tools for specific application requirements, while EdgeMark lowers the barriers to adoption of eAI technologies.

### Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection 
[[arxiv](https://arxiv.org/abs/2502.01699)] [[cool](https://papers.cool/arxiv/2502.01699)] [[pdf](https://arxiv.org/pdf/2502.01699)]
> **Authors**: Tianlin Zhang,En Yu,Yi Shao,Shuai Li,Sujuan Hou,Jiande Sun
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别,信息检索,多媒体
- **Abstract**: Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.

### Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01693)] [[cool](https://papers.cool/arxiv/2502.01693)] [[pdf](https://arxiv.org/pdf/2502.01693)]
> **Authors**: Priodyuti Pradhan,Amit Reza
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 13 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,适应和自组织系统
- **Abstract**: In complex systems, information propagation can be defined as diffused or delocalized, weakly localized, and strongly localized. This study investigates the application of graph neural network models to learn the behavior of a linear dynamical system on networks. A graph convolution and attention-based neural network framework has been developed to identify the steady-state behavior of the linear dynamical system. We reveal that our trained model distinguishes the different states with high accuracy. Furthermore, we have evaluated model performance with real-world data. In addition, to understand the explainability of our model, we provide an analytical derivation for the forward and backward propagation of our framework.

### Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation 
[[arxiv](https://arxiv.org/abs/2502.01692)] [[cool](https://papers.cool/arxiv/2502.01692)] [[pdf](https://arxiv.org/pdf/2502.01692)]
> **Authors**: Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct

### BrainOOD: Out-of-distribution Generalizable Brain Network Analysis 
[[arxiv](https://arxiv.org/abs/2502.01688)] [[cool](https://papers.cool/arxiv/2502.01688)] [[pdf](https://arxiv.org/pdf/2502.01688)]
> **Authors**: Jiaxing Xu,Yongqiang Chen,Xia Dong,Mengcheng Lan,Tiancheng Huang,Qingtian Bian,James Cheng,Yiping Ke
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经元和认知
- **Abstract**: In neuroscience, identifying distinct patterns linked to neurological disorders, such as Alzheimer's and Autism, is critical for early diagnosis and effective intervention. Graph Neural Networks (GNNs) have shown promising in analyzing brain networks, but there are two major challenges in using GNNs: (1) distribution shifts in multi-site brain network data, leading to poor Out-of-Distribution (OOD) generalization, and (2) limited interpretability in identifying key brain regions critical to neurological disorders. Existing graph OOD methods, while effective in other domains, struggle with the unique characteristics of brain networks. To bridge these gaps, we introduce BrainOOD, a novel framework tailored for brain networks that enhances GNNs' OOD generalization and interpretability. BrainOOD framework consists of a feature selector and a structure extractor, which incorporates various auxiliary losses including an improved Graph Information Bottleneck (GIB) objective to recover causal subgraphs. By aligning structure selection across brain networks and filtering noisy features, BrainOOD offers reliable interpretations of critical brain regions. Our approach outperforms 16 existing methods and improves generalization to OOD subjects by up to 8.5%. Case studies highlight the scientific validity of the patterns extracted, which aligns with the findings in known neuroscience literature. We also propose the first OOD brain network benchmark, which provides a foundation for future research in this field. Our code is available at https://github.com/AngusMonroe/BrainOOD.

### Leveraging Joint Predictive Embedding and Bayesian Inference in Graph Self Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.01684)] [[cool](https://papers.cool/arxiv/2502.01684)] [[pdf](https://arxiv.org/pdf/2502.01684)]
> **Authors**: Srinitish Srinivasan,Omkumar CU
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络
- **Abstract**: Graph representation learning has emerged as a cornerstone for tasks like node classification and link prediction, yet prevailing self-supervised learning (SSL) methods face challenges such as computational inefficiency, reliance on contrastive objectives, and representation collapse. Existing approaches often depend on feature reconstruction, negative sampling, or complex decoders, which introduce training overhead and hinder generalization. Further, current techniques which address such limitations fail to account for the contribution of node embeddings to a certain prediction in the absence of labeled nodes. To address these limitations, we propose a novel joint embedding predictive framework for graph SSL that eliminates contrastive objectives and negative sampling while preserving semantic and structural information. Additionally, we introduce a semantic-aware objective term that incorporates pseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node discriminability by evaluating latent feature contributions. Extensive experiments demonstrate that our framework outperforms state-of-the-art graph SSL methods across benchmarks, achieving superior performance without contrastive loss or complex decoders. Key innovations include (1) a non-contrastive, view-invariant joint embedding predictive architecture, (2) Leveraging single context and multiple targets relationship between subgraphs, and (3) GMM-based pseudo-label scoring to capture semantic contributions. This work advances graph SSL by offering a computationally efficient, collapse-resistant paradigm that bridges spatial and semantic graph features for downstream tasks. The code for our paper can be found at https://github.com/Deceptrax123/JPEB-GSSL

### DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale 
[[arxiv](https://arxiv.org/abs/2502.01681)] [[cool](https://papers.cool/arxiv/2502.01681)] [[pdf](https://arxiv.org/pdf/2502.01681)]
> **Authors**: Ziyang Zheng,Shan Huang,Jianyuan Zhong,Zhengyuan Shi,Guohao Dai,Ningyi Xu,Qiang Xu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,硬件架构
- **Abstract**: Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce DeepGate4, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) an update strategy tailored for circuit graphs, which reduce memory complexity to sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse transformer with global and local structural encodings for AIGs; and (3) an inference acceleration CUDA kernel that fully exploit the unique sparsity patterns of AIGs. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5% and 31.1% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory usage by 46.8%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.

### Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01680)] [[cool](https://papers.cool/arxiv/2502.01680)] [[pdf](https://arxiv.org/pdf/2502.01680)]
> **Authors**: Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 5 figures, this paper is under review in the conference
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the interpretability of symbolic reasoning and the predictive power of neural learning. The framework utilizes data from diverse sources, including geospatial, economic, and mobility datasets, to build a comprehensive feature set. DTs are employed to extract interpretable if-then rules that capture key patterns, which are then incorporated as additional features into a NN to enhance its predictive capabilities. Experimental results show that the combined dataset, enriched with symbolic rules, consistently outperforms standalone datasets across multiple evaluation metrics, including Mean Absolute Error (MAE), \(R^2\), and Common Part of Commuters (CPC). Rules selected at finer variance thresholds (e.g., 0.0001) demonstrate superior effectiveness in capturing nuanced relationships, reducing prediction errors, and aligning with observed commuter patterns. By merging symbolic and neural learning paradigms, this Neurosymbolic approach achieves both interpretability and accuracy.

### LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection 
[[arxiv](https://arxiv.org/abs/2502.01678)] [[cool](https://papers.cool/arxiv/2502.01678)] [[pdf](https://arxiv.org/pdf/2502.01678)]
> **Authors**: Yihe Wang,Nan Huang,Nadia Mammone,Marco Cecchi,Xiang Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学,信号处理
- **Abstract**: Electroencephalogram (EEG) provides a non-invasive, highly accessible, and cost-effective solution for Alzheimer's Disease (AD) detection. However, existing methods, whether based on manual feature extraction or deep learning, face two major challenges: the lack of large-scale datasets for robust feature learning and evaluation, and poor detection performance due to inter-subject variations. To address these challenges, we curate an EEG-AD corpus containing 813 subjects, which forms the world's largest EEG-AD dataset to the best of our knowledge. Using this unique dataset, we propose LEAD, the first large foundation model for EEG-based AD detection. Our method encompasses an entire pipeline, from data selection and preprocessing to self-supervised contrastive pretraining, fine-tuning, and key setups such as subject-independent evaluation and majority voting for subject-level detection. We pre-train the model on 11 EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised pre-training design includes sample-level and subject-level contrasting to extract useful general EEG features. Fine-tuning is performed on 5 channel-aligned datasets together. The backbone encoder incorporates temporal and channel embeddings to capture features across both temporal and spatial dimensions. Our method demonstrates outstanding AD detection performance, achieving up to a 9.86% increase in F1 score at the sample-level and up to a 9.31% at the subject-level compared to state-of-the-art methods. The results of our model strongly confirm the effectiveness of contrastive pre-training and channel-aligned unified fine-tuning for addressing inter-subject variation. The source code is at https://github.com/DL4mHealth/LEAD.

### AI Scaling: From Up to Down and Out 
[[arxiv](https://arxiv.org/abs/2502.01677)] [[cool](https://papers.cool/arxiv/2502.01677)] [[pdf](https://arxiv.org/pdf/2502.01677)]
> **Authors**: Yunke Wang,Yanxi Li,Chang Xu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: AI Scaling has traditionally been synonymous with Scaling Up, which builds larger and more powerful models. However, the growing demand for efficiency, adaptability, and collaboration across diverse applications necessitates a broader perspective. This position paper presents a holistic framework for AI scaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that while Scaling Up of models faces inherent bottlenecks, the future trajectory of AI scaling lies in Scaling Down and Scaling Out. These paradigms address critical technical and societal challenges, such as reducing carbon footprint, ensuring equitable access, and enhancing cross-domain collaboration. We explore transformative applications in healthcare, smart manufacturing, and content creation, demonstrating how AI Scaling can enable breakthroughs in efficiency, personalization, and global connectivity. Additionally, we highlight key challenges, including balancing model complexity with interpretability, managing resource constraints, and fostering ethical development. By synthesizing these approaches, we propose a unified roadmap that redefines the future of AI research and application, paving the way for advancements toward Artificial General Intelligence (AGI).

### Addressing Delayed Feedback in Conversion Rate Prediction via Influence Functions 
[[arxiv](https://arxiv.org/abs/2502.01669)] [[cool](https://papers.cool/arxiv/2502.01669)] [[pdf](https://arxiv.org/pdf/2502.01669)]
> **Authors**: Chenlu Ding,Jiancan Wu,Yancheng Yuan,Junfeng Fang,Cunchun Li,Xiang Wang,Xiangnan He
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,信息检索
- **Abstract**: In the realm of online digital advertising, conversion rate (CVR) prediction plays a pivotal role in maximizing revenue under cost-per-conversion (CPA) models, where advertisers are charged only when users complete specific actions, such as making a purchase. A major challenge in CVR prediction lies in the delayed feedback problem-conversions may occur hours or even weeks after initial user interactions. This delay complicates model training, as recent data may be incomplete, leading to biases and diminished performance. Although existing methods attempt to address this issue, they often fall short in adapting to evolving user behaviors and depend on auxiliary models, which introduces computational inefficiencies and the risk of model inconsistency. In this work, we propose an Influence Function-empowered framework for Delayed Feedback Modeling (IF-DFM). IF-DFM leverages influence functions to estimate how newly acquired and delayed conversion data impact model parameters, enabling efficient parameter updates without the need for full retraining. Additionally, we present a scalable algorithm that efficiently computes parameter updates by reframing the inverse Hessian-vector product as an optimization problem, striking a balance between computational efficiency and effectiveness. Extensive experiments on benchmark datasets demonstrate that IF-DFM consistently surpasses state-of-the-art methods, significantly enhancing both prediction accuracy and model adaptability.

### Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference Ranking 
[[arxiv](https://arxiv.org/abs/2502.01667)] [[cool](https://papers.cool/arxiv/2502.01667)] [[pdf](https://arxiv.org/pdf/2502.01667)]
> **Authors**: Jie Ren,Yuhang Zhang,Dongrui Liu,Xiaopeng Zhang,Qi Tian
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference. Previous approaches typically assume a consistent preference label between final generations and noisy samples at intermediate steps, and directly apply DPO to these noisy samples for fine-tuning. However, we theoretically identify inherent issues in this assumption and its impacts on the effectiveness of preference alignment. We first demonstrate the inherent issues from two perspectives: gradient direction and preference order, and then propose a Tailored Preference Optimization (TailorPO) framework for aligning diffusion models with human preference, underpinned by some theoretical insights. Our approach directly ranks intermediate noisy samples based on their step-wise reward, and effectively resolves the gradient direction issues through a simple yet efficient design. Additionally, we incorporate the gradient guidance of diffusion models into preference alignment to further enhance the optimization effectiveness. Experimental results demonstrate that our method significantly improves the model's ability to generate aesthetically pleasing and human-preferred images.

### Employee Turnover Prediction: A Cross-component Attention Transformer with Consideration of Competitor Influence and Contagious Effect 
[[arxiv](https://arxiv.org/abs/2502.01660)] [[cool](https://papers.cool/arxiv/2502.01660)] [[pdf](https://arxiv.org/pdf/2502.01660)]
> **Authors**: Hao Liu,Yong Ge
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Employee turnover refers to an individual's termination of employment from the current organization. It is one of the most persistent challenges for firms, especially those ones in Information Technology (IT) industry that confront high turnover rates. Effective prediction of potential employee turnovers benefits multiple stakeholders such as firms and online recruiters. Prior studies have focused on either the turnover prediction within a single firm or the aggregated employee movement among firms. How to predict the individual employees' turnovers among multiple firms has gained little attention in literature, and thus remains a great research challenge. In this study, we propose a novel deep learning approach based on job embeddedness theory to predict the turnovers of individual employees across different firms. Through extensive experimental evaluations using a real-world dataset, our developed method demonstrates superior performance over several state-of-the-art benchmark methods. Additionally, we estimate the cost saving for recruiters by using our turnover prediction solution and interpret the attributions of various driving factors to employee's turnover to showcase its practical business value.

### Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques 
[[arxiv](https://arxiv.org/abs/2502.01659)] [[cool](https://papers.cool/arxiv/2502.01659)] [[pdf](https://arxiv.org/pdf/2502.01659)]
> **Authors**: Nathaniel Tomczak,Sanmukh Kuppannagari
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算,表现
- **Abstract**: Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve "true sparsity" are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).

### Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations 
[[arxiv](https://arxiv.org/abs/2502.01657)] [[cool](https://papers.cool/arxiv/2502.01657)] [[pdf](https://arxiv.org/pdf/2502.01657)]
> **Authors**: Varun Dhanraj,Chris Eliasmith
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly tasks that involve precise rule following, as often found in mathematical reasoning tasks. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, allowing for problem-solving within a neurosymbolic vector space. The results are decoded and combined with the original hidden state, boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method improves efficiency, reliability, and interpretability. Our experimental results demonstrate an average of $82.86\%$ lower cross entropy loss and $24.50$ times more problems correctly solved on a suite of mathematical reasoning problems compared to chain-of-thought prompting and supervised fine-tuning (LoRA), while at the same time not hindering the performance of the LLM on other tasks.

### A binary PSO based ensemble under-sampling model for rebalancing imbalanced training data 
[[arxiv](https://arxiv.org/abs/2502.01655)] [[cool](https://papers.cool/arxiv/2502.01655)] [[pdf](https://arxiv.org/pdf/2502.01655)]
> **Authors**: Jinyan Li,Yaoyang Wu,Simon Fong,Antonio J. Tallón-Ballesteros,Xin-she Yang,Sabah Mohammed,Feng Wu
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 22 pages, 18 figures
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Ensemble technique and under-sampling technique are both effective tools used for imbalanced dataset classification problems. In this paper, a novel ensemble method combining the advantages of both ensemble learning for biasing classifiers and a new under-sampling method is proposed. The under-sampling method is named Binary PSO instance selection; it gathers with ensemble classifiers to find the most suitable length and combination of the majority class samples to build a new dataset with minority class samples. The proposed method adopts multi-objective strategy, and contribution of this method is a notable improvement of the performances of imbalanced classification, and in the meantime guaranteeing a best integrity possible for the original dataset. We experimented the proposed method and compared its performance of processing imbalanced datasets with several other conventional basic ensemble methods. Experiment is also conducted on these imbalanced datasets using an improved version where ensemble classifiers are wrapped in the Binary PSO instance selection. According to experimental results, our proposed methods outperform single ensemble methods, state-of-the-art under-sampling methods, and also combinations of these methods with the traditional PSO instance selection algorithm.

### Predicting concentration levels of air pollutants by transfer learning and recurrent neural network 
[[arxiv](https://arxiv.org/abs/2502.01654)] [[cool](https://papers.cool/arxiv/2502.01654)] [[pdf](https://arxiv.org/pdf/2502.01654)]
> **Authors**: Iat Hang Fong,Tengyue Li,Simon Fong,Raymond K. Wong,Antonio J. Tallón-Ballesteros
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: Forecasting, environment monitoring, transferlearning, recurrentneuralnetwork, airborne particle matter
- **标题**: None
- **领域**: 机器学习,神经和进化计算,大气和海洋物理
- **Abstract**: Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy. The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks.

### Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization 
[[arxiv](https://arxiv.org/abs/2502.01652)] [[cool](https://papers.cool/arxiv/2502.01652)] [[pdf](https://arxiv.org/pdf/2502.01652)]
> **Authors**: Soham Sane
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 11 Pages, 18 Equations, 1 Table
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Hybrid Group Relative Policy Optimization (Hybrid GRPO) is a reinforcement learning framework that extends Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) by incorporating empirical multi-sample action evaluation while preserving the stability of value function-based learning. Unlike DeepSeek GRPO, which eliminates the value function in favor of purely empirical reward estimation, Hybrid GRPO introduces a structured advantage computation method that balances empirical action sampling with bootstrapped value estimation. This approach enhances sample efficiency, improves learning stability, and mitigates variance amplification observed in purely empirical methods. A detailed mathematical comparison between PPO, DeepSeek GRPO, and Hybrid GRPO is presented, highlighting key differences in advantage estimation and policy updates. Experimental validation in a controlled reinforcement learning environment demonstrates that Hybrid GRPO achieves superior convergence speed, more stable policy updates, and improved sample efficiency compared to existing methods. Several extensions to Hybrid GRPO are explored, including entropy-regularized sampling, hierarchical multi-step sub-sampling, adaptive reward normalization, and value-based action selection. Beyond reinforcement learning in simulated environments, Hybrid GRPO provides a scalable framework for bridging the gap between large language models (LLMs) and real-world agent-based decision-making. By integrating structured empirical sampling with reinforcement learning stability mechanisms, Hybrid GRPO has potential applications in autonomous robotics, financial modeling, and AI-driven control systems. These findings suggest that Hybrid GRPO serves as a robust and adaptable reinforcement learning methodology, paving the way for further advancements in policy optimization.

### Fine-tuning LLaMA 2 interference: a comparative study of language implementations for optimal efficiency 
[[arxiv](https://arxiv.org/abs/2502.01651)] [[cool](https://papers.cool/arxiv/2502.01651)] [[pdf](https://arxiv.org/pdf/2502.01651)]
> **Authors**: Sazzad Hossain,Touhidul Alam Seyam,Avijit Chowdhury,Munis Xamidov,Rajib Ghose,Abhijit Pathak
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 11 pages, conference paper. International conference on Artificial Intelligence and Future Civilization
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper presents a comparative study aimed at optimizing Llama2 inference, a critical aspect of machine learning and natural language processing (NLP). We evaluate various programming languages and frameworks, including TensorFlow, PyTorch, Python, Mojo, C++, and Java, analyzing their performance in terms of speed, memory consumption, and ease of implementation through extensive benchmarking. Strengths and limitations of each approach are highlighted, along with proposed optimization strategies for parallel processing and hardware utilization. Furthermore, we investigate the Mojo SDK, a novel framework designed for large language model (LLM) inference on Apple Silicon, benchmarking its performance against implementations in C, C++, Rust, Zig, Go, and Julia. Our experiments, conducted on an Apple M1 Max, demonstrate Mojo SDK's competitive performance, ease of use, and seamless Python compatibility, positioning it as a strong alternative for LLM inference on Apple Silicon. We also discuss broader implications for LLM deployment on resource-constrained hardware and identify potential directions for future research.

### Online Gradient Boosting Decision Tree: In-Place Updates for Efficient Adding/Deleting Data 
[[arxiv](https://arxiv.org/abs/2502.01634)] [[cool](https://papers.cool/arxiv/2502.01634)] [[pdf](https://arxiv.org/pdf/2502.01634)]
> **Authors**: Huawei Lin,Jun Woo Chung,Yingjie Lao,Weijie Zhao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 25 pages, 11 figures, 16 tables. Keywords: DecrementalLearning, IncrementalLearning,MachineUnlearning, OnlineLearning, Gradient Boosting Decision Trees, GBDTs
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,机器学习
- **Abstract**: Gradient Boosting Decision Tree (GBDT) is one of the most popular machine learning models in various applications. However, in the traditional settings, all data should be simultaneously accessed in the training procedure: it does not allow to add or delete any data instances after training. In this paper, we propose an efficient online learning framework for GBDT supporting both incremental and decremental learning. To the best of our knowledge, this is the first work that considers an in-place unified incremental and decremental learning on GBDT. To reduce the learning cost, we present a collection of optimizations for our framework, so that it can add or delete a small fraction of data on the fly. We theoretically show the relationship between the hyper-parameters of the proposed optimizations, which enables trading off accuracy and cost on incremental and decremental learning. The backdoor attack results show that our framework can successfully inject and remove backdoor in a well-trained model using incremental and decremental learning, and the empirical results on public datasets confirm the effectiveness and efficiency of our proposed online learning framework and optimizations.

### Adversarial Reasoning at Jailbreaking Time 
[[arxiv](https://arxiv.org/abs/2502.01633)] [[cool](https://papers.cool/arxiv/2502.01633)] [[pdf](https://arxiv.org/pdf/2502.01633)]
> **Authors**: Mahdi Sabbaghi,Paul Kassianik,George Pappas,Yaron Singer,Amin Karbasi,Hamed Hassani
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks. In this paper, we apply these advances to the task of model jailbreaking: eliciting harmful responses from aligned LLMs. We develop an adversarial reasoning approach to automatic jailbreaking via test-time computation that achieves SOTA attack success rates (ASR) against many aligned LLMs, even the ones that aim to trade inference-time compute for adversarial robustness. Our approach introduces a new paradigm in understanding LLM vulnerabilities, laying the foundation for the development of more robust and trustworthy AI systems.

### Harmonic Loss Trains Interpretable AI Models 
[[arxiv](https://arxiv.org/abs/2502.01628)] [[cool](https://papers.cool/arxiv/2502.01628)] [[pdf](https://arxiv.org/pdf/2502.01628)]
> **Authors**: David D. Baek,Ziming Liu,Riya Tyagi,Max Tegmark
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 7 figures; The first two authors contributed equally
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we introduce **harmonic loss** as an alternative to the standard cross-entropy loss for training neural networks and large language models (LLMs). Harmonic loss enables improved interpretability and faster convergence, owing to its scale invariance and finite convergence point by design, which can be interpreted as a class center. We first validate the performance of harmonic models across algorithmic, vision, and language datasets. Through extensive experiments, we demonstrate that models trained with harmonic loss outperform standard models by: (a) enhancing interpretability, (b) requiring less data for generalization, and (c) reducing grokking. Moreover, we compare a GPT-2 model trained with harmonic loss to the standard GPT-2, illustrating that the harmonic model develops more interpretable representations. Looking forward, we believe harmonic loss has the potential to become a valuable tool in domains with limited data availability or in high-stakes applications where interpretability and reliability are paramount, paving the way for more robust and efficient neural network models.

### A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods 
[[arxiv](https://arxiv.org/abs/2502.01618)] [[cool](https://papers.cool/arxiv/2502.01618)] [[pdf](https://arxiv.org/pdf/2502.01618)]
> **Authors**: Isha Puri,Shivchander Sudalairaj,Guangxuan Xu,Kai Xu,Akash Srivastava
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at https://probabilistic-inference-scaling.github.io.

### Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01616)] [[cool](https://papers.cool/arxiv/2502.01616)] [[pdf](https://arxiv.org/pdf/2502.01616)]
> **Authors**: Udita Ghosh,Dripta S. Raychaudhuri,Jiachen Li,Konstantinos Karydis,Amit Roy-Chowdhury
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Preference-based reinforcement learning (RL) offers a promising approach for aligning policies with human intent but is often constrained by the high cost of human feedback. In this work, we introduce PrefVLM, a framework that integrates Vision-Language Models (VLMs) with selective human feedback to significantly reduce annotation requirements while maintaining performance. Our method leverages VLMs to generate initial preference labels, which are then filtered to identify uncertain cases for targeted human annotation. Additionally, we adapt VLMs using a self-supervised inverse dynamics loss to improve alignment with evolving policies. Experiments on Meta-World manipulation tasks demonstrate that PrefVLM achieves comparable or superior success rates to state-of-the-art methods while using up to 2 x fewer human annotations. Furthermore, we show that adapted VLMs enable efficient knowledge transfer across tasks, further minimizing feedback needs. Our results highlight the potential of combining VLMs with selective human supervision to make preference-based RL more scalable and practical.

### Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges 
[[arxiv](https://arxiv.org/abs/2502.01612)] [[cool](https://papers.cool/arxiv/2502.01612)] [[pdf](https://arxiv.org/pdf/2502.01612)]
> **Authors**: Nayoung Lee,Ziyang Cai,Avi Schwarzschild,Kangwook Lee,Dimitris Papailiopoulos
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Added references
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models often struggle with length generalization and solving complex problem instances beyond their training distribution. We present a self-improvement approach where models iteratively generate and learn from their own solutions, progressively tackling harder problems while maintaining a standard transformer architecture. Across diverse tasks including arithmetic, string manipulation, and maze solving, self-improving enables models to solve problems far beyond their initial training distribution-for instance, generalizing from 10-digit to 100-digit addition without apparent saturation. We observe that in some cases filtering for correct self-generated examples leads to exponential improvements in out-of-distribution performance across training rounds. Additionally, starting from pretrained models significantly accelerates this self-improvement process for several tasks. Our results demonstrate how controlled weak-to-strong curricula can systematically teach a model logical extrapolation without any changes to the positional embeddings, or the model architecture.

### Reinforcement Learning for Long-Horizon Interactive LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.01600)] [[cool](https://papers.cool/arxiv/2502.01600)] [[pdf](https://arxiv.org/pdf/2502.01600)]
> **Authors**: Kevin Chen,Marco Cusumano-Towner,Brody Huval,Aleksei Petrenko,Jackson Hamburger,Vladlen Koltun,Philipp Krähenbühl
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive LOOP, a data- and memory-efficient variant of proximal policy optimization. LOOP uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with LOOP in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.

### Faster Adaptive Optimization via Expected Gradient Outer Product Reparameterization 
[[arxiv](https://arxiv.org/abs/2502.01594)] [[cool](https://papers.cool/arxiv/2502.01594)] [[pdf](https://arxiv.org/pdf/2502.01594)]
> **Authors**: Adela DePavia,Vasileios Charisopoulos,Rebecca Willett
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Adaptive optimization algorithms -- such as Adagrad, Adam, and their variants -- have found widespread use in machine learning, signal processing and many other settings. Several methods in this family are not rotationally equivariant, meaning that simple reparameterizations (i.e. change of basis) can drastically affect their convergence. However, their sensitivity to the choice of parameterization has not been systematically studied; it is not clear how to identify a "favorable" change of basis in which these methods perform best. In this paper we propose a reparameterization method and demonstrate both theoretically and empirically its potential to improve their convergence behavior. Our method is an orthonormal transformation based on the expected gradient outer product (EGOP) matrix, which can be approximated using either full-batch or stochastic gradient oracles. We show that for a broad class of functions, the sensitivity of adaptive algorithms to choice-of-basis is influenced by the decay of the EGOP matrix spectrum. We illustrate the potential impact of EGOP reparameterization by presenting empirical evidence and theoretical arguments that common machine learning tasks with "natural" data exhibit EGOP spectral decay.

### Improving Transformer World Models for Data-Efficient RL 
[[arxiv](https://arxiv.org/abs/2502.01591)] [[cool](https://papers.cool/arxiv/2502.01591)] [[pdf](https://arxiv.org/pdf/2502.01591)]
> **Authors**: Antoine Dedieu,Joseph Ortiz,Xinghua Lou,Carter Wendelken,Wolfgang Lehrach,J Swaroop Guntupalli,Miguel Lazaro-Gredilla,Kevin Patrick Murphy
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.

### A Differentiable Alignment Framework for Sequence-to-Sequence Modeling via Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.01588)] [[cool](https://papers.cool/arxiv/2502.01588)] [[pdf](https://arxiv.org/pdf/2502.01588)]
> **Authors**: Yacouba Kaloga,Shashi Kumar,Petr Motlicek,Ina Kodrasi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音,音频和语音处理,机器学习
- **Abstract**: Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies. In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner. We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties. Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance, though with a trade-off in ASR performance when compared to CTC. We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community.

### SubTrack your Grad: Gradient Subspace Tracking for Memory and Time Efficient Full-Parameter LLM Training 
[[arxiv](https://arxiv.org/abs/2502.01586)] [[cool](https://papers.cool/arxiv/2502.01586)] [[pdf](https://arxiv.org/pdf/2502.01586)]
> **Authors**: Sahar Rajabi,Nayeema Nonta,Sirisha Rambhatla
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Training Large Language Models (LLMs) demand significant time and computational resources due to their large model sizes and optimizer states. To overcome these challenges, recent methods, such as BAdam, employ partial weight updates to enhance time and memory efficiency, though sometimes at the cost of performance. Others, like GaLore, focus on maintaining performance while optimizing memory usage through full parameter training, but may incur higher time complexity. By leveraging the low-rank structure of the gradient and the Grassmannian geometry, we propose SubTrack-Grad, a subspace tracking-based optimization method that efficiently tracks the evolving gradient subspace by incorporating estimation errors and previously identified subspaces. SubTrack-Grad delivers better or on-par results compared to GaLore, while significantly outperforming BAdam, which, despite being time-efficient, compromises performance. SubTrack-Grad reduces wall-time by up to 20.57% on GLUE tasks (15% average reduction) and up to 65% on SuperGLUE tasks (22% average reduction) compared to GaLore. Notably, for a 3B parameter model, GaLore incurred a substantial 157% increase in wall-time compared to full-rank training, whereas SubTrack-Grad exhibited a 31% increase, representing a 49% reduction in wall-time, while enjoying the same memory reductions as GaLore.

### Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization 
[[arxiv](https://arxiv.org/abs/2502.01562)] [[cool](https://papers.cool/arxiv/2502.01562)] [[pdf](https://arxiv.org/pdf/2502.01562)]
> **Authors**: Minttu Alakuijala,Ya Gao,Georgy Ananov,Samuel Kaski,Pekka Marttinen,Alexander Ilin,Harri Valpola
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: As the general capabilities of artificial intelligence (AI) agents continue to evolve, their ability to learn to master multiple complex tasks through experience remains a key challenge. Current LLM agents, particularly those based on proprietary language models, typically rely on prompts to incorporate knowledge about the target tasks. This approach does not allow the agent to internalize this information and instead relies on ever-expanding prompts to sustain its functionality in diverse scenarios. This resembles a system of notes used by a person affected by anterograde amnesia, the inability to form new memories. In this paper, we propose a novel method to train AI agents to incorporate knowledge and skills for multiple tasks without the need for either cumbersome note systems or prior high-quality demonstration data. Our approach employs an iterative process where the agent collects new experiences, receives corrective feedback from humans in the form of hints, and integrates this feedback into its weights via a context distillation training procedure. We demonstrate the efficacy of our approach by implementing it in a Llama-3-based agent which, after only a few rounds of feedback, outperforms advanced models GPT-4o and DeepSeek-V3 in a taskset requiring correct sequencing of information retrieval, tool use, and question answering.

### Search-Based Adversarial Estimates for Improving Sample Efficiency in Off-Policy Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01558)] [[cool](https://papers.cool/arxiv/2502.01558)] [[pdf](https://arxiv.org/pdf/2502.01558)]
> **Authors**: Federico Malato,Ville Hautamaki
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Submitted to International Conference onMachineLearning2025. Currently under peer-review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Sample inefficiency is a long-lasting challenge in deep reinforcement learning (DRL). Despite dramatic improvements have been made, the problem is far from being solved and is especially challenging in environments with sparse or delayed rewards. In our work, we propose to use Adversarial Estimates as a new, simple and efficient approach to mitigate this problem for a class of feedback-based DRL algorithms. Our approach leverages latent similarity search from a small set of human-collected trajectories to boost learning, using only five minutes of human-recorded experience. The results of our study show algorithms trained with Adversarial Estimates converge faster than their original version. Moreover, we discuss how our approach could enable learning in feedback-based algorithms in extreme scenarios with very sparse rewards.

### Training in reverse: How iteration order influences convergence and stability in deep learning 
[[arxiv](https://arxiv.org/abs/2502.01557)] [[cool](https://papers.cool/arxiv/2502.01557)] [[pdf](https://arxiv.org/pdf/2502.01557)]
> **Authors**: Benoit Dherin,Benny Avelin,Anders Karlsson,Hanna Mazzawi,Javier Gonzalvo,Michael Munn
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,动力系统,机器学习
- **Abstract**: Despite exceptional achievements, training neural networks remains computationally expensive and is often plagued by instabilities that can degrade convergence. While learning rate schedules can help mitigate these issues, finding optimal schedules is time-consuming and resource-intensive. This work explores theoretical issues concerning training stability in the constant-learning-rate (i.e., without schedule) and small-batch-size regime. Surprisingly, we show that the order of gradient updates affects stability and convergence in gradient-based optimizers. We illustrate this new line of thinking using backward-SGD, which processes batch gradient updates like SGD but in reverse order. Our theoretical analysis shows that in contractive regions (e.g., around minima) backward-SGD converges to a point while the standard forward-SGD generally only converges to a distribution. This leads to improved stability and convergence which we demonstrate experimentally. While full backward-SGD is computationally intensive in practice, it highlights opportunities to exploit reverse training dynamics (or more generally alternate iteration orders) to improve training. To our knowledge, this represents a new and unexplored avenue in deep learning optimization.

### Observation Noise and Initialization in Wide Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01556)] [[cool](https://papers.cool/arxiv/2502.01556)] [[pdf](https://arxiv.org/pdf/2502.01556)]
> **Authors**: Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Alvaro Cartea,Jose Miguel Hernandez-Lobato,Konstantina Palla,Kamil Ciosek
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Work under review, 22 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Performing gradient descent in a wide neural network is equivalent to computing the posterior mean of a Gaussian Process with the Neural Tangent Kernel (NTK-GP), for a specific choice of prior mean and with zero observation noise. However, existing formulations of this result have two limitations: i) the resultant NTK-GP assumes no noise in the observed target variables, which can result in suboptimal predictions with noisy data; ii) it is unclear how to extend the equivalence to an arbitrary prior mean, a crucial aspect of formulating a well-specified model. To address the first limitation, we introduce a regularizer into the neural network's training objective, formally showing its correspondence to incorporating observation noise into the NTK-GP model. To address the second, we introduce a \textit{shifted network} that enables arbitrary prior mean functions. This approach allows us to perform gradient descent on a single neural network, without expensive ensembling or kernel matrix inversion. Our theoretical insights are validated empirically, with experiments exploring different values of observation noise and network architectures.

### Unsupervised anomaly detection in large-scale estuarine acoustic telemetry data 
[[arxiv](https://arxiv.org/abs/2502.01543)] [[cool](https://papers.cool/arxiv/2502.01543)] [[pdf](https://arxiv.org/pdf/2502.01543)]
> **Authors**: Siphendulwe Zaza,Marcellin Atemkeng,Taryn S. Murray,John David Filmalter,Paul D. Cowley
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Acoustic telemetry data plays a vital role in understanding the behaviour and movement of aquatic animals. However, these datasets, which often consist of millions of individual data points, frequently contain anomalous movements that pose significant challenges. Traditionally, anomalous movements are identified either manually or through basic statistical methods, approaches that are time-consuming and prone to high rates of unidentified anomalies in large datasets. This study focuses on the development of automated classifiers for a large telemetry dataset comprising detections from fifty acoustically tagged dusky kob monitored in the Breede Estuary, South Africa. Using an array of 16 acoustic receivers deployed throughout the estuary between 2016 and 2021, we collected over three million individual data points. We present detailed guidelines for data pre-processing, resampling strategies, labelling process, feature engineering, data splitting methodologies, and the selection and interpretation of machine learning and deep learning models for anomaly detection. Among the evaluated models, neural networks autoencoder (NN-AE) demonstrated superior performance, aided by our proposed threshold-finding algorithm. NN-AE achieved a high recall with no false normal (i.e., no misclassifications of anomalous movements as normal patterns), a critical factor in ensuring that no true anomalies are overlooked. In contrast, other models exhibited false normal fractions exceeding 0.9, indicating they failed to detect the majority of true anomalies; a significant limitation for telemetry studies where undetected anomalies can distort interpretations of movement patterns. While the NN-AE's performance highlights its reliability and robustness in detecting anomalies, it faced challenges in accurately learning normal movement patterns when these patterns gradually deviated from anomalous ones.

### FedGES: A Federated Learning Approach for BN Structure Learning 
[[arxiv](https://arxiv.org/abs/2502.01538)] [[cool](https://papers.cool/arxiv/2502.01538)] [[pdf](https://arxiv.org/pdf/2502.01538)]
> **Authors**: Pablo Torrijos,José A. Gámez,José M. Puerta
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ef:Discovery Science. DS 2024. Lecture Notes in Computer Science(), vol 15244. Springer, Cham
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Bayesian Network (BN) structure learning traditionally centralizes data, raising privacy concerns when data is distributed across multiple entities. This research introduces Federated GES (FedGES), a novel Federated Learning approach tailored for BN structure learning in decentralized settings using the Greedy Equivalence Search (GES) algorithm. FedGES uniquely addresses privacy and security challenges by exchanging only evolving network structures, not parameters or data. It realizes collaborative model development, using structural fusion to combine the limited models generated by each client in successive iterations. A controlled structural fusion is also proposed to enhance client consensus when adding any edge. Experimental results on various BNs from {\sf bnlearn}'s BN Repository validate the effectiveness of FedGES, particularly in high-dimensional (a large number of variables) and sparse data scenarios, offering a practical and privacy-preserving solution for real-world BN structure learning.

### Preference Leakage: A Contamination Problem in LLM-as-a-judge 
[[arxiv](https://arxiv.org/abs/2502.01534)] [[cool](https://papers.cool/arxiv/2502.01534)] [[pdf](https://arxiv.org/pdf/2502.01534)]
> **Authors**: Dawei Li,Renliang Sun,Yue Huang,Ming Zhong,Bohan Jiang,Jiawei Han,Xiangliang Zhang,Wei Wang,Huan Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.

### Transformers trained on proteins can learn to attend to Euclidean distance 
[[arxiv](https://arxiv.org/abs/2502.01533)] [[cool](https://papers.cool/arxiv/2502.01533)] [[pdf](https://arxiv.org/pdf/2502.01533)]
> **Authors**: Isaac Ellmen,Constantin Schneider,Matthew I. J. Raybould,Charlotte M. Deane
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,生物分子
- **Abstract**: While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models.

### Federated Learning with Discriminative Naive Bayes Classifier 
[[arxiv](https://arxiv.org/abs/2502.01532)] [[cool](https://papers.cool/arxiv/2502.01532)] [[pdf](https://arxiv.org/pdf/2502.01532)]
> **Authors**: Pablo Torrijos,Juan C. Alfaro,José A. Gámez,José M. Puerta
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ef:Intelligent Data Engineering and AutomatedLearning. IDEAL 2024. Lecture Notes in Computer Science, vol 15347. Springer, Cham
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated Learning has emerged as a promising approach to train machine learning models on decentralized data sources while preserving data privacy. This paper proposes a new federated approach for Naive Bayes (NB) classification, assuming discrete variables. Our approach federates a discriminative variant of NB, sharing meaningless parameters instead of conditional probability tables. Therefore, this process is more reliable against possible attacks. We conduct extensive experiments on 12 datasets to validate the efficacy of our approach, comparing federated and non-federated settings. Additionally, we benchmark our method against the generative variant of NB, which serves as a baseline for comparison. Our experimental results demonstrate the effectiveness of our method in achieving accurate classification.

### Enhancing Bayesian Network Structural Learning with Monte Carlo Tree Search 
[[arxiv](https://arxiv.org/abs/2502.01527)] [[cool](https://papers.cool/arxiv/2502.01527)] [[pdf](https://arxiv.org/pdf/2502.01527)]
> **Authors**: Jorge D. Laborda,Pablo Torrijos,José M. Puerta,José A. Gámez
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ef:Information Processing and Management of Uncertainty in Knowledge-Based Systems. IPMU 2024. Lecture Notes in Networks and Systems, vol 1174. Springer, Cham
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This article presents MCTS-BN, an adaptation of the Monte Carlo Tree Search (MCTS) algorithm for the structural learning of Bayesian Networks (BNs). Initially designed for game tree exploration, MCTS has been repurposed to address the challenge of learning BN structures by exploring the search space of potential ancestral orders in Bayesian Networks. Then, it employs Hill Climbing (HC) to derive a Bayesian Network structure from each order. In large BNs, where the search space for variable orders becomes vast, using completely random orders during the rollout phase is often unreliable and impractical. We adopt a semi-randomized approach to address this challenge by incorporating variable orders obtained from other heuristic search algorithms such as Greedy Equivalent Search (GES), PC, or HC itself. This hybrid strategy mitigates the computational burden and enhances the reliability of the rollout process. Experimental evaluations demonstrate the effectiveness of MCTS-BN in improving BNs generated by traditional structural learning algorithms, exhibiting robust performance even when base algorithm orders are suboptimal and surpassing the gold standard when provided with favorable orders.

### Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01521)] [[cool](https://papers.cool/arxiv/2502.01521)] [[pdf](https://arxiv.org/pdf/2502.01521)]
> **Authors**: Kaixi Bao,Chenhao Li,Yarden As,Andreas Krause,Marco Hutter
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: In reinforcement learning (RL), agents often struggle to perform well on tasks that differ from those encountered during training. This limitation presents a challenge to the broader deployment of RL in diverse and dynamic task settings. In this work, we introduce memory augmentation, a memory-based RL approach to improve task generalization. Our approach leverages task-structured augmentations to simulate plausible out-of-distribution scenarios and incorporates memory mechanisms to enable context-aware policy adaptation. Trained on a predefined set of tasks, our policy demonstrates the ability to generalize to unseen tasks through memory augmentation without requiring additional interactions with the environment. Through extensive simulation experiments and real-world hardware evaluations on legged locomotion tasks, we demonstrate that our approach achieves zero-shot generalization to unseen tasks while maintaining robust in-distribution performance and high sample efficiency.

### Compact Yet Highly Accurate Printed Classifiers Using Sequential Support Vector Machine Circuits 
[[arxiv](https://arxiv.org/abs/2502.01498)] [[cool](https://papers.cool/arxiv/2502.01498)] [[pdf](https://arxiv.org/pdf/2502.01498)]
> **Authors**: Ilias Sertaridis,Spyridon Besias,Florentia Afentaki,Konstantinos Balaskas,Georgios Zervakis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at the 2025 IEEE International Symposium on Circuits and Systems (ISCAS), May 25-28 2025, London, UK
- **标题**: None
- **领域**: 机器学习,硬件架构
- **Abstract**: Printed Electronics (PE) technology has emerged as a promising alternative to silicon-based computing. It offers attractive properties such as on-demand ultra-low-cost fabrication, mechanical flexibility, and conformality. However, PE are governed by large feature sizes, prohibiting the realization of complex printed Machine Learning (ML) classifiers. Leveraging PE's ultra-low non-recurring engineering and fabrication costs, designers can fully customize hardware to a specific ML model and dataset, significantly reducing circuit complexity. Despite significant advancements, state-of-the-art solutions achieve area efficiency at the expense of considerable accuracy loss. Our work mitigates this by designing area- and power-efficient printed ML classifiers with little to no accuracy degradation. Specifically, we introduce the first sequential Support Vector Machine (SVM) classifiers, exploiting the hardware efficiency of bespoke control and storage units and a single Multiply-Accumulate compute engine. Our SVMs yield on average 6x lower area and 4.6% higher accuracy compared to the printed state of the art.

### Explaining Context Length Scaling and Bounds for Language Models 
[[arxiv](https://arxiv.org/abs/2502.01481)] [[cool](https://papers.cool/arxiv/2502.01481)] [[pdf](https://arxiv.org/pdf/2502.01481)]
> **Authors**: Jingzhe Shi,Qinwei Ma,Hongyi Liu,Hang Zhao,Jeng-Neng Hwang,Serge Belongie,Lei Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 14 figures
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Long Context Language Models have drawn great attention in the past few years. There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impact Language Modeling. In this work, we (1) propose a clean and effective theoretical framework on explaining the impact of context length to Language Modeling, from an Intrinsic Space perspective; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain case. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models. Code for our experiments is available at this url: https://github.com/JingzheShi/NLPCtlScalingAndBounds.

### Position: Empowering Time Series Reasoning with Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.01477)] [[cool](https://papers.cool/arxiv/2502.01477)] [[pdf](https://arxiv.org/pdf/2502.01477)]
> **Authors**: Yaxuan Kong,Yiyuan Yang,Shiyu Wang,Chenghao Liu,Yuxuan Liang,Ming Jin,Stefan Zohren,Dan Pei,Yan Liu,Qingsong Wen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Understanding time series data is crucial for multiple real-world applications. While large language models (LLMs) show promise in time series tasks, current approaches often rely on numerical data alone, overlooking the multimodal nature of time-dependent information, such as textual descriptions, visual data, and audio signals. Moreover, these methods underutilize LLMs' reasoning capabilities, limiting the analysis to surface-level interpretations instead of deeper temporal and multimodal reasoning. In this position paper, we argue that multimodal LLMs (MLLMs) can enable more powerful and flexible reasoning for time series analysis, enhancing decision-making and real-world applications. We call on researchers and practitioners to leverage this potential by developing strategies that prioritize trust, interpretability, and robust reasoning in MLLMs. Lastly, we highlight key research directions, including novel reasoning paradigms, architectural innovations, and domain-specific applications, to advance time series reasoning with MLLMs.

### Neuro-Symbolic AI for Analytical Solutions of Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.01476)] [[cool](https://papers.cool/arxiv/2502.01476)] [[pdf](https://arxiv.org/pdf/2502.01476)]
> **Authors**: Orestis Oikonomou,Levi Lingsch,Dana Grund,Siddhartha Mishra,Georgios Kissas
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Analytical solutions of differential equations offer exact insights into fundamental behaviors of physical processes. Their application, however, is limited as finding these solutions is difficult. To overcome this limitation, we combine two key insights. First, constructing an analytical solution requires a composition of foundational solution components. Second, iterative solvers define parameterized function spaces with constraint-based updates. Our approach merges compositional differential equation solution techniques with iterative refinement by using formal grammars, building a rich space of candidate solutions that are embedded into a low-dimensional (continuous) latent manifold for probabilistic exploration. This integration unifies numerical and symbolic differential equation solvers via a neuro-symbolic AI framework to find analytical solutions of a wide variety of differential equations. By systematically constructing candidate expressions and applying constraint-based refinement, we overcome longstanding barriers to extract such closed-form solutions. We illustrate advantages over commercial solvers, symbolic methods, and approximate neural networks on a diverse set of problems, demonstrating both generality and accuracy.

### Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention 
[[arxiv](https://arxiv.org/abs/2502.01473)] [[cool](https://papers.cool/arxiv/2502.01473)] [[pdf](https://arxiv.org/pdf/2502.01473)]
> **Authors**: Arya Honarpisheh,Mustafa Bozdag,Mario Sznaier,Octavia Camps
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: State-space models (SSMs) are a new class of foundation models that have emerged as a compelling alternative to Transformers and their attention mechanisms for sequence processing tasks. This paper provides a detailed theoretical analysis of selective SSMs, the core components of the Mamba and Mamba-2 architectures. We leverage the connection between selective SSMs and the self-attention mechanism to highlight the fundamental similarities between these models. Building on this connection, we establish a length independent covering number-based generalization bound for selective SSMs, providing a deeper understanding of their theoretical performance guarantees. We analyze the effects of state matrix stability and input-dependent discretization, shedding light on the critical role played by these factors in the generalization capabilities of selective SSMs. Finally, we empirically demonstrate the sequence length independence of the derived bounds on two tasks.

### Docking-Aware Attention: Dynamic Protein Representations through Molecular Context Integration 
[[arxiv](https://arxiv.org/abs/2502.01461)] [[cool](https://papers.cool/arxiv/2502.01461)] [[pdf](https://arxiv.org/pdf/2502.01461)]
> **Authors**: Amitay Sicherman,Kira Radinsky
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,生物分子
- **Abstract**: Computational prediction of enzymatic reactions represents a crucial challenge in sustainable chemical synthesis across various scientific domains, ranging from drug discovery to materials science and green chemistry. These syntheses rely on proteins that selectively catalyze complex molecular transformations. These protein catalysts exhibit remarkable substrate adaptability, with the same protein often catalyzing different chemical transformations depending on its molecular partners. Current approaches to protein representation in reaction prediction either ignore protein structure entirely or rely on static embeddings, failing to capture how proteins dynamically adapt their behavior to different substrates. We present Docking-Aware Attention (DAA), a novel architecture that generates dynamic, context-dependent protein representations by incorporating molecular docking information into the attention mechanism. DAA combines physical interaction scores from docking predictions with learned attention patterns to focus on protein regions most relevant to specific molecular interactions. We evaluate our method on enzymatic reaction prediction, where it outperforms previous state-of-the-art methods, achieving 62.2\% accuracy versus 56.79\% on complex molecules and 55.54\% versus 49.45\% on innovative reactions. Through detailed ablation studies and visualizations, we demonstrate how DAA generates interpretable attention patterns that adapt to different molecular contexts. Our approach represents a general framework for context-aware protein representation in biocatalysis prediction, with potential applications across enzymatic synthesis planning. We open-source our implementation and pre-trained models to facilitate further research.

### Understanding the Capabilities and Limitations of Weak-to-Strong Generalization 
[[arxiv](https://arxiv.org/abs/2502.01458)] [[cool](https://papers.cool/arxiv/2502.01458)] [[pdf](https://arxiv.org/pdf/2502.01458)]
> **Authors**: Wei Yao,Wenkai Yang,Ziqiao Wang,Yankai Lin,Yong Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Weak-to-strong generalization, where weakly supervised strong models outperform their weaker teachers, offers a promising approach to aligning superhuman models with human values. To deepen the understanding of this approach, we provide theoretical insights into its capabilities and limitations. First, in the classification setting, we establish upper and lower generalization error bounds for the strong model, identifying the primary limitations as stemming from the weak model's generalization error and the optimization objective itself. Additionally, we derive lower and upper bounds on the calibration error of the strong model. These theoretical bounds reveal two critical insights: (1) the weak model should demonstrate strong generalization performance and maintain well-calibrated predictions, and (2) the strong model's training process must strike a careful balance, as excessive optimization could undermine its generalization capability by over-relying on the weak supervision signals. Finally, in the regression setting, we extend the work of Charikar et al. (2024) to a loss function based on Kullback-Leibler (KL) divergence, offering guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement. We conduct sufficient experiments to validate our theory.

### Process Reinforcement through Implicit Rewards 
[[arxiv](https://arxiv.org/abs/2502.01456)] [[cool](https://papers.cool/arxiv/2502.01456)] [[pdf](https://arxiv.org/pdf/2502.01456)]
> **Authors**: Ganqu Cui,Lifan Yuan,Zefan Wang,Hanbin Wang,Wendi Li,Bingxiang He,Yuchen Fan,Tianyu Yu,Qixin Xu,Weize Chen,Jiarui Yuan,Huayu Chen,Kaiyan Zhang,Xingtai Lv,Shuo Wang,Yuan Yao,Xu Han,Hao Peng,Yu Cheng,Zhiyuan Liu,Maosong Sun,Bowen Zhou,Ning Ding
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 20 pages.Model&Code&Data available at https://github.com/PRIME-RL/PRIME
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.

### Molecular Odor Prediction Based on Multi-Feature Graph Attention Networks 
[[arxiv](https://arxiv.org/abs/2502.01430)] [[cool](https://papers.cool/arxiv/2502.01430)] [[pdf](https://arxiv.org/pdf/2502.01430)]
> **Authors**: HongXin Xie,JianDe Sun,Yi Shao,Shuai Li,Sujuan Hou,YuLong Sun,Jian Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Olfactory perception plays a critical role in both human and organismal interactions, yet understanding of its underlying mechanisms and influencing factors remain insufficient. Molecular structures influence odor perception through intricate biochemical interactions, and accurately quantifying structure-odor relationships presents significant challenges. The Quantitative Structure-Odor Relationship (QSOR) task, which involves predicting the associations between molecular structures and their corresponding odors, seeks to address these challenges. To this end, we propose a method for QSOR, utilizing Graph Attention Networks to model molecular structures and capture both local and global features. Unlike conventional QSOR approaches reliant on predefined descriptors, our method leverages diverse molecular feature extraction techniques to automatically learn comprehensive representations. This integration enhances the model's capacity to handle complex molecular information, improves prediction accuracy. Our approach demonstrates clear advantages in QSOR prediction tasks, offering valuable insights into the application of deep learning in cheminformatics.

### An Algorithm for Fixed Budget Best Arm Identification with Combinatorial Exploration 
[[arxiv](https://arxiv.org/abs/2502.01429)] [[cool](https://papers.cool/arxiv/2502.01429)] [[pdf](https://arxiv.org/pdf/2502.01429)]
> **Authors**: Siddhartha Parupudi,Gourab Ghatak
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We consider the best arm identification (BAI) problem in the $K-$armed bandit framework with a modification - the agent is allowed to play a subset of arms at each time slot instead of one arm. Consequently, the agent observes the sample average of the rewards of the arms that constitute the probed subset. Several trade-offs arise here - e.g., sampling a larger number of arms together results in a wider view of the environment, while sampling fewer arms enhances the information about individual reward distributions. Furthermore, grouping a large number of suboptimal arms together albeit reduces the variance of the reward of the group, it may enhance the group mean to make it close to that containing the optimal arm. To solve this problem, we propose an algorithm that constructs $\log_2 K$ groups and performs a likelihood ratio test to detect the presence of the best arm in each of these groups. Then a Hamming decoding procedure determines the unique best arm. We derive an upper bound for the error probability of the proposed algorithm based on a new hardness parameter $H_4$. Finally, we demonstrate cases under which it outperforms the state-of-the-art algorithms for the single play case.

### Structural features of the fly olfactory circuit mitigate the stability-plasticity dilemma in continual learning 
[[arxiv](https://arxiv.org/abs/2502.01427)] [[cool](https://papers.cool/arxiv/2502.01427)] [[pdf](https://arxiv.org/pdf/2502.01427)]
> **Authors**: Heming Zou,Yunliang Zang,Xiangyang Ji
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,神经元和认知
- **Abstract**: Artificial neural networks face the stability-plasticity dilemma in continual learning, while the brain can maintain memories and remain adaptable. However, the biological strategies for continual learning and their potential to inspire learning algorithms in neural networks are poorly understood. This study presents a minimal model of the fly olfactory circuit to investigate the biological strategies that support continual odor learning. We introduce the fly olfactory circuit as a plug-and-play component, termed the Fly Model, which can integrate with modern machine learning methods to address this dilemma. Our findings demonstrate that the Fly Model enhances both memory stability and learning plasticity, overcoming the limitations of current continual learning strategies. We validated its effectiveness across various challenging continual learning scenarios using commonly used datasets. The fly olfactory system serves as an elegant biological circuit for lifelong learning, offering a module that enhances continual learning with minimal additional computational cost for machine learning.

### The Batch Complexity of Bandit Pure Exploration 
[[arxiv](https://arxiv.org/abs/2502.01425)] [[cool](https://papers.cool/arxiv/2502.01425)] [[pdf](https://arxiv.org/pdf/2502.01425)]
> **Authors**: Adrienne Tuynman,Rémy Degenne
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In a fixed-confidence pure exploration problem in stochastic multi-armed bandits, an algorithm iteratively samples arms and should stop as early as possible and return the correct answer to a query about the arms distributions. We are interested in batched methods, which change their sampling behaviour only a few times, between batches of observations. We give an instance-dependent lower bound on the number of batches used by any sample efficient algorithm for any pure exploration task. We then give a general batched algorithm and prove upper bounds on its expected sample complexity and batch complexity. We illustrate both lower and upper bounds on best-arm identification and thresholding bandits.

### Categorical Schrödinger Bridge Matching 
[[arxiv](https://arxiv.org/abs/2502.01416)] [[cool](https://papers.cool/arxiv/2502.01416)] [[pdf](https://arxiv.org/pdf/2502.01416)]
> **Authors**: Grigoriy Ksenofontov,Alexander Korotin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The Schrödinger Bridge (SB) is a powerful framework for solving generative modeling tasks such as unpaired domain translation. Most SB-related research focuses on continuous data space $\mathbb{R}^{D}$ and leaves open theoretical and algorithmic questions about applying SB methods to discrete data, e.g, on finite spaces $\mathbb{S}^{D}$. Notable examples of such sets $\mathbb{S}$ are codebooks of vector-quantized (VQ) representations of modern autoencoders, tokens in texts, categories of atoms in molecules, etc. In this paper, we provide a theoretical and algorithmic foundation for solving SB in discrete spaces using the recently introduced Iterative Markovian Fitting (IMF) procedure. Specifically, we theoretically justify the convergence of discrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop a practical computational algorithm for SB which we call Categorical Schrödinger Bridge Matching (CSBM). We show the performance of CSBM via a series of experiments with synthetic data and VQ representations of images.

### GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models 
[[arxiv](https://arxiv.org/abs/2502.01406)] [[cool](https://papers.cool/arxiv/2502.01406)] [[pdf](https://arxiv.org/pdf/2502.01406)]
> **Authors**: Jonathan Drechsel,Steffen Herbold
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: AI systems frequently exhibit and amplify social biases, including gender bias, leading to harmful consequences in critical areas. This study introduces a novel encoder-decoder approach that leverages model gradients to learn a single monosemantic feature neuron encoding gender information. We show that our method can be used to debias transformer-based language models, while maintaining other capabilities. We demonstrate the effectiveness of our approach across multiple encoder-only based models and highlight its potential for broader applications.

### Can message-passing GNN approximate triangular factorizations of sparse matrices? 
[[arxiv](https://arxiv.org/abs/2502.01397)] [[cool](https://papers.cool/arxiv/2502.01397)] [[pdf](https://arxiv.org/pdf/2502.01397)]
> **Authors**: Vladislav Trifonov,Ekaterina Muravleva,Ivan Oseledets
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,数值分析
- **Abstract**: We study fundamental limitations of Graph Neural Networks (GNNs) for learning sparse matrix preconditioners. While recent works have shown promising results using GNNs to predict incomplete factorizations, we demonstrate that the local nature of message passing creates inherent barriers for capturing non-local dependencies required for optimal preconditioning. We introduce a new benchmark dataset of matrices where good sparse preconditioners exist but require non-local computations, constructed using both synthetic examples and real-world matrices. Our experimental results show that current GNN architectures struggle to approximate these preconditioners, suggesting the need for new architectural approaches beyond traditional message passing networks. We provide theoretical analysis and empirical evidence to explain these limitations, with implications for the broader use of GNNs in numerical linear algebra.

### Learning Traffic Anomalies from Generative Models on Real-Time Observations 
[[arxiv](https://arxiv.org/abs/2502.01391)] [[cool](https://papers.cool/arxiv/2502.01391)] [[pdf](https://arxiv.org/pdf/2502.01391)]
> **Authors**: Fotis I. Giasemis,Alexandros Sopasakis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.

### Detecting Backdoor Samples in Contrastive Language Image Pretraining 
[[arxiv](https://arxiv.org/abs/2502.01385)] [[cool](https://papers.cool/arxiv/2502.01385)] [[pdf](https://arxiv.org/pdf/2502.01385)]
> **Authors**: Hanxun Huang,Sarah Erfani,Yige Li,Xingjun Ma,James Bailey
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ICLR2025
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Contrastive language-image pretraining (CLIP) has been found to be vulnerable to poisoning backdoor attacks where the adversary can achieve an almost perfect attack success rate on CLIP models by poisoning only 0.01\% of the training dataset. This raises security concerns on the current practice of pretraining large-scale models on unscrutinized web data using CLIP. In this work, we analyze the representations of backdoor-poisoned samples learned by CLIP models and find that they exhibit unique characteristics in their local subspace, i.e., their local neighborhoods are far more sparse than that of clean samples. Based on this finding, we conduct a systematic study on detecting CLIP backdoor attacks and show that these attacks can be easily and efficiently detected by traditional density ratio-based local outlier detectors, whereas existing backdoor sample detection methods fail. Our experiments also reveal that an unintentional backdoor already exists in the original CC3M dataset and has been trained into a popular open-source model released by OpenCLIP. Based on our detector, one can clean up a million-scale web dataset (e.g., CC3M) efficiently within 15 minutes using 4 Nvidia A100 GPUs. The code is publicly available in our \href{https://github.com/HanxunH/Detect-CLIP-Backdoor-Samples}{GitHub repository}.

### InfoBridge: Mutual Information estimation via Bridge Matching 
[[arxiv](https://arxiv.org/abs/2502.01383)] [[cool](https://papers.cool/arxiv/2502.01383)] [[pdf](https://arxiv.org/pdf/2502.01383)]
> **Authors**: Sergei Kholkin,Ivan Butakov,Evgeny Burnaev,Nikita Gushchin,Alexander Korotin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Diffusion bridge models have recently become a powerful tool in the field of generative modeling. In this work, we leverage their power to address another important problem in machine learning and information theory - the estimation of the mutual information (MI) between two random variables. We show that by using the theory of diffusion bridges, one can construct an unbiased estimator for data posing difficulties for conventional MI estimators. We showcase the performance of our estimator on a series of standard MI estimation benchmarks.

### CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models 
[[arxiv](https://arxiv.org/abs/2502.01378)] [[cool](https://papers.cool/arxiv/2502.01378)] [[pdf](https://arxiv.org/pdf/2502.01378)]
> **Authors**: Guanduo Chen,Yutong He,Yipeng Hu,Kun Yuan,Binhang Yuan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Models (LLMs) demonstrate exceptional performance across various tasks but demand substantial computational resources even for fine-tuning computation. Although Low-Rank Adaptation (LoRA) significantly alleviates memory consumption during fine-tuning, its impact on computational cost reduction is limited. This paper identifies the computation of activation gradients as the primary bottleneck in LoRA's backward propagation and introduces the Computation-Efficient LoRA (CE-LoRA) algorithm, which enhances computational efficiency while preserving memory efficiency. CE-LoRA leverages two key techniques: Approximated Matrix Multiplication, which replaces dense multiplications of large and complete matrices with sparse multiplications involving only critical rows and columns, and the Double-LoRA technique, which reduces error propagation in activation gradients. Theoretically, CE-LoRA converges at the same rate as LoRA, $ \mathcal{O}(1/\sqrt{T}) $, where $T$ is the number of iteartions. Empirical evaluations confirm that CE-LoRA significantly reduces computational costs compared to LoRA without notable performance degradation.

### Compact Rule-Based Classifier Learning via Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.01375)] [[cool](https://papers.cool/arxiv/2502.01375)] [[pdf](https://arxiv.org/pdf/2502.01375)]
> **Authors**: Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学中的逻辑
- **Abstract**: Rule-based models play a crucial role in scenarios that require transparency and accountable decision-making. However, they primarily consist of discrete parameters and structures, which presents challenges for scalability and optimization. In this work, we introduce a new rule-based classifier trained using gradient descent, in which the user can control the maximum number and length of the rules. For numerical partitions, the user can also control the partitions used with fuzzy sets, which also helps keep the number of partitions small. We perform a series of exhaustive experiments on $40$ datasets to show how this classifier performs in terms of accuracy and rule base size. Then, we compare our results with a genetic search that fits an equivalent classifier and with other explainable and non-explainable state-of-the-art classifiers. Our results show how our method can obtain compact rule bases that use significantly fewer patterns than other rule-based methods and perform better than other explainable classifiers.

### Trajectory World Models for Heterogeneous Environments 
[[arxiv](https://arxiv.org/abs/2502.01366)] [[cool](https://papers.cool/arxiv/2502.01366)] [[pdf](https://arxiv.org/pdf/2502.01366)]
> **Authors**: Shaofeng Yin,Jialong Wu,Siqiao Huang,Xingjian Su,Xu He,Jianye Hao,Mingsheng Long
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Heterogeneity in sensors and actuators across environments poses a significant challenge to building large-scale pre-trained world models on top of this low-dimensional sensor information. In this work, we explore pre-training world models for heterogeneous environments by addressing key transfer barriers in both data diversity and model flexibility. We introduce UniTraj, a unified dataset comprising over one million trajectories from 80 environments, designed to scale data while preserving critical diversity. Additionally, we propose TrajWorld, a novel architecture capable of flexibly handling varying sensor and actuator information and capturing environment dynamics in-context. Pre-training TrajWorld on UniTraj demonstrates significant improvements in transition prediction and achieves a new state-of-the-art for off-policy evaluation. To the best of our knowledge, this work, for the first time, demonstrates the transfer benefits of world models across heterogeneous and complex control environments.

### Inverse Bridge Matching Distillation 
[[arxiv](https://arxiv.org/abs/2502.01362)] [[cool](https://papers.cool/arxiv/2502.01362)] [[pdf](https://arxiv.org/pdf/2502.01362)]
> **Authors**: Nikita Gushchin,David Li,Daniil Selikhanovych,Evgeny Burnaev,Dmitry Baranchuk,Alexander Korotin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.

### A Relative Homology Theory of Representation in Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01360)] [[cool](https://papers.cool/arxiv/2502.01360)] [[pdf](https://arxiv.org/pdf/2502.01360)]
> **Authors**: Kosio Beshkov
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,代数拓扑,神经元和认知
- **Abstract**: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain into convex polyhedra $G_J$ over which the network $Φ$ operates in an affine manner. In this work, we leverage these properties to define the equivalence class of inputs $\sim_Φ$, which can be split into two sets related to the local rank of $Φ_J$ and the intersections $\cap \text{Im}Φ_{J_i}$. We refer to the latter as the overlap decomposition $O_Φ$ and prove that if the intersections between each polyhedron and the input manifold are convex, the homology groups of neural representations are isomorphic to relative homology groups $H_k(Φ(M)) \simeq H_k(M,O_Φ)$. This lets us compute Betti numbers without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm. Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our relative homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on various classification problems while varying network width and depth and discuss some shortcomings of our method.

### Metric Privacy in Federated Learning for Medical Imaging: Improving Convergence and Preventing Client Inference Attacks 
[[arxiv](https://arxiv.org/abs/2502.01352)] [[cool](https://papers.cool/arxiv/2502.01352)] [[pdf](https://arxiv.org/pdf/2502.01352)]
> **Authors**: Judith Sáinz-Pardo Díaz,Andreas Athanasiou,Kangsoo Jung,Catuscia Palamidessi,Álvaro López García
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Federated learning is a distributed learning technique that allows training a global model with the participation of different data owners without the need to share raw data. This architecture is orchestrated by a central server that aggregates the local models from the clients. This server may be trusted, but not all nodes in the network. Then, differential privacy (DP) can be used to privatize the global model by adding noise. However, this may affect convergence across the rounds of the federated architecture, depending also on the aggregation strategy employed. In this work, we aim to introduce the notion of metric-privacy to mitigate the impact of classical server side global-DP on the convergence of the aggregated model. Metric-privacy is a relaxation of DP, suitable for domains provided with a notion of distance. We apply it from the server side by computing a distance for the difference between the local models. We compare our approach with standard DP by analyzing the impact on six classical aggregation strategies. The proposed methodology is applied to an example of medical imaging and different scenarios are simulated across homogeneous and non-i.i.d clients. Finally, we introduce a novel client inference attack, where a semi-honest client tries to find whether another client participated in the training and study how it can be mitigated using DP and metric-privacy. Our evaluation shows that metric-privacy can increase the performance of the model compared to standard DP, while offering similar protection against client inference attacks.

### Activation by Interval-wise Dropout: A Simple Way to Prevent Neural Networks from Plasticity Loss 
[[arxiv](https://arxiv.org/abs/2502.01342)] [[cool](https://papers.cool/arxiv/2502.01342)] [[pdf](https://arxiv.org/pdf/2502.01342)]
> **Authors**: Sangyeon Park,Isaac Han,Seungwon Oh,Kyung-Joong Kim
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Plasticity loss, a critical challenge in neural network training, limits a model's ability to adapt to new tasks or shifts in data distribution. This paper introduces AID (Activation by Interval-wise Dropout), a novel method inspired by Dropout, designed to address plasticity loss. Unlike Dropout, AID generates subnetworks by applying Dropout with different probabilities on each preactivation interval. Theoretical analysis reveals that AID regularizes the network, promoting behavior analogous to that of deep linear networks, which do not suffer from plasticity loss. We validate the effectiveness of AID in maintaining plasticity across various benchmarks, including continual learning tasks on standard image classification datasets such as CIFAR10, CIFAR100, and TinyImageNet. Furthermore, we show that AID enhances reinforcement learning performance in the Arcade Learning Environment benchmark.

### Accelerating Linear Recurrent Neural Networks for the Edge with Unstructured Sparsity 
[[arxiv](https://arxiv.org/abs/2502.01330)] [[cool](https://papers.cool/arxiv/2502.01330)] [[pdf](https://arxiv.org/pdf/2502.01330)]
> **Authors**: Alessandro Pierro,Steven Abreu,Jonathan Timcheck,Philipp Stratmann,Andreas Wild,Sumit Bam Shrestha
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: Linear recurrent neural networks enable powerful long-range sequence modeling with constant memory usage and time-per-token during inference. These architectures hold promise for streaming applications at the edge, but deployment in resource-constrained environments requires hardware-aware optimizations to minimize latency and energy consumption. Unstructured sparsity offers a compelling solution, enabling substantial reductions in compute and memory requirements--when accelerated by compatible hardware platforms. In this paper, we conduct a scaling study to investigate the Pareto front of performance and efficiency across inference compute budgets. We find that highly sparse linear RNNs consistently achieve better efficiency-performance trade-offs than dense baselines, with 2x less compute and 36% less memory at iso-accuracy. Our models achieve state-of-the-art results on a real-time streaming task for audio denoising. By quantizing our sparse models to fixed-point arithmetic and deploying them on the Intel Loihi 2 neuromorphic chip for real-time processing, we translate model compression into tangible gains of 42x lower latency and 149x lower energy consumption compared to a dense model on an edge GPU. Our findings showcase the transformative potential of unstructured sparsity, paving the way for highly efficient recurrent neural networks in real-world, resource-constrained environments.

### Learning Fused State Representations for Control from Multi-View Observations 
[[arxiv](https://arxiv.org/abs/2502.01316)] [[cool](https://papers.cool/arxiv/2502.01316)] [[pdf](https://arxiv.org/pdf/2502.01316)]
> **Authors**: Zeyu Wang,Yao-Hui Li,Xin Li,Hongyu Zang,Romain Laroche,Riashat Islam
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.

### Strategic Classification with Randomised Classifiers 
[[arxiv](https://arxiv.org/abs/2502.01313)] [[cool](https://papers.cool/arxiv/2502.01313)] [[pdf](https://arxiv.org/pdf/2502.01313)]
> **Authors**: Jack Geary,Henry Gouk
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We consider the problem of strategic classification, where a learner must build a model to classify agents based on features that have been strategically modified. Previous work in this area has concentrated on the case when the learner is restricted to deterministic classifiers. In contrast, we perform a theoretical analysis of an extension to this setting that allows the learner to produce a randomised classifier. We show that, under certain conditions, the optimal randomised classifier can achieve better accuracy than the optimal deterministic classifier, but under no conditions can it be worse. When a finite set of training data is available, we show that the excess risk of Strategic Empirical Risk Minimisation over the class of randomised classifiers is bounded in a similar manner as the deterministic case. In both the deterministic and randomised cases, the risk of the classifier produced by the learner converges to that of the corresponding optimal classifier as the volume of available training data grows. Moreover, this convergence happens at the same rate as in the i.i.d. case. Our findings are compared with previous theoretical work analysing the problem of strategic classification. We conclude that randomisation has the potential to alleviate some issues that could be faced in practice without introducing any substantial downsides.

### TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional Networks to Predict Transcription Factor Binding Sites 
[[arxiv](https://arxiv.org/abs/2502.01311)] [[cool](https://papers.cool/arxiv/2502.01311)] [[pdf](https://arxiv.org/pdf/2502.01311)]
> **Authors**: Nimisha Ghosh,Pratik Dutta,Daniele Santoni
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,基因组学
- **Abstract**: Transcription factors are proteins that regulate the expression of genes by binding to specific genomic regions known as Transcription Factor Binding Sites (TFBSs), typically located in the promoter regions of those genes. Accurate prediction of these binding sites is essential for understanding the complex gene regulatory networks underlying various cellular functions. In this regard, many deep learning models have been developed for such prediction, but there is still scope of improvement. In this work, we have developed a deep learning model which uses pre-trained DNABERT, a Convolutional Neural Network (CNN) module, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale Convolutions with Attention (MSCA) module and an output module. The pre-trained DNABERT is used for sequence embedding, thereby capturing the long-term dependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are useful in extracting higher-order local features. TFBS-Finder is trained and tested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies as well as cross-cell line validations and comparisons with other models. The experimental results show the superiority of the proposed method in predicting TFBSs compared to the existing methodologies. The codes and the relevant datasets are publicly available at https://github.com/NimishaGhosh/TFBS-Finder/.

### A Statistical Learning Perspective on Semi-dual Adversarial Neural Optimal Transport Solvers 
[[arxiv](https://arxiv.org/abs/2502.01310)] [[cool](https://papers.cool/arxiv/2502.01310)] [[pdf](https://arxiv.org/pdf/2502.01310)]
> **Authors**: Roman Tarasov,Petr Mokrov,Milena Gazdieva,Evgeny Burnaev,Alexander Korotin
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Neural network based Optimal Transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing approaches to OT, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural networks). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for more general OT formulations, paving the promising direction for future research.

### Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01307)] [[cool](https://papers.cool/arxiv/2502.01307)] [[pdf](https://arxiv.org/pdf/2502.01307)]
> **Authors**: Henrik Müller,Daniel Kudenko
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 4 figures, accepted as extended abstract at AAMAS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Potential-based reward shaping is commonly used to incorporate prior knowledge of how to solve the task into reinforcement learning because it can formally guarantee policy invariance. As such, the optimal policy and the ordering of policies by their returns are not altered by potential-based reward shaping. In this work, we highlight the dependence of effective potential-based reward shaping on the initial Q-values and external rewards, which determine the agent's ability to exploit the shaping rewards to guide its exploration and achieve increased sample efficiency. We formally derive how a simple linear shift of the potential function can be used to improve the effectiveness of reward shaping without changing the encoded preferences in the potential function, and without having to adjust the initial Q-values, which can be challenging and undesirable in deep reinforcement learning. We show the theoretical limitations of continuous potential functions for correctly assigning positive and negative reward shaping values. We verify our theoretical findings empirically on Gridworld domains with sparse and uninformative reward functions, as well as on the Cart Pole and Mountain Car environments, where we demonstrate the application of our results in deep reinforcement learning.

### Molecular Odor Prediction with Harmonic Modulated Feature Mapping and Chemically-Informed Loss 
[[arxiv](https://arxiv.org/abs/2502.01296)] [[cool](https://papers.cool/arxiv/2502.01296)] [[pdf](https://arxiv.org/pdf/2502.01296)]
> **Authors**: HongXin Xie,JianDe Sun,Yi Shao,Shuai Li,Sujuan Hou,YuLong Sun,Yuxiang Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Molecular odor prediction has great potential across diverse fields such as chemistry, pharmaceuticals, and environmental science, enabling the rapid design of new materials and enhancing environmental monitoring. However, current methods face two main challenges: First, existing models struggle with non-smooth objective functions and the complexity of mixed feature dimensions; Second, datasets suffer from severe label imbalance, which hampers model training, particularly in learning minority class labels. To address these issues, we introduce a novel feature mapping method and a molecular ensemble optimization loss function. By incorporating feature importance learning and frequency modulation, our model adaptively adjusts the contribution of each feature, efficiently capturing the intricate relationship between molecular structures and odor descriptors. Our feature mapping preserves feature independence while enhancing the model's efficiency in utilizing molecular features through frequency modulation. Furthermore, the proposed loss function dynamically adjusts label weights, improves structural consistency, and strengthens label correlations, effectively addressing data imbalance and label co-occurrence challenges. Experimental results show that our method significantly can improves the accuracy of molecular odor prediction across various deep learning models, demonstrating its promising potential in molecular structure representation and chemoinformatics.

### A Framework for Double-Blind Federated Adaptation of Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.01289)] [[cool](https://papers.cool/arxiv/2502.01289)] [[pdf](https://arxiv.org/pdf/2502.01289)]
> **Authors**: Nurbek Tastan,Karthik Nandakumar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别,分布式、并行和集群计算
- **Abstract**: The availability of foundational models (FMs) pre-trained on large-scale data has advanced the state-of-the-art in many computer vision tasks. While FMs have demonstrated good zero-shot performance on many image classification tasks, there is often scope for performance improvement by adapting the FM to the downstream task. However, the data that is required for this adaptation typically exists in silos across multiple entities (data owners) and cannot be collated at a central location due to regulations and privacy concerns. At the same time, a learning service provider (LSP) who owns the FM cannot share the model with the data owners due to proprietary reasons. In some cases, the data owners may not even have the resources to store such large FMs. Hence, there is a need for algorithms to adapt the FM in a double-blind federated manner, i.e., the data owners do not know the FM or each other's data, and the LSP does not see the data for the downstream tasks. In this work, we propose a framework for double-blind federated adaptation of FMs using fully homomorphic encryption (FHE). The proposed framework first decomposes the FM into a sequence of FHE-friendly blocks through knowledge distillation. The resulting FHE-friendly model is adapted for the downstream task via low-rank parallel adapters that can be learned without backpropagation through the FM. Since the proposed framework requires the LSP to share intermediate representations with the data owners, we design a privacy-preserving permutation scheme to prevent the data owners from learning the FM through model extraction attacks. Finally, a secure aggregation protocol is employed for federated learning of the low-rank parallel adapters. Empirical results on four datasets demonstrate the practical feasibility of the proposed framework.

### HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance 
[[arxiv](https://arxiv.org/abs/2502.01276)] [[cool](https://papers.cool/arxiv/2502.01276)] [[pdf](https://arxiv.org/pdf/2502.01276)]
> **Authors**: Marcel Wever,Maximilian Muschalik,Fabian Fumagalli,Marius Lindauer
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance. However, the impact of individual hyperparameters on model generalization is highly context-dependent, prohibiting a one-size-fits-all solution and requiring opaque automated machine learning (AutoML) systems to find optimal configurations. The black-box nature of most AutoML systems undermines user trust and discourages adoption. To address this, we propose a game-theoretic explainability framework for HPO that is based on Shapley values and interactions. Our approach provides an additive decomposition of a performance measure across hyperparameters, enabling local and global explanations of hyperparameter importance and interactions. The framework, named HyperSHAP, offers insights into ablations, the tunability of learning algorithms, and optimizer behavior across different hyperparameter spaces. We evaluate HyperSHAP on various HPO benchmarks by analyzing the interaction structure of the HPO problem. Our results show that while higher-order interactions exist, most performance improvements can be explained by focusing on lower-order representations.

### Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity Perspective 
[[arxiv](https://arxiv.org/abs/2502.01272)] [[cool](https://papers.cool/arxiv/2502.01272)] [[pdf](https://arxiv.org/pdf/2502.01272)]
> **Authors**: Chang Liu,Hai Huang,Yujie Xing,Xingquan Zuo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Neural Networks (GNNs) have achieved notable success in tasks such as social and transportation networks. However, recent studies have highlighted the vulnerability of GNNs to backdoor attacks, raising significant concerns about their reliability in real-world applications. Despite initial efforts to defend against specific graph backdoor attacks, existing defense methods face two main challenges: either the inability to establish a clear distinction between triggers and clean nodes, resulting in the removal of many clean nodes, or the failure to eliminate the impact of triggers, making it challenging to restore the target nodes to their pre-attack state. Through empirical analysis of various existing graph backdoor attacks, we observe that the triggers generated by these methods exhibit over-similarity in both features and structure. Based on this observation, we propose a novel graph backdoor defense method SimGuard. We first utilizes a similarity-based metric to detect triggers and then employs contrastive learning to train a backdoor detector that generates embeddings capable of separating triggers from clean nodes, thereby improving detection efficiency. Extensive experiments conducted on real-world datasets demonstrate that our proposed method effectively defends against various graph backdoor attacks while preserving performance on clean nodes. The code will be released upon acceptance.

### Exploratory Utility Maximization Problem with Tsallis Entropy 
[[arxiv](https://arxiv.org/abs/2502.01269)] [[cool](https://papers.cool/arxiv/2502.01269)] [[pdf](https://arxiv.org/pdf/2502.01269)]
> **Authors**: Chen Ziyi,Gu Jia-wen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数学金融
- **Abstract**: We study expected utility maximization problem with constant relative risk aversion utility function in a complete market under the reinforcement learning framework. To induce exploration, we introduce the Tsallis entropy regularizer, which generalizes the commonly used Shannon entropy. Unlike the classical Merton's problem, which is always well-posed and admits closed-form solutions, we find that the utility maximization exploratory problem is ill-posed in certain cases, due to over-exploration. With a carefully selected primary temperature function, we investigate two specific examples, for which we fully characterize their well-posedness and provide semi-closed-form solutions. It is interesting to find that one example has the well-known Gaussian distribution as the optimal strategy, while the other features the rare Wigner semicircle distribution, which is equivalent to a scaled Beta distribution. The means of the two optimal exploratory policies coincide with that of the classical counterpart. In addition, we examine the convergence of the value function and optimal exploratory strategy as the exploration vanishes. Finally, we design a reinforcement learning algorithm and conduct numerical experiments to demonstrate the advantages of reinforcement learning.

### Counterfactual Situation Testing: From Single to Multidimensional Discrimination 
[[arxiv](https://arxiv.org/abs/2502.01267)] [[cool](https://papers.cool/arxiv/2502.01267)] [[pdf](https://arxiv.org/pdf/2502.01267)]
> **Authors**: Jose M. Alvarez,Salvatore Ruggieri
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We present counterfactual situation testing (CST), a causal data mining framework for detecting individual discrimination in a dataset of classifier decisions. CST answers the question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing (ST) of Thanh et al. (2011) by operationalizing the notion of fairness given the difference via counterfactual reasoning. ST finds for each complainant similar protected and non-protected instances in the dataset; constructs, respectively, a control and test group; and compares the groups such that a difference in outcomes implies a potential case of individual discrimination. CST, instead, avoids this idealized comparison by establishing the test group on the complainant's generated counterfactual, which reflects how the protected attribute when changed influences other seemingly neutral attributes of the complainant. Under CST we test for discrimination for each complainant by comparing similar individuals within each group but dissimilar individuals across groups. We consider single (e.g., gender) and multidimensional (e.g., gender and race) discrimination testing. For multidimensional discrimination we study multiple and intersectional discrimination and, as feared by legal scholars, find evidence that the former fails to account for the latter kind. Using a k-nearest neighbor implementation, we showcase CST on synthetic and real data. Experimental results show that CST uncovers a higher number of cases than ST, even when the model is counterfactually fair. In fact, CST extends counterfactual fairness (CF) of Kusner et al. (2017) by equipping CF with confidence intervals.

### On Exact Learning of $d$-Monotone Functions 
[[arxiv](https://arxiv.org/abs/2502.01265)] [[cool](https://papers.cool/arxiv/2502.01265)] [[pdf](https://arxiv.org/pdf/2502.01265)]
> **Authors**: Nader H. Bshouty
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: In this paper, we study the learnability of the Boolean class of $d$-monotone functions $f:{\cal X}\to\{0,1\}$ from membership and equivalence queries, where $({\cal X},\le)$ is a finite lattice. We show that the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function $F:\{0,1\}^d\to\{0,1\}$ and $g_1,\ldots,g_d:{\cal X}\to \{0,1\}$ are any monotone functions, is learnable in time $σ({\cal X})\cdot (size(f)/d+1)^{d}$ where $σ({\cal X})$ is the maximum sum of the number of immediate predecessors in a chain from the largest element to the smallest element in the lattice ${\cal X}$ and $size(f)=size(g_1)+\cdots+size(g_d)$, where $size(g_i)$ is the number of minimal elements in $g_i^{-1}(1)$. For the Boolean function $f:\{0,1\}^n\to\{0,1\}$, the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function and $g_1,\ldots,g_d$ are any monotone DNF, is learnable in time $O(n^2)\cdot (size(f)/d+1)^{d}$ where $size(f)=size(g_1)+\cdots+size(g_d)$. In particular, this class is learnable in polynomial time when $d$ is constant. Additionally, this class is learnable in polynomial time when $size(g_i)$ is constant for all $i$ and $d=O(\log n)$.

### Beyond Win Rates: A Clustering-Based Approach to Character Balance Analysis in Team-Based Games 
[[arxiv](https://arxiv.org/abs/2502.01250)] [[cool](https://papers.cool/arxiv/2502.01250)] [[pdf](https://arxiv.org/pdf/2502.01250)]
> **Authors**: Haokun Zhou
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Character diversity in competitive games, while enriching gameplay, often introduces balance challenges that can negatively impact player experience and strategic depth. Traditional balance assessments rely on aggregate metrics like win rates and pick rates, which offer limited insight into the intricate dynamics of team-based games and nuanced character roles. This paper proposes a novel clustering-based methodology to analyze character balance, leveraging in-game data from Valorant to account for team composition influences and reveal latent character roles. By applying hierarchical agglomerative clustering with Jensen-Shannon Divergence to professional match data from the Valorant Champions Tour 2022, our approach identifies distinct clusters of agents exhibiting similar co-occurrence patterns within team compositions. This method not only complements existing quantitative metrics but also provides a more holistic and interpretable perspective on character synergies and potential imbalances, offering game developers a valuable tool for informed and context-aware balance adjustments.

### Learnable polynomial, trigonometric, and tropical activations 
[[arxiv](https://arxiv.org/abs/2502.01247)] [[cool](https://papers.cool/arxiv/2502.01247)] [[pdf](https://arxiv.org/pdf/2502.01247)]
> **Authors**: Ismail Khalfaoui-Hassani,Stefan Kesselheim
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,代数几何
- **Abstract**: This paper investigates scalable neural networks with learnable activation functions based on orthogonal function bases and tropical polynomials, targeting ImageNet-1K classification and next token prediction on OpenWebText. Traditional activations, such as ReLU, are static. In contrast, learnable activations enable the network to adapt dynamically during training. However, stability issues, such as vanishing or exploding gradients, arise with improper variance management in deeper networks. To remedy this, we propose an initialization scheme that single-handedly preserves unitary variance in transformers and convolutional networks, ensuring stable gradient flow even in deep architectures. Extensive experiments demonstrate that networks with Hermite, Fourier, and Tropical-based learnable activations significantly improve over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in train and test, highlighting the viability of learnable activations in large-scale tasks. The activation functions developed here are the subject of a library coded entirely in pure PyTorch: torchortho, available at https://github.com/K-H-Ismail/torchortho.

### The Differences Between Direct Alignment Algorithms are a Blur 
[[arxiv](https://arxiv.org/abs/2502.01237)] [[cool](https://papers.cool/arxiv/2502.01237)] [[pdf](https://arxiv.org/pdf/2502.01237)]
> **Authors**: Alexey Gorbatovski,Boris Shaposhnikov,Viacheslav Sinii,Alexey Malakhov,Daniil Gavrilov
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the $β$ parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +$3.46$ (ORPO) and +$8.27$ (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.

### Eliciting Language Model Behaviors with Investigator Agents 
[[arxiv](https://arxiv.org/abs/2502.01236)] [[cool](https://papers.cool/arxiv/2502.01236)] [[pdf](https://arxiv.org/pdf/2502.01236)]
> **Authors**: Xiang Lisa Li,Neil Chowdhury,Daniel D. Johnson,Tatsunori Hashimoto,Percy Liang,Sarah Schwettmann,Jacob Steinhardt
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate.

### Efficient Prior Selection in Gaussian Process Bandits with Thompson Sampling 
[[arxiv](https://arxiv.org/abs/2502.01226)] [[cool](https://papers.cool/arxiv/2502.01226)] [[pdf](https://arxiv.org/pdf/2502.01226)]
> **Authors**: Jack Sandberg,Morteza Haghir Chehreghani
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 16 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Gaussian process (GP) bandits provide a powerful framework for solving blackbox optimization of unknown functions. The characteristics of the unknown function depends heavily on the assumed GP prior. Most work in the literature assume that this prior is known but in practice this seldom holds. Instead, practitioners often rely on maximum likelihood estimation to select the hyperparameters of the prior - which lacks theoretical guarantees. In this work, we propose two algorithms for joint prior selection and regret minimization in GP bandits based on GP Thompson sampling (GP-TS): Prior-Elimination GP-TS (PE-GP-TS) and HyperPrior GP-TS (HP-GP-TS). We theoretically analyze the algorithms and establish upper bounds for their respective regret. In addition, we demonstrate the effectiveness of our algorithms compared to the alternatives through experiments with synthetic and real-world data.

### Privilege Scores 
[[arxiv](https://arxiv.org/abs/2502.01211)] [[cool](https://papers.cool/arxiv/2502.01211)] [[pdf](https://arxiv.org/pdf/2502.01211)]
> **Authors**: Ludwig Bothmann,Philip A. Boustani,Jose M. Alvarez,Giuseppe Casalicchio,Bernd Bischl,Susanne Dandl
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Bias-transforming methods of fairness-aware machine learning aim to correct a non-neutral status quo with respect to a protected attribute (PA). Current methods, however, lack an explicit formulation of what drives non-neutrality. We introduce privilege scores (PS) to measure PA-related privilege by comparing the model predictions in the real world with those in a fair world in which the influence of the PA is removed. At the individual level, PS can identify individuals who qualify for affirmative action; at the global level, PS can inform bias-transforming policies. After presenting estimation methods for PS, we propose privilege score contributions (PSCs), an interpretation method that attributes the origin of privilege to mediating features and direct effects. We provide confidence intervals for both PS and PSCs. Experiments on simulated and real-world data demonstrate the broad applicability of our methods and provide novel insights into gender and racial privilege in mortgage and college admissions applications.

### Almost Surely Safe Alignment of Large Language Models at Inference-Time 
[[arxiv](https://arxiv.org/abs/2502.01208)] [[cool](https://papers.cool/arxiv/2502.01208)] [[pdf](https://arxiv.org/pdf/2502.01208)]
> **Authors**: Xiaotong Ji,Shyam Sundhar Ramesh,Matthieu Zimmer,Ilija Bogunovic,Jun Wang,Haitham Bou Ammar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.

### Land Surface Temperature Super-Resolution with a Scale-Invariance-Free Neural Approach: Application to MODIS 
[[arxiv](https://arxiv.org/abs/2502.01204)] [[cool](https://papers.cool/arxiv/2502.01204)] [[pdf](https://arxiv.org/pdf/2502.01204)]
> **Authors**: Romuald Ait-Bachir,Carlos Granero-Belinchon,Aurélie Michel,Julien Michel,Xavier Briottet,Lucas Drumetz
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Due to the trade-off between the temporal and spatial resolution of thermal spaceborne sensors, super-resolution methods have been developed to provide fine-scale Land SurfaceTemperature (LST) maps. Most of them are trained at low resolution but applied at fine resolution, and so they require a scale-invariance hypothesis that is not always adapted. Themain contribution of this work is the introduction of a Scale-Invariance-Free approach for training Neural Network (NN) models, and the implementation of two NN models, calledScale-Invariance-Free Convolutional Neural Network for Super-Resolution (SIF-CNN-SR) for the super-resolution of MODIS LST products. The Scale-Invariance-Free approach consists ontraining the models in order to provide LST maps at high spatial resolution that recover the initial LST when they are degraded at low resolution and that contain fine-scale texturesinformed by the high resolution NDVI. The second contribution of this work is the release of a test database with ASTER LST images concomitant with MODIS ones that can be usedfor evaluation of super-resolution algorithms. We compare the two proposed models, SIF-CNN-SR1 and SIF-CNN-SR2, with four state-of-the-art methods, Bicubic, DMS, ATPRK, Tsharp,and a CNN sharing the same architecture as SIF-CNN-SR but trained under the scale-invariance hypothesis. We show that SIF-CNN-SR1 outperforms the state-of-the-art methods and the other two CNN models as evaluated with LPIPS and Fourier space metrics focusing on the analysis of textures. These results and the available ASTER-MODIS database for evaluation are promising for future studies on super-resolution of LST.

### Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models 
[[arxiv](https://arxiv.org/abs/2502.01203)] [[cool](https://papers.cool/arxiv/2502.01203)] [[pdf](https://arxiv.org/pdf/2502.01203)]
> **Authors**: Gholamali Aminian,Amir R. Asadi,Idan Shenfeld,Youssef Mroueh
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, particularly in reverse KL-regularization, where achieving exact solutions has remained an open problem. This paper presents the first \emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems.

### Dance recalibration for dance coherency with recurrent convolution block 
[[arxiv](https://arxiv.org/abs/2502.01190)] [[cool](https://papers.cool/arxiv/2502.01190)] [[pdf](https://arxiv.org/pdf/2502.01190)]
> **Authors**: Seungho Eum,Ihjoon Cho,Junghyeon Kim
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the recent advancements in generative AI such as GAN, Diffusion, and VAE, the use of generative AI for dance generation has seen significant progress and received considerable interest. In this study, We propose R-Lodge, an enhanced version of Lodge. R-Lodge incorporates Recurrent Sequential Representation Learning named Dance Recalibration to original coarse-to-fine long dance generation model. R-Lodge utilizes Dance Recalibration method using $N$ Dance Recalibration Block to address the lack of consistency in the coarse dance representation of the Lodge model. By utilizing this method, each generated dance motion incorporates a bit of information from the previous dance motions. We evaluate R-Lodge on FineDance dataset and the results show that R-Lodge enhances the consistency of the whole generated dance motions.

### FairUDT: Fairness-aware Uplift Decision Trees 
[[arxiv](https://arxiv.org/abs/2502.01188)] [[cool](https://papers.cool/arxiv/2502.01188)] [[pdf](https://arxiv.org/pdf/2502.01188)]
> **Authors**: Anam Zahid,Abdur Rehman Ali,Shaina Raza,Rai Shahnawaz,Faisal Kamiran,Asim Karim
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Published in Knowledge-based Systems (2025)
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Training data used for developing machine learning classifiers can exhibit biases against specific protected attributes. Such biases typically originate from historical discrimination or certain underlying patterns that disproportionately under-represent minority groups, such as those identified by their gender, religion, or race. In this paper, we propose a novel approach, FairUDT, a fairness-aware Uplift-based Decision Tree for discrimination identification. FairUDT demonstrates how the integration of uplift modeling with decision trees can be adapted to include fair splitting criteria. Additionally, we introduce a modified leaf relabeling approach for removing discrimination. We divide our dataset into favored and deprived groups based on a binary sensitive attribute, with the favored dataset serving as the treatment group and the deprived dataset as the control group. By applying FairUDT and our leaf relabeling approach to preprocess three benchmark datasets, we achieve an acceptable accuracy-discrimination tradeoff. We also show that FairUDT is inherently interpretable and can be utilized in discrimination detection tasks. The code for this project is available https://github.com/ara-25/FairUDT

### FragmentNet: Adaptive Graph Fragmentation for Graph-to-Sequence Molecular Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.01184)] [[cool](https://papers.cool/arxiv/2502.01184)] [[pdf](https://arxiv.org/pdf/2502.01184)]
> **Authors**: Ankur Samanta,Rohan Gupta,Aditi Misra,Christian McIntosh Clarke,Jayakumar Rajadas
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 22 pages, 13 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,人工智能,化学物理,定量方法
- **Abstract**: Molecular property prediction uses molecular structure to infer chemical properties. Chemically interpretable representations that capture meaningful intramolecular interactions enhance the usability and effectiveness of these predictions. However, existing methods often rely on atom-based or rule-based fragment tokenization, which can be chemically suboptimal and lack scalability. We introduce FragmentNet, a graph-to-sequence foundation model with an adaptive, learned tokenizer that decomposes molecular graphs into chemically valid fragments while preserving structural connectivity. FragmentNet integrates VQVAE-GCN for hierarchical fragment embeddings, spatial positional encodings for graph serialization, global molecular descriptors, and a transformer. Pre-trained with Masked Fragment Modeling and fine-tuned on MoleculeNet tasks, FragmentNet outperforms models with similarly scaled architectures and datasets while rivaling larger state-of-the-art models requiring significantly more resources. This novel framework enables adaptive decomposition, serialization, and reconstruction of molecular graphs, facilitating fragment-based editing and visualization of property trends in learned embeddings - a powerful tool for molecular design and optimization.

### Insights from Network Science can advance Deep Graph Learning 
[[arxiv](https://arxiv.org/abs/2502.01177)] [[cool](https://papers.cool/arxiv/2502.01177)] [[pdf](https://arxiv.org/pdf/2502.01177)]
> **Authors**: Christopher Blöcker,Martin Rosvall,Ingo Scholtes,Jevin D. West
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep graph learning and network science both analyze graphs but approach similar problems from different perspectives. Whereas network science focuses on models and measures that reveal the organizational principles of complex systems with explicit assumptions, deep graph learning focuses on flexible and generalizable models that learn patterns in graph data in an automated fashion. Despite these differences, both fields share the same goal: to better model and understand patterns in graph-structured data. Early efforts to integrate methods, models, and measures from network science and deep graph learning indicate significant untapped potential. In this position, we explore opportunities at their intersection. We discuss open challenges in deep graph learning, including data augmentation, improved evaluation practices, higher-order models, and pooling methods. Likewise, we highlight challenges in network science, including scaling to massive graphs, integrating continuous gradient-based optimization, and developing standardized benchmarks.

### Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity 
[[arxiv](https://arxiv.org/abs/2502.01171)] [[cool](https://papers.cool/arxiv/2502.01171)] [[pdf](https://arxiv.org/pdf/2502.01171)]
> **Authors**: Erpai Luo,Xinran Wei,Lin Huang,Yunyang Li,Han Yang,Zun Wang,Chang Liu,Zaishuo Xia,Jia Zhang,Bin Shao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算物理
- **Abstract**: Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost-driven by high-order tensor product (TP) operations-restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce SPHNet, an efficient and scalable equivariant network that incorporates adaptive sparsity into Hamiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70 percent. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact.

### Label Distribution Learning with Biased Annotations by Learning Multi-Label Representation 
[[arxiv](https://arxiv.org/abs/2502.01170)] [[cool](https://papers.cool/arxiv/2502.01170)] [[pdf](https://arxiv.org/pdf/2502.01170)]
> **Authors**: Zhiqiang Kou,Si Qin,Hailin Wang,Mingkun Xie,Shuo Chen,Yuheng Jia,Tongliang Liu,Masashi Sugiyama,Xin Geng
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-label learning (MLL) has gained attention for its ability to represent real-world data. Label Distribution Learning (LDL), an extension of MLL to learning from label distributions, faces challenges in collecting accurate label distributions. To address the issue of biased annotations, based on the low-rank assumption, existing works recover true distributions from biased observations by exploring the label correlations. However, recent evidence shows that the label distribution tends to be full-rank, and naive apply of low-rank approximation on biased observation leads to inaccurate recovery and performance degradation. In this paper, we address the LDL with biased annotations problem from a novel perspective, where we first degenerate the soft label distribution into a hard multi-hot label and then recover the true label information for each instance. This idea stems from an insight that assigning hard multi-hot labels is often easier than assigning a soft label distribution, and it shows stronger immunity to noise disturbances, leading to smaller label bias. Moreover, assuming that the multi-label space for predicting label distributions is low-rank offers a more reasonable approach to capturing label correlations. Theoretical analysis and experiments confirm the effectiveness and robustness of our method on real-world datasets.

### AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science 
[[arxiv](https://arxiv.org/abs/2502.01159)] [[cool](https://papers.cool/arxiv/2502.01159)] [[pdf](https://arxiv.org/pdf/2502.01159)]
> **Authors**: Chenyue Li,Wen Deng,Mengqian Lu,Binhang Yuan
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 16 pages, 3 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. To address this need, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. We employ a template-based question generation framework, enabling scalable and diverse multiple-choice questions curated from graduate-level atmospheric science problems. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench.

### MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks 
[[arxiv](https://arxiv.org/abs/2502.01158)] [[cool](https://papers.cool/arxiv/2502.01158)] [[pdf](https://arxiv.org/pdf/2502.01158)]
> **Authors**: Alejandro Guerra-Manzanares,Farah E. Shamout
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Published in Transactions onMachineLearningResearch (01/2025), https://openreview.net/forum?id=BhOJreYmur&noteId=ymnAhncuez
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Multimodal fusion leverages information across modalities to learn better feature representations with the goal of improving performance in fusion-based tasks. However, multimodal datasets, especially in medical settings, are typically smaller than their unimodal counterparts, which can impede the performance of multimodal models. Additionally, the increase in the number of modalities is often associated with an overall increase in the size of the multimodal network, which may be undesirable in medical use cases. Utilizing smaller unimodal encoders may lead to sub-optimal performance, particularly when dealing with high-dimensional clinical data. In this paper, we propose the Modality-INformed knowledge Distillation (MIND) framework, a multimodal model compression approach based on knowledge distillation that transfers knowledge from ensembles of pre-trained deep neural networks of varying sizes into a smaller multimodal student. The teacher models consist of unimodal networks, allowing the student to learn from diverse representations. MIND employs multi-head joint fusion models, as opposed to single-head models, enabling the use of unimodal encoders in the case of unimodal samples without requiring imputation or masking of absent modalities. As a result, MIND generates an optimized multimodal model, enhancing both multimodal and unimodal representations. It can also be leveraged to balance multimodal learning during training. We evaluate MIND on binary and multilabel clinical prediction tasks using time series data and chest X-ray images. Additionally, we assess the generalizability of the MIND framework on three non-medical multimodal multiclass datasets. Experimental results demonstrate that MIND enhances the performance of the smaller multimodal network across all five tasks, as well as various fusion methods and multimodal architectures, compared to state-of-the-art baselines.

### Tackling Feature and Sample Heterogeneity in Decentralized Multi-Task Learning: A Sheaf-Theoretic Approach 
[[arxiv](https://arxiv.org/abs/2502.01145)] [[cool](https://papers.cool/arxiv/2502.01145)] [[pdf](https://arxiv.org/pdf/2502.01145)]
> **Authors**: Chaouki Ben Issaid,Praneeth Vepakomma,Mehdi Bennis
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated multi-task learning (FMTL) aims to simultaneously learn multiple related tasks across clients without sharing sensitive raw data. However, in the decentralized setting, existing FMTL frameworks are limited in their ability to capture complex task relationships and handle feature and sample heterogeneity across clients. To address these challenges, we introduce a novel sheaf-theoretic-based approach for FMTL. By representing client relationships using cellular sheaves, our framework can flexibly model interactions between heterogeneous client models. We formulate the sheaf-based FMTL optimization problem using sheaf Laplacian regularization and propose the Sheaf-FMTL algorithm to solve it. We show that the proposed framework provides a unified view encompassing many existing federated learning (FL) and FMTL approaches. Furthermore, we prove that our proposed algorithm, Sheaf-FMTL, achieves a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Extensive experiments demonstrate that Sheaf-FMTL exhibits communication savings by sending significantly fewer bits compared to decentralized FMTL baselines.

### Beyond Yes or No: Predictive Compliance Monitoring Approaches for Quantifying the Magnitude of Compliance Violations 
[[arxiv](https://arxiv.org/abs/2502.01141)] [[cool](https://papers.cool/arxiv/2502.01141)] [[pdf](https://arxiv.org/pdf/2502.01141)]
> **Authors**: Qian Chen,Stefanie Rinderle-Ma,Lijie Wen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Most existing process compliance monitoring approaches detect compliance violations in an ex post manner. Only predicate prediction focuses on predicting them. However, predicate prediction provides a binary yes/no notion of compliance, lacking the ability to measure to which extent an ongoing process instance deviates from the desired state as specified in constraints. Here, being able to quantify the magnitude of violation would provide organizations with deeper insights into their operational performance, enabling informed decision making to reduce or mitigate the risk of non-compliance. Thus, we propose two predictive compliance monitoring approaches to close this research gap. The first approach reformulates the binary classification problem as a hybrid task that considers both classification and regression, while the second employs a multi-task learning method to explicitly predict the compliance status and the magnitude of violation for deviant cases simultaneously. In this work, we focus on temporal constraints as they are significant in almost any application domain, e.g., health care. The evaluation on synthetic and real-world event logs demonstrates that our approaches are capable of quantifying the magnitude of violations while maintaining comparable performance for compliance predictions achieved by state-of-the-art approaches.

### Simple Linear Neuron Boosting 
[[arxiv](https://arxiv.org/abs/2502.01131)] [[cool](https://papers.cool/arxiv/2502.01131)] [[pdf](https://arxiv.org/pdf/2502.01131)]
> **Authors**: Daniel Munoz
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Given a differentiable network architecture and loss function, we revisit optimizing the network's neurons in function space using Boosted Backpropagation (Grubb & Bagnell, 2010), in contrast to optimizing in parameter space. From this perspective, we reduce descent in the space of linear functions that optimizes the network's backpropagated-errors to a preconditioned gradient descent algorithm. We show that this preconditioned update rule is equivalent to reparameterizing the network to whiten each neuron's features, with the benefit that the normalization occurs outside of inference. In practice, we use this equivalence to construct an online estimator for approximating the preconditioner and we propose an online, matrix-free learning algorithm with adaptive step sizes. The algorithm is applicable whenever autodifferentiation is available, including convolutional networks and transformers, and it is simple to implement for both the local and distributed training settings. We demonstrate fast convergence both in terms of epochs and wall clock time on a variety of tasks and networks.

### Learning Efficient Positional Encodings with Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01122)] [[cool](https://papers.cool/arxiv/2502.01122)] [[pdf](https://arxiv.org/pdf/2502.01122)]
> **Authors**: Charilaos I. Kanatsoulis,Evelyn Choi,Stephanie Jegelka,Jure Leskovec,Alejandro Ribeiro
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs for graphs poses significant challenges due to the absence of canonical node ordering and the scale of the graph. {In this work, we identify four key properties that graph PEs should satisfy}: stability, expressive power, scalability, and genericness. We find that existing eigenvector-based PE methods often fall short of jointly satisfying these criteria. To address this gap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our primary insight is that message-passing GNNs function as nonlinear mappings of eigenvectors, enabling the design of GNN architectures for generating powerful and efficient PEs. A crucial challenge lies in initializing node attributes in a manner that is both expressive and permutation equivariant. We tackle this by initializing GNNs with random node inputs or standard basis vectors, thereby unlocking the expressive power of message-passing operations, while employing statistical pooling functions to maintain permutation equivariance. Our analysis demonstrates that PEARL approximates equivariant functions of eigenvectors with linear complexity, while rigorously establishing its stability and high expressive power. Experimental evaluations show that PEARL outperforms lightweight versions of eigenvector-based PEs and achieves comparable performance to full eigenvector-based PEs, but with one or two orders of magnitude lower complexity. Our code is available at https://github.com/ehejin/Pearl-PE.

### Large Language Model-Enhanced Multi-Armed Bandits 
[[arxiv](https://arxiv.org/abs/2502.01118)] [[cool](https://papers.cool/arxiv/2502.01118)] [[pdf](https://arxiv.org/pdf/2502.01118)]
> **Authors**: Jiahang Sun,Zhiyong Wang,Runhan Yang,Chenjun Xiao,John C. S. Lui,Zhongxiang Dai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) have been adopted to solve sequential decision-making tasks such as multi-armed bandits (MAB), in which an LLM is directly instructed to select the arms to pull in every iteration. However, this paradigm of direct arm selection using LLMs has been shown to be suboptimal in many MAB tasks. Therefore, we propose an alternative approach which combines the strengths of classical MAB and LLMs. Specifically, we adopt a classical MAB algorithm as the high-level framework and leverage the strong in-context learning capability of LLMs to perform the sub-task of reward prediction. Firstly, we incorporate the LLM-based reward predictor into the classical Thompson sampling (TS) algorithm and adopt a decaying schedule for the LLM temperature to ensure a transition from exploration to exploitation. Next, we incorporate the LLM-based reward predictor (with a temperature of 0) into a regression oracle-based MAB algorithm equipped with an explicit exploration mechanism. We also extend our TS-based algorithm to dueling bandits where only the preference feedback between pairs of arms is available, which requires non-trivial algorithmic modifications. We conduct empirical evaluations using both synthetic MAB tasks and experiments designed using real-world text datasets, in which the results show that our algorithms consistently outperform previous baseline methods based on direct arm selection. Interestingly, we also demonstrate that in challenging tasks where the arms lack semantic meanings that can be exploited by the LLM, our approach achieves considerably better performance than LLM-based direct arm selection.

### Learning to Learn Weight Generation via Trajectory Diffusion 
[[arxiv](https://arxiv.org/abs/2502.01117)] [[cool](https://papers.cool/arxiv/2502.01117)] [[pdf](https://arxiv.org/pdf/2502.01117)]
> **Authors**: Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Serge Belongie,Jenq-Neng Hwang,Lei Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Di's higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning.Our code is released at https://github.com/tuantuange/Lt-Di.

### Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings 
[[arxiv](https://arxiv.org/abs/2502.01108)] [[cool](https://papers.cool/arxiv/2502.01108)] [[pdf](https://arxiv.org/pdf/2502.01108)]
> **Authors**: Mithun Saha,Maxwell A. Xu,Wanting Mao,Sameer Neupane,James M. Rehg,Santosh Kumar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: The first two listed authors contributed equally to this research
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: Photoplethysmography (PPG)-based foundation models are gaining traction due to the widespread use of PPG in biosignal monitoring and their potential to generalize across diverse health applications. In this paper, we introduce Pulse-PPG, the first open-source PPG foundation model trained exclusively on raw PPG data collected over a 100-day field study with 120 participants. Existing PPG foundation models are either open-source but trained on clinical data or closed-source, limiting their applicability in real-world settings. We evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its performance against a state-of-the-art foundation model trained on clinical data. Our results demonstrate that Pulse-PPG, trained on uncurated field data, exhibits superior generalization across clinical and mobile health applications in both lab and field settings. This suggests that exposure to real-world variability enables the model to learn fine-grained representations, making it more adaptable across tasks. Furthermore, pre-training on field data surprisingly outperforms its pre-training on clinical data in many tasks, reinforcing the importance of training on real-world, diverse datasets. To encourage further advancements in robust foundation models leveraging field data, we plan to release Pulse-PPG, providing researchers with a powerful resource for developing more generalizable PPG-based models.

### GTG: Generalizable Trajectory Generation Model for Urban Mobility 
[[arxiv](https://arxiv.org/abs/2502.01107)] [[cool](https://papers.cool/arxiv/2502.01107)] [[pdf](https://arxiv.org/pdf/2502.01107)]
> **Authors**: Jingyuan Wang,Yujing Lin,Yudong Li
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 12 pages, 5 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Trajectory data mining is crucial for smart city management. However, collecting large-scale trajectory datasets is challenging due to factors such as commercial conflicts and privacy regulations. Therefore, we urgently need trajectory generation techniques to address this issue. Existing trajectory generation methods rely on the global road network structure of cities. When the road network structure changes, these methods are often not transferable to other cities. In fact, there exist invariant mobility patterns between different cities: 1) People prefer paths with the minimal travel cost; 2) The travel cost of roads has an invariant relationship with the topological features of the road network. Based on the above insight, this paper proposes a Generalizable Trajectory Generation model (GTG). The model consists of three parts: 1) Extracting city-invariant road representation based on Space Syntax method; 2) Cross-city travel cost prediction through disentangled adversarial training; 3) Travel preference learning by shortest path search and preference update. By learning invariant movement patterns, the model is capable of generating trajectories in new cities. Experiments on three datasets demonstrates that our model significantly outperforms existing models in terms of generalization ability.

### Can We Validate Counterfactual Estimations in the Presence of General Network Interference? 
[[arxiv](https://arxiv.org/abs/2502.01106)] [[cool](https://papers.cool/arxiv/2502.01106)] [[pdf](https://arxiv.org/pdf/2502.01106)]
> **Authors**: Sadegh Shirani,Yuwei Luo,William Overman,Ruoxuan Xiong,Mohsen Bayati
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计量经济学,方法论,机器学习
- **Abstract**: In experimental settings with network interference, a unit's treatment can influence outcomes of other units, challenging both causal effect estimation and its validation. Classic validation approaches fail as outcomes are only observable under one treatment scenario and exhibit complex correlation patterns due to interference. To address these challenges, we introduce a new framework enabling cross-validation for counterfactual estimation. At its core is our distribution-preserving network bootstrap method -- a theoretically-grounded approach inspired by approximate message passing. This method creates multiple subpopulations while preserving the underlying distribution of network effects. We extend recent causal message-passing developments by incorporating heterogeneous unit-level characteristics and varying local interactions, ensuring reliable finite-sample performance through non-asymptotic analysis. We also develop and publicly release a comprehensive benchmark toolbox with diverse experimental environments, from networks of interacting AI agents to opinion formation in real-world communities and ride-sharing applications. These environments provide known ground truth values while maintaining realistic complexities, enabling systematic examination of causal inference methods. Extensive evaluation across these environments demonstrates our method's robustness to diverse forms of network interference. Our work provides researchers with both a practical estimation framework and a standardized platform for testing future methodological developments.

### Federated Linear Dueling Bandits 
[[arxiv](https://arxiv.org/abs/2502.01085)] [[cool](https://papers.cool/arxiv/2502.01085)] [[pdf](https://arxiv.org/pdf/2502.01085)]
> **Authors**: Xuhan Huang,Yan Hu,Zhiyan Li,Zhiyong Wang,Benyou Wang,Zhongxiang Dai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Contextual linear dueling bandits have recently garnered significant attention due to their widespread applications in important domains such as recommender systems and large language models. Classical dueling bandit algorithms are typically only applicable to a single agent. However, many applications of dueling bandits involve multiple agents who wish to collaborate for improved performance yet are unwilling to share their data. This motivates us to draw inspirations from federated learning, which involves multiple agents aiming to collaboratively train their neural networks via gradient descent (GD) without sharing their raw data. Previous works have developed federated linear bandit algorithms which rely on closed-form updates of the bandit parameters (e.g., the linear function parameter) to achieve collaboration. However, in linear dueling bandits, the linear function parameter lacks a closed-form expression and its estimation requires minimizing a loss function. This renders these previous methods inapplicable. In this work, we overcome this challenge through an innovative and principled combination of online gradient descent (for minimizing the loss function to estimate the linear function parameters) and federated learning, hence introducing the first federated linear dueling bandit algorithms. Through rigorous theoretical analysis, we prove that our algorithms enjoy a sub-linear upper bound on its cumulative regret. We also use empirical experiments to demonstrate the effectiveness of our algorithms and the practical benefit of collaboration.

### Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis 
[[arxiv](https://arxiv.org/abs/2502.01084)] [[cool](https://papers.cool/arxiv/2502.01084)] [[pdf](https://arxiv.org/pdf/2502.01084)]
> **Authors**: Weiwei Lin,Chenghan He
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,声音,音频和语音处理
- **Abstract**: We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.

### Tool Unlearning for Tool-Augmented LLMs 
[[arxiv](https://arxiv.org/abs/2502.01083)] [[cool](https://papers.cool/arxiv/2502.01083)] [[pdf](https://arxiv.org/pdf/2502.01083)]
> **Authors**: Jiali Cheng,Hadi Amiri
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: https://clu-uml.github.io/MU-Bench-Project-Page/
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to forget learned tools due to security vulnerabilities, privacy regulations, or tool deprecations. However, ``tool unlearning'' has not been investigated in unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete, the first approach for unlearning tools from tool-augmented LLMs. It implements three key properties to address the above challenges for effective tool unlearning and introduces a new membership inference attack (MIA) model for effective evaluation. Extensive experiments on multiple tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns randomly selected tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks.

### qNBO: quasi-Newton Meets Bilevel Optimization 
[[arxiv](https://arxiv.org/abs/2502.01076)] [[cool](https://papers.cool/arxiv/2502.01076)] [[pdf](https://arxiv.org/pdf/2502.01076)]
> **Authors**: Sheng Fang,Yong-Jin Liu,Wei Yao,Chengming Yu,Jin Zhang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Bilevel optimization, addressing challenges in hierarchical learning tasks, has gained significant interest in machine learning. The practical implementation of the gradient descent method to bilevel optimization encounters computational hurdles, notably the computation of the exact lower-level solution and the inverse Hessian of the lower-level objective. Although these two aspects are inherently connected, existing methods typically handle them separately by solving the lower-level problem and a linear system for the inverse Hessian-vector product. In this paper, we introduce a general framework to address these computational challenges in a coordinated manner. Specifically, we leverage quasi-Newton algorithms to accelerate the resolution of the lower-level problem while efficiently approximating the inverse Hessian-vector product. Furthermore, by exploiting the superlinear convergence properties of BFGS, we establish the non-asymptotic convergence analysis of the BFGS adaptation within our framework. Numerical experiments demonstrate the comparable or superior performance of the proposed algorithms in real-world learning tasks, including hyperparameter optimization, data hyper-cleaning, and few-shot meta-learning.

### Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks 
[[arxiv](https://arxiv.org/abs/2502.01074)] [[cool](https://papers.cool/arxiv/2502.01074)] [[pdf](https://arxiv.org/pdf/2502.01074)]
> **Authors**: Chengxin Hu,Hao Li,Yihe Yuan,Zezheng Song,Haixin Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 30 pages, 13 figures, 7 tables, paper under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The dataset, code and weights of the powerful AI-driven chemistry generalist are open-sourced.

### An Investigation of FP8 Across Accelerators for LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.01070)] [[cool](https://papers.cool/arxiv/2502.01070)] [[pdf](https://arxiv.org/pdf/2502.01070)]
> **Authors**: Jiwoo Kim,Joonhyung Lee,Gunho Park,Byeongwook Kim,Se Jung Kwon,Dongsoo Lee,Youngjoo Lee
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,表现
- **Abstract**: The introduction of 8-bit floating-point (FP8) computation units in modern AI accelerators has generated significant interest in FP8-based large language model (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep learning requires a shared scaling factor. Additionally, while E4M3 and E5M2 are well-defined at the individual value level, their scaling and accumulation methods remain unspecified and vary across hardware and software implementations. As a result, FP8 behaves more like a quantization format than a standard numeric representation. In this work, we provide the first comprehensive analysis of FP8 computation and acceleration on two AI accelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that the Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency during LLM inference, offering valuable insights into the practical implications of FP8 adoption for datacenter-scale LLM serving.

### FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation 
[[arxiv](https://arxiv.org/abs/2502.01068)] [[cool](https://papers.cool/arxiv/2502.01068)] [[pdf](https://arxiv.org/pdf/2502.01068)]
> **Authors**: Dongwon Jo,Jiwon Song,Yulhwa Kim,Jae-Joon Kim
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\times$ and 1.40$\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.

### Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits with Known Optimality Gap 
[[arxiv](https://arxiv.org/abs/2502.01067)] [[cool](https://papers.cool/arxiv/2502.01067)] [[pdf](https://arxiv.org/pdf/2502.01067)]
> **Authors**: Nikolai Karpov,Chen Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: AAAI 2025
- **标题**: None
- **领域**: 机器学习,数据结构和算法
- **Abstract**: We investigate the sample-memory-pass trade-offs for pure exploration in multi-pass streaming multi-armed bandits (MABs) with the *a priori* knowledge of the optimality gap $Δ_{[2]}$. Here, and throughout, the optimality gap $Δ_{[i]}$ is defined as the mean reward gap between the best and the $i$-th best arms. A recent line of results by Jin, Huang, Tang, and Xiao [ICML'21] and Assadi and Wang [COLT'24] have shown that if there is no known $Δ_{[2]}$, a pass complexity of $Θ(\log(1/Δ_{[2]}))$ (up to $\log\log(1/Δ_{[2]})$ terms) is necessary and sufficient to obtain the *worst-case optimal* sample complexity of $O(n/Δ^{2}_{[2]})$ with a single-arm memory. However, our understanding of multi-pass algorithms with known $Δ_{[2]}$ is still limited. Here, the key open problem is how many passes are required to achieve the complexity, i.e., $O( \sum_{i=2}^{n}1/Δ^2_{[i]})$ arm pulls, with a sublinear memory size. In this work, we show that the ``right answer'' for the question is $Θ(\log{n})$ passes (up to $\log\log{n}$ terms). We first present a lower bound, showing that any algorithm that finds the best arm with slightly sublinear memory -- a memory of $o({n}/{\text{polylog}({n})})$ arms -- and $O(\sum_{i=2}^{n}{1}/{Δ^{2}_{[i]}}\cdot \log{(n)})$ arm pulls has to make $Ω(\frac{\log{n}}{\log\log{n}})$ passes over the stream. We then show a nearly-matching algorithm that assuming the knowledge of $Δ_{[2]}$, finds the best arm with $O( \sum_{i=2}^{n}1/Δ^2_{[i]} \cdot \log{n})$ arm pulls and a *single arm* memory.

### Learning Nonlinearity of Boolean Functions: An Experimentation with Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01060)] [[cool](https://papers.cool/arxiv/2502.01060)] [[pdf](https://arxiv.org/pdf/2502.01060)]
> **Authors**: Sriram Ranga,Nandish Chattopadhyay,Anupam Chattopadhyay
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: To be published in International conference on Artificial Intelligence and Sustainable Computing, AISC 2024
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: This paper investigates the learnability of the nonlinearity property of Boolean functions using neural networks. We train encoder style deep neural networks to learn to predict the nonlinearity of Boolean functions from examples of functions in the form of a truth table and their corresponding nonlinearity values. We report empirical results to show that deep neural networks are able to learn to predict the property for functions in 4 and 5 variables with an accuracy above 95%. While these results are positive and a disciplined analysis is being presented for the first time in this regard, we should also underline the statutory warning that it seems quite challenging to extend the idea to higher number of variables, and it is also not clear whether one can get advantage in terms of time and space complexity over the existing combinatorial algorithms.

### Internal Activation as the Polar Star for Steering Unsafe LLM Behavior 
[[arxiv](https://arxiv.org/abs/2502.01042)] [[cool](https://papers.cool/arxiv/2502.01042)] [[pdf](https://arxiv.org/pdf/2502.01042)]
> **Authors**: Peixuan Han,Cheng Qian,Xiusi Chen,Yuji Zhang,Denghui Zhang,Heng Ji
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks but also pose significant risks due to their potential to generate harmful content. Although existing safety mechanisms can improve model safety, they often lead to overly cautious behavior and fail to fully utilize LLMs' internal cognitive processes. Drawing inspiration from cognitive science, where humans rely on reflective reasoning (System 2 thinking) to regulate language and behavior, we empirically demonstrate that LLMs also possess a similar capacity for internal assessment and regulation, which can be actively detected. Building on this insight, we introduce SafeSwitch, a framework that dynamically regulates unsafe outputs by monitoring and utilizing the model's internal states. Our empirical results show that SafeSwitch reduces harmful outputs by over 80% on safety benchmarks while maintaining strong utility. Compared to traditional safety alignment methods, SafeSwitch delivers more informative and context-aware refusals, demonstrates resilience to unseen queries, and achieves these benefits while only tuning less than 6% of the original parameters. These features make SafeSwitch a promising approach for implementing nuanced safety controls in LLMs.

### Geoinformatics-Guided Machine Learning for Power Plant Classification 
[[arxiv](https://arxiv.org/abs/2502.01039)] [[cool](https://papers.cool/arxiv/2502.01039)] [[pdf](https://arxiv.org/pdf/2502.01039)]
> **Authors**: Blessing Austin-Gabriel,Aparna S. Varde,Hao Liu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: :I.2.6; I.2.m; J.2
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper proposes an approach in the area of Knowledge-Guided Machine Learning (KGML) via a novel integrated framework comprising CNN (Convolutional Neural Networks) and ViT (Vision Transformers) along with GIS (Geographic Information Systems) to enhance power plant classification in the context of energy management. Knowledge from geoinformatics derived through Spatial Masks (SM) in GIS is infused into an architecture of CNN and ViT, in this proposed KGML approach. It is found to provide much better performance compared to the baseline of CNN and ViT only in the classification of multiple types of power plants from real satellite imagery, hence emphasizing the vital role of the geoinformatics-guided approach. This work makes a contribution to the main theme of KGML that can be beneficial in many AI systems today. It makes broader impacts on AI in Smart Cities, and Environmental Computing.

### eagle: early approximated gradient based learning rate estimator 
[[arxiv](https://arxiv.org/abs/2502.01036)] [[cool](https://papers.cool/arxiv/2502.01036)] [[pdf](https://arxiv.org/pdf/2502.01036)]
> **Authors**: Takumi Fujimoto,Hiroaki Nishi
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 43pages, 24figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We propose EAGLE update rule, a novel optimization method that accelerates loss convergence during the early stages of training by leveraging both current and previous step parameter and gradient values. The update algorithm estimates optimal parameters by computing the changes in parameters and gradients between consecutive training steps and leveraging the local curvature of the loss landscape derived from these changes. However, this update rule has potential instability, and to address that, we introduce an adaptive switching mechanism that dynamically selects between Adam and EAGLE update rules to enhance training stability. Experiments on standard benchmark datasets demonstrate that EAGLE optimizer, which combines this novel update rule with the switching mechanism achieves rapid training loss convergence with fewer epochs, compared to conventional optimization methods.

### Converting MLPs into Polynomials in Closed Form 
[[arxiv](https://arxiv.org/abs/2502.01032)] [[cool](https://papers.cool/arxiv/2502.01032)] [[pdf](https://arxiv.org/pdf/2502.01032)]
> **Authors**: Nora Belrose,Alice Rigg
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent work has shown that purely quadratic functions can replace MLPs in transformers with no significant loss in performance, while enabling new methods of interpretability based on linear algebra. In this work, we theoretically derive closed-form least-squares optimal approximations of feedforward networks (multilayer perceptrons and gated linear units) using polynomial functions of arbitrary degree. When the $R^2$ is high, this allows us to interpret MLPs and GLUs by visualizing the eigendecomposition of the coefficients of their linear and quadratic approximants. We also show that these approximants can be used to create SVD-based adversarial examples. By tracing the $R^2$ of linear and quadratic approximants across training time, we find new evidence that networks start out simple, and get progressively more complex. Even at the end of training, however, our quadratic approximants explain over 95% of the variance in network outputs.

### DiffIM: Differentiable Influence Minimization with Surrogate Modeling and Continuous Relaxation 
[[arxiv](https://arxiv.org/abs/2502.01031)] [[cool](https://papers.cool/arxiv/2502.01031)] [[pdf](https://arxiv.org/pdf/2502.01031)]
> **Authors**: Junghun Lee,Hyunju Kim,Fanchen Bu,Jihoon Ko,Kijung Shin
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Accepted to AAAI'25
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: In social networks, people influence each other through social links, which can be represented as propagation among nodes in graphs. Influence minimization (IMIN) is the problem of manipulating the structures of an input graph (e.g., removing edges) to reduce the propagation among nodes. IMIN can represent time-critical real-world applications, such as rumor blocking, but IMIN is theoretically difficult and computationally expensive. Moreover, the discrete nature of IMIN hinders the usage of powerful machine learning techniques, which requires differentiable computation. In this work, we propose DiffIM, a novel method for IMIN with two differentiable schemes for acceleration: (1) surrogate modeling for efficient influence estimation, which avoids time-consuming simulations (e.g., Monte Carlo), and (2) the continuous relaxation of decisions, which avoids the evaluation of individual discrete decisions (e.g., removing an edge). We further propose a third accelerating scheme, gradient-driven selection, that chooses edges instantly based on gradients without optimization (spec., gradient descent iterations) on each test instance. Through extensive experiments on real-world graphs, we show that each proposed scheme significantly improves speed with little (or even no) IMIN performance degradation. Our method is Pareto-optimal (i.e., no baseline is faster and more effective than it) and typically several orders of magnitude (spec., up to 15,160X) faster than the most effective baseline while being more effective.

### Comprehensive Modeling Approaches for Forecasting Bitcoin Transaction Fees: A Comparative Study 
[[arxiv](https://arxiv.org/abs/2502.01029)] [[cool](https://papers.cool/arxiv/2502.01029)] [[pdf](https://arxiv.org/pdf/2502.01029)]
> **Authors**: Jiangqin Ma,Erfan Mahmoudinia
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transaction fee prediction in Bitcoin's ecosystem represents a crucial challenge affecting both user costs and miner revenue optimization. This study presents a systematic evaluation of six predictive models for forecasting Bitcoin transaction fees across a 24-hour horizon (144 blocks): SARIMAX, Prophet, Time2Vec, Time2Vec with Attention, a Hybrid model combining SARIMAX with Gradient Boosting, and the Temporal Fusion Transformer (TFT). Our approach integrates comprehensive feature engineering spanning mempool metrics, network parameters, and historical fee patterns to capture the multifaceted dynamics of fee behavior. Through rigorous 5-fold cross-validation and independent testing, our analysis reveals that traditional statistical approaches outperform more complex deep learning architectures. The SARIMAX model achieves superior accuracy on the independent test set, while Prophet demonstrates strong performance during cross-validation. Notably, sophisticated deep learning models like Time2Vec and TFT show comparatively lower predictive power despite their architectural complexity. This performance disparity likely stems from the relatively constrained training dataset of 91 days, suggesting that deep learning models may achieve enhanced results with extended historical data. These findings offer significant practical implications for cryptocurrency stakeholders, providing empirically-validated guidance for fee-sensitive decision making while illuminating critical considerations in model selection based on data constraints. The study establishes a foundation for advanced fee prediction while highlighting the current advantages of traditional statistical methods in this domain.

### Efficient Model Editing with Task Vector Bases: A Theoretical Framework and Scalable Approach 
[[arxiv](https://arxiv.org/abs/2502.01015)] [[cool](https://papers.cool/arxiv/2502.01015)] [[pdf](https://arxiv.org/pdf/2502.01015)]
> **Authors**: Siqi Zeng,Yifei He,Weiqiu You,Yifan Hao,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 25 pages, 11 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Task vectors, which are derived from the difference between pre-trained and fine-tuned model weights, enable flexible task adaptation and model merging through arithmetic operations such as addition and negation. However, existing approaches often rely on heuristics with limited theoretical support, often leading to performance gaps comparing to direct task fine tuning. Meanwhile, although it is easy to manipulate saved task vectors with arithmetic for different purposes, such compositional flexibility demands high memory usage, especially when dealing with a huge number of tasks, limiting scalability. This work addresses these issues with a theoretically grounded framework that explains task vector arithmetic and introduces the task vector bases framework. Building upon existing task arithmetic literature, our method significantly reduces the memory cost for downstream arithmetic with little effort, while achieving competitive performance and maintaining compositional advantage, providing a practical solution for large-scale task arithmetic.

### Refining Adaptive Zeroth-Order Optimization at Ease 
[[arxiv](https://arxiv.org/abs/2502.01014)] [[cool](https://papers.cool/arxiv/2502.01014)] [[pdf](https://arxiv.org/pdf/2502.01014)]
> **Authors**: Yao Shu,Qixin Zhang,Kun He,Zhongxiang Dai
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces Refined Adaptive Zeroth-Order Optimization (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show (I) the first analysis to the variance reduction of first moment estimate in ZO optimization, (II) the improved second moment estimates with a more accurate approximation of its variance-free ideal, (III) the first variance-aware convergence framework for adaptive ZO methods, which may be of independent interest, and (IV) the faster convergence of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges.

### Deep Active Learning based Experimental Design to Uncover Synergistic Genetic Interactions for Host Targeted Therapeutics 
[[arxiv](https://arxiv.org/abs/2502.01012)] [[cool](https://papers.cool/arxiv/2502.01012)] [[pdf](https://arxiv.org/pdf/2502.01012)]
> **Authors**: Haonan Zhu,Mary Silva,Jose Cadena,Braden Soper,Michał Lisicki,Braian Peetoom,Sergio E. Baranzini,Shivshankar Sundaram,Priyadip Ray,Jeff Drocco
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法,方法论
- **Abstract**: Recent technological advances have introduced new high-throughput methods for studying host-virus interactions, but testing synergistic interactions between host gene pairs during infection remains relatively slow and labor intensive. Identification of multiple gene knockdowns that effectively inhibit viral replication requires a search over the combinatorial space of all possible target gene pairs and is infeasible via brute-force experiments. Although active learning methods for sequential experimental design have shown promise, existing approaches have generally been restricted to single-gene knockdowns or small-scale double knockdown datasets. In this study, we present an integrated Deep Active Learning (DeepAL) framework that incorporates information from a biological knowledge graph (SPOKE, the Scalable Precision Medicine Open Knowledge Engine) to efficiently search the configuration space of a large dataset of all pairwise knockdowns of 356 human genes in HIV infection. Through graph representation learning, the framework is able to generate task-specific representations of genes while also balancing the exploration-exploitation trade-off to pinpoint highly effective double-knockdown pairs. We additionally present an ensemble method for uncertainty quantification and an interpretation of the gene pairs selected by our algorithm via pathway analysis. To our knowledge, this is the first work to show promising results on double-gene knockdown experimental data of appreciable scale (356 by 356 matrix).

### CausalCOMRL: Context-Based Offline Meta-Reinforcement Learning with Causal Representation 
[[arxiv](https://arxiv.org/abs/2502.00983)] [[cool](https://papers.cool/arxiv/2502.00983)] [[pdf](https://arxiv.org/pdf/2502.00983)]
> **Authors**: Zhengzhe Zhang,Wenjia Meng,Haoliang Sun,Gang Pan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Context-based offline meta-reinforcement learning (OMRL) methods have achieved appealing success by leveraging pre-collected offline datasets to develop task representations that guide policy learning. However, current context-based OMRL methods often introduce spurious correlations, where task components are incorrectly correlated due to confounders. These correlations can degrade policy performance when the confounders in the test task differ from those in the training task. To address this problem, we propose CausalCOMRL, a context-based OMRL method that integrates causal representation learning. This approach uncovers causal relationships among the task components and incorporates the causal relationships into task representations, enhancing the generalizability of RL agents. We further improve the distinction of task representations from different tasks by using mutual information optimization and contrastive learning. Utilizing these causal task representations, we employ SAC to optimize policies on meta-RL benchmarks. Experimental results show that CausalCOMRL achieves better performance than other methods on most benchmarks.

### Forecasting VIX using interpretable Kolmogorov-Arnold networks 
[[arxiv](https://arxiv.org/abs/2502.00980)] [[cool](https://papers.cool/arxiv/2502.00980)] [[pdf](https://arxiv.org/pdf/2502.00980)]
> **Authors**: So-Yoon Cho,Sungchul Lee,Hyun-Gyoon Kim
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **Abstract**: This paper presents the use of Kolmogorov-Arnold Networks (KANs) for forecasting the CBOE Volatility Index (VIX). Unlike traditional MLP-based neural networks that are often criticized for their black-box nature, KAN offers an interpretable approach via learnable spline-based activation functions and symbolification. Based on a parsimonious architecture with symbolic functions, KAN expresses a forecast of the VIX as a closed-form in terms of explanatory variables, and provide interpretable insights into key characteristics of the VIX, including mean reversion and the leverage effect. Through in-depth empirical analysis across multiple datasets and periods, we show that KANs achieve competitive forecasting performance while requiring significantly fewer parameters compared to MLP-based neural network models. Our findings demonstrate the capacity and potential of KAN as an interpretable financial time-series forecasting method.

### A Wearable Device Dataset for Mental Health Assessment Using Laser Doppler Flowmetry and Fluorescence Spectroscopy Sensors 
[[arxiv](https://arxiv.org/abs/2502.00973)] [[cool](https://papers.cool/arxiv/2502.00973)] [[pdf](https://arxiv.org/pdf/2502.00973)]
> **Authors**: Minh Ngoc Nguyen,Khai Le-Duc,Tan-Hanh Pham,Trang Nguyen,Quang Minh Luu,Ba Kien Tran,Truong-Son Hy,Viktor Dremin,Sergei Sokolovsky,Edik Rafailov
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Preprint, 55 pages
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: In this study, we introduce a novel method to predict mental health by building machine learning models for a non-invasive wearable device equipped with Laser Doppler Flowmetry (LDF) and Fluorescence Spectroscopy (FS) sensors. Besides, we present the corresponding dataset to predict mental health, e.g. depression, anxiety, and stress levels via the DAS-21 questionnaire. To our best knowledge, this is the world's largest and the most generalized dataset ever collected for both LDF and FS studies. The device captures cutaneous blood microcirculation parameters, and wavelet analysis of the LDF signal extracts key rhythmic oscillations. The dataset, collected from 132 volunteers aged 18-94 from 19 countries, explores relationships between physiological features, demographics, lifestyle habits, and health conditions. We employed a variety of machine learning methods to classify stress detection, in which LightGBM is identified as the most effective model for stress detection, achieving a ROC AUC of 0.7168 and a PR AUC of 0.8852. In addition, we also incorporated Explainable Artificial Intelligence (XAI) techniques into our analysis to investigate deeper insights into the model's predictions. Our results suggest that females, younger individuals and those with a higher Body Mass Index (BMI) or heart rate have a greater likelihood of experiencing mental health conditions like stress and anxiety. All related code and data are published online: https://github.com/leduckhai/Wearable_LDF-FS.

### PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs 
[[arxiv](https://arxiv.org/abs/2502.00963)] [[cool](https://papers.cool/arxiv/2502.00963)] [[pdf](https://arxiv.org/pdf/2502.00963)]
> **Authors**: Mauricio Soroco,Jialin Song,Mengzhou Xia,Kye Emond,Weiran Sun,Wuyang Chen
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms prompting the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We will release all data, model checkpoints, and code at https://pde-controller.github.io/.

### Analysis of static and dynamic batching algorithms for graph neural networks 
[[arxiv](https://arxiv.org/abs/2502.00944)] [[cool](https://papers.cool/arxiv/2502.00944)] [[pdf](https://arxiv.org/pdf/2502.00944)]
> **Authors**: Daniel Speckhard,Tim Bechtel,Sebastian Kehl,Jonathan Godwin,Claudia Draxl
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph neural networks (GNN) have shown promising results for several domains such as materials science, chemistry, and the social sciences. GNN models often contain millions of parameters, and like other neural network (NN) models, are often fed only a fraction of the graphs that make up the training dataset in batches to update model parameters. The effect of batching algorithms on training time and model performance has been thoroughly explored for NNs but not yet for GNNs. We analyze two different batching algorithms for graph based models, namely static and dynamic batching. We use the Jraph library built on JAX to perform our experiments, where we compare the two batching methods for two datasets, the QM9 dataset of small molecules and the AFLOW materials database. Our experiments show that significant training time savings can be found from changing the batching algorithm, but the fastest algorithm depends on the data, model, batch size and number of training steps run. Experiments show no significant difference in model learning between the algorithms.

### Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.00922)] [[cool](https://papers.cool/arxiv/2502.00922)] [[pdf](https://arxiv.org/pdf/2502.00922)]
> **Authors**: Patrick Yubeaton,Tareq Mahmoud,Shehab Naga,Pooria Taheri,Tianhua Xia,Arun George,Yasmein Khalil,Sai Qian Zhang,Siddharth Joshi,Chinmay Hegde,Siddharth Garg
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,硬件架构
- **Abstract**: As they become more capable, large language models (LLMs) have continued to rapidly increase in size. This has exacerbated the difficulty in running state of the art LLMs on small, edge devices. Standard techniques advocate solving this problem through lossy compression techniques such as quantization or pruning. However, such compression techniques are lossy, and have been shown to change model behavior in unpredictable manners. We propose Huff-LLM, an \emph{end-to-end, lossless} model compression method that lets users store LLM weights in compressed format \emph{everywhere} -- cloud, disk, main memory, and even in on-chip memory/buffers. This allows us to not only load larger models in main memory, but also reduces bandwidth required to load weights on chip, and makes more efficient use of on-chip weight buffers. In addition to the memory savings achieved via compression, we also show latency and energy efficiency improvements when performing inference with the compressed model.

### Blink of an eye: a simple theory for feature localization in generative models 
[[arxiv](https://arxiv.org/abs/2502.00921)] [[cool](https://papers.cool/arxiv/2502.00921)] [[pdf](https://arxiv.org/pdf/2502.00921)]
> **Authors**: Marvin Li,Aayush Karan,Sitan Chen
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) can exhibit undesirable and unexpected behavior in the blink of an eye. In a recent Anthropic demo, Claude switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow ``critical windows'' of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. We show that it emerges generically as the generation process localizes to a sub-population of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes no distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic tools and no stochastic calculus or statistical physics-based machinery. We also identify an intriguing connection to the all-or-nothing phenomenon from statistical inference. Finally, we validate our predictions empirically for LLMs and find that critical windows often coincide with failures in problem solving for various math and reasoning benchmarks.

### Fundamental limits of learning in sequence multi-index models and deep attention networks: High-dimensional asymptotics and sharp thresholds 
[[arxiv](https://arxiv.org/abs/2502.00901)] [[cool](https://papers.cool/arxiv/2502.00901)] [[pdf](https://arxiv.org/pdf/2502.00901)]
> **Authors**: Emanuele Troiani,Hugo Cui,Yatin Dandi,Florent Krzakala,Lenka Zdeborová
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络
- **Abstract**: In this manuscript, we study the learning of deep attention neural networks, defined as the composition of multiple self-attention layers, with tied and low-rank weights. We first establish a mapping of such models to sequence multi-index models, a generalization of the widely studied multi-index model to sequential covariates, for which we establish a number of general results. In the context of Bayesian-optimal learning, in the limit of large dimension $D$ and commensurably large number of samples $N$, we derive a sharp asymptotic characterization of the optimal performance as well as the performance of the best-known polynomial-time algorithm for this setting --namely approximate message-passing--, and characterize sharp thresholds on the minimal sample complexity required for better-than-random prediction performance. Our analysis uncovers, in particular, how the different layers are learned sequentially. Finally, we discuss how this sequential learning can also be observed in a realistic setup.

### Multi-frequency wavefield solutions for variable velocity models using meta-learning enhanced low-rank physics-informed neural network 
[[arxiv](https://arxiv.org/abs/2502.00897)] [[cool](https://papers.cool/arxiv/2502.00897)] [[pdf](https://arxiv.org/pdf/2502.00897)]
> **Authors**: Shijun Cheng,Tariq Alkhalifah
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,地球物理学
- **Abstract**: Physics-informed neural networks (PINNs) face significant challenges in modeling multi-frequency wavefields in complex velocity models due to their slow convergence, difficulty in representing high-frequency details, and lack of generalization to varying frequencies and velocity scenarios. To address these issues, we propose Meta-LRPINN, a novel framework that combines low-rank parameterization using singular value decomposition (SVD) with meta-learning and frequency embedding. Specifically, we decompose the weights of PINN's hidden layers using SVD and introduce an innovative frequency embedding hypernetwork (FEH) that links input frequencies with the singular values, enabling efficient and frequency-adaptive wavefield representation. Meta-learning is employed to provide robust initialization, improving optimization stability and reducing training time. Additionally, we implement adaptive rank reduction and FEH pruning during the meta-testing phase to further enhance efficiency. Numerical experiments, which are presented on multi-frequency scattered wavefields for different velocity models, demonstrate that Meta-LRPINN achieves much fast convergence speed and much high accuracy compared to baseline methods such as Meta-PINN and vanilla PINN. Also, the proposed framework shows strong generalization to out-of-distribution frequencies while maintaining computational efficiency. These results highlight the potential of our Meta-LRPINN for scalable and adaptable seismic wavefield modeling.

### SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters 
[[arxiv](https://arxiv.org/abs/2502.00883)] [[cool](https://papers.cool/arxiv/2502.00883)] [[pdf](https://arxiv.org/pdf/2502.00883)]
> **Authors**: Teng Xiao,Yige Yuan,Zhengyu Chen,Mingxiao Li,Shangsong Liang,Zhaochun Ren,Vasant G Honavar
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches-even without any hyperparameters or a reference model . For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: https://github.com/tengxiao1/SimPER.

### Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing 
[[arxiv](https://arxiv.org/abs/2502.00882)] [[cool](https://papers.cool/arxiv/2502.00882)] [[pdf](https://arxiv.org/pdf/2502.00882)]
> **Authors**: Gil Goldshlager,Jiang Hu,Lin Lin
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 25 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,数值分析,优化与控制,机器学习
- **Abstract**: Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time. In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps. To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a $\textit{weighted}$ least-squares solution. Unfortunately, for general problems the condition number of the weight matrix and the variance of the iterates can become arbitrarily large. We resolve these issues by incorporating regularization into the RBK iterations. Numerical experiments, including examples arising from natural gradient optimization, suggest that the regularized algorithm, ReBlocK, outperforms minibatch stochastic gradient descent for realistic problems that exhibit fast singular value decay.

### Towards Automation of Cognitive Modeling using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00879)] [[cool](https://papers.cool/arxiv/2502.00879)] [[pdf](https://arxiv.org/pdf/2502.00879)]
> **Authors**: Milena Rmus,Akshay K. Jagadish,Marvin Mathony,Tobias Ludwig,Eric Schulz
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. Previous work has demonstrated that Large Language Models (LLMs) are adept at pattern recognition in-context, solving complex problems, and generating executable code. In this work, we leverage these abilities to explore the potential of LLMs in automating the generation of cognitive models based on behavioral data. We evaluated the LLM in two different tasks: model identification (relating data to a source model), and model generation (generating the underlying cognitive model). We performed these tasks across two cognitive domains - decision making and learning. In the case of data simulated from canonical cognitive models, we found that the LLM successfully identified and generated the ground truth model. In the case of human data, where behavioral noise and lack of knowledge of the true underlying process pose significant challenges, the LLM generated models that are identical or close to the winning model from cognitive science literature. Our findings suggest that LLMs can have a transformative impact on cognitive modeling. With this project, we aim to contribute to an ongoing effort of automating scientific discovery in cognitive science.

### Modified Adaptive Tree-Structured Parzen Estimator for Hyperparameter Optimization 
[[arxiv](https://arxiv.org/abs/2502.00871)] [[cool](https://papers.cool/arxiv/2502.00871)] [[pdf](https://arxiv.org/pdf/2502.00871)]
> **Authors**: Szymon Sieradzki,Jacek Mańdziuk
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 21 pages, 10 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we review hyperparameter optimization methods for machine learning models, with a particular focus on the Adaptive Tree-Structured Parzen Estimator (ATPE) algorithm. We propose several modifications to ATPE and assess their efficacy on a diverse set of standard benchmark functions. Experimental results demonstrate that the proposed modifications significantly improve the effectiveness of ATPE hyperparameter optimization on selected benchmarks, a finding that holds practical relevance for their application in real-world machine learning / optimization tasks.

### FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation 
[[arxiv](https://arxiv.org/abs/2502.00870)] [[cool](https://papers.cool/arxiv/2502.00870)] [[pdf](https://arxiv.org/pdf/2502.00870)]
> **Authors**: Wenzheng Jiang,Ji Wang,Xiongtao Zhang,Weidong Bao,Cheston Tan,Flint Xiaofeng Fan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: This preprint presents the full version of the Extended Abstract accepted by AAMAS 2025, including all the proofs and experiments
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal details. Knowledge Distillation (KD) is a promising method for facilitating knowledge sharing among heterogeneous models, but it faces challenges related to the scarcity of public datasets and limitations in knowledge representation when applied to FedRL. To address these challenges, we propose Federated Heterogeneous Policy Distillation (FedHPD), which solves the problem of heterogeneous FedRL by utilizing action probability distributions as a medium for knowledge sharing. We provide a theoretical analysis of FedHPD's convergence under standard assumptions. Extensive experiments corroborate that FedHPD shows significant improvements across various reinforcement learning benchmark tasks, further validating our theoretical findings. Moreover, additional experiments demonstrate that FedHPD operates effectively without the need for an elaborate selection of public datasets.

### FedRIR: Rethinking Information Representation in Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.00859)] [[cool](https://papers.cool/arxiv/2502.00859)] [[pdf](https://arxiv.org/pdf/2502.00859)]
> **Authors**: Yongqiang Huang,Zerui Shao,Ziyuan Yang,Zexin Lu,Yi Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Mobile and Web-of-Things (WoT) devices at the network edge generate vast amounts of data for machine learning applications, yet privacy concerns hinder centralized model training. Federated Learning (FL) allows clients (devices) to collaboratively train a shared model coordinated by a central server without transfer private data, but inherent statistical heterogeneity among clients presents challenges, often leading to a dilemma between clients' needs for personalized local models and the server's goal of building a generalized global model. Existing FL methods typically prioritize either global generalization or local personalization, resulting in a trade-off between these two objectives and limiting the full potential of diverse client data. To address this challenge, we propose a novel framework that simultaneously enhances global generalization and local personalization by Rethinking Information Representation in the Federated learning process (FedRIR). Specifically, we introduce Masked Client-Specific Learning (MCSL), which isolates and extracts fine-grained client-specific features tailored to each client's unique data characteristics, thereby enhancing personalization. Concurrently, the Information Distillation Module (IDM) refines the global shared features by filtering out redundant client-specific information, resulting in a purer and more robust global representation that enhances generalization. By integrating the refined global features with the isolated client-specific features, we construct enriched representations that effectively capture both global patterns and local nuances, thereby improving the performance of downstream tasks on the client. The code is available at https://github.com/Deep-Imaging-Group/FedRIR.

### Dual Alignment Maximin Optimization for Offline Model-based RL 
[[arxiv](https://arxiv.org/abs/2502.00850)] [[cool](https://papers.cool/arxiv/2502.00850)] [[pdf](https://arxiv.org/pdf/2502.00850)]
> **Authors**: Chi Zhou,Wang Luo,Haoran Li,Congying Han,Tiande Guo,Zicheng Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.

### Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework 
[[arxiv](https://arxiv.org/abs/2502.00846)] [[cool](https://papers.cool/arxiv/2502.00846)] [[pdf](https://arxiv.org/pdf/2502.00846)]
> **Authors**: Terje Mildner,Oliver Hamelijnck,Paris Giampouras,Theodoros Damoulas
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is provably robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness. Additionally, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.

### Boosting Adversarial Robustness and Generalization with Structural Prior 
[[arxiv](https://arxiv.org/abs/2502.00834)] [[cool](https://papers.cool/arxiv/2502.00834)] [[pdf](https://arxiv.org/pdf/2502.00834)]
> **Authors**: Zhichao Hou,Weizhi Gao,Hamid Krim,Xiaorui Liu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,神经和进化计算
- **Abstract**: This work investigates a novel approach to boost adversarial robustness and generalization by incorporating structural prior into the design of deep learning models. Specifically, our study surprisingly reveals that existing dictionary learning-inspired convolutional neural networks (CNNs) provide a false sense of security against adversarial attacks. To address this, we propose Elastic Dictionary Learning Networks (EDLNets), a novel ResNet architecture that significantly enhances adversarial robustness and generalization. This novel and effective approach is supported by a theoretical robustness analysis using influence functions. Moreover, extensive and reliable experiments demonstrate consistent and significant performance improvement on open robustness leaderboards such as RobustBench, surpassing state-of-the-art baselines. To the best of our knowledge, this is the first work to discover and validate that structural prior can reliably enhance deep learning robustness under strong adaptive attacks, unveiling a promising direction for future research.

### A Comprehensive Analysis on LLM-based Node Classification Algorithms 
[[arxiv](https://arxiv.org/abs/2502.00829)] [[cool](https://papers.cool/arxiv/2502.00829)] [[pdf](https://arxiv.org/pdf/2502.00829)]
> **Authors**: Xixi Wu,Yifei Shen,Fangzhou Ge,Caihua Shan,Yizhu Jiao,Xiangguo Sun,Hong Cheng
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application. In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes ten datasets, eight LLM-based algorithms, and three learning paradigms, and is designed for easy extension with new methods and datasets. Subsequently, we conducted extensive experiments, training and evaluating over 2,200 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size) that affect performance. Our findings uncover eight insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field. Codes and datasets are released at \href{https://llmnodebed.github.io/}{https://llmnodebed.github.io/}.

### OOD Detection with immature Models 
[[arxiv](https://arxiv.org/abs/2502.00820)] [[cool](https://papers.cool/arxiv/2502.00820)] [[pdf](https://arxiv.org/pdf/2502.00820)]
> **Authors**: Behrooz Montazeran,Ullrich Köthe
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 17 pages, 2 Tables, 9 Figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Likelihood-based deep generative models (DGMs) have gained significant attention for their ability to approximate the distributions of high-dimensional data. However, these models lack a performance guarantee in assigning higher likelihood values to in-distribution (ID) inputs, data the models are trained on, compared to out-of-distribution (OOD) inputs. This counter-intuitive behaviour is particularly pronounced when ID inputs are more complex than OOD data points. One potential approach to address this challenge involves leveraging the gradient of a data point with respect to the parameters of the DGMs. A recent OOD detection framework proposed estimating the joint density of layer-wise gradient norms for a given data point as a model-agnostic method, demonstrating superior performance compared to the Typicality Test across likelihood-based DGMs and image dataset pairs. In particular, most existing methods presuppose access to fully converged models, the training of which is both time-intensive and computationally demanding. In this work, we demonstrate that using immature models,stopped at early stages of training, can mostly achieve equivalent or even superior results on this downstream task compared to mature models capable of generating high-quality samples that closely resemble ID data. This novel finding enhances our understanding of how DGMs learn the distribution of ID data and highlights the potential of leveraging partially trained models for downstream tasks. Furthermore, we offer a possible explanation for this unexpected behaviour through the concept of support overlap.

### Sundial: A Family of Highly Capable Time Series Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.00816)] [[cool](https://papers.cool/arxiv/2502.00816)] [[pdf](https://arxiv.org/pdf/2502.00816)]
> **Authors**: Yong Liu,Guo Qin,Zhiyuan Shi,Zhi Chen,Caiyin Yang,Xiangdong Huang,Jianmin Wang,Mingsheng Long
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on time series without discrete tokenization. Conditioned on arbitrary-length time series, our model is pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving flexibility in representation learning beyond using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with 1 trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse through TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which exhibit unprecedented model capacity and generalization performance on zero-shot forecasting. In addition to presenting good scaling behavior, Sundial achieves new state-of-the-art on both point forecasting and probabilistic forecasting benchmarks. We believe that Sundial's pioneering generative paradigm will facilitate a wide variety of forecasting scenarios.

### Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling 
[[arxiv](https://arxiv.org/abs/2502.00814)] [[cool](https://papers.cool/arxiv/2502.00814)] [[pdf](https://arxiv.org/pdf/2502.00814)]
> **Authors**: Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Yue Wang,Li Li,Wengang Zhou,Houqiang Li
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) has achieved considerable success in aligning large language models (LLMs) by modeling human preferences with a learnable reward model and employing a reinforcement learning algorithm to maximize the reward model's scores. However, these reward models are susceptible to exploitation through various superficial confounding factors, with length bias emerging as a particularly significant concern. Moreover, while the pronounced impact of length bias on preference modeling suggests that LLMs possess an inherent sensitivity to length perception, our preliminary investigations reveal that fine-tuned LLMs consistently struggle to adhere to explicit length instructions. To address these two limitations, we propose a novel framework wherein the reward model explicitly differentiates between human semantic preferences and response length requirements. Specifically, we introduce a Response-conditioned Bradley-Terry (Rc-BT) model that enhances the reward model's capability in length bias mitigating and length instruction following, through training on our augmented dataset. Furthermore, we propose the Rc-DPO algorithm to leverage the Rc-BT model for direct policy optimization (DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to length instructions. Extensive evaluations demonstrate that our approach substantially improves both preference modeling and length instruction compliance, with its effectiveness validated across various foundational models and preference datasets.

### Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage in Downstream Applications 
[[arxiv](https://arxiv.org/abs/2502.00808)] [[cool](https://papers.cool/arxiv/2502.00808)] [[pdf](https://arxiv.org/pdf/2502.00808)]
> **Authors**: Yixin Wu,Ziqing Yang,Yun Shen,Michael Backes,Yang Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: To Appear in the 34th USENIX Security Symposium, August 13-15, 2025
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机与社会
- **Abstract**: Large language models (LLMs) have facilitated the generation of high-quality, cost-effective synthetic data for developing downstream models and conducting statistical analyses in various domains. However, the increased reliance on synthetic data may pose potential negative impacts. Numerous studies have demonstrated that LLM-generated synthetic data can perpetuate and even amplify societal biases and stereotypes, and produce erroneous outputs known as ``hallucinations'' that deviate from factual knowledge. In this paper, we aim to audit artifacts, such as classifiers, generators, or statistical plots, to identify those trained on or derived from synthetic data and raise user awareness, thereby reducing unexpected consequences and risks in downstream applications. To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data. We then propose an auditing framework with three methods including metric-based auditing, tuning-based auditing, and classification-based auditing. These methods operate without requiring the artifact owner to disclose proprietary training details. We evaluate our auditing framework on three text classification tasks, two text summarization tasks, and two data visualization tasks across three training scenarios. Our evaluation demonstrates the effectiveness of all proposed auditing methods across all these tasks. For instance, black-box metric-based auditing can achieve an average accuracy of $0.868 \pm 0.071$ for auditing classifiers and $0.880 \pm 0.052$ for auditing generators using only 200 random queries across three scenarios. We hope our research will enhance model transparency and regulatory compliance, ensuring the ethical and responsible use of synthetic data.

### UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs 
[[arxiv](https://arxiv.org/abs/2502.00806)] [[cool](https://papers.cool/arxiv/2502.00806)] [[pdf](https://arxiv.org/pdf/2502.00806)]
> **Authors**: Yufei He,Yuan Sui,Xiaoxin He,Yue Liu,Yifei Sun,Bryan Hooi
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: WWW 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Existing foundation models, such as CLIP, aim to learn a unified embedding space for multimodal data, enabling a wide range of downstream web-based applications like search, recommendation, and content classification. However, these models often overlook the inherent graph structures in multimodal datasets, where entities and their relationships are crucial. Multimodal graphs (MMGs) represent such graphs where each node is associated with features from different modalities, while the edges capture the relationships between these entities. On the other hand, existing graph foundation models primarily focus on text-attributed graphs (TAGs) and are not designed to handle the complexities of MMGs. To address these limitations, we propose UniGraph2, a novel cross-domain graph foundation model that enables general representation learning on MMGs, providing a unified embedding space. UniGraph2 employs modality-specific encoders alongside a graph neural network (GNN) to learn a unified low-dimensional embedding space that captures both the multimodal information and the underlying graph structure. We propose a new cross-domain multi-graph pre-training algorithm at scale to ensure effective transfer learning across diverse graph domains and modalities. Additionally, we adopt a Mixture of Experts (MoE) component to align features from different domains and modalities, ensuring coherent and robust embeddings that unify the information across modalities. Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as representation learning, transfer learning, and multimodal generative tasks, offering a scalable and flexible solution for learning on MMGs.

### ProPINN: Demystifying Propagation Failures in Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00803)] [[cool](https://papers.cool/arxiv/2502.00803)] [[pdf](https://arxiv.org/pdf/2502.00803)]
> **Authors**: Haixu Wu,Yuezhou Ma,Hang Zhou,Huikun Weng,Jianmin Wang,Mingsheng Long
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physics-informed neural networks (PINNs) have earned high expectations in solving partial differential equations (PDEs), but their optimization usually faces thorny challenges due to the unique derivative-dependent loss function. By analyzing the loss distribution, previous research observed the propagation failure phenomenon of PINNs, intuitively described as the correct supervision for model outputs cannot ``propagate'' from initial states or boundaries to the interior domain. Going beyond intuitive understanding, this paper provides the first formal and in-depth study of propagation failure and its root cause. Based on a detailed comparison with classical finite element methods, we ascribe the failure to the conventional single-point-processing architecture of PINNs and further prove that propagation failure is essentially caused by the lower gradient correlation of PINN models on nearby collocation points. Compared to superficial loss maps, this new perspective provides a more precise quantitative criterion to identify where and why PINN fails. The theoretical finding also inspires us to present a new PINN architecture, named ProPINN, which can effectively unite the gradient of region points for better propagation. ProPINN can reliably resolve PINN failure modes and significantly surpass advanced Transformer-based models with 46% relative promotion.

### Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.00802)] [[cool](https://papers.cool/arxiv/2502.00802)] [[pdf](https://arxiv.org/pdf/2502.00802)]
> **Authors**: Massimiliano Falzari,Matthia Sabatelli
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep Reinforcement Learning (DRL) systems often tend to overfit to early experiences, a phenomenon known as the primacy bias (PB). This bias can severely hinder learning efficiency and final performance, particularly in complex environments. This paper presents a comprehensive investigation of PB through the lens of the Fisher Information Matrix (FIM). We develop a framework characterizing PB through distinct patterns in the FIM trace, identifying critical memorization and reorganization phases during learning. Building on this understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a novel method that leverages the geometric structure of the parameter space to selectively modify network weights, preventing early experiences from dominating the learning process. Empirical results across DeepMind Control Suite (DMC) environments show that FGSF consistently outperforms baselines, particularly in complex tasks. We analyze the different impacts of PB on actor and critic networks, the role of replay ratios in exacerbating the effect, and the effectiveness of even simple noise injection methods. Our findings provide a deeper understanding of PB and practical mitigation strategies, offering a FIM-based geometric perspective for advancing DRL.

### Transfer Learning in Physics-Informed Neural Networks: Full Fine-Tuning, Lightweight Fine-Tuning, and Low-Rank Adaptation 
[[arxiv](https://arxiv.org/abs/2502.00782)] [[cool](https://papers.cool/arxiv/2502.00782)] [[pdf](https://arxiv.org/pdf/2502.00782)]
> **Authors**: Yizheng Wang,Jinshuai Bai,Mohammad Sadegh Eshaghi,Cosmin Anitescu,Xiaoying Zhuang,Timon Rabczuk,Yinghua Liu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: AI for PDEs has garnered significant attention, particularly Physics-Informed Neural Networks (PINNs). However, PINNs are typically limited to solving specific problems, and any changes in problem conditions necessitate retraining. Therefore, we explore the generalization capability of transfer learning in the strong and energy form of PINNs across different boundary conditions, materials, and geometries. The transfer learning methods we employ include full finetuning, lightweight finetuning, and Low-Rank Adaptation (LoRA). The results demonstrate that full finetuning and LoRA can significantly improve convergence speed while providing a slight enhancement in accuracy.

### Role of Mixup in Topological Persistence Based Knowledge Distillation for Wearable Sensor Data 
[[arxiv](https://arxiv.org/abs/2502.00779)] [[cool](https://papers.cool/arxiv/2502.00779)] [[pdf](https://arxiv.org/pdf/2502.00779)]
> **Authors**: Eun Som Jeon,Hongjun Choi,Matthew P. Buman,Pavan Turaga
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: IEEE Sensors Journal (2024)
- **标题**: None
- **领域**: 机器学习,人工智能,信号处理
- **Abstract**: The analysis of wearable sensor data has enabled many successes in several applications. To represent the high-sampling rate time-series with sufficient detail, the use of topological data analysis (TDA) has been considered, and it is found that TDA can complement other time-series features. Nonetheless, due to the large time consumption and high computational resource requirements of extracting topological features through TDA, it is difficult to deploy topological knowledge in various applications. To tackle this problem, knowledge distillation (KD) can be adopted, which is a technique facilitating model compression and transfer learning to generate a smaller model by transferring knowledge from a larger network. By leveraging multiple teachers in KD, both time-series and topological features can be transferred, and finally, a superior student using only time-series data is distilled. On the other hand, mixup has been popularly used as a robust data augmentation technique to enhance model performance during training. Mixup and KD employ similar learning strategies. In KD, the student model learns from the smoothed distribution generated by the teacher model, while mixup creates smoothed labels by blending two labels. Hence, this common smoothness serves as the connecting link that establishes a connection between these two methods. In this paper, we analyze the role of mixup in KD with time-series as well as topological persistence, employing multiple teachers. We present a comprehensive analysis of various methods in KD and mixup on wearable sensor data.

### ATA: Adaptive Task Allocation for Efficient Resource Management in Distributed Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.00775)] [[cool](https://papers.cool/arxiv/2502.00775)] [[pdf](https://arxiv.org/pdf/2502.00775)]
> **Authors**: Artavazd Maranjyan,El Mehdi Saad,Peter Richtárik,Francesco Orabona
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,优化与控制,机器学习
- **Abstract**: Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers.

### Learning-Based TSP-Solvers Tend to Be Overly Greedy 
[[arxiv](https://arxiv.org/abs/2502.00767)] [[cool](https://papers.cool/arxiv/2502.00767)] [[pdf](https://arxiv.org/pdf/2502.00767)]
> **Authors**: Xiayang Li,Shihua Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能,数据结构和算法
- **Abstract**: Deep learning has shown significant potential in solving combinatorial optimization problems such as the Euclidean traveling salesman problem (TSP). However, most training and test instances for existing TSP algorithms are generated randomly from specific distributions like uniform distribution. This has led to a lack of analysis and understanding of the performance of deep learning algorithms in out-of-distribution (OOD) generalization scenarios, which has a close relationship with the worst-case performance in the combinatorial optimization field. For data-driven algorithms, the statistical properties of randomly generated datasets are critical. This study constructs a statistical measure called nearest-neighbor density to verify the asymptotic properties of randomly generated datasets and reveal the greedy behavior of learning-based solvers, i.e., always choosing the nearest neighbor nodes to construct the solution path. Based on this statistical measure, we develop interpretable data augmentation methods that rely on distribution shifts or instance perturbations and validate that the performance of the learning-based solvers degenerates much on such augmented data. Moreover, fine-tuning learning-based solvers with augmented data further enhances their generalization abilities. In short, we decipher the limitations of learning-based TSP solvers tending to be overly greedy, which may have profound implications for AI-empowered combinatorial optimization solvers.

### Privacy Preserving Properties of Vision Classifiers 
[[arxiv](https://arxiv.org/abs/2502.00760)] [[cool](https://papers.cool/arxiv/2502.00760)] [[pdf](https://arxiv.org/pdf/2502.00760)]
> **Authors**: Pirzada Suhail,Amit Sethi
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,计算机视觉和模式识别
- **Abstract**: Vision classifiers are often trained on proprietary datasets containing sensitive information, yet the models themselves are frequently shared openly under the privacy-preserving assumption. Although these models are assumed to protect sensitive information in their training data, the extent to which this assumption holds for different architectures remains unexplored. This assumption is challenged by inversion attacks which attempt to reconstruct training data from model weights, exposing significant privacy vulnerabilities. In this study, we systematically evaluate the privacy-preserving properties of vision classifiers across diverse architectures, including Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision Transformers (ViTs). Using network inversion-based reconstruction techniques, we assess the extent to which these architectures memorize and reveal training data, quantifying the relative ease of reconstruction across models. Our analysis highlights how architectural differences, such as input representation, feature extraction mechanisms, and weight structures, influence privacy risks. By comparing these architectures, we identify which are more resilient to inversion attacks and examine the trade-offs between model performance and privacy preservation, contributing to the development of secure and privacy-respecting machine learning models for sensitive applications. Our findings provide actionable insights into the design of secure and privacy-aware machine learning systems, emphasizing the importance of evaluating architectural decisions in sensitive applications involving proprietary or personal data.

### Continuity-Preserving Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images 
[[arxiv](https://arxiv.org/abs/2502.00754)] [[cool](https://papers.cool/arxiv/2502.00754)] [[pdf](https://arxiv.org/pdf/2502.00754)]
> **Authors**: Aiqing Zhu,Yuting Pan,Qianxiao Li
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Continuous dynamical systems are cornerstones of many scientific and engineering disciplines. While machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are discrete in nature. Consequently, a naive application of a convolutional autoencoder can result in latent coordinates that are discontinuous in time. To resolve this, we propose continuity-preserving convolutional autoencoders (CpAEs) to learn continuous latent states and their corresponding continuous latent dynamical models from discrete image frames. We present a mathematical formulation for learning dynamics from image frames, which illustrates issues with previous approaches and motivates our methodology based on promoting the continuity of convolution filters, thereby preserving the continuity of the latent states. This approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics, leading to more accurate latent dynamical models. Extensive experiments across various scenarios demonstrate the effectiveness of CpAEs.

### BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts 
[[arxiv](https://arxiv.org/abs/2502.00745)] [[cool](https://papers.cool/arxiv/2502.00745)] [[pdf](https://arxiv.org/pdf/2502.00745)]
> **Authors**: Divya Jyoti Bajpai,Manjesh Kumar Hanawal
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Published at International Conference onLearningRepresentations (ICLR) 2025
- **标题**: None
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **Abstract**: Early Exit (EE) techniques have emerged as a means to reduce inference latency in Deep Neural Networks (DNNs). The latency improvement and accuracy in these techniques crucially depend on the criteria used to make exit decisions. We propose a new decision criterion where exit classifiers are treated as experts BEEM and aggregate their confidence scores. The confidence scores are aggregated only if neighbouring experts are consistent in prediction as the samples pass through them, thus capturing their ensemble effect. A sample exits when the aggregated confidence value exceeds a threshold. The threshold is set using the error rates of the intermediate exits aiming to surpass the performance of conventional DNN inference. Experimental results on the COCO dataset for Image captioning and GLUE datasets for various language tasks demonstrate that our method enhances the performance of state-of-the-art EE methods, achieving improvements in speed-up by a factor 1.5x to 2.1x. When compared to the final layer, its accuracy is comparable in harder Image Captioning and improves in the easier language tasks. The source code for this work is publicly available at https://github.com/Div290/BEEM1/tree/main

### CoNNect: A Swiss-Army-Knife Regularizer for Pruning of Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00744)] [[cool](https://papers.cool/arxiv/2502.00744)] [[pdf](https://arxiv.org/pdf/2502.00744)]
> **Authors**: Christian Franssen,Jinyang Jiang,Yijie Peng,Bernd Heidergott
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Pruning encompasses a range of techniques aimed at increasing the sparsity of neural networks (NNs). These techniques can generally be framed as minimizing a loss function subject to an $L_0$-norm constraint. This paper introduces CoNNect, a novel differentiable regularizer for sparse NN training that ensures connectivity between input and output layers. CoNNect integrates with established pruning strategies and supports both structured and unstructured pruning. We proof that CoNNect approximates $L_0$-regularization, guaranteeing maximally connected network structures while avoiding issues like layer collapse. Numerical experiments demonstrate that CoNNect improves classical pruning strategies and enhances state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner.

### Meta-Prompt Optimization for LLM-Based Sequential Decision Making 
[[arxiv](https://arxiv.org/abs/2502.00728)] [[cool](https://papers.cool/arxiv/2502.00728)] [[pdf](https://arxiv.org/pdf/2502.00728)]
> **Authors**: Mingze Kong,Zhiyong Wang,Yao Shu,Zhongxiang Dai
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large language models (LLMs) have recently been employed as agents to solve sequential decision-making tasks such as Bayesian optimization and multi-armed bandits (MAB). These works usually adopt an LLM for sequential action selection by providing it with a fixed, manually designed meta-prompt. However, numerous previous works have found that the prompt has a significant impact on the performance of the LLM, which calls for a method to automatically optimize the meta-prompt for LLM-based agents. Unfortunately, the non-stationarity in the reward observations during LLM-based sequential decision-making makes meta-prompt optimization highly challenging. To address this challenge, we draw inspirations from adversarial bandit algorithms, which are inherently capable of handling non-stationary reward observations. Building on this foundation, we propose our EXPonential-weight algorithm for prompt Optimization} (EXPO) to automatically optimize the task description and meta-instruction in the meta-prompt for LLM-based agents. We also extend EXPO to additionally optimize the exemplars (i.e., history of interactions) in the meta-prompt to further enhance the performance, hence introducing our EXPO-ES algorithm. We use extensive experiments to show that our algorithms significantly improve the performance of LLM-based sequential decision-making.

### Understanding and Mitigating the High Computational Cost in Path Data Diffusion 
[[arxiv](https://arxiv.org/abs/2502.00725)] [[cool](https://papers.cool/arxiv/2502.00725)] [[pdf](https://arxiv.org/pdf/2502.00725)]
> **Authors**: Dingyuan Shi,Lulu Zhang,Yongxin Tong,Ke Xu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Advancements in mobility services, navigation systems, and smart transportation technologies have made it possible to collect large amounts of path data. Modeling the distribution of this path data, known as the Path Generation (PG) problem, is crucial for understanding urban mobility patterns and developing intelligent transportation systems. Recent studies have explored using diffusion models to address the PG problem due to their ability to capture multimodal distributions and support conditional generation. A recent work devises a diffusion process explicitly in graph space and achieves state-of-the-art performance. However, this method suffers a high computation cost in terms of both time and memory, which prohibits its application. In this paper, we analyze this method both theoretically and experimentally and find that the main culprit of its high computation cost is its explicit design of the diffusion process in graph space. To improve efficiency, we devise a Latent-space Path Diffusion (LPD) model, which operates in latent space instead of graph space. Our LPD significantly reduces both time and memory costs by up to 82.8% and 83.1%, respectively. Despite these reductions, our approach does not suffer from performance degradation. It outperforms the state-of-the-art method in most scenarios by 24.5%~34.0%.

### "I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models 
[[arxiv](https://arxiv.org/abs/2502.00718)] [[cool](https://papers.cool/arxiv/2502.00718)] [[pdf](https://arxiv.org/pdf/2502.00718)]
> **Authors**: Isha Gupta,David Khachaturov,Robert Mullins
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音,音频和语音处理
- **Abstract**: The rise of multimodal large language models has introduced innovative human-machine interaction paradigms but also significant challenges in machine learning safety. Audio-Language Models (ALMs) are especially relevant due to the intuitive nature of spoken communication, yet little is known about their failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on their ability to bypass alignment mechanisms. We construct adversarial perturbations that generalize across prompts, tasks, and even base audio samples, demonstrating the first universal jailbreaks in the audio modality, and show that these remain effective in simulated real-world conditions. Beyond demonstrating attack feasibility, we analyze how ALMs interpret these audio adversarial examples and reveal them to encode imperceptible first-person toxic speech - suggesting that the most effective perturbations for eliciting toxic outputs specifically embed linguistic features within the audio signal. These results have important implications for understanding the interactions between different modalities in multimodal models, and offer actionable insights for enhancing defenses against adversarial audio attacks.

### UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node Classification 
[[arxiv](https://arxiv.org/abs/2502.00716)] [[cool](https://papers.cool/arxiv/2502.00716)] [[pdf](https://arxiv.org/pdf/2502.00716)]
> **Authors**: Mohammad T. Teimuri,Zahra Dehghanian,Gholamali Aminian,Hamid R. Rabiee
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph-structured datasets often suffer from class imbalance, which complicates node classification tasks. In this work, we address this issue by first providing an upper bound on population risk for imbalanced transductive node classification. We then propose a simple and novel algorithm, Uncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels assigned to unlabeled nodes to mitigate the adverse effects of imbalance on classification accuracy. Furthermore, the UPL algorithm enhances the accuracy of pseudo-labeling by reducing training noise of pseudo-labels through a novel uncertainty-aware approach. We comprehensively evaluate the UPL algorithm across various benchmark datasets, demonstrating its superior performance compared to existing state-of-the-art methods.

### Optimization for Neural Operators can Benefit from Width 
[[arxiv](https://arxiv.org/abs/2502.00705)] [[cool](https://papers.cool/arxiv/2502.00705)] [[pdf](https://arxiv.org/pdf/2502.00705)]
> **Authors**: Pedro Cisneros-Velarde,Bhavesh Shrimali,Arindam Banerjee
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Neural Operators that directly learn mappings between function spaces, such as Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs), have received considerable attention. Despite the universal approximation guarantees for DONs and FNOs, there is currently no optimization convergence guarantee for learning such networks using gradient descent (GD). In this paper, we address this open problem by presenting a unified framework for optimization based on GD and applying it to establish convergence guarantees for both DONs and FNOs. In particular, we show that the losses associated with both of these neural operators satisfy two conditions -- restricted strong convexity (RSC) and smoothness -- that guarantee a decrease on their loss values due to GD. Remarkably, these two conditions are satisfied for each neural operator due to different reasons associated with the architectural differences of the respective models. One takeaway that emerges from the theory is that wider networks should lead to better optimization convergence for both DONs and FNOs. We present empirical results on canonical operator learning problems to support our theoretical results.

### Leveraging Large Language Models to Predict Antibody Biological Activity Against Influenza A Hemagglutinin 
[[arxiv](https://arxiv.org/abs/2502.00694)] [[cool](https://papers.cool/arxiv/2502.00694)] [[pdf](https://arxiv.org/pdf/2502.00694)]
> **Authors**: Ella Barkan,Ibrahim Siddiqui,Kevin J. Cheng,Alex Golts,Yoel Shoshan,Jeffrey K. Weber,Yailin Campos Mota,Michal Ozery-Flato,Giuseppe A. Sautto
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,定量方法
- **Abstract**: Monoclonal antibodies (mAbs) represent one of the most prevalent FDA-approved modalities for treating autoimmune diseases, infectious diseases, and cancers. However, discovery and development of therapeutic antibodies remains a time-consuming and expensive process. Recent advancements in machine learning (ML) and artificial intelligence (AI) have shown significant promise in revolutionizing antibody discovery and optimization. In particular, models that predict antibody biological activity enable in-silico evaluation of binding and functional properties; such models can prioritize antibodies with the highest likelihoods of success in costly and time-intensive laboratory testing procedures. We here explore an AI model for predicting the binding and receptor blocking activity of antibodies against influenza A hemagglutinin (HA) antigens. Our present model is developed with the MAMMAL framework for biologics discovery to predict antibody-antigen interactions using only sequence information. To evaluate the model's performance, we tested it under various data split conditions to mimic real-world scenarios. Our models achieved an AUROC $\geq$ 0.91 for predicting the activity of existing antibodies against seen HAs and an AUROC of 0.9 for unseen HAs. For novel antibody activity prediction, the AUROC was 0.73, which further declined to 0.63-0.66 under stringent constraints on similarity to existing antibodies. These results demonstrate the potential of AI foundation models to transform antibody design by reducing dependence on extensive laboratory testing and enabling more efficient prioritization of antibody candidates. Moreover, our findings emphasize the critical importance of diverse and comprehensive antibody datasets to improve the generalization of prediction models, particularly for novel antibody development.

### Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis of Fairness in AI Conference Policies 
[[arxiv](https://arxiv.org/abs/2502.00690)] [[cool](https://papers.cool/arxiv/2502.00690)] [[pdf](https://arxiv.org/pdf/2502.00690)]
> **Authors**: Yuefan Cao,Xiaoyu Li,Yingyu Liang,Zhizhou Sha,Zhenmei Shi,Zhao Song,Jiahao Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会,数字图书馆
- **Abstract**: As AI research surges in both impact and volume, conferences have imposed submission limits to maintain paper quality and alleviate organizational pressure. In this work, we examine the fairness of desk-rejection systems under submission limits and reveal that existing practices can result in substantial inequities. Specifically, we formally define the paper submission limit problem and identify a critical dilemma: when the number of authors exceeds three, it becomes impossible to reject papers solely based on excessive submissions without negatively impacting innocent authors. Thus, this issue may unfairly affect early-career researchers, as their submissions may be penalized due to co-authors with significantly higher submission counts, while senior researchers with numerous papers face minimal consequences. To address this, we propose an optimization-based fairness-aware desk-rejection mechanism and formally define two fairness metrics: individual fairness and group fairness. We prove that optimizing individual fairness is NP-hard, whereas group fairness can be efficiently optimized via linear programming. Through case studies, we demonstrate that our proposed system ensures greater equity than existing methods, including those used in CVPR 2025, offering a more socially just approach to managing excessive submissions in AI conferences.

### Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.00684)] [[cool](https://papers.cool/arxiv/2502.00684)] [[pdf](https://arxiv.org/pdf/2502.00684)]
> **Authors**: Zeyu Jiang,Hai Huang,Xingquan Zuo
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 3 figures, IJCAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep reinforcement learning (DRL), through learning policies or values represented by neural networks, has successfully addressed many complex control problems. However, the neural networks introduced by DRL lack interpretability and transparency. Current DRL interpretability methods largely treat neural networks as black boxes, with few approaches delving into the internal mechanisms of policy/value networks. This limitation undermines trust in both the neural network models that represent policies and the explanations derived from them. In this work, we propose a novel concept-based interpretability method that provides fine-grained explanations of DRL models at the neuron level. Our method formalizes atomic concepts as binary functions over the state space and constructs complex concepts through logical operations. By analyzing the correspondence between neuron activations and concept functions, we establish interpretable explanations for individual neurons in policy/value networks. Experimental results on both continuous control tasks and discrete decision-making environments demonstrate that our method can effectively identify meaningful concepts that align with human understanding while faithfully reflecting the network's decision-making logic.

### A Survey of Quantized Graph Representation Learning: Connecting Graph Structures with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00681)] [[cool](https://papers.cool/arxiv/2502.00681)] [[pdf](https://arxiv.org/pdf/2502.00681)]
> **Authors**: Qika Lin,Zhen Peng,Kaize Shi,Kai He,Yiming Xu,Erik Cambria,Mengling Feng
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Recent years have witnessed rapid advances in graph representation learning, with the continuous embedding approach emerging as the dominant paradigm. However, such methods encounter issues regarding parameter efficiency, interpretability, and robustness. Thus, Quantized Graph Representation (QGR) learning has recently gained increasing interest, which represents the graph structure with discrete codes instead of conventional continuous embeddings. Given its analogous representation form to natural language, QGR also possesses the capability to seamlessly integrate graph structures with large language models (LLMs). As this emerging paradigm is still in its infancy yet holds significant promise, we undertake this thorough survey to promote its rapid future prosperity. We first present the background of the general quantization methods and their merits. Moreover, we provide an in-depth demonstration of current QGR studies from the perspectives of quantized strategies, training objectives, distinctive designs, knowledge graph quantization, and applications. We further explore the strategies for code dependence learning and integration with LLMs. At last, we give discussions and conclude future directions, aiming to provide a comprehensive picture of QGR and inspire future research.

### How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence 
[[arxiv](https://arxiv.org/abs/2502.00678)] [[cool](https://papers.cool/arxiv/2502.00678)] [[pdf](https://arxiv.org/pdf/2502.00678)]
> **Authors**: Hyeong Kyu Choi,Maxim Khanov,Hongxin Wei,Yixuan Li
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that quantifies dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings.

### Safety Alignment Depth in Large Language Models: A Markov Chain Perspective 
[[arxiv](https://arxiv.org/abs/2502.00669)] [[cool](https://papers.cool/arxiv/2502.00669)] [[pdf](https://arxiv.org/pdf/2502.00669)]
> **Authors**: Ching-Chia Kao,Chia-Mu Yu,Chun-Shien Lu,Chu-Song Chen
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Large Language Models (LLMs) are increasingly adopted in high-stakes scenarios, yet their safety mechanisms often remain fragile. Simple jailbreak prompts or even benign fine-tuning can bypass these protocols, underscoring the need to understand where and how they fail. Recent findings suggest that vulnerabilities emerge when alignment is confined to only the initial output tokens. Unfortunately, even with the introduction of deep safety alignment, determining the optimal safety depth remains an unresolved challenge. By leveraging the equivalence between autoregressive language models and Markov chains, this paper offers the first theoretical result on how to identify the ideal depth for safety alignment, and demonstrates how permutation-based data augmentation can tighten these bounds. Crucially, we reveal a fundamental interaction between alignment depth and ensemble width-indicating that broader ensembles can compensate for shallower alignments. These insights provide a theoretical foundation for designing more robust, scalable safety strategies that complement existing alignment approaches, opening new avenues for research into safer, more reliable LLMs.

### Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration 
[[arxiv](https://arxiv.org/abs/2502.00666)] [[cool](https://papers.cool/arxiv/2502.00666)] [[pdf](https://arxiv.org/pdf/2502.00666)]
> **Authors**: Mingyu Chen,Yiding Chen,Wen Sun,Xuezhou Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024).. Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design. The code is available at https://github.com/MYC000801/SE-POPO.

### LLM Safety Alignment is Divergence Estimation in Disguise 
[[arxiv](https://arxiv.org/abs/2502.00657)] [[cool](https://papers.cool/arxiv/2502.00657)] [[pdf](https://arxiv.org/pdf/2502.00657)]
> **Authors**: Rajdeep Haldar,Ziyi Wang,Qifan Song,Guang Lin,Yue Xing
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会,机器学习
- **Abstract**: We propose a theoretical framework demonstrating that popular Large Language Model (LLM) alignment methods, including Reinforcement Learning from Human Feedback (RLHF) and alternatives, fundamentally function as divergence estimators between aligned (preferred or safe) and unaligned (less-preferred or harmful) distributions. This explains the separation phenomenon between safe and harmful prompts in the model hidden representation after alignment. Inspired by the theoretical results, we identify that some alignment methods are better than others in terms of separation and, introduce a new method, KLDO, and further demonstrate the implication of our theories. We advocate for compliance-refusal datasets over preference datasets to enhance safety alignment, supported by both theoretical reasoning and empirical evidence. Additionally, to quantify safety separation, we leverage a distance metric in the representation space and statistically validate its efficacy as a statistical significant indicator of LLM resilience against jailbreak attacks.

### Reformulation is All You Need: Addressing Malicious Text Features in DNNs 
[[arxiv](https://arxiv.org/abs/2502.00652)] [[cool](https://papers.cool/arxiv/2502.00652)] [[pdf](https://arxiv.org/pdf/2502.00652)]
> **Authors**: Yi Jiang,Oubo Ma,Yong Yang,Tong Zhang,Shouling Ji
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,密码学和安全
- **Abstract**: Human language encompasses a wide range of intricate and diverse implicit features, which attackers can exploit to launch adversarial or backdoor attacks, compromising DNN models for NLP tasks. Existing model-oriented defenses often require substantial computational resources as model size increases, whereas sample-oriented defenses typically focus on specific attack vectors or schemes, rendering them vulnerable to adaptive attacks. We observe that the root cause of both adversarial and backdoor attacks lies in the encoding process of DNN models, where subtle textual features, negligible for human comprehension, are erroneously assigned significant weight by less robust or trojaned models. Based on it we propose a unified and adaptive defense framework that is effective against both adversarial and backdoor attacks. Our approach leverages reformulation modules to address potential malicious features in textual inputs while preserving the original semantic integrity. Extensive experiments demonstrate that our framework outperforms existing sample-oriented defense baselines across a diverse range of malicious textual features.

### Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions 
[[arxiv](https://arxiv.org/abs/2502.00620)] [[cool](https://papers.cool/arxiv/2502.00620)] [[pdf](https://arxiv.org/pdf/2502.00620)]
> **Authors**: Yihao Xue,Jiping Li,Baharan Mirzasoleiman
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.

### Using Causality for Enhanced Prediction of Web Traffic Time Series 
[[arxiv](https://arxiv.org/abs/2502.00612)] [[cool](https://papers.cool/arxiv/2502.00612)] [[pdf](https://arxiv.org/pdf/2502.00612)]
> **Authors**: Chang Tian,Mingzhe Xing,Zenglin Shi,Matthew B. Blaschko,Yinliang Yue,Marie-Francine Moens
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: time series, web service, web traffic, causality
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: Predicting web service traffic has significant social value, as it can be applied to various practical scenarios, including but not limited to dynamic resource scaling, load balancing, system anomaly detection, service-level agreement compliance, and fraud detection. Web service traffic is characterized by frequent and drastic fluctuations over time and are influenced by heterogeneous web user behaviors, making accurate prediction a challenging task. Previous research has extensively explored statistical approaches, and neural networks to mine features from preceding service traffic time series for prediction. However, these methods have largely overlooked the causal relationships between services. Drawing inspiration from causality in ecological systems, we empirically recognize the causal relationships between web services. To leverage these relationships for improved web service traffic prediction, we propose an effective neural network module, CCMPlus, designed to extract causal relationship features across services. This module can be seamlessly integrated with existing time series models to consistently enhance the performance of web service traffic predictions. We theoretically justify that the causal correlation matrix generated by the CCMPlus module captures causal relationships among services. Empirical results on real-world datasets from Microsoft Azure, Alibaba Group, and Ant Group confirm that our method surpasses state-of-the-art approaches in Mean Squared Error (MSE) and Mean Absolute Error (MAE) for predicting service traffic time series. These findings highlight the efficacy of leveraging causal relationships for improved predictions.

### PAC Learning is just Bipartite Matching (Sort of) 
[[arxiv](https://arxiv.org/abs/2502.00607)] [[cool](https://papers.cool/arxiv/2502.00607)] [[pdf](https://arxiv.org/pdf/2502.00607)]
> **Authors**: Shaddin Dughmi
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Position paper
- **标题**: None
- **领域**: 机器学习,数据结构和算法,机器学习
- **Abstract**: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.

### Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective 
[[arxiv](https://arxiv.org/abs/2502.00604)] [[cool](https://papers.cool/arxiv/2502.00604)] [[pdf](https://arxiv.org/pdf/2502.00604)]
> **Authors**: Sifan Wang,Ananyae Kumar Bhartari,Bowen Li,Paris Perdikaris
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 39 pages, 22 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算物理
- **Abstract**: Multi-task learning through composite loss functions is fundamental to modern deep learning, yet optimizing competing objectives remains challenging. We present new theoretical and practical approaches for addressing directional conflicts between loss terms, demonstrating their effectiveness in physics-informed neural networks (PINNs) where such conflicts are particularly challenging to resolve. Through theoretical analysis, we demonstrate how these conflicts limit first-order methods and show that second-order optimization naturally resolves them through implicit gradient alignment. We prove that SOAP, a recently proposed quasi-Newton method, efficiently approximates the Hessian preconditioner, enabling breakthrough performance in PINNs: state-of-the-art results on 10 challenging PDE benchmarks, including the first successful application to turbulent flows with Reynolds numbers up to 10,000, with 2-10x accuracy improvements over existing methods. We also introduce a novel gradient alignment score that generalizes cosine similarity to multiple gradients, providing a practical tool for analyzing optimization dynamics. Our findings establish frameworks for understanding and resolving gradient conflicts, with broad implications for optimization beyond scientific computing.

### Enhancing Offline Reinforcement Learning with Curriculum Learning-Based Trajectory Valuation 
[[arxiv](https://arxiv.org/abs/2502.00601)] [[cool](https://papers.cool/arxiv/2502.00601)] [[pdf](https://arxiv.org/pdf/2502.00601)]
> **Authors**: Amir Abolfazli,Zekun Song,Avishek Anand,Wolfgang Nejdl
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted at AAMAS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The success of deep reinforcement learning (DRL) relies on the availability and quality of training data, often requiring extensive interactions with specific environments. In many real-world scenarios, where data collection is costly and risky, offline reinforcement learning (RL) offers a solution by utilizing data collected by domain experts and searching for a batch-constrained optimal policy. This approach is further augmented by incorporating external data sources, expanding the range and diversity of data collection possibilities. However, existing offline RL methods often struggle with challenges posed by non-matching data from these external sources. In this work, we specifically address the problem of source-target domain mismatch in scenarios involving mixed datasets, characterized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data generated from higher-quality policies. To tackle this problem, we introduce Transition Scoring (TS), a novel method that assigns scores to transitions based on their similarity to the target domain, and propose Curriculum Learning-Based Trajectory Valuation (CLTV), which effectively leverages these transition scores to identify and prioritize high-quality trajectories through a curriculum learning approach. Our extensive experiments across various offline RL methods and MuJoCo environments, complemented by rigorous theoretical analysis, demonstrate that CLTV enhances the overall performance and transferability of policies learned by offline RL algorithms.

### Converting Transformers into DGNNs Form 
[[arxiv](https://arxiv.org/abs/2502.00585)] [[cool](https://papers.cool/arxiv/2502.00585)] [[pdf](https://arxiv.org/pdf/2502.00585)]
> **Authors**: Jie Zhang,Kuan-Chieh Wang,Bo-Wei Chiu,Min-Te Sun
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 21 pages, 3 figures, and 8 tables; pseudocode improved
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Recent advances in deep learning have established Transformer architectures as the predominant modeling paradigm. Central to the success of Transformers is the self-attention mechanism, which scores the similarity between query and key matrices to modulate a value matrix. This operation bears striking similarities to digraph convolution, prompting an investigation into whether digraph convolution could serve as an alternative to self-attention. In this study, we formalize this concept by introducing a synthetic unitary digraph convolution based on the digraph Fourier transform. The resulting model, which we term Converter, effectively converts a Transformer into a Directed Graph Neural Network (DGNN) form. We have tested Converter on Long-Range Arena benchmark, long document classification, and DNA sequence-based taxonomy classification. Our experimental results demonstrate that Converter achieves superior performance while maintaining computational efficiency and architectural simplicity, which establishes it as a lightweight yet powerful Transformer variant.

### Optimal Sensor Placement in Power Transformers Using Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00552)] [[cool](https://papers.cool/arxiv/2502.00552)] [[pdf](https://arxiv.org/pdf/2502.00552)]
> **Authors**: Sirui Li,Federica Bragone,Matthieu Barreau,Tor Laneryd,Kateryna Morozovska
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: Our work aims at simulating and predicting the temperature conditions inside a power transformer using Physics-Informed Neural Networks (PINNs). The predictions obtained are then used to determine the optimal placement for temperature sensors inside the transformer under the constraint of a limited number of sensors, enabling efficient performance monitoring. The method consists of combining PINNs with Mixed Integer Optimization Programming to obtain the optimal temperature reconstruction inside the transformer. First, we extend our PINN model for the thermal modeling of power transformers to solve the heat diffusion equation from 1D to 2D space. Finally, we construct an optimal sensor placement model inside the transformer that can be applied to problems in 1D and 2D.

### Muti-Fidelity Prediction and Uncertainty Quantification with Laplace Neural Operators for Parametric Partial Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.00550)] [[cool](https://papers.cool/arxiv/2502.00550)] [[pdf](https://arxiv.org/pdf/2502.00550)]
> **Authors**: Haoyang Zheng,Guang Lin
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 30 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,数值分析,计算物理
- **Abstract**: Laplace Neural Operators (LNOs) have recently emerged as a promising approach in scientific machine learning due to the ability to learn nonlinear maps between functional spaces. However, this framework often requires substantial amounts of high-fidelity (HF) training data, which is often prohibitively expensive to acquire. To address this, we propose multi-fidelity Laplace Neural Operators (MF-LNOs), which combine a low-fidelity (LF) base model with parallel linear/nonlinear HF correctors and dynamic inter-fidelity weighting. This allows us to exploit correlations between LF and HF datasets and achieve accurate inference of quantities of interest even with sparse HF data. We further incorporate a modified replica exchange stochastic gradient Langevin algorithm, which enables a more effective posterior distribution estimation and uncertainty quantification in model predictions. Extensive validation across four canonical dynamical systems (the Lorenz system, Duffing oscillator, Burgers equation, and Brusselator reaction-diffusion system) demonstrates the framework's effectiveness. The results show significant improvements, with testing losses reduced by 40% to 80% compared to traditional approaches. This validates MF-LNO as a versatile tool for surrogate modeling in parametric PDEs, offering significant improvements in data efficiency and uncertainty-aware prediction.

### Integrating Frequency Guidance into Multi-source Domain Generalization for Bearing Fault Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.00545)] [[cool](https://papers.cool/arxiv/2502.00545)] [[pdf](https://arxiv.org/pdf/2502.00545)]
> **Authors**: Xiaotong Tu,Chenyu Ma,Qingyao Wu,Yinhao Liu,Hongyang Zhang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Recent generalizable fault diagnosis researches have effectively tackled the distributional shift between unseen working conditions. Most of them mainly focus on learning domain-invariant representation through feature-level methods. However, the increasing numbers of unseen domains may lead to domain-invariant features contain instance-level spurious correlations, which impact the previous models' generalizable ability. To address the limitations, we propose the Fourier-based Augmentation Reconstruction Network, namely FARNet.The methods are motivated by the observation that the Fourier phase component and amplitude component preserve different semantic information of the signals, which can be employed in domain augmentation techniques. The network comprises an amplitude spectrum sub-network and a phase spectrum sub-network, sequentially reducing the discrepancy between the source and target domains. To construct a more robust generalized model, we employ a multi-source domain data augmentation strategy in the frequency domain. Specifically, a Frequency-Spatial Interaction Module (FSIM) is introduced to handle global information and local spatial features, promoting representation learning between the two sub-networks. To refine the decision boundary of our model output compared to conventional triplet loss, we propose a manifold triplet loss to contribute to generalization. Through extensive experiments on the CWRU and SJTU datasets, FARNet demonstrates effective performance and achieves superior results compared to current cross-domain approaches on the benchmarks.

### Enhancing Field-Oriented Control of Electric Drives with Tiny Neural Network Optimized for Micro-controllers 
[[arxiv](https://arxiv.org/abs/2502.00532)] [[cool](https://papers.cool/arxiv/2502.00532)] [[pdf](https://arxiv.org/pdf/2502.00532)]
> **Authors**: Martin Joel Mouk Elele,Danilo Pau,Shixin Zhuang,Tullio Facchinetti
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: This paper has been submitted to the EDGEAIResearch Symposium 2025 (https://conf.researchr.org/home/tinyml-symp-2025#Call-for-papers previously known as tinyML Research Symposium). It was peer reviewed and camera ready updated accordingly to reviewer's feedback. It will be presented in EDGEAIFOUNDATION Austin 2025 (https://www.edgeaifoundation.org/events/austin-2024)
- **标题**: None
- **领域**: 机器学习,系统与控制
- **Abstract**: The deployment of neural networks on resource-constrained micro-controllers has gained momentum, driving many advancements in Tiny Neural Networks. This paper introduces a tiny feed-forward neural network, TinyFC, integrated into the Field-Oriented Control (FOC) of Permanent Magnet Synchronous Motors (PMSMs). Proportional-Integral (PI) controllers are widely used in FOC for their simplicity, although their limitations in handling nonlinear dynamics hinder precision. To address this issue, a lightweight 1,400 parameters TinyFC was devised to enhance the FOC performance while fitting into the computational and memory constraints of a micro-controller. Advanced optimization techniques, including pruning, hyperparameter tuning, and quantization to 8-bit integers, were applied to reduce the model's footprint while preserving the network effectiveness. Simulation results show the proposed approach significantly reduced overshoot by up to 87.5%, with the pruned model achieving complete overshoot elimination, highlighting the potential of tiny neural networks in real-time motor control applications.

### Generic Multimodal Spatially Graph Network for Spatially Embedded Network Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.00530)] [[cool](https://papers.cool/arxiv/2502.00530)] [[pdf](https://arxiv.org/pdf/2502.00530)]
> **Authors**: Xudong Fan,Jürgen Hackl
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络
- **Abstract**: Spatially embedded networks (SENs) represent a special type of complex graph, whose topologies are constrained by the networks' embedded spatial environments. The graph representation of such networks is thereby influenced by the embedded spatial features of both nodes and edges. Accurate network representation of the graph structure and graph features is a fundamental task for various graph-related tasks. In this study, a Generic Multimodal Spatially Graph Convolutional Network (GMu-SGCN) is developed for efficient representation of spatially embedded networks. The developed GMu-SGCN model has the ability to learn the node connection pattern via multimodal node and edge features. In order to evaluate the developed model, a river network dataset and a power network dataset have been used as test beds. The river network represents the naturally developed SENs, whereas the power network represents a man-made network. Both types of networks are heavily constrained by the spatial environments and uncertainties from nature. Comprehensive evaluation analysis shows the developed GMu-SGCN can improve accuracy of the edge existence prediction task by 37.1\% compared to a GraphSAGE model which only considers the node's position feature in a power network test bed. Our model demonstrates the importance of considering the multidimensional spatial feature for spatially embedded network representation.

### PolarQuant: Leveraging Polar Transformation for Efficient Key Cache Quantization and Decoding Acceleration 
[[arxiv](https://arxiv.org/abs/2502.00527)] [[cool](https://papers.cool/arxiv/2502.00527)] [[pdf](https://arxiv.org/pdf/2502.00527)]
> **Authors**: Songhao Wu,Ang Lv,Xiao Feng,Yufei Zhang,Xun Zhang,Guojun Yin,Wei Lin,Rui Yan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: preprint
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The KV cache in large language models is a dominant factor in memory usage, limiting their broader applicability. Quantizing the cache to lower bit widths is an effective way to reduce computational costs; however, previous methods struggle with quantizing key vectors due to outliers, resulting in excessive overhead. We propose a novel quantization approach called PolarQuant, which efficiently addresses the outlier challenge. We observe that outliers typically appear in only one of two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied. When represented as two-dimensional vectors, these dimensions exhibit well-structured patterns, with radii and angles smoothly distributed in polar coordinates. This alleviates the challenge of outliers on per-channel quantization, making them well-suited for quantization. Thus, PolarQuant divides key vectors into groups of two-dimensional sub-vectors, encoding them as the corresponding quantized radius and the polar angle, rather than quantizing original key vectors directly. PolarQuant achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models.

### Bridging Internal Probability and Self-Consistency for Effective and Efficient LLM Reasoning 
[[arxiv](https://arxiv.org/abs/2502.00511)] [[cool](https://papers.cool/arxiv/2502.00511)] [[pdf](https://arxiv.org/pdf/2502.00511)]
> **Authors**: Zhi Zhou,Tan Yuhao,Zenan Li,Yuan Yao,Lan-Zhe Guo,Xiaoxing Ma,Yu-Feng Li
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Preliminary work
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Recent advancements in large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, single-shot inference often yields unreliable results for complex reasoning tasks, leading researchers to explore multiple reasoning paths through methods such as perplexity and self-consistency. In this paper, we present the first theoretical error decomposition analysis of these techniques, breaking down their error into estimation error and model error. Our analysis reveals a fundamental trade-off: perplexity methods suffer from substantial model error due to the absence of a proper consistency function, while self-consistency exhibits high estimation error due to a slow error convergence rate. To overcome these limitations, we propose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines Perplexity Consistency, which seamlessly integrates LLM perplexity with self-consistency, and Reasoning Pruning, which eliminates low-probability reasoning paths to effectively prevent the degeneration of estimation error reduction. Theoretical analysis demonstrates that RPC not only accelerates the convergence rate of estimation error to an exponential level but also holds strong potential for further reducing model error. Extensive empirical evaluations on seven benchmark datasets confirm that RPC can significantly improve reasoning performance, sample efficiency, and confidence reliability.

### Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency Approach for ECG Classification 
[[arxiv](https://arxiv.org/abs/2502.00497)] [[cool](https://papers.cool/arxiv/2502.00497)] [[pdf](https://arxiv.org/pdf/2502.00497)]
> **Authors**: Sam Jeong,Hae Yong Kim
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Machine learning has transformed the classification of biomedical signals such as electrocardiograms (ECGs). Advances in deep learning, particularly convolutional neural networks (CNNs), enable automatic feature extraction, raising the question: Can combining time- and frequency-domain attributes enhance classification accuracy? To explore this, we evaluated three ECG classification tasks: (1) arrhythmia classification, (2) identity recognition, and (3) apnea detection. We initially tested three methods: (i) 2-D spectrogram-based frequency-time classification (SPECT), (ii) time-domain classification using a 1-D CNN (CNN1D), and (iii) frequency-domain classification using a Fourier transform-based CNN (FFT1D). Performance was validated using K-fold cross-validation. Among these, CNN1D (time only) performed best, followed by SPECT (time-frequency) and FFT1D (frequency only). Surprisingly, SPECT, which integrates time- and frequency-domain features, performed worse than CNN1D, suggesting a need for a more effective time and frequency fusion approach. To address this, we tested the recently proposed Fourier Analysis Network (FAN), which combines time- and frequency-domain features. However, FAN performed comparably to CNN1D, excelling in some tasks while underperforming in others. To enhance this approach, we developed the Convolutional Fourier Analysis Network (CFAN), which integrates FAN with CNN. CFAN outperformed all previous methods across all classification tasks. These findings underscore the advantages of combining time- and frequency-domain features, demonstrating CFAN's potential as a powerful and versatile solution for ECG classification and broader biomedical signal analysis

### Oscillations Make Neural Networks Robust to Quantization 
[[arxiv](https://arxiv.org/abs/2502.00490)] [[cool](https://papers.cool/arxiv/2502.00490)] [[pdf](https://arxiv.org/pdf/2502.00490)]
> **Authors**: Jonathan Wenshøj,Bob Pepin,Raghavendra Selvan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We challenge the prevailing view that oscillations in Quantization Aware Training (QAT) are merely undesirable artifacts caused by the Straight-Through Estimator (STE). Through theoretical analysis of QAT in linear models, we demonstrate that the gradient of the loss function can be decomposed into two terms: the original full-precision loss and a term that causes quantization oscillations. Based on these insights, we propose a novel regularization method that induces oscillations to improve quantization robustness. Contrary to traditional methods that focuses on minimizing the effects of oscillations, our approach leverages the beneficial aspects of weight oscillations to preserve model performance under quantization. Our empirical results on ResNet-18 and Tiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy at >= 3-bit weight quantization, while maintaining close to full precision accuracy at bits greater than the target bit. Our work therefore provides a new perspective on model preparation for quantization, particularly for finding weights that are robust to changes in the bit of the quantizer -- an area where current methods struggle to match the accuracy of QAT at specific bits.

### Learn Sharp Interface Solution by Homotopy Dynamics 
[[arxiv](https://arxiv.org/abs/2502.00488)] [[cool](https://papers.cool/arxiv/2502.00488)] [[pdf](https://arxiv.org/pdf/2502.00488)]
> **Authors**: Chuqi Chen,Yahong Yang,Yang Xiang,Wenrui Hao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Solving partial differential equations (PDEs) using neural networks has become a central focus in scientific machine learning. Training neural networks for sharp interface problems is particularly challenging due to certain parameters in the PDEs that introduce near-singularities in the loss function. In this study, we overcome this challenge by introducing a novel method based on homotopy dynamics to effectively manipulate these parameters. From a theoretical perspective, we analyze the effects of these parameters on training difficulty in sharp interface problems and establish the convergence of the proposed homotopy dynamics method. Experimentally, we demonstrate that our approach significantly accelerates convergence and improves the accuracy of sharp interface capturing. These findings present an efficient optimization strategy leveraging homotopy dynamics, offering a robust framework to extend the applicability of neural networks for solving PDEs with sharp

### Weak-to-Strong Diffusion with Reflection 
[[arxiv](https://arxiv.org/abs/2502.00473)] [[cool](https://papers.cool/arxiv/2502.00473)] [[pdf](https://arxiv.org/pdf/2502.00473)]
> **Authors**: Lichen Bai,Masashi Sugiyama,Zeke Xie
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 20 pages, 19 figures, 14 tables
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.

### Binned Spectral Power Loss for Improved Prediction of Chaotic Systems 
[[arxiv](https://arxiv.org/abs/2502.00472)] [[cool](https://papers.cool/arxiv/2502.00472)] [[pdf](https://arxiv.org/pdf/2502.00472)]
> **Authors**: Dibyajyoti Chakraborty,Arvind T. Mohan,Romit Maulik
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,动力系统,流体动力学
- **Abstract**: Forecasting multiscale chaotic dynamical systems with deep learning remains a formidable challenge due to the spectral bias of neural networks, which hinders the accurate representation of fine-scale structures in long-term predictions. This issue is exacerbated when models are deployed autoregressively, leading to compounding errors and instability. In this work, we introduce a novel approach to mitigate the spectral bias which we call the Binned Spectral Power (BSP) Loss. The BSP loss is a frequency-domain loss function that adaptively weighs errors in predicting both larger and smaller scales of the dataset. Unlike traditional losses that focus on pointwise misfits, our BSP loss explicitly penalizes deviations in the energy distribution across different scales, promoting stable and physically consistent predictions. We demonstrate that the BSP loss mitigates the well-known problem of spectral bias in deep learning. We further validate our approach for the data-driven high-dimensional time-series forecasting of a range of benchmark chaotic systems which are typically intractable due to spectral bias. Our results demonstrate that the BSP loss significantly improves the stability and spectral accuracy of neural forecasting models without requiring architectural modifications. By directly targeting spectral consistency, our approach paves the way for more robust deep learning models for long-term forecasting of chaotic dynamical systems.

### Enhancing Memory and Imagination Consistency in Diffusion-based World Models via Linear-Time Sequence Modeling 
[[arxiv](https://arxiv.org/abs/2502.00466)] [[cool](https://papers.cool/arxiv/2502.00466)] [[pdf](https://arxiv.org/pdf/2502.00466)]
> **Authors**: Jia-Hua Lee,Bor-Jiun Lin,Wei-Fang Sun,Chun-Yi Lee
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 26 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: World models are crucial for enabling agents to simulate and plan within environments, yet existing approaches struggle with long-term dependencies and inconsistent predictions. We introduce EDELINE, a novel framework that integrates diffusion models with linear-time state space modelsto enhance memory retention and temporal consistency. EDELINE employs a recurrent embedding module based on Mamba SSMs for processing unbounded sequences, a unified architecture for joint reward and termination prediction, and dynamic loss harmonization to balance multi-task learning. Our results across multiple benchmarks demonstrate EDELINE's superiority and robustness over prior baselines in long-horizon tasks.

### Enhance Learning Efficiency of Oblique Decision Tree via Feature Concatenation 
[[arxiv](https://arxiv.org/abs/2502.00465)] [[cool](https://papers.cool/arxiv/2502.00465)] [[pdf](https://arxiv.org/pdf/2502.00465)]
> **Authors**: Shen-Huan Lyu,Yi-Xiao He,Yanyan Wang,Zhihao Qu,Bin Tang,Baoliu Ye
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Oblique Decision Tree (ODT) separates the feature space by linear projections, as opposed to the conventional Decision Tree (DT) that forces axis-parallel splits. ODT has been proven to have a stronger representation ability than DT, as it provides a way to create shallower tree structures while still approximating complex decision boundaries. However, its learning efficiency is still insufficient, since the linear projections cannot be transmitted to the child nodes, resulting in a waste of model parameters. In this work, we propose an enhanced ODT method with Feature Concatenation (\texttt{FC-ODT}), which enables in-model feature transformation to transmit the projections along the decision paths. Theoretically, we prove that our method enjoys a faster consistency rate w.r.t. the tree depth, indicating that our method possesses a significant advantage in generalization performance, especially for shallow trees. Experiments show that \texttt{FC-ODT} can outperform the other state-of-the-art decision trees with a limited tree depth.

### Efficient Over-parameterized Matrix Sensing from Noisy Measurements via Alternating Preconditioned Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.00463)] [[cool](https://papers.cool/arxiv/2502.00463)] [[pdf](https://arxiv.org/pdf/2502.00463)]
> **Authors**: Zhiyu Liu,Zhi Han,Yandong Tang,Hai Zhang,Shaojie Tang,Yao Wang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorized form $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r > r_\star $, with the true rank $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + λI)^{-1} $ and $ (R^\top R + λI)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $λ$ and are sensitive to initial points and step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter and enabling faster convergence with larger step sizes. We theoretically prove that APGD achieves near-optimal error convergence at a linear rate, starting from arbitrary random initializations. Through extensive experiments, we validate our theoretical results and demonstrate that APGD outperforms other methods, achieving the fastest convergence rate. Notably, both our theoretical analysis and experimental results illustrate that APGD does not rely on the initialization procedure, making it more practical and versatile.

### Explorations of the Softmax Space: Knowing When the Neural Network Doesn't Know... 
[[arxiv](https://arxiv.org/abs/2502.00456)] [[cool](https://papers.cool/arxiv/2502.00456)] [[pdf](https://arxiv.org/pdf/2502.00456)]
> **Authors**: Daniel Sikar,Artur d'Avila Garcez,Tillman Weyde
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 5 figures, 1 table. arXiv admin note: substantial text overlap with arXiv:2407.07821
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Ensuring the reliability and safety of automated decision-making is crucial. This paper proposes a new approach for measuring the reliability of predictions in machine learning models. We analyze how the outputs of a trained neural network change using clustering to measure distances between outputs and class centroids. We propose this distance as a metric to evaluate the confidence of predictions. We assign each prediction to a cluster with centroid representing the mean softmax output for all correct predictions of a given class. We then define a safety threshold for a class as the smallest distance from an incorrect prediction to the given class centroid. We evaluate the approach on the MNIST and CIFAR-10 datasets using a Convolutional Neural Network and a Vision Transformer, respectively. The results show that our approach is consistent across these data sets and network models, and indicate that the proposed metric can offer an efficient way of determining when automated predictions are acceptable and when they should be deferred to human operators.

### Stochastic Linear Bandits with Latent Heterogeneity 
[[arxiv](https://arxiv.org/abs/2502.00423)] [[cool](https://papers.cool/arxiv/2502.00423)] [[pdf](https://arxiv.org/pdf/2502.00423)]
> **Authors**: Elynn Chen,Xi Chen,Wenbo Jing,Xiao Liu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: This paper addresses the critical challenge of latent heterogeneity in online decision-making, where individual responses to business actions vary due to unobserved characteristics. While existing approaches in data-driven decision-making have focused on observable heterogeneity through contextual features, they fall short when heterogeneity stems from unobservable factors such as lifestyle preferences and personal experiences. We propose a novel latent heterogeneous bandit framework that explicitly models this unobserved heterogeneity in customer responses, with promotion targeting as our primary example. Our methodology introduces an innovative algorithm that simultaneously learns latent group memberships and group-specific reward functions. Through theoretical analysis and empirical validation using data from a mobile commerce platform, we establish high-probability bounds for parameter estimation, convergence rates for group classification, and comprehensive regret bounds. Notably, our theoretical analysis reveals two distinct types of regret measures: a ``strong regret'' against an oracle with perfect knowledge of customer memberships, which remains non-sub-linear due to inherent classification uncertainty, and a ``regular regret'' against an oracle aware only of deterministic components, for which our algorithm achieves a sub-linear rate that is minimax optimal in horizon length and dimension. We further demonstrate that existing bandit algorithms ignoring latent heterogeneity incur constant average regret that accumulates linearly over time. Our framework provides practitioners with new tools for decision-making under latent heterogeneity and extends to various business applications, including personalized pricing, resource allocation, and inventory management.

### Predictive modeling and anomaly detection in large-scale web portals through the CAWAL framework 
[[arxiv](https://arxiv.org/abs/2502.00413)] [[cool](https://papers.cool/arxiv/2502.00413)] [[pdf](https://arxiv.org/pdf/2502.00413)]
> **Authors**: Ozkan Canay,Umit Kocabicak
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 15 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,信息检索
- **Abstract**: This study presents an approach that uses session and page view data collected through the CAWAL framework, enriched through specialized processes, for advanced predictive modeling and anomaly detection in web usage mining (WUM) applications. Traditional WUM methods often rely on web server logs, which limit data diversity and quality. Integrating application logs with web analytics, the CAWAL framework creates comprehensive session and page view datasets, providing a more detailed view of user interactions and effectively addressing these limitations. This integration enhances data diversity and quality while eliminating the preprocessing stage required in conventional WUM, leading to greater process efficiency. The enriched datasets, created by cross-integrating session and page view data, were applied to advanced machine learning models, such as Gradient Boosting and Random Forest, which are known for their effectiveness in capturing complex patterns and modeling non-linear relationships. These models achieved over 92% accuracy in predicting user behavior and significantly improved anomaly detection capabilities. The results show that this approach offers detailed insights into user behavior and system performance metrics, making it a reliable solution for improving large-scale web portals' efficiency, reliability, and scalability.

### Causal Abstraction Learning based on the Semantic Embedding Principle 
[[arxiv](https://arxiv.org/abs/2502.00407)] [[cool](https://papers.cool/arxiv/2502.00407)] [[pdf](https://arxiv.org/pdf/2502.00407)]
> **Authors**: Gabriele D'Acunto,Fabio Massimo Zennaro,Yorgos Felekis,Paolo Di Lorenzo
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is $\textit{semantic embedding}$, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the $\textit{Stiefel manifold}$. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.

### Spectro-Riemannian Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00401)] [[cool](https://papers.cool/arxiv/2502.00401)] [[pdf](https://arxiv.org/pdf/2502.00401)]
> **Authors**: Karish Grover,Haiyang Yu,Xiang Song,Qi Zhu,Han Xie,Vassilis N. Ioannidis,Christos Faloutsos
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Can integrating spectral and curvature signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph structures like scale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excels at processing signal variations across graphs, making it effective in homophilic and heterophilic settings. Leveraging both can significantly enhance the learned representations. To this end, we propose Spectro-Riemannian Graph Neural Networks (CUSP) - the first graph representation learning paradigm that unifies both CUrvature (geometric) and SPectral insights. CUSP is a mixed-curvature spectral GNN that learns spectral filters to optimize node embeddings in products of constant-curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically, CUSP introduces three novel components: (a) Cusp Laplacian, an extension of the traditional graph Laplacian based on Ollivier-Ricci curvature, designed to capture the curvature signals better; (b) Cusp Filtering, which employs multiple Riemannian graph filters to obtain cues from various bands in the eigenspectrum; and (c) Cusp Pooling, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently curved substructures in our graph. Empirical evaluation across eight homophilic and heterophilic datasets demonstrates the superiority of CUSP in node classification and link prediction tasks, with a gain of up to 5.3% over state-of-the-art models.

### CoHiRF: A Scalable and Interpretable Clustering Framework for High-Dimensional Data 
[[arxiv](https://arxiv.org/abs/2502.00380)] [[cool](https://papers.cool/arxiv/2502.00380)] [[pdf](https://arxiv.org/pdf/2502.00380)]
> **Authors**: Bruno Belucci,Karim Lounici,Katia Meziani
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Clustering high-dimensional data poses significant challenges due to the curse of dimensionality, scalability issues, and the presence of noisy and irrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF), a novel clustering method designed to address these challenges effectively. CoHiRF leverages random feature selection to mitigate noise and dimensionality effects, repeatedly applies K-Means clustering in reduced feature spaces, and combines results through a unanimous consensus criterion. This iterative approach constructs a cluster assignment matrix, where each row records the cluster assignments of a sample across repetitions, enabling the identification of stable clusters by comparing identical rows. Clusters are organized hierarchically, enabling the interpretation of the hierarchy to gain insights into the dataset. CoHiRF is computationally efficient with a running time comparable to K-Means, scalable to massive datasets, and exhibits robust performance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and OPTICS. Experimental results on synthetic and real-world datasets confirm the method's ability to reveal meaningful patterns while maintaining scalability, making it a powerful tool for high-dimensional data analysis.

### SSRepL-ADHD: Adaptive Complex Representation Learning Framework for ADHD Detection from Visual Attention Tasks 
[[arxiv](https://arxiv.org/abs/2502.00376)] [[cool](https://papers.cool/arxiv/2502.00376)] [[pdf](https://arxiv.org/pdf/2502.00376)]
> **Authors**: Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: ef:2024 IEEE International Conference on Big Data (BigData)
- **标题**: None
- **领域**: 机器学习,人机交互,信号处理
- **Abstract**: Self Supervised Representation Learning (SSRepL) can capture meaningful and robust representations of the Attention Deficit Hyperactivity Disorder (ADHD) data and have the potential to improve the model's performance on also downstream different types of Neurodevelopmental disorder (NDD) detection. In this paper, a novel SSRepL and Transfer Learning (TL)-based framework that incorporates a Long Short-Term Memory (LSTM) and a Gated Recurrent Units (GRU) model is proposed to detect children with potential symptoms of ADHD. This model uses Electroencephalogram (EEG) signals extracted during visual attention tasks to accurately detect ADHD by preprocessing EEG signal quality through normalization, filtering, and data balancing. For the experimental analysis, we use three different models: 1) SSRepL and TL-based LSTM-GRU model named as SSRepL-ADHD, which integrates LSTM and GRU layers to capture temporal dependencies in the data, 2) lightweight SSRepL-based DNN model (LSSRepL-DNN), and 3) Random Forest (RF). In the study, these models are thoroughly evaluated using well-known performance metrics (i.e., accuracy, precision, recall, and F1-score). The results show that the proposed SSRepL-ADHD model achieves the maximum accuracy of 81.11% while admitting the difficulties associated with dataset imbalance and feature selection.

### Generalized Lie Symmetries in Physics-Informed Neural Operators 
[[arxiv](https://arxiv.org/abs/2502.00373)] [[cool](https://papers.cool/arxiv/2502.00373)] [[pdf](https://arxiv.org/pdf/2502.00373)]
> **Authors**: Amy Xiang Wang,Zakhar Shumaylov,Peter Zaika,Ferdia Sherry,Carola-Bibiane Schönlieb
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: SCML 2025 Oral
- **标题**: None
- **领域**: 机器学习,计算物理
- **Abstract**: Physics-informed neural operators (PINOs) have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). Recent research has demonstrated that incorporating Lie point symmetry information can significantly enhance the training efficiency of PINOs, primarily through techniques like data, architecture, and loss augmentation. In this work, we focus on the latter, highlighting that point symmetries oftentimes result in no training signal, limiting their effectiveness in many problems. To address this, we propose a novel loss augmentation strategy that leverages evolutionary representatives of point symmetries, a specific class of generalized symmetries of the underlying PDE. These generalized symmetries provide a richer set of generators compared to standard symmetries, leading to a more informative training signal. We demonstrate that leveraging evolutionary representatives enhances the performance of neural operators, resulting in improved data efficiency and accuracy during training.

### What should an AI assessor optimise for? 
[[arxiv](https://arxiv.org/abs/2502.00365)] [[cool](https://papers.cool/arxiv/2502.00365)] [[pdf](https://arxiv.org/pdf/2502.00365)]
> **Authors**: Daniel Romero-Alvarado,Fernando Martínez-Plumed,José Hernández-Orallo
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: An AI assessor is an external, ideally indepen-dent system that predicts an indicator, e.g., a loss value, of another AI system. Assessors can lever-age information from the test results of many other AI systems and have the flexibility of be-ing trained on any loss function or scoring rule: from squared error to toxicity metrics. Here we address the question: is it always optimal to train the assessor for the target metric? Or could it be better to train for a different metric and then map predictions back to the target metric? Us-ing twenty regression and classification problems with tabular data, we experimentally explore this question for, respectively, regression losses and classification scores with monotonic and non-monotonic mappings and find that, contrary to intuition, optimising for more informative met-rics is not generally better. Surprisingly, some monotonic transformations are promising. For example, the logistic loss is useful for minimis-ing absolute or quadratic errors in regression, and the logarithmic score helps maximise quadratic or spherical scores in classification.

### Machine Learning Models for Reinforced Concrete Pipes Condition Prediction: The State-of-the-Art Using Artificial Neural Networks and Multiple Linear Regression in a Wisconsin Case Study 
[[arxiv](https://arxiv.org/abs/2502.00363)] [[cool](https://papers.cool/arxiv/2502.00363)] [[pdf](https://arxiv.org/pdf/2502.00363)]
> **Authors**: Mohsen Mohammadagha,Mohammad Najafi,Vinayak Kaushal,Ahmad Mahmoud Ahmad Jibreen
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: The aging sewer infrastructure in the U.S., covering 2.1 million kilometers, encounters increasing structural issues, resulting in around 75,000 yearly sanitary sewer overflows that present serious economic, environmental, and public health hazards. Conventional inspection techniques and deterministic models do not account for the unpredictable nature of sewer decline, whereas probabilistic methods depend on extensive historical data, which is frequently lacking or incomplete. This research intends to enhance predictive accuracy for the condition of sewer pipelines through machine learning models artificial neural networks (ANNs) and multiple linear regression (MLR) by integrating factors such as pipe age, material, diameter, environmental influences, and PACP ratings. ANNs utilized ReLU activation functions and Adam optimization, whereas MLR applied regularization to address multicollinearity, with both models assessed through metrics like RMSE, MAE, and R2. The findings indicated that ANNs surpassed MLR, attaining an R2 of 0.9066 compared to MLRs 0.8474, successfully modeling nonlinear relationships while preserving generalization. MLR, on the other hand, offered enhanced interpretability by pinpointing significant predictors such as residual buildup. As a result, pipeline degradation is driven by pipe length, age, and pipe diameter as key predictors, while depth, soil type, and segment show minimal influence in this analysis. Future studies ought to prioritize hybrid models that merge the accuracy of ANNs with the interpretability of MLR, incorporating advanced methods such as SHAP analysis and transfer learning to improve scalability in managing infrastructure and promoting environmental sustainability.

### Soft Diffusion Actor-Critic: Efficient Online Reinforcement Learning for Diffusion Policy 
[[arxiv](https://arxiv.org/abs/2502.00361)] [[cool](https://papers.cool/arxiv/2502.00361)] [[pdf](https://arxiv.org/pdf/2502.00361)]
> **Authors**: Haitong Ma,Tianyi Chen,Kai Wang,Na Li,Bo Dai
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 19 pages, 4 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the vanilla diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy, making training diffusion policies highly non-trivial in online RL. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and impractical. To enable efficient diffusion policy training for online RL, we propose Soft Diffusion Actor-Critic (SDAC), exploiting the viewpoint of diffusion models as noise-perturbed energy-based models. The proposed SDAC relies solely on the state-action value function as the energy functions to train diffusion policies, bypassing sampling from the optimal policy while maintaining lightweight computations. We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that SDAC outperforms all recent diffusion-policy online RLs on most tasks, and improves more than 120% over soft actor-critic on complex locomotion tasks such as Humanoid and Ant.

### Exploring Representation-Aligned Latent Space for Better Generation 
[[arxiv](https://arxiv.org/abs/2502.00359)] [[cool](https://papers.cool/arxiv/2502.00359)] [[pdf](https://arxiv.org/pdf/2502.00359)]
> **Authors**: Wanghan Xu,Xiaoyu Yue,Zidong Wang,Yao Teng,Wenlong Zhang,Xihui Liu,Luping Zhou,Wanli Ouyang,Lei Bai
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Generative models serve as powerful tools for modeling the real world, with mainstream diffusion models, particularly those based on the latent diffusion model paradigm, achieving remarkable progress across various tasks, such as image and video synthesis. Latent diffusion models are typically trained using Variational Autoencoders (VAEs), interacting with VAE latents rather than the real samples. While this generative paradigm speeds up training and inference, the quality of the generated outputs is limited by the latents' quality. Traditional VAE latents are often seen as spatial compression in pixel space and lack explicit semantic representations, which are essential for modeling the real world. In this paper, we introduce ReaLS (Representation-Aligned Latent Space), which integrates semantic priors to improve generation performance. Extensive experiments show that fundamental DiT and SiT trained on ReaLS can achieve a 15% improvement in FID metric. Furthermore, the enhanced semantic latent space enables more perceptual downstream tasks, such as segmentation and depth estimation.

### Sampling in High-Dimensions using Stochastic Interpolants and Forward-Backward Stochastic Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.00355)] [[cool](https://papers.cool/arxiv/2502.00355)] [[pdf](https://arxiv.org/pdf/2502.00355)]
> **Authors**: Anand Jerry George,Nicolas Macris
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present a class of diffusion-based algorithms to draw samples from high-dimensional probability distributions given their unnormalized densities. Ideally, our methods can transport samples from a Gaussian distribution to a specified target distribution in finite time. Our approach relies on the stochastic interpolants framework to define a time-indexed collection of probability densities that bridge a Gaussian distribution to the target distribution. Subsequently, we derive a diffusion process that obeys the aforementioned probability density at each time instant. Obtaining such a diffusion process involves solving certain Hamilton-Jacobi-Bellman PDEs. We solve these PDEs using the theory of forward-backward stochastic differential equations (FBSDE) together with machine learning-based methods. Through numerical experiments, we demonstrate that our algorithm can effectively draw samples from distributions that conventional methods struggle to handle.

### PM-MOE: Mixture of Experts on Private Model Parameters for Personalized Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.00354)] [[cool](https://papers.cool/arxiv/2502.00354)] [[pdf](https://arxiv.org/pdf/2502.00354)]
> **Authors**: Yu Feng,Yangli-ao Geng,Yifan Zhu,Zongfu Han,Xie Yu,Kaiwen Xue,Haoran Luo,Mengyang Sun,Guangwei Zhang,Meina Song
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Federated learning (FL) has gained widespread attention for its privacy-preserving and collaborative learning capabilities. Due to significant statistical heterogeneity, traditional FL struggles to generalize a shared model across diverse data domains. Personalized federated learning addresses this issue by dividing the model into a globally shared part and a locally private part, with the local model correcting representation biases introduced by the global model. Nevertheless, locally converged parameters more accurately capture domain-specific knowledge, and current methods overlook the potential benefits of these parameters. To address these limitations, we propose PM-MoE architecture. This architecture integrates a mixture of personalized modules and an energy-based personalized modules denoising, enabling each client to select beneficial personalized parameters from other clients. We applied the PM-MoE architecture to nine recent model-split-based personalized federated learning algorithms, achieving performance improvements with minimal additional training. Extensive experiments on six widely adopted datasets and two heterogeneity settings validate the effectiveness of our approach. The source code is available at \url{https://github.com/dannis97500/PM-MOE}.

### The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.00345)] [[cool](https://papers.cool/arxiv/2502.00345)] [[pdf](https://arxiv.org/pdf/2502.00345)]
> **Authors**: Yurui Li,Yuxuan Chen,Li Zhang,Shijian Li,Gang Pan
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications.Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies.Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks.To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation.In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design.We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations.We evaluate 10 cooperative MARL methods on the proposed tasks.The results indicate that all baselines perform poorly on these tasks.To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks.Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks.The source files is available at https://github.com/Yurui-Li/CTC.

### Enhancing Token Filtering Efficiency in Large Language Model Training with Collider 
[[arxiv](https://arxiv.org/abs/2502.00340)] [[cool](https://papers.cool/arxiv/2502.00340)] [[pdf](https://arxiv.org/pdf/2502.00340)]
> **Authors**: Di Chai,Pengbo Li,Feiyuan Zhang,Yilun Jin,Han Tian,Junxue Zhang,Kai Chen
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,分布式、并行和集群计算
- **Abstract**: Token filtering has been proposed to enhance utility of large language models (LLMs) by eliminating inconsequential tokens during training. While using fewer tokens should reduce computational workloads, existing studies have not succeeded in achieving higher efficiency. This is primarily due to the insufficient sparsity caused by filtering tokens only in the output layers, as well as inefficient sparse GEMM (General Matrix Multiplication), even when having sufficient sparsity. This paper presents Collider, a system unleashing the full efficiency of token filtering in LLM training. At its core, Collider filters activations of inconsequential tokens across all layers to maintain sparsity. Additionally, it features an automatic workflow that transforms sparse GEMM into dimension-reduced dense GEMM for optimized efficiency. Evaluations on three LLMs-TinyLlama-1.1B, Qwen2.5-1.5B, and Phi1.5-1.4B-demonstrate that Collider reduces backpropagation time by up to 35.1% and end-to-end training time by up to 22.0% when filtering 40% of tokens. Utility assessments of training TinyLlama on 15B tokens indicate that Collider sustains the utility advancements of token filtering by relatively improving model utility by 16.3% comparing to regular training, and reduces training time from 4.7 days to 3.5 days using 8 GPUs. Collider is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code.

### OneForecast: A Universal Framework for Global and Regional Weather Forecasting 
[[arxiv](https://arxiv.org/abs/2502.00338)] [[cool](https://papers.cool/arxiv/2502.00338)] [[pdf](https://arxiv.org/pdf/2502.00338)]
> **Authors**: Yuan Gao,Hao Wu,Ruiqi Shu,Huanshuo Dong,Fan Xu,Rui Chen,Yibo Yan,Qingsong Wen,Xuming Hu,Kun Wang,Jiahao Wu,Qing Li,Hui Xiong,Xiaomeng Huang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,大气和海洋物理
- **Abstract**: Accurate weather forecasts are important for disaster prevention, agricultural planning, and water resource management. Traditional numerical weather prediction (NWP) methods offer physically interpretable high-accuracy predictions but are computationally expensive and fail to fully leverage rapidly growing historical data. In recent years, deep learning methods have made significant progress in weather forecasting, but challenges remain, such as balancing global and regional high-resolution forecasts, excessive smoothing in extreme event predictions, and insufficient dynamic system modeling. To address these issues, this paper proposes a global-regional nested weather forecasting framework based on graph neural networks (GNNs). By combining a dynamic system perspective with multi-grid theory, we construct a multi-scale graph structure and densify the target region to capture local high-frequency features. We introduce an adaptive information propagation mechanism, using dynamic gating units to deeply integrate node and edge features for more accurate extreme event forecasting. For high-resolution regional forecasts, we propose a neural nested grid method to mitigate boundary information loss. Experimental results show that the proposed method performs excellently across global to regional scales and short-term to long-term forecasts, especially in extreme event predictions (e.g., typhoons), significantly improving forecast accuracy. Our codes are available at https://github.com/YuanGao-YG/OneForecast.

### Denoising Score Matching with Random Features: Insights on Diffusion Models from Precise Learning Curves 
[[arxiv](https://arxiv.org/abs/2502.00336)] [[cool](https://papers.cool/arxiv/2502.00336)] [[pdf](https://arxiv.org/pdf/2502.00336)]
> **Authors**: Anand Jerry George,Rodrigo Veiga,Nicolas Macris
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We derive asymptotically precise expressions for test and train errors of denoising score matching (DSM) in generative diffusion models. The score function is parameterized by random features neural networks, with the target distribution being $d$-dimensional standard Gaussian. We operate in a regime where the dimension $d$, number of data samples $n$, and number of features $p$ tend to infinity while keeping the ratios $ψ_n=\frac{n}{d}$ and $ψ_p=\frac{p}{d}$ fixed. By characterizing the test and train errors, we identify regimes of generalization and memorization in diffusion models. Furthermore, our work sheds light on the conditions enhancing either generalization or memorization. Consistent with prior empirical observations, our findings indicate that the model complexity ($p$) and the number of noise samples per data sample ($m$) used during DSM significantly influence generalization and memorization behaviors.

### From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation 
[[arxiv](https://arxiv.org/abs/2502.00330)] [[cool](https://papers.cool/arxiv/2502.00330)] [[pdf](https://arxiv.org/pdf/2502.00330)]
> **Authors**: Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Sercan Ö. Arık
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Expanded version of the ICLR 2025 paper
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples ("optimize") and using them as demonstrations to regenerate new examples ("generate") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation.

### $k$-SVD with Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.00320)] [[cool](https://papers.cool/arxiv/2502.00320)] [[pdf](https://arxiv.org/pdf/2502.00320)]
> **Authors**: Emily Gan,Yassir Jedra,Devavrat Shah
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: We show that a gradient-descent with a simple, universal rule for step-size selection provably finds $k$-SVD, i.e., the $k\geq 1$ largest singular values and corresponding vectors, of any matrix, despite nonconvexity. There has been substantial progress towards this in the past few years where existing results are able to establish such guarantees for the \emph{exact-parameterized} and \emph{over-parameterized} settings, with choice of oracle-provided step size. But guarantees for generic setting with a step size selection that does not require oracle-provided information has remained a challenge. We overcome this challenge and establish that gradient descent with an appealingly simple adaptive step size (akin to preconditioning) and random initialization enjoys global linear convergence for generic setting. Our convergence analysis reveals that the gradient method has an attracting region, and within this attracting region, the method behaves like Heron's method (a.k.a. the Babylonian method). Empirically, we validate the theoretical results. The emergence of modern compute infrastructure for iterative optimization coupled with this work is likely to provide means to solve $k$-SVD for very large matrices.

### Physics-Inspired Distributed Radio Map Estimation 
[[arxiv](https://arxiv.org/abs/2502.00319)] [[cool](https://papers.cool/arxiv/2502.00319)] [[pdf](https://arxiv.org/pdf/2502.00319)]
> **Authors**: Dong Yang,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,信号处理
- **Abstract**: To gain panoramic awareness of spectrum coverage in complex wireless environments, data-driven learning approaches have recently been introduced for radio map estimation (RME). While existing deep learning based methods conduct RME given spectrum measurements gathered from dispersed sensors in the region of interest, they rely on centralized data at a fusion center, which however raises critical concerns on data privacy leakages and high communication overloads. Federated learning (FL) enhance data security and communication efficiency in RME by allowing multiple clients to collaborate in model training without directly sharing local data. However, the performance of the FL-based RME can be hindered by the problem of task heterogeneity across clients due to their unavailable or inaccurate landscaping information. To fill this gap, in this paper, we propose a physics-inspired distributed RME solution in the absence of landscaping information. The main idea is to develop a novel distributed RME framework empowered by leveraging the domain knowledge of radio propagation models, and by designing a new distributed learning approach that splits the entire RME model into two modules. A global autoencoder module is shared among clients to capture the common pathloss influence on radio propagation pattern, while a client-specific autoencoder module focuses on learning the individual features produced by local shadowing effects from the unique building distributions in local environment. Simulation results show that our proposed method outperforms the benchmarks in achieving higher performance.

### Sub-Sequential Physics-Informed Learning with State Space Model 
[[arxiv](https://arxiv.org/abs/2502.00318)] [[cool](https://papers.cool/arxiv/2502.00318)] [[pdf](https://arxiv.org/pdf/2502.00318)]
> **Authors**: Chenhui Xu,Dancheng Liu,Yuting Hu,Jiajie Li,Ruiyang Qin,Qingxiao Zheng,Jinjun Xiong
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Physics-Informed Neural Networks (PINNs) are a kind of deep-learning-based numerical solvers for partial differential equations (PDEs). Existing PINNs often suffer from failure modes of being unable to propagate patterns of initial conditions. We discover that these failure modes are caused by the simplicity bias of neural networks and the mismatch between PDE's continuity and PINN's discrete sampling. We reveal that the State Space Model (SSM) can be a continuous-discrete articulation allowing initial condition propagation, and that simplicity bias can be eliminated by aligning a sequence of moderate granularity. Accordingly, we propose PINNMamba, a novel framework that introduces sub-sequence modeling with SSM. Experimental results show that PINNMamba can reduce errors by up to 86.3\% compared with state-of-the-art architecture. Our code is available at https://github.com/miniHuiHui/PINNMamba.

### Sparse Gradient Compression for Fine-Tuning Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00311)] [[cool](https://papers.cool/arxiv/2502.00311)] [[pdf](https://arxiv.org/pdf/2502.00311)]
> **Authors**: David H. Yang,Mohammad Mohammadi Amiri,Tejaswini Pedapati,Subhajit Chaudhury,Pin-Yu Chen
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Fine-tuning large language models (LLMs) for downstream tasks has become increasingly crucial due to their widespread use and the growing availability of open-source models. However, the high memory costs associated with fine-tuning remain a significant challenge, especially as models increase in size. To address this, parameter efficient fine-tuning (PEFT) methods have been proposed to minimize the number of parameters required for fine-tuning LLMs. However, these approaches often tie the number of optimizer states to dimensions of model parameters, limiting flexibility and control during fine-tuning. In this paper, we propose sparse gradient compression (SGC), a training regime designed to address these limitations. Our approach leverages inherent sparsity in gradients to compress optimizer states by projecting them onto a low-dimensonal subspace, with dimensionality independent of the original model's parameters. By enabling optimizer state updates in an arbitrary low-dimensional subspace, SGC offers a flexible tradeoff between memory efficiency and performance. We demonstrate through experiments that SGC can decrease memory usage in optimizer states more effectively than existing PEFT methods. Furthermore, by fine-tuning LLMs on various downstream tasks, we show that SGC can deliver superior performance while substantially lowering optimizer state memory requirements, particularly in both data-limited and memory-limited settings.

### HoP: Homeomorphic Polar Learning for Hard Constrained Optimization 
[[arxiv](https://arxiv.org/abs/2502.00304)] [[cool](https://papers.cool/arxiv/2502.00304)] [[pdf](https://arxiv.org/pdf/2502.00304)]
> **Authors**: Ke Deng,Hanwen Zhang,Jin Lu,Haijian Sun
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: in submission
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制
- **Abstract**: Constrained optimization demands highly efficient solvers which promotes the development of learn-to-optimize (L2O) approaches. As a data-driven method, L2O leverages neural networks to efficiently produce approximate solutions. However, a significant challenge remains in ensuring both optimality and feasibility of neural networks' output. To tackle this issue, we introduce Homeomorphic Polar Learning (HoP) to solve the star-convex hard-constrained optimization by embedding homeomorphic mapping in neural networks. The bijective structure enables end-to-end training without extra penalty or correction. For performance evaluation, we evaluate HoP's performance across a variety of synthetic optimization tasks and real-world applications in wireless communications. In all cases, HoP achieves solutions closer to the optimum than existing L2O methods while strictly maintaining feasibility.

### Uncertainty Quantification of Wind Gust Predictions in the Northeast US: An Evidential Neural Network and Explainable Artificial Intelligence Approach 
[[arxiv](https://arxiv.org/abs/2502.00300)] [[cool](https://papers.cool/arxiv/2502.00300)] [[pdf](https://arxiv.org/pdf/2502.00300)]
> **Authors**: Israt Jahan,John S. Schreck,David John Gagne,Charlie Becker,Marina Astitha
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Main body 27 pages with 12 figures
- **标题**: None
- **领域**: 机器学习,大气和海洋物理,机器学习
- **Abstract**: Machine learning has shown promise in reducing bias in numerical weather model predictions of wind gusts. Yet, they underperform to predict high gusts even with additional observations due to the right-skewed distribution of gusts. Uncertainty quantification (UQ) addresses this by identifying when predictions are reliable or needs cautious interpretation. Using data from 61 extratropical storms in the Northeastern USA, we introduce evidential neural network (ENN) as a novel approach for UQ in gust predictions, leveraging atmospheric variables from the Weather Research and Forecasting (WRF) model as features and gust observations as targets. Explainable artificial intelligence (XAI) techniques demonstrated that key predictive features also contributed to higher uncertainty. Estimated uncertainty correlated with storm intensity and spatial gust gradients. ENN allowed constructing gust prediction intervals without requiring an ensemble. From an operational perspective, providing gust forecasts with quantified uncertainty enhances stakeholders' confidence in risk assessment and response planning for extreme gust events.

### The Price of Linear Time: Error Analysis of Structured Kernel Interpolation 
[[arxiv](https://arxiv.org/abs/2502.00298)] [[cool](https://papers.cool/arxiv/2502.00298)] [[pdf](https://arxiv.org/pdf/2502.00298)]
> **Authors**: Alexander Moreno,Justin Xiao,Jonathan Mei
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Structured Kernel Interpolation (SKI) (Wilson et al. 2015) helps scale Gaussian Processes (GPs) by approximating the kernel matrix via interpolation at inducing points, achieving linear computational complexity. However, it lacks rigorous theoretical error analysis. This paper bridges the gap: we prove error bounds for the SKI Gram matrix and examine the error's effect on hyperparameter estimation and posterior inference. We further provide a practical guide to selecting the number of inducing points under convolutional cubic interpolation: they should grow as $n^{d/3}$ for error control. Crucially, we identify two dimensionality regimes governing the trade-off between SKI Gram matrix spectral norm error and computational complexity. For $d \leq 3$, any error tolerance can achieve linear time for sufficiently large sample size. For $d > 3$, the error must increase with sample size to maintain linear time. Our analysis provides key insights into SKI's scalability-accuracy trade-offs, establishing precise conditions for achieving linear-time GP inference with controlled approximation error.

### Learning from Suboptimal Data in Continuous Control via Auto-Regressive Soft Q-Network 
[[arxiv](https://arxiv.org/abs/2502.00288)] [[cool](https://papers.cool/arxiv/2502.00288)] [[pdf](https://arxiv.org/pdf/2502.00288)]
> **Authors**: Jijia Liu,Feng Gao,Qingmin Liao,Chao Yu,Yu Wang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器人技术
- **Abstract**: Reinforcement learning (RL) for continuous control often requires large amounts of online interaction data. Value-based RL methods can mitigate this burden by offering relatively high sample efficiency. Some studies further enhance sample efficiency by incorporating offline demonstration data to "kick-start" training, achieving promising results in continuous control. However, they typically compute the Q-function independently for each action dimension, neglecting interdependencies and making it harder to identify optimal actions when learning from suboptimal data, such as non-expert demonstration and online-collected data during the training process. To address these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner. First, ARSQ decomposes the continuous action space into discrete spaces in a coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained continuous control tasks. Next, it auto-regressively predicts dimensional action advantages within each decision step, enabling more effective decision-making in continuous control tasks. We evaluate ARSQ on two continuous control benchmarks, RLBench and D4RL, integrating demonstration data into online training. On D4RL, which includes non-expert demonstrations, ARSQ achieves an average $1.62\times$ performance improvement over SOTA value-based baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses various baselines, demonstrating its effectiveness in learning from suboptimal online-collected data.

### K Nearest Neighbor-Guided Trajectory Similarity Learning 
[[arxiv](https://arxiv.org/abs/2502.00285)] [[cool](https://papers.cool/arxiv/2502.00285)] [[pdf](https://arxiv.org/pdf/2502.00285)]
> **Authors**: Yanchuan Chang,Xu Cai,Christian S. Jensen,Jianzhong Qi
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,数据库
- **Abstract**: Trajectory similarity is fundamental to many spatio-temporal data mining applications. Recent studies propose deep learning models to approximate conventional trajectory similarity measures, exploiting their fast inference time once trained. Although efficient inference has been reported, challenges remain in similarity approximation accuracy due to difficulties in trajectory granularity modeling and in exploiting similarity signals in the training data. To fill this gap, we propose TSMini, a highly effective trajectory similarity model with a sub-view modeling mechanism capable of learning multi-granularity trajectory patterns and a k nearest neighbor-based loss that guides TSMini to learn not only absolute similarity values between trajectories but also their relative similarity ranks. Together, these two innovations enable highly accurate trajectory similarity approximation. Experiments show that TSMini can outperform the state-of-the-art models by 22% in accuracy on average when learning trajectory similarity measures.

### GraphMinNet: Learning Dependencies in Graphs with Light Complexity Minimal Architecture 
[[arxiv](https://arxiv.org/abs/2502.00282)] [[cool](https://papers.cool/arxiv/2502.00282)] [[pdf](https://arxiv.org/pdf/2502.00282)]
> **Authors**: Md Atik Ahamed,Andrew Cheng,Qiang Ye,Qiang Cheng
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Neural Networks (GNNs) have demonstrated remarkable success in various applications, yet they often struggle to capture long-range dependencies (LRD) effectively. This paper introduces GraphMinNet, a novel GNN architecture that generalizes the idea of minimal Gated Recurrent Units to graph-structured data. Our approach achieves efficient LRD modeling with linear computational complexity while maintaining permutation equivariance and stability. The model incorporates both structural and positional information through a unique combination of feature and positional encodings, leading to provably stronger expressiveness than the 1-WL test. Theoretical analysis establishes that GraphMinNet maintains non-decaying gradients over long distances, ensuring effective long-range information propagation. Extensive experiments on ten diverse datasets, including molecular graphs, image graphs, and synthetic networks, demonstrate that GraphMinNet achieves state-of-the-art performance while being computationally efficient. Our results show superior performance on 6 out of 10 datasets and competitive results on the others, validating the effectiveness of our approach in capturing both local and global graph structures.

### Sigmoid Self-Attention is Better than Softmax Self-Attention: A Mixture-of-Experts Perspective 
[[arxiv](https://arxiv.org/abs/2502.00281)] [[cool](https://papers.cool/arxiv/2502.00281)] [[pdf](https://arxiv.org/pdf/2502.00281)]
> **Authors**: Fanqi Yan,Huy Nguyen,Pedram Akbarian,Nhat Ho,Alessandro Rinaldo
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Fanqi Yan, Huy Nguyen contributed equally to this work. 51 pages, 2 figures, 3 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: At the core of the popular Transformer architecture is the self-attention mechanism, which dynamically assigns softmax weights to each input token so that the model can focus on the most salient information. However, the softmax structure slows down the attention computation due to its row-wise nature, and inherently introduces competition among tokens: as the weight assigned to one token increases, the weights of others decrease. This competitive dynamic may narrow the focus of self-attention to a limited set of features, potentially overlooking other informative characteristics. Recent experimental studies have shown that using the element-wise sigmoid function helps eliminate token competition and reduce the computational overhead. Despite these promising empirical results, a rigorous comparison between sigmoid and softmax self-attention mechanisms remains absent in the literature. This paper closes this gap by theoretically demonstrating that sigmoid self-attention is more sample-efficient than its softmax counterpart. Toward that goal, we illustrate that each row of the self-attention matrix can be represented as a mixture of experts. Our analysis shows that ''experts'' in sigmoid self-attention require significantly less data to achieve the same approximation error as those in softmax self-attention. We corroborate our theoretical findings through extensive experiments on both synthetic and real-world datasets.

### On the study of frequency control and spectral bias in Wavelet-Based Kolmogorov Arnold networks: A path to physics-informed KANs 
[[arxiv](https://arxiv.org/abs/2502.00280)] [[cool](https://papers.cool/arxiv/2502.00280)] [[pdf](https://arxiv.org/pdf/2502.00280)]
> **Authors**: Juan Daniel Meshir,Abel Palafox,Edgar Alejandro Guerrero
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 29 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Spectral bias, the tendency of neural networks to prioritize learning low-frequency components of functions during the initial training stages, poses a significant challenge when approximating solutions with high-frequency details. This issue is particularly pronounced in physics-informed neural networks (PINNs), widely used to solve differential equations that describe physical phenomena. In the literature, contributions such as Wavelet Kolmogorov Arnold Networks (Wav-KANs) have demonstrated promising results in capturing both low- and high-frequency components. Similarly, Fourier features (FF) are often employed to address this challenge. However, the theoretical foundations of Wav-KANs, particularly the relationship between the frequency of the mother wavelet and spectral bias, remain underexplored. A more in-depth understanding of how Wav-KANs manage high-frequency terms could offer valuable insights for addressing oscillatory phenomena encountered in parabolic, elliptic, and hyperbolic differential equations. In this work, we analyze the eigenvalues of the neural tangent kernel (NTK) of Wav-KANs to enhance their ability to converge on high-frequency components, effectively mitigating spectral bias. Our theoretical findings are validated through numerical experiments, where we also discuss the limitations of traditional approaches, such as standard PINNs and Fourier features, in addressing multi-frequency problems.

### Improving realistic semi-supervised learning with doubly robust estimation 
[[arxiv](https://arxiv.org/abs/2502.00279)] [[cool](https://papers.cool/arxiv/2502.00279)] [[pdf](https://arxiv.org/pdf/2502.00279)]
> **Authors**: Khiem Pham,Charles Herrmann,Ramin Zabih
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: A major challenge in Semi-Supervised Learning (SSL) is the limited information available about the class distribution in the unlabeled data. In many real-world applications this arises from the prevalence of long-tailed distributions, where the standard pseudo-label approach to SSL is biased towards the labeled class distribution and thus performs poorly on unlabeled data. Existing methods typically assume that the unlabeled class distribution is either known a priori, which is unrealistic in most situations, or estimate it on-the-fly using the pseudo-labels themselves. We propose to explicitly estimate the unlabeled class distribution, which is a finite-dimensional parameter, \emph{as an initial step}, using a doubly robust estimator with a strong theoretical guarantee; this estimate can then be integrated into existing methods to pseudo-label the unlabeled data during training more accurately. Experimental results demonstrate that incorporating our techniques into common pseudo-labeling approaches improves their performance.

### Regularized Langevin Dynamics for Combinatorial Optimization 
[[arxiv](https://arxiv.org/abs/2502.00277)] [[cool](https://papers.cool/arxiv/2502.00277)] [[pdf](https://arxiv.org/pdf/2502.00277)]
> **Authors**: Shengyu Feng,Yiming Yang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative algorithm. However, we observed that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA) and the other one based on neural network (NN). Empirical results on three classical CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA and NN-based solvers. In particular, our SA algorithm reduces the running time of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems.

### DUET: Optimizing Training Data Mixtures via Feedback from Unseen Evaluation Tasks 
[[arxiv](https://arxiv.org/abs/2502.00270)] [[cool](https://papers.cool/arxiv/2502.00270)] [[pdf](https://arxiv.org/pdf/2502.00270)]
> **Authors**: Zhiliang Chen,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Bryan Kian Hsiang Low
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: The performance of a machine learning (ML) model depends heavily on the relevance of its training data to the domain of the downstream evaluation task. However, in practice, the data involved in an unseen evaluation task is often not known to us (e.g., conversations between an LLM and a user are end-to-end encrypted). So, it is not obvious what data would be relevant for training/fine-tuning the ML model to maximize its task performance. Instead, one can only deploy the ML model in the unseen evaluation task to gather multiple rounds of coarse feedback on how well the model has performed. This paper presents a novel global-to-local algorithm called DUET that can exploit the feedback loop by interleaving a data selection method with Bayesian optimization. As a result, DUET can efficiently refine the training data mixture from a pool of data domains to maximize the model's performance on the unseen evaluation task and its convergence to the optimal data mixture can be theoretically guaranteed by analyzing its cumulative regret. Empirical evaluation on image and LLM evaluation tasks shows that DUET finds better training data mixtures than conventional baselines.

### Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion 
[[arxiv](https://arxiv.org/abs/2502.00264)] [[cool](https://papers.cool/arxiv/2502.00264)] [[pdf](https://arxiv.org/pdf/2502.00264)]
> **Authors**: Binchi Zhang,Zaiyi Zheng,Zhengzhang Chen,Jundong Li
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry.

### ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for Pretrained LLMs 
[[arxiv](https://arxiv.org/abs/2502.00258)] [[cool](https://papers.cool/arxiv/2502.00258)] [[pdf](https://arxiv.org/pdf/2502.00258)]
> **Authors**: Hongyi Liu,Rajarshi Saha,Zhen Jia,Youngsuk Park,Jiaji Huang,Shoham Sabach,Yu-Xiang Wang,George Karypis
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large Language Models (LLMs) have demonstrated exceptional performance in natural language processing tasks, yet their massive size makes serving them inefficient and costly. Semi-structured pruning has emerged as an effective method for model acceleration, but existing approaches are suboptimal because they focus on local, layer-wise optimizations using heuristic rules, failing to leverage global feedback. We present ProxSparse, a learning-based framework for mask selection enabled by regularized optimization. ProxSparse transforms the rigid, non-differentiable mask selection process into a smoother optimization procedure, allowing gradual mask exploration with flexibility. ProxSparse does not involve additional weight updates once the mask is determined. Our extensive evaluations on 7 widely used models show that ProxSparse consistently outperforms previously proposed semi-structured mask selection methods with significant improvement, demonstrating the effectiveness of our learned approach towards semi-structured pruning.

### Contrastive Private Data Synthesis via Weighted Multi-PLM Fusion 
[[arxiv](https://arxiv.org/abs/2502.00245)] [[cool](https://papers.cool/arxiv/2502.00245)] [[pdf](https://arxiv.org/pdf/2502.00245)]
> **Authors**: Tianyuan Zou,Yang Liu,Peng Li,Yufei Xiong,Jianqing Zhang,Jingjing Liu,Xiaozhou Ye,Ye Ouyang,Ya-Qin Zhang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 16 pages, 11 tables, 7 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Substantial quantity and high quality are the golden rules of making a good training dataset with sample privacy protection equally important. Generating synthetic samples that resemble high-quality private data while ensuring Differential Privacy (DP), a formal privacy guarantee, promises scalability and practicality. However, existing methods relying on pre-trained models for data synthesis %that avoid fine-tuning large pre-trained generative models often struggle in data-deficient scenarios, suffering from limited sample size, inevitable generation noise and existing pre-trained model bias. To address these challenges, we propose a novel contrAstive private data Synthesis via Weighted multiple Pre-trained language models (PLM) framework, named as WASP. WASP utilizes limited private samples for more accurate private data distribution estimation via a Top-Q voting mechanism, and leverages low-quality synthetic samples for contrastive generation via collaboration among dynamically weighted multiple pre-trained models.Extensive experiments on 6 well-developed datasets with 6 open-source and 3 closed-source PLMs demonstrate the superiority of WASP in improving model performance over diverse downstream tasks. Code is available at https://anonymous.4open.science/r/WASP.

### Mordal: Automated Pretrained Model Selection for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.00241)] [[cool](https://papers.cool/arxiv/2502.00241)] [[pdf](https://arxiv.org/pdf/2502.00241)]
> **Authors**: Shiqi He,Insu Jang,Mosharaf Chowdhury
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category of multimodal models because of their many practical use cases, including in healthcare, robotics, and accessibility. Unfortunately, even though different VLMs in the literature demonstrate impressive visual capabilities in different benchmarks, they are handcrafted by human experts; there is no automated framework to create task-specific multimodal models. We introduce Mordal, an automated multimodal model search framework that efficiently finds the best VLM for a user-defined task without manual intervention. Mordal achieves this both by reducing the number of candidates to consider during the search process and by minimizing the time required to evaluate each remaining candidate. Our evaluation shows that Mordal can find the best VLM for a given problem using up to $8.9\times$--$11.6\times$ lower GPU hours than grid search. In the process of our evaluation, we have also discovered new VLMs that outperform their state-of-the-art counterparts.

### Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms 
[[arxiv](https://arxiv.org/abs/2502.00234)] [[cool](https://papers.cool/arxiv/2502.00234)] [[pdf](https://arxiv.org/pdf/2502.00234)]
> **Authors**: Yinuo Ren,Haoxuan Chen,Yuchen Zhu,Wei Guo,Yongxin Chen,Grant M. Rotskoff,Molei Tao,Lexing Ying
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 38 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,数值分析,计算物理,机器学习
- **Abstract**: Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $τ$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $τ$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $θ$-trapezoidal method in KL divergence. Empirical evaluations on GPT-2 level text and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints.

### HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems 
[[arxiv](https://arxiv.org/abs/2502.00226)] [[cool](https://papers.cool/arxiv/2502.00226)] [[pdf](https://arxiv.org/pdf/2502.00226)]
> **Authors**: Jun Xing,Mayur Bhatia,Sahil Phulwani,Darshan Suresh,Rafik Matta
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 24 pages, 25 figures
- **标题**: None
- **领域**: 机器学习,软件工程
- **Abstract**: Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.

### Should You Use Your Large Language Model to Explore or Exploit? 
[[arxiv](https://arxiv.org/abs/2502.00225)] [[cool](https://papers.cool/arxiv/2502.00225)] [[pdf](https://arxiv.org/pdf/2502.00225)]
> **Authors**: Keegan Harris,Aleksandrs Slivkins
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.

### Algorithmic Clustering based on String Compression to Extract P300 Structure in EEG Signals 
[[arxiv](https://arxiv.org/abs/2502.00220)] [[cool](https://papers.cool/arxiv/2502.00220)] [[pdf](https://arxiv.org/pdf/2502.00220)]
> **Authors**: Guillermo Sarasa,Ana Granados,Francisco B Rodríguez
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: ef:Computer Methods and Programs in Biomedicine 2019
- **标题**: None
- **领域**: 机器学习,信息论,信号处理
- **Abstract**: P300 is an Event-Related Potential widely used in Brain-Computer Interfaces, but its detection is challenging due to inter-subject and temporal variability. This work introduces a clustering methodology based on Normalized Compression Distance (NCD) to extract the P300 structure, ensuring robustness against variability. We propose a novel signal-to-ASCII transformation to generate compression-friendly objects, which are then clustered using a hierarchical tree-based method and a multidimensional projection approach. Experimental results on two datasets demonstrate the method's ability to reveal relevant P300 structures, showing clustering performance comparable to state-of-the-art approaches. Furthermore, analysis at the electrode level suggests that the method could assist in electrode selection for P300 detection. This compression-driven clustering methodology offers a complementary tool for EEG analysis and P300 identification.

### Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone 
[[arxiv](https://arxiv.org/abs/2502.00217)] [[cool](https://papers.cool/arxiv/2502.00217)] [[pdf](https://arxiv.org/pdf/2502.00217)]
> **Authors**: Negar Hassanpour,Muhammad Kamran Janjua,Kunlin Zhang,Sepehr Lavasani,Xiaowen Zhang,Chunhua Zhou,Chao Gao
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 16 pages, 7 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Balancing competing objectives remains a fundamental challenge in multi-task learning (MTL), primarily due to conflicting gradients across individual tasks. A common solution relies on computing a dynamic gradient update vector that balances competing tasks as optimization progresses. Building on this idea, we propose ConicGrad, a principled, scalable, and robust MTL approach formulated as a constrained optimization problem. Our method introduces an angular constraint to dynamically regulate gradient update directions, confining them within a cone centered on the reference gradient of the overall objective. By balancing task-specific gradients without over-constraining their direction or magnitude, ConicGrad effectively resolves inter-task gradient conflicts. Moreover, our framework ensures computational efficiency and scalability to high-dimensional parameter spaces. We conduct extensive experiments on standard supervised learning and reinforcement learning MTL benchmarks, and demonstrate that ConicGrad achieves state-of-the-art performance across diverse tasks.

### Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers 
[[arxiv](https://arxiv.org/abs/2502.00213)] [[cool](https://papers.cool/arxiv/2502.00213)] [[pdf](https://arxiv.org/pdf/2502.00213)]
> **Authors**: Akiyoshi Tomihari,Issei Sato
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算
- **Abstract**: Transformer models are challenging to optimize with SGD and typically require adaptive optimizers such as Adam. However, the reasons behind the superior performance of Adam over SGD remain unclear. In this study, we investigate the optimization of transformer models by focusing on \emph{gradient heterogeneity}, defined as the disparity in gradient norms among parameters. Our analysis shows that gradient heterogeneity hinders gradient-based optimization, including SGD, while sign-based optimization, a simplified variant of Adam, is less affected. We further examine gradient heterogeneity in transformer models and show that it is influenced by the placement of layer normalization. Additionally, we show that the momentum term in sign-based optimization is important for preventing the excessive growth of linear-head parameters in tasks with many classes. Experimental results from fine-tuning transformer models in both NLP and vision domains validate our theoretical analyses. This study provides insights into the optimization challenges of transformer models and offers guidance for designing future optimization algorithms. Code is available at \url{https://github.com/tom4649/gradient-heterogeneity}.

### STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving 
[[arxiv](https://arxiv.org/abs/2502.00212)] [[cool](https://papers.cool/arxiv/2502.00212)] [[pdf](https://arxiv.org/pdf/2502.00212)]
> **Authors**: Kefan Dong,Tengyu Ma
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 23 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算机科学中的逻辑
- **Abstract**: A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens generated during the training in Lean, STP proves 26.3% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (61.7%, pass@3200), Proofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).

### BICompFL: Stochastic Federated Learning with Bi-Directional Compression 
[[arxiv](https://arxiv.org/abs/2502.00206)] [[cool](https://papers.cool/arxiv/2502.00206)] [[pdf](https://arxiv.org/pdf/2502.00206)]
> **Authors**: Maximilian Egger,Rawad Bitar,Antonia Wachter-Zeh,Nir Weinberger,Deniz Gündüz
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算,信息论,机器学习
- **Abstract**: We address the prominent communication bottleneck in federated learning (FL). We specifically consider stochastic FL, in which models or compressed model updates are specified by distributions rather than deterministic parameters. Stochastic FL offers a principled approach to compression, and has been shown to reduce the communication load under perfect downlink transmission from the federator to the clients. However, in practice, both the uplink and downlink communications are constrained. We show that bi-directional compression for stochastic FL has inherent challenges, which we address by introducing BICompFL. Our BICompFL is experimentally shown to reduce the communication cost by an order of magnitude compared to multiple benchmarks, while maintaining state-of-the-art accuracies. Theoretically, we study the communication cost of BICompFL through a new analysis of an importance-sampling based technique, which exposes the interplay between uplink and downlink communication costs.

### Nearly-Optimal Bandit Learning in Stackelberg Games with Side Information 
[[arxiv](https://arxiv.org/abs/2502.00204)] [[cool](https://papers.cool/arxiv/2502.00204)] [[pdf](https://arxiv.org/pdf/2502.00204)]
> **Authors**: Maria-Florina Balcan,Martino Bernasconi,Matteo Castiglioni,Andrea Celli,Keegan Harris,Zhiwei Steven Wu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: We study the problem of online learning in Stackelberg games with side information between a leader and a sequence of followers. In every round the leader observes contextual information and commits to a mixed strategy, after which the follower best-responds. We provide learning algorithms for the leader which achieve $O(T^{1/2})$ regret under bandit feedback, an improvement from the previously best-known rates of $O(T^{2/3})$. Our algorithms rely on a reduction to linear contextual bandits in the utility space: In each round, a linear contextual bandit algorithm recommends a utility vector, which our algorithm inverts to determine the leader's mixed strategy. We extend our algorithms to the setting in which the leader's utility function is unknown, and also apply it to the problems of bidding in second-price auctions with side information and online Bayesian persuasion with public and private states. Finally, we observe that our algorithms empirically outperform previous results on numerical simulations.

### Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment 
[[arxiv](https://arxiv.org/abs/2502.00203)] [[cool](https://papers.cool/arxiv/2502.00203)] [[pdf](https://arxiv.org/pdf/2502.00203)]
> **Authors**: Shengyang Sun,Yian Zhang,Alexander Bukharin,David Mosallanezhad,Jiaqi Zeng,Soumye Singhal,Gerald Shen,Adithya Renduchintala,Tugrul Konuk,Yi Dong,Zhilin Wang,Dmitry Chichkov,Olivier Delalleau,Oleksii Kuchaiev
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 4 figures; update author names
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: The rapid development of large language model (LLM) alignment algorithms has resulted in a complex and fragmented landscape, with limited clarity on the effectiveness of different methods and their inter-connections. This paper introduces Reward-Aware Preference Optimization (RPO), a mathematical framework that unifies popular preference optimization techniques in LLM alignment, including DPO, IPO, SimPO, and REINFORCE (LOO), among others. RPO provides a structured approach to disentangle and systematically study the impact of various design choices, such as the optimization objective, the number of responses per prompt, and the use of implicit versus explicit reward models, on LLM preference optimization. We additionally propose a new experimental setup that enables the clean and direct ablation of such design choices. Through an extensive series of ablation studies within the RPO framework, we gain insights into the critical factors shaping model alignment, offering practical guidance on the most effective strategies for improving LLM alignment.

### Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review 
[[arxiv](https://arxiv.org/abs/2502.00201)] [[cool](https://papers.cool/arxiv/2502.00201)] [[pdf](https://arxiv.org/pdf/2502.00201)]
> **Authors**: Yisong Chen,Chuqing Zhao,Yixin Xu,Chuanhao Nie
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,统计金融
- **Abstract**: This paper systematically reviews advancements in deep learning (DL) techniques for financial fraud detection, a critical issue in the financial sector. Using the Kitchenham systematic literature review approach, 57 studies published between 2019 and 2024 were analyzed. The review highlights the effectiveness of various deep learning models such as Convolutional Neural Networks, Long Short-Term Memory, and transformers across domains such as credit card transactions, insurance claims, and financial statement audits. Performance metrics such as precision, recall, F1-score, and AUC-ROC were evaluated. Key themes explored include the impact of data privacy frameworks and advancements in feature engineering and data preprocessing. The study emphasizes challenges such as imbalanced datasets, model interpretability, and ethical considerations, alongside opportunities for automation and privacy-preserving techniques such as blockchain integration and Principal Component Analysis. By examining trends over the past five years, this review identifies critical gaps and promising directions for advancing DL applications in financial fraud detection, offering actionable insights for researchers and practitioners.

### Model Successor Functions 
[[arxiv](https://arxiv.org/abs/2502.00197)] [[cool](https://papers.cool/arxiv/2502.00197)] [[pdf](https://arxiv.org/pdf/2502.00197)]
> **Authors**: Yingshan Chang,Yonatan Bisk
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). Recently, there is a growing focus on inductive generalization, where a progression of difficulty implicitly governs the direction of domain shifts. In inductive generalization, it is often assumed that the training data lie in the easier side, while the testing data lie in the harder side. The challenge is that training data are always finite, but a learner is expected to infer an inductive principle that could be applied in an unbounded manner. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. This work provides such a formalization that centers on the concept of model successors. Then we outline directions to adapt well-established techniques towards the learning of model successors. This work calls for restructuring of the research discussion around inductive generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation.

### Physics-Informed Neural Network based Damage Identification for Truss Railroad Bridges 
[[arxiv](https://arxiv.org/abs/2502.00194)] [[cool](https://papers.cool/arxiv/2502.00194)] [[pdf](https://arxiv.org/pdf/2502.00194)]
> **Authors**: Althaf Shajihan,Kirill Mechitov,Girish Chowdhary,Billie F. Spencer Jr
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 30 pages, 15 figures
- **标题**: None
- **领域**: 机器学习,人工智能,计算物理
- **Abstract**: Railroad bridges are a crucial component of the U.S. freight rail system, which moves over 40 percent of the nation's freight and plays a critical role in the economy. However, aging bridge infrastructure and increasing train traffic pose significant safety hazards and risk service disruptions. The U.S. rail network includes over 100,000 railroad bridges, averaging one every 1.4 miles of track, with steel bridges comprising over 50% of the network's total bridge length. Early identification and assessment of damage in these bridges remain challenging tasks. This study proposes a physics-informed neural network (PINN) based approach for damage identification in steel truss railroad bridges. The proposed approach employs an unsupervised learning approach, eliminating the need for large datasets typically required by supervised methods. The approach utilizes train wheel load data and bridge response during train crossing events as inputs for damage identification. The PINN model explicitly incorporates the governing differential equations of the linear time-varying (LTV) bridge-train system. Herein, this model employs a recurrent neural network (RNN) based architecture incorporating a custom Runge-Kutta (RK) integrator cell, designed for gradient-based learning. The proposed approach updates the bridge finite element model while also quantifying damage severity and localizing the affected structural members. A case study on the Calumet Bridge in Chicago, Illinois, with simulated damage scenarios, is used to demonstrate the model's effectiveness in identifying damage while maintaining low false-positive rates. Furthermore, the damage identification pipeline is designed to seamlessly integrate prior knowledge from inspections and drone surveys, also enabling context-aware updating and assessment of bridge's condition.

### Byzantine-Resilient Zero-Order Optimization for Communication-Efficient Heterogeneous Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.00193)] [[cool](https://papers.cool/arxiv/2502.00193)] [[pdf](https://arxiv.org/pdf/2502.00193)]
> **Authors**: Maximilian Egger,Mayank Bakshi,Rawad Bitar
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,分布式、并行和集群计算,机器学习
- **Abstract**: We introduce CyBeR-0, a Byzantine-resilient federated zero-order optimization method that is robust under Byzantine attacks and provides significant savings in uplink and downlink communication costs. We introduce transformed robust aggregation to give convergence guarantees for general non-convex objectives under client data heterogeneity. Empirical evaluations for standard learning tasks and fine-tuning large language models show that CyBeR-0 exhibits stable performance with only a few scalars per-round communication cost and reduced memory requirements.

### On the Effectiveness of Random Weights in Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00190)] [[cool](https://papers.cool/arxiv/2502.00190)] [[pdf](https://arxiv.org/pdf/2502.00190)]
> **Authors**: Thu Bui,Carola-Bibiane Schönlieb,Bruno Ribeiro,Beatrice Bevilacqua,Moshe Eliasof
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Graph Neural Networks (GNNs) have achieved remarkable success across diverse tasks on graph-structured data, primarily through the use of learned weights in message passing layers. In this paper, we demonstrate that random weights can be surprisingly effective, achieving performance comparable to end-to-end training counterparts, across various tasks and datasets. Specifically, we show that by replacing learnable weights with random weights, GNNs can retain strong predictive power, while significantly reducing training time by up to 6$\times$ and memory usage by up to 3$\times$. Moreover, the random weights combined with our construction yield random graph propagation operators, which we show to reduce the problem of feature rank collapse in GNNs. These understandings and empirical results highlight random weights as a lightweight and efficient alternative, offering a compelling perspective on the design and training of GNN architectures.

### Understanding Federated Learning from IID to Non-IID dataset: An Experimental Study 
[[arxiv](https://arxiv.org/abs/2502.00182)] [[cool](https://papers.cool/arxiv/2502.00182)] [[pdf](https://arxiv.org/pdf/2502.00182)]
> **Authors**: Jungwon Seo,Ferhat Ozgur Catak,Chunming Rong
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: ef:36th Norwegian ICT Conference for Research and Education, NIKT 2024
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field.

### Designing Scheduling for Diffusion Models via Spectral Analysis 
[[arxiv](https://arxiv.org/abs/2502.00180)] [[cool](https://papers.cool/arxiv/2502.00180)] [[pdf](https://arxiv.org/pdf/2502.00180)]
> **Authors**: Roi Benita,Michael Elad,Joseph Keshet
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity and shift-invariance assumptions, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged for optimizing the noise schedule, ensuring the best alignment with the original dataset's characteristics. Our results lead to scheduling curves that are dependent on the frequency content of the data, offering a theoretical justification for some of the heuristics taken by practitioners.

### Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants Using Sighted Participants 
[[arxiv](https://arxiv.org/abs/2502.00177)] [[cool](https://papers.cool/arxiv/2502.00177)] [[pdf](https://arxiv.org/pdf/2502.00177)]
> **Authors**: Eirini Schoinas,Adyah Rastogi,Anissa Carter,Jacob Granley,Michael Beyeler
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: :I.2.10
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,人机交互
- **Abstract**: Human-in-the-loop optimization (HILO) is a promising approach for personalizing visual prostheses by iteratively refining stimulus parameters based on user feedback. Previous work demonstrated HILO's efficacy in simulation, but its performance with human participants remains untested. Here we evaluate HILO using sighted participants viewing simulated prosthetic vision to assess its ability to optimize stimulation strategies under realistic conditions. Participants selected between phosphenes generated by competing encoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in three conditions: standard optimization, threshold misspecifications, and out-of-distribution parameter sampling. Participants consistently preferred HILO-generated stimuli over both a naïve encoder and the DSE alone, with log odds favoring HILO across all conditions. We also observed key differences between human and simulated decision-making, highlighting the importance of validating optimization strategies with human participants. These findings support HILO as a viable approach for adapting visual prostheses to individuals.

### Distribution-Specific Agnostic Conditional Classification With Halfspaces 
[[arxiv](https://arxiv.org/abs/2502.00172)] [[cool](https://papers.cool/arxiv/2502.00172)] [[pdf](https://arxiv.org/pdf/2502.00172)]
> **Authors**: Jizhou Huang,Brendan Juba
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算复杂度,机器学习
- **Abstract**: We study ``selective'' or ``conditional'' classification problems under an agnostic setting. Classification tasks commonly focus on modeling the relationship between features and categories that captures the vast majority of data. In contrast to common machine learning frameworks, conditional classification intends to model such relationships only on a subset of the data defined by some selection rule. Most work on conditional classification either solves the problem in a realizable setting or does not guarantee the error is bounded compared to an optimal solution. In this work, we consider selective/conditional classification by sparse linear classifiers for subsets defined by halfspaces, and give both positive as well as negative results for Gaussian feature distributions. On the positive side, we present the first PAC-learning algorithm for homogeneous halfspace selectors with error guarantee $\bigO*{\sqrt{\mathrm{opt}}}$, where $\mathrm{opt}$ is the smallest conditional classification error over the given class of classifiers and homogeneous halfspaces. On the negative side, we find that, under cryptographic assumptions, approximating the conditional classification loss within a small additive error is computationally hard even under Gaussian distribution. We prove that approximating conditional classification is at least as hard as approximating agnostic classification in both additive and multiplicative form.

### Demystifying MPNNs: Message Passing as Merely Efficient Matrix Multiplication 
[[arxiv](https://arxiv.org/abs/2502.00140)] [[cool](https://papers.cool/arxiv/2502.00140)] [[pdf](https://arxiv.org/pdf/2502.00140)]
> **Authors**: Qin Jiang,Chengjia Wang,Michael Lones,Wei Pang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,神经和进化计算,社交和信息网络
- **Abstract**: While Graph Neural Networks (GNNs) have achieved remarkable success, their design largely relies on empirical intuition rather than theoretical understanding. In this paper, we present a comprehensive analysis of GNN behavior through three fundamental aspects: (1) we establish that \textbf{$k$-layer} Message Passing Neural Networks efficiently aggregate \textbf{$k$-hop} neighborhood information through iterative computation, (2) analyze how different loop structures influence neighborhood computation, and (3) examine behavior across structure-feature hybrid and structure-only tasks. For deeper GNNs, we demonstrate that gradient-related issues, rather than just over-smoothing, can significantly impact performance in sparse graphs. We also analyze how different normalization schemes affect model performance and how GNNs make predictions with uniform node features, providing a theoretical framework that bridges the gap between empirical success and theoretical understanding.

### SAGRAD: A Program for Neural Network Training with Simulated Annealing and the Conjugate Gradient Method 
[[arxiv](https://arxiv.org/abs/2502.00112)] [[cool](https://papers.cool/arxiv/2502.00112)] [[pdf](https://arxiv.org/pdf/2502.00112)]
> **Authors**: Javier Bernal,Jose Torres-Jimenez
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: ef:Journal of Research of the National Institute of Standards and Technology Volume 120 (2015)
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: SAGRAD (Simulated Annealing GRADient), a Fortran 77 program for computing neural networks for classification using batch learning, is discussed. Neural network training in SAGRAD is based on a combination of simulated annealing and Møller's scaled conjugate gradient algorithm, the latter a variation of the traditional conjugate gradient method, better suited for the nonquadratic nature of neural networks. Different aspects of the implementation of the training process in SAGRAD are discussed, such as the efficient computation of gradients and multiplication of vectors by Hessian matrices that are required by Møller's algorithm; the (re)initialization of weights with simulated annealing required to (re)start Møller's algorithm the first time and each time thereafter that it shows insufficient progress in reaching a possibly local minimum; and the use of simulated annealing when Møller's algorithm, after possibly making considerable progress, becomes stuck at a local minimum or flat area of weight space. Outlines of the scaled conjugate gradient algorithm, the simulated annealing procedure and the training process used in SAGRAD are presented together with results from running SAGRAD on two examples of training data.

### Tracking Most Significant Shifts in Infinite-Armed Bandits 
[[arxiv](https://arxiv.org/abs/2502.00108)] [[cool](https://papers.cool/arxiv/2502.00108)] [[pdf](https://arxiv.org/pdf/2502.00108)]
> **Authors**: Joe Suk,Jung-hun Kim
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We study an infinite-armed bandit problem where actions' mean rewards are initially sampled from a reservoir distribution. Most prior works in this setting focused on stationary rewards (Berry et al., 1997; Wang et al., 2008; Bonald and Proutiere, 2013; Carpentier and Valko, 2015) with the more challenging adversarial/non-stationary variant only recently studied in the context of rotting/decreasing rewards (Kim et al., 2022; 2024). Furthermore, optimal regret upper bounds were only achieved using parameter knowledge of non-stationarity and only known for certain regimes of regularity of the reservoir. This work shows the first parameter-free optimal regret bounds for all regimes while also relaxing distributional assumptions on the reservoir. We first introduce a blackbox scheme to convert a finite-armed MAB algorithm designed for near-stationary environments into a parameter-free algorithm for the infinite-armed non-stationary problem with optimal regret guarantees. We next study a natural notion of significant shift for this problem inspired by recent developments in finite-armed MAB (Suk & Kpotufe, 2022). We show that tighter regret bounds in terms of significant shifts can be adaptively attained by employing a randomized variant of elimination within our blackbox scheme. Our enhanced rates only depend on the rotting non-stationarity and thus exhibit an interesting phenomenon for this problem where rising rewards do not factor into the difficulty of non-stationarity.

### Re-Visiting Explainable AI Evaluation Metrics to Identify The Most Informative Features 
[[arxiv](https://arxiv.org/abs/2502.00088)] [[cool](https://papers.cool/arxiv/2502.00088)] [[pdf](https://arxiv.org/pdf/2502.00088)]
> **Authors**: Ahmed M. Salih
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Functionality or proxy-based approach is one of the used approaches to evaluate the quality of explainable artificial intelligence methods. It uses statistical methods, definitions and new developed metrics for the evaluation without human intervention. Among them, Selectivity or RemOve And Retrain (ROAR), and Permutation Importance (PI) are the most commonly used metrics to evaluate the quality of explainable artificial intelligence methods to highlight the most significant features in machine learning models. They state that the model performance should experience a sharp reduction if the most informative feature is removed from the model or permuted. However, the efficiency of both metrics is significantly affected by multicollinearity, number of significant features in the model and the accuracy of the model. This paper shows with empirical examples that both metrics suffer from the aforementioned limitations. Accordingly, we propose expected accuracy interval (EAI), a metric to predict the upper and lower bounds of the the accuracy of the model when ROAR or IP is implemented. The proposed metric found to be very useful especially with collinear features.

### From Data to Action: Charting A Data-Driven Path to Combat Antimicrobial Resistance 
[[arxiv](https://arxiv.org/abs/2502.00061)] [[cool](https://papers.cool/arxiv/2502.00061)] [[pdf](https://arxiv.org/pdf/2502.00061)]
> **Authors**: Qian Fu,Yuzhe Zhang,Yanfeng Shu,Ming Ding,Lina Yao,Chen Wang
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 29 pages, 3 figures, 4 tables, survey paper
- **标题**: None
- **领域**: 机器学习,人工智能,种群与进化
- **Abstract**: Antimicrobial-resistant (AMR) microbes are a growing challenge in healthcare, rendering modern medicines ineffective. AMR arises from antibiotic production and bacterial evolution, but quantifying its transmission remains difficult. With increasing AMR-related data, data-driven methods offer promising insights into its causes and treatments. This paper reviews AMR research from a data analytics and machine learning perspective, summarizing the state-of-the-art and exploring key areas such as surveillance, prediction, drug discovery, stewardship, and driver analysis. It discusses data sources, methods, and challenges, emphasizing standardization and interoperability. Additionally, it surveys statistical and machine learning techniques for AMR analysis, addressing issues like data noise and bias. Strategies for denoising and debiasing are highlighted to enhance fairness and robustness in AMR research. The paper underscores the importance of interdisciplinary collaboration and awareness of data challenges in advancing AMR research, pointing to future directions for innovation and improved methodologies.

### Large Language Models are Few-shot Multivariate Time Series Classifiers 
[[arxiv](https://arxiv.org/abs/2502.00059)] [[cool](https://papers.cool/arxiv/2502.00059)] [[pdf](https://arxiv.org/pdf/2502.00059)]
> **Authors**: Yakun Chen,Zihao Li,Chao Yang,Xianzhi Wang,Guandong Xu
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) have been extensively applied in time series analysis. Yet, their utility in the few-shot classification (i.e., a crucial training scenario due to the limited training data available in industrial applications) concerning multivariate time series data remains underexplored. We aim to leverage the extensive pre-trained knowledge in LLMs to overcome the data scarcity problem within multivariate time series. Specifically, we propose LLMFew, an LLM-enhanced framework to investigate the feasibility and capacity of LLMs for few-shot multivariate time series classification. This model introduces a Patch-wise Temporal Convolution Encoder (PTCEnc) to align time series data with the textual embedding input of LLMs. We further fine-tune the pre-trained LLM decoder with Low-rank Adaptations (LoRA) to enhance its feature representation learning ability in time series data. Experimental results show that our model outperformed state-of-the-art baselines by a large margin, achieving 125.2% and 50.2% improvement in classification accuracy on Handwriting and EthanolConcentration datasets, respectively. Moreover, our experimental results demonstrate that LLM-based methods perform well across a variety of datasets in few-shot MTSC, delivering reliable results compared to traditional models. This success paves the way for their deployment in industrial environments where data are limited.

### Bridging Contrastive Learning and Domain Adaptation: Theoretical Perspective and Practical Application 
[[arxiv](https://arxiv.org/abs/2502.00052)] [[cool](https://papers.cool/arxiv/2502.00052)] [[pdf](https://arxiv.org/pdf/2502.00052)]
> **Authors**: Gonzalo Iñaki Quintana,Laurence Vancamberg,Vincent Jugnon,Agnès Desolneux,Mathilde Mougeot
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This work studies the relationship between Contrastive Learning and Domain Adaptation from a theoretical perspective. The two standard contrastive losses, NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely used for Domain Adaptation. Our work shows that minimizing the contrastive losses decreases the CMMD and simultaneously improves class-separability, laying the theoretical groundwork for the use of Contrastive Learning in the context of Domain Adaptation. Due to the relevance of Domain Adaptation in medical imaging, we focused the experiments on mammography images. Extensive experiments on three mammography datasets - synthetic patches, clinical (real) patches, and clinical (real) images - show improved Domain Adaptation, class-separability, and classification performance, when minimizing the Supervised Contrastive loss.

### Contextually Entangled Gradient Mapping for Optimized LLM Comprehension 
[[arxiv](https://arxiv.org/abs/2502.00048)] [[cool](https://papers.cool/arxiv/2502.00048)] [[pdf](https://arxiv.org/pdf/2502.00048)]
> **Authors**: Colin Sisate,Alistair Goldfinch,Vincent Waterstone,Sebastian Kingsley,Mariana Blackthorn
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to gradient optimization, redefining the relationship between contextual embeddings and gradient updates to enhance semantic coherence and reasoning capabilities in neural architectures. By treating gradients as dynamic carriers of contextual dependencies rather than isolated numerical entities, the proposed methodology bridges critical gaps in existing optimization strategies. The integration of entangled gradient dynamics into a loss regularization framework demonstrated significant improvements in tasks involving long-form reasoning, contextual retention, and adaptability to unseen domains. Experimental evaluations showed that the CEGM-enhanced model consistently outperformed baseline approaches, achieving higher accuracy in token-level predictions and greater resilience to noisy inputs. Practical implementations involved modifications to training pipelines, introducing entanglement layers and dynamic coefficient adjustments that seamlessly align with existing architectures. Results further highlighted reductions in semantic drift during sequential transformations and improvements in embedding coherence across paraphrased sentences, showing the robustness and versatility of the proposed methodology. The findings demonstrate the broader implications of gradient entanglement for both theoretical advancements and practical applications in optimization strategies.

### HadamRNN: Binary and Sparse Ternary Orthogonal RNNs 
[[arxiv](https://arxiv.org/abs/2502.00047)] [[cool](https://papers.cool/arxiv/2502.00047)] [[pdf](https://arxiv.org/pdf/2502.00047)]
> **Authors**: Armand Foucault,Franck Mamalet,François Malgouyres
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-04
> **comment**: ef:International Conference onLearningRepresentations (ICLR), Apr 2025, Singapour, Singapore
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNs (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and lock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, and IMDB dataset. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps.

### Optimization Strategies for Enhancing Resource Efficiency in Transformers & Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.00046)] [[cool](https://papers.cool/arxiv/2502.00046)] [[pdf](https://arxiv.org/pdf/2502.00046)]
> **Authors**: Tom Wallace,Naser Ezzati-Jivan,Beatrice Ombuki-Berman
> **First submission**: 2025-01-16
> **First announcement**: 2025-02-04
> **comment**: Accepted for ACM's ICPE 2025 in Short Paper format
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Advancements in Natural Language Processing are heavily reliant on the Transformer architecture, whose improvements come at substantial resource costs due to ever-growing model sizes. This study explores optimization techniques, including Quantization, Knowledge Distillation, and Pruning, focusing on energy and computational efficiency while retaining performance. Among standalone methods, 4-bit Quantization significantly reduces energy use with minimal accuracy loss. Hybrid approaches, like NVIDIA's Minitron approach combining KD and Structured Pruning, further demonstrate promising trade-offs between size reduction and accuracy retention. A novel optimization equation is introduced, offering a flexible framework for comparing various methods. Through the investigation of these compression methods, we provide valuable insights for developing more sustainable and efficient LLMs, shining a light on the often-ignored concern of energy efficiency.

### Restless Multi-armed Bandits under Frequency and Window Constraints for Public Service Inspections 
[[arxiv](https://arxiv.org/abs/2502.00045)] [[cool](https://papers.cool/arxiv/2502.00045)] [[pdf](https://arxiv.org/pdf/2502.00045)]
> **Authors**: Yi Mao,Andrew Perrault
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学,计算机与社会
- **Abstract**: Municipal inspections are an important part of maintaining the quality of goods and services. In this paper, we approach the problem of intelligently scheduling service inspections to maximize their impact, using the case of food establishment inspections in Chicago as a case study. The Chicago Department of Public Health (CDPH) inspects thousands of establishments each year, with a substantial fail rate (over 3,000 failed inspection reports in 2023). To balance the objectives of ensuring adherence to guidelines, minimizing disruption to establishments, and minimizing inspection costs, CDPH assigns each establishment an inspection window every year and guarantees that they will be inspected exactly once during that window. These constraints create a challenge for a restless multi-armed bandit (RMAB) approach, for which there are no existing methods. We develop an extension to Whittle index-based systems for RMABs that can guarantee action window constraints and frequencies, and furthermore can be leveraged to optimize action window assignments themselves. Briefly, we combine MDP reformulation and integer programming-based lookahead to maximize the impact of inspections subject to constraints. A neural network-based supervised learning model is developed to model state transitions of real Chicago establishments using public CDPH inspection records, which demonstrates 10\% AUC improvements compared with directly predicting establishments' failures. Our experiments not only show up to 24\% (in simulation) or 33\% (on real data) reward improvements resulting from our approach but also give insight into the impact of scheduling constraints.

### Multi-Objective Reinforcement Learning for Power Grid Topology Control 
[[arxiv](https://arxiv.org/abs/2502.00040)] [[cool](https://papers.cool/arxiv/2502.00040)] [[pdf](https://arxiv.org/pdf/2502.00040)]
> **Authors**: Thomas Lautenbacher,Ali Rajaei,Davide Barbieri,Jan Viebahn,Jochen L. Cremer
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,系统与控制
- **Abstract**: Transmission grid congestion increases as the electrification of various sectors requires transmitting more power. Topology control, through substation reconfiguration, can reduce congestion but its potential remains under-exploited in operations. A challenge is modeling the topology control problem to align well with the objectives and constraints of operators. Addressing this challenge, this paper investigates the application of multi-objective reinforcement learning (MORL) to integrate multiple conflicting objectives for power grid topology control. We develop a MORL approach using deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate a set of Pareto-optimal policies that balance objectives such as minimizing line loading, topological deviation, and switching frequency. Initial case studies show that the MORL approach can provide valuable insights into objective trade-offs and improve Pareto front approximation compared to a random search baseline. The generated multi-objective RL policies are 30% more successful in preventing grid failure under contingencies and 20% more effective when training budget is reduced - compared to the common single objective RL policy.

### Efficient Client Selection in Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.00036)] [[cool](https://papers.cool/arxiv/2502.00036)] [[pdf](https://arxiv.org/pdf/2502.00036)]
> **Authors**: William Marfo,Deepak K. Tosh,Shirley V. Moore
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Federated Learning (FL) enables decentralized machine learning while preserving data privacy. This paper proposes a novel client selection framework that integrates differential privacy and fault tolerance. The adaptive client selection adjusts the number of clients based on performance and system constraints, with noise added to protect privacy. Evaluated on the UNSW-NB15 and ROAD datasets for network anomaly detection, the method improves accuracy by 7% and reduces training time by 25% compared to baselines. Fault tolerance enhances robustness with minimal performance trade-offs.

### Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients 
[[arxiv](https://arxiv.org/abs/2502.00025)] [[cool](https://papers.cool/arxiv/2502.00025)] [[pdf](https://arxiv.org/pdf/2502.00025)]
> **Authors**: Abdulaziz Ahmed,Mohammad Saleem,Mohammed Alzeen,Badari Birur,Rachel E Fargason,Bradley G Burk,Hannah Rose Harkins,Ahmed Alhassan,Mohammed Ali Al-Garadi
> **First submission**: 2025-01-21
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会
- **Abstract**: Importance: Emergency department (ED) returns for mental health conditions pose a major healthcare burden, with 24-27% of patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical use. Objective: To assess whether integrating large language models (LLMs) with machine learning improves predictive accuracy and clinical interpretability of ED mental health return risk models. Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an academic medical center in the Deep South from January 2018 to December 2022. Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability using a novel LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values with clinical knowledge. Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89), while Exercise and Home Environment showed lower performance (F1: 0.70-0.67). The LLM-based interpretability framework achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61. Conclusions and Relevance: Integrating LLMs with machine learning models yielded modest but consistent accuracy gains while significantly enhancing interpretability through automated, clinically relevant explanations. This approach provides a framework for translating predictive analytics into actionable clinical insights.

### PixelBrax: Learning Continuous Control from Pixels End-to-End on the GPU 
[[arxiv](https://arxiv.org/abs/2502.00021)] [[cool](https://papers.cool/arxiv/2502.00021)] [[pdf](https://arxiv.org/pdf/2502.00021)]
> **Authors**: Trevor McInroe,Samuel Garcin
> **First submission**: 2025-01-16
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,表现
- **Abstract**: We present PixelBrax, a set of continuous control tasks with pixel observations. We combine the Brax physics engine with a pure JAX renderer, allowing reinforcement learning (RL) experiments to run end-to-end on the GPU. PixelBrax can render observations over thousands of parallel environments and can run two orders of magnitude faster than existing benchmarks that rely on CPU-based rendering. Additionally, PixelBrax supports fully reproducible experiments through its explicit handling of any stochasticity within the environments and supports color and video distractors for benchmarking generalization. We open-source PixelBrax alongside JAX implementations of several RL algorithms at github.com/trevormcinroe/pixelbrax.

## 多代理系统(cs.MA:Multiagent Systems)

### Position: Towards a Responsible LLM-empowered Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.01714)] [[cool](https://papers.cool/arxiv/2502.01714)] [[pdf](https://arxiv.org/pdf/2502.01714)]
> **Authors**: Jinwei Hu,Yi Dong,Shuang Ao,Zhuoyun Li,Boxuan Wang,Lokesh Singh,Guangliang Cheng,Sarvapali D. Ramchurn,Xiaowei Huang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under Review
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: The rise of Agent AI and Large Language Model-powered Multi-Agent Systems (LLM-MAS) has underscored the need for responsible and dependable system operation. Tools like LangChain and Retrieval-Augmented Generation have expanded LLM capabilities, enabling deeper integration into MAS through enhanced knowledge retrieval and reasoning. However, these advancements introduce critical challenges: LLM agents exhibit inherent unpredictability, and uncertainties in their outputs can compound across interactions, threatening system stability. To address these risks, a human-centered design approach with active dynamic moderation is essential. Such an approach enhances traditional passive oversight by facilitating coherent inter-agent communication and effective system governance, allowing MAS to achieve desired outcomes more efficiently.

### Musical Agent Systems: MACAT and MACataRT 
[[arxiv](https://arxiv.org/abs/2502.00023)] [[cool](https://papers.cool/arxiv/2502.00023)] [[pdf](https://arxiv.org/pdf/2502.00023)]
> **Authors**: Keon Ju M. Lee,Philippe Pasquier
> **First submission**: 2025-01-19
> **First announcement**: 2025-02-04
> **comment**: In Proceedings of the Creativity and GenerativeAINIPS (NeuralInformation Processing Systems) Workshop 2024
- **标题**: None
- **领域**: 多代理系统,人工智能,人机交互,声音,音频和语音处理
- **Abstract**: Our research explores the development and application of musical agents, human-in-the-loop generative AI systems designed to support music performance and improvisation within co-creative spaces. We introduce MACAT and MACataRT, two distinct musical agent systems crafted to enhance interactive music-making between human musicians and AI. MACAT is optimized for agent-led performance, employing real-time synthesis and self-listening to shape its output autonomously, while MACataRT provides a flexible environment for collaborative improvisation through audio mosaicing and sequence-based learning. Both systems emphasize training on personalized, small datasets, fostering ethical and transparent AI engagement that respects artistic integrity. This research highlights how interactive, artist-centred generative AI can expand creative possibilities, empowering musicians to explore new forms of artistic expression in real-time, performance-driven and music improvisation contexts.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### TESS: A Scalable Temporally and Spatially Local Learning Rule for Spiking Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01837)] [[cool](https://papers.cool/arxiv/2502.01837)] [[pdf](https://arxiv.org/pdf/2502.01837)]
> **Authors**: Marco Paul E. Apolinario,Kaushik Roy,Charlotte Frenkel
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 神经和进化计算,人工智能,机器学习
- **Abstract**: The demand for low-power inference and training of deep neural networks (DNNs) on edge devices has intensified the need for algorithms that are both scalable and energy-efficient. While spiking neural networks (SNNs) allow for efficient inference by processing complex spatio-temporal dynamics in an event-driven fashion, training them on resource-constrained devices remains challenging due to the high computational and memory demands of conventional error backpropagation (BP)-based approaches. In this work, we draw inspiration from biological mechanisms such as eligibility traces, spike-timing-dependent plasticity, and neural activity synchronization to introduce TESS, a temporally and spatially local learning rule for training SNNs. Our approach addresses both temporal and spatial credit assignments by relying solely on locally available signals within each neuron, thereby allowing computational and memory overheads to scale linearly with the number of neurons, independently of the number of time steps. Despite relying on local mechanisms, we demonstrate performance comparable to the backpropagation through time (BPTT) algorithm, within $\sim1.4$ accuracy points on challenging computer vision scenarios relevant at the edge, such as the IBM DVS Gesture dataset, CIFAR10-DVS, and temporal versions of CIFAR10, and CIFAR100. Being able to produce comparable performance to BPTT while keeping low time and memory complexity, TESS enables efficient and scalable on-device learning at the edge.

### Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity 
[[arxiv](https://arxiv.org/abs/2502.00593)] [[cool](https://papers.cool/arxiv/2502.00593)] [[pdf](https://arxiv.org/pdf/2502.00593)]
> **Authors**: Ryan Bahlous-Boldi,Maxence Faldor,Luca Grillotti,Hannah Janmohamed,Lisa Coiffard,Lee Spector,Antoine Cully
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: Quality-Diversity is a family of evolutionary algorithms that generate diverse, high-performing solutions through local competition principles inspired by natural evolution. While research has focused on improving specific aspects of Quality-Diversity algorithms, surprisingly little attention has been paid to investigating alternative formulations of local competition itself -- the core mechanism distinguishing Quality-Diversity from traditional evolutionary algorithms. Most approaches implement local competition through explicit collection mechanisms like fixed grids or unstructured archives, imposing artificial constraints that require predefined bounds or hard-to-tune parameters. We show that Quality-Diversity methods can be reformulated as Genetic Algorithms where local competition occurs through fitness transformations rather than explicit collection mechanisms. Building on this insight, we introduce Dominated Novelty Search, a Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. Our experiments show that Dominated Novelty Search significantly outperforms existing approaches across standard Quality-Diversity benchmarks, while maintaining its advantage in challenging scenarios like high-dimensional and unsupervised spaces.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Advanced Architectures Integrated with Agentic AI for Next-Generation Wireless Networks 
[[arxiv](https://arxiv.org/abs/2502.01089)] [[cool](https://papers.cool/arxiv/2502.01089)] [[pdf](https://arxiv.org/pdf/2502.01089)]
> **Authors**: Kapal Dev,Sunder Ali Khowaja,Engin Zeydan,Merouane Debbah
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 6 Pages
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: This paper investigates a range of cutting-edge technologies and architectural innovations aimed at simplifying network operations, reducing operational expenditure (OpEx), and enabling the deployment of new service models. The focus is on (i) Proposing novel, more efficient 6G architectures, with both Control and User planes enabling the seamless expansion of services, while addressing long-term 6G network evolution. (ii) Exploring advanced techniques for constrained artificial intelligence (AI) operations, particularly the design of AI agents for real-time learning, optimizing energy consumption, and the allocation of computational resources. (iii) Identifying technologies and architectures that support the orchestration of backend services using serverless computing models across multiple domains, particularly for vertical industries. (iv) Introducing optically-based, ultra-high-speed, low-latency network architectures, with fast optical switching and real-time control, replacing conventional electronic switching to reduce power consumption by an order of magnitude.

## 机器人技术(cs.RO:Robotics)

### DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents 
[[arxiv](https://arxiv.org/abs/2502.01956)] [[cool](https://papers.cool/arxiv/2502.01956)] [[pdf](https://arxiv.org/pdf/2502.01956)]
> **Authors**: Shashank Sharma,Janina Hoffmann,Vinay Namboodiri
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: In this paper, we address the challenge of long-horizon visual planning tasks using Hierarchical Reinforcement Learning (HRL). Our key contribution is a Discrete Hierarchical Planning (DHP) method, an alternative to traditional distance-based approaches. We provide theoretical foundations for the method and demonstrate its effectiveness through extensive empirical evaluations. Our agent recursively predicts subgoals in the context of a long-term goal and receives discrete rewards for constructing plans as compositions of abstract actions. The method introduces a novel advantage estimation strategy for tree trajectories, which inherently encourages shorter plans and enables generalization beyond the maximum tree depth. The learned policy function allows the agent to plan efficiently, requiring only $\log N$ computational steps, making re-planning highly efficient. The agent, based on a soft-actor critic (SAC) framework, is trained using on-policy imagination data. Additionally, we propose a novel exploration strategy that enables the agent to generate relevant training examples for the planning modules. We evaluate our method on long-horizon visual planning tasks in a 25-room environment, where it significantly outperforms previous benchmarks at success rate and average episode length. Furthermore, an ablation study highlights the individual contributions of key modules to the overall performance.

### VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play 
[[arxiv](https://arxiv.org/abs/2502.01932)] [[cool](https://papers.cool/arxiv/2502.01932)] [[pdf](https://arxiv.org/pdf/2502.01932)]
> **Authors**: Zelai Xu,Chao Yu,Ruize Zhang,Huining Yuan,Xiangmin Yi,Shilong Ji,Chuqi Wang,Wenhao Tang,Yu Wang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Multi-agent reinforcement learning (MARL) has made significant progress, largely fueled by the development of specialized testbeds that enable systematic evaluation of algorithms in controlled yet challenging scenarios. However, existing testbeds often focus on purely virtual simulations or limited robot morphologies such as robotic arms, quadrupeds, and humanoids, leaving high-mobility platforms with real-world physical constraints like drones underexplored. To bridge this gap, we present VolleyBots, a new MARL testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots features a turn-based interaction model under volleyball rules, a hierarchical decision-making process that combines motion control and strategic play, and a high-fidelity simulation for seamless sim-to-real transfer. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative MARL and game-theoretic algorithms. Results in simulation show that while existing algorithms handle simple tasks effectively, they encounter difficulty in complex tasks that require both low-level control and high-level strategy. We further demonstrate zero-shot deployment of a simulation-learned policy to real-world drones, highlighting VolleyBots' potential to propel MARL research involving agile robotic platforms. The project page is at https://sites.google.com/view/thu-volleybots/home.

### Wake-Informed 3D Path Planning for Autonomous Underwater Vehicles Using A* and Neural Network Approximations 
[[arxiv](https://arxiv.org/abs/2502.01918)] [[cool](https://papers.cool/arxiv/2502.01918)] [[pdf](https://arxiv.org/pdf/2502.01918)]
> **Authors**: Zachary Cooper-Baldock,Stephen Turnock,Karl Sammut
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 11 pages, 6 figures, preprint of journal paper
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Autonomous Underwater Vehicles (AUVs) encounter significant energy, control and navigation challenges in complex underwater environments, particularly during close-proximity operations, such as launch and recovery (LAR), where fluid interactions and wake effects present additional navigational and energy challenges. Traditional path planning methods fail to incorporate these detailed wake structures, resulting in increased energy consumption, reduced control stability, and heightened safety risks. This paper presents a novel wake-informed, 3D path planning approach that fully integrates localized wake effects and global currents into the planning algorithm. Two variants of the A* algorithm - a current-informed planner and a wake-informed planner - are created to assess its validity and two neural network models are then trained to approximate these planners for real-time applications. Both the A* planners and NN models are evaluated using important metrics such as energy expenditure, path length, and encounters with high-velocity and turbulent regions. The results demonstrate a wake-informed A* planner consistently achieves the lowest energy expenditure and minimizes encounters with high-velocity regions, reducing energy consumption by up to 11.3%. The neural network models are observed to offer computational speedup of 6 orders of magnitude, but exhibit 4.51 - 19.79% higher energy expenditures and 9.81 - 24.38% less optimal paths. These findings underscore the importance of incorporating detailed wake structures into traditional path planning algorithms and the benefits of neural network approximations to enhance energy efficiency and operational safety for AUVs in complex 3D domains.

### Generalizable and Fast Surrogates: Model Predictive Control of Articulated Soft Robots using Physics-Informed Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.01916)] [[cool](https://papers.cool/arxiv/2502.01916)] [[pdf](https://arxiv.org/pdf/2502.01916)]
> **Authors**: Tim-Lukas Habich,Aran Mohammad,Simon F. G. Ehlers,Martin Bensch,Thomas Seel,Moritz Schappler
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Soft robots can revolutionize several applications with high demands on dexterity and safety. When operating these systems, real-time estimation and control require fast and accurate models. However, prediction with first-principles (FP) models is slow, and learned black-box models have poor generalizability. Physics-informed machine learning offers excellent advantages here, but it is currently limited to simple, often simulated systems without considering changes after training. We propose physics-informed neural networks (PINNs) for articulated soft robots (ASRs) with a focus on data efficiency. The amount of expensive real-world training data is reduced to a minimum - one dataset in one system domain. Two hours of data in different domains are used for a comparison against two gold-standard approaches: In contrast to a recurrent neural network, the PINN provides a high generalizability. The prediction speed of an accurate FP model is improved with the PINN by up to a factor of 466 at slightly reduced accuracy. This enables nonlinear model predictive control (MPC) of the pneumatic ASR. In nine dynamic MPC experiments, an average joint-tracking error of 1.3° is achieved.

### Composite Gaussian Processes Flows for Learning Discontinuous Multimodal Policies 
[[arxiv](https://arxiv.org/abs/2502.01913)] [[cool](https://papers.cool/arxiv/2502.01913)] [[pdf](https://arxiv.org/pdf/2502.01913)]
> **Authors**: Shu-yuan Wang,Hikaru Sasaki,Takamitsu Matsubara
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Learning control policies for real-world robotic tasks often involve challenges such as multimodality, local discontinuities, and the need for computational efficiency. These challenges arise from the complexity of robotic environments, where multiple solutions may coexist. To address these issues, we propose Composite Gaussian Processes Flows (CGP-Flows), a novel semi-parametric model for robotic policy. CGP-Flows integrate Overlapping Mixtures of Gaussian Processes (OMGPs) with the Continuous Normalizing Flows (CNFs), enabling them to model complex policies addressing multimodality and local discontinuities. This hybrid approach retains the computational efficiency of OMGPs while incorporating the flexibility of CNFs. Experiments conducted in both simulated and real-world robotic tasks demonstrate that CGP-flows significantly improve performance in modeling control policies. In a simulation task, we confirmed that CGP-Flows had a higher success rate compared to the baseline method, and the success rate of GCP-Flow was significantly different from the success rate of other baselines in chi-square tests.

### From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment 
[[arxiv](https://arxiv.org/abs/2502.01828)] [[cool](https://papers.cool/arxiv/2502.01828)] [[pdf](https://arxiv.org/pdf/2502.01828)]
> **Authors**: Yilin Wu,Ran Tian,Gokul Swamy,Andrea Bajcsy
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM's burden of predicting action outcomes (foresight) from evaluation (forethought). For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. For forethought, we align the VLM with these predicted latent states to reason about the consequences of actions in its native representation--natural language--and effectively filter proposed plans. We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering. Videos can be found on the project website: https://yilin-wu98.github.io/forewarn/.

### Flow-based Domain Randomization for Learning and Sequencing Robotic Skills 
[[arxiv](https://arxiv.org/abs/2502.01800)] [[cool](https://papers.cool/arxiv/2502.01800)] [[pdf](https://arxiv.org/pdf/2502.01800)]
> **Authors**: Aidan Curtis,Eric Li,Michael Noseworthy,Nishad Gothoskar,Sachin Chitta,Hui Li,Leslie Pack Kaelbling,Nicole Carey
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies trained in simulation. By randomizing environment properties during training, the learned policy can become robust to uncertainties along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate automatically discovering a sampling distribution via entropy-regularized reward maximization of a normalizing-flow-based neural sampling distribution. We show that this architecture is more flexible and provides greater robustness than existing approaches that learn simpler, parameterized sampling distributions, as demonstrated in six simulated and one real-world robotics domain. Lastly, we explore how these learned sampling distributions, combined with a privileged value function, can be used for out-of-distribution detection in an uncertainty-aware multi-step manipulation planner.

### VILP: Imitation Learning with Latent Video Planning 
[[arxiv](https://arxiv.org/abs/2502.01784)] [[cool](https://papers.cool/arxiv/2502.01784)] [[pdf](https://arxiv.org/pdf/2502.01784)]
> **Authors**: Zhengtong Xu,Qiang Qiu,Yu She
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: In the era of generative AI, integrating video generation models into robotics opens new possibilities for the general-purpose robot agent. This paper introduces imitation learning with latent video planning (VILP). We propose a latent video diffusion model to generate predictive robot videos that adhere to temporal consistency to a good degree. Our method is able to generate highly time-aligned videos from multiple views, which is crucial for robot policy learning. Our video generation model is highly time-efficient. For example, it can generate videos from two distinct perspectives, each consisting of six frames with a resolution of 96x160 pixels, at a rate of 5 Hz. In the experiments, we demonstrate that VILP outperforms the existing video generation robot policy across several metrics: training costs, inference speed, temporal consistency of generated videos, and the performance of the policy. We also compared our method with other imitation learning methods. Our findings indicate that VILP can rely less on extensive high-quality task-specific robot action data while still maintaining robust performance. In addition, VILP possesses robust capabilities in representing multi-modal action distributions. Our paper provides a practical example of how to effectively integrate video generation models into robot policies, potentially offering insights for related fields and directions. For more details, please refer to our open-source repository https://github.com/ZhengtongXu/VILP.

### Coarse-to-Fine 3D Keyframe Transporter 
[[arxiv](https://arxiv.org/abs/2502.01773)] [[cool](https://papers.cool/arxiv/2502.01773)] [[pdf](https://arxiv.org/pdf/2502.01773)]
> **Authors**: Xupeng Zhu,David Klee,Dian Wang,Boce Hu,Haojie Huang,Arsh Tangri,Robin Walters,Robert Platt
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Recent advances in Keyframe Imitation Learning (IL) have enabled learning-based agents to solve a diverse range of manipulation tasks. However, most approaches ignore the rich symmetries in the problem setting and, as a consequence, are sample-inefficient. This work identifies and utilizes the bi-equivariant symmetry within Keyframe IL to design a policy that generalizes to transformations of both the workspace and the objects grasped by the gripper. We make two main contributions: First, we analyze the bi-equivariance properties of the keyframe action scheme and propose a Keyframe Transporter derived from the Transporter Networks, which evaluates actions using cross-correlation between the features of the grasped object and the features of the scene. Second, we propose a computationally efficient coarse-to-fine SE(3) action evaluation scheme for reasoning the intertwined translation and rotation action. The resulting method outperforms strong Keyframe IL baselines by an average of >10% on a wide range of simulation tasks, and by an average of 55% in 4 physical experiments.

### Dynamic object goal pushing with mobile manipulators through model-free constrained reinforcement learning 
[[arxiv](https://arxiv.org/abs/2502.01546)] [[cool](https://papers.cool/arxiv/2502.01546)] [[pdf](https://arxiv.org/pdf/2502.01546)]
> **Authors**: Ioannis Dadiotis,Mayank Mittal,Nikos Tsagarakis,Marco Hutter
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: accepted for ICRA 2025
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: Non-prehensile pushing to move and reorient objects to a goal is a versatile loco-manipulation skill. In the real world, the object's physical properties and friction with the floor contain significant uncertainties, which makes the task challenging for a mobile manipulator. In this paper, we develop a learning-based controller for a mobile manipulator to move an unknown object to a desired position and yaw orientation through a sequence of pushing actions. The proposed controller for the robotic arm and the mobile base motion is trained using a constrained Reinforcement Learning (RL) formulation. We demonstrate its capability in experiments with a quadrupedal robot equipped with an arm. The learned policy achieves a success rate of 91.35% in simulation and at least 80% on hardware in challenging scenarios. Through our extensive hardware experiments, we show that the approach demonstrates high robustness against unknown objects of different masses, materials, sizes, and shapes. It reactively discovers the pushing location and direction, thus achieving contact-rich behavior while observing only the pose of the object. Additionally, we demonstrate the adaptive behavior of the learned policy towards preventing the object from toppling.

### Resilient UAV Trajectory Planning via Few-Shot Meta-Offline Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.01268)] [[cool](https://papers.cool/arxiv/2502.01268)] [[pdf](https://arxiv.org/pdf/2502.01268)]
> **Authors**: Eslam Eldeeb,Hirley Alves
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Reinforcement learning (RL) has been a promising essence in future 5G-beyond and 6G systems. Its main advantage lies in its robust model-free decision-making in complex and large-dimension wireless environments. However, most existing RL frameworks rely on online interaction with the environment, which might not be feasible due to safety and cost concerns. Another problem with online RL is the lack of scalability of the designed algorithm with dynamic or new environments. This work proposes a novel, resilient, few-shot meta-offline RL algorithm combining offline RL using conservative Q-learning (CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed algorithm can train RL models using static offline datasets without any online interaction with the environments. In addition, with the aid of MAML, the proposed model can be scaled up to new unseen environments. We showcase the proposed algorithm for optimizing an unmanned aerial vehicle (UAV) 's trajectory and scheduling policy to minimize the age-of-information (AoI) and transmission power of limited-power devices. Numerical results show that the proposed few-shot meta-offline RL algorithm converges faster than baseline schemes, such as deep Q-networks and CQL. In addition, it is the only algorithm that can achieve optimal joint AoI and transmission power using an offline dataset with few shots of data points and is resilient to network failures due to unprecedented environmental changes.

### Neural Cellular Automata for Decentralized Sensing using a Soft Inductive Sensor Array for Distributed Manipulator Systems 
[[arxiv](https://arxiv.org/abs/2502.01242)] [[cool](https://papers.cool/arxiv/2502.01242)] [[pdf](https://arxiv.org/pdf/2502.01242)]
> **Authors**: Bailey Dacre,Nicolas Bessone,Matteo Lo Preti,Diana Cafiso,Rodrigo Moreno,Andrés Faíña,Lucia Beccai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 8 figures
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: In Distributed Manipulator Systems (DMS), decentralization is a highly desirable property as it promotes robustness and facilitates scalability by distributing computational burden and eliminating singular points of failure. However, current DMS typically utilize a centralized approach to sensing, such as single-camera computer vision systems. This centralization poses a risk to system reliability and offers a significant limiting factor to system size. In this work, we introduce a decentralized approach for sensing and in a Distributed Manipulator Systems using Neural Cellular Automata (NCA). Demonstrating a decentralized sensing in a hardware implementation, we present a novel inductive sensor board designed for distributed sensing and evaluate its ability to estimate global object properties, such as the geometric center, through local interactions and computations. Experiments demonstrate that NCA-based sensing networks accurately estimate object position at 0.24 times the inter sensor distance. They maintain resilience under sensor faults and noise, and scale seamlessly across varying network sizes. These findings underscore the potential of local, decentralized computations to enable scalable, fault-tolerant, and noise-resilient object property estimation in DMS

### Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents 
[[arxiv](https://arxiv.org/abs/2502.01218)] [[cool](https://papers.cool/arxiv/2502.01218)] [[pdf](https://arxiv.org/pdf/2502.01218)]
> **Authors**: Zhizhen Zhang,Lei Zhu,Zhen Fang,Zi Huang,Yadan Luo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments across varying numbers of demonstrations show that the pretrained features significantly enhance downstream manipulation tasks by up to 49% with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents. The source code is included in the supplementary material for reference.

### ConditionNET: Learning Preconditions and Effects for Execution Monitoring 
[[arxiv](https://arxiv.org/abs/2502.01167)] [[cool](https://papers.cool/arxiv/2502.01167)] [[pdf](https://arxiv.org/pdf/2502.01167)]
> **Authors**: Daniel Sliwowski,Dongheui Lee
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 5 figures, 3 tables
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: The introduction of robots into everyday scenarios necessitates algorithms capable of monitoring the execution of tasks. In this paper, we propose ConditionNET, an approach for learning the preconditions and effects of actions in a fully data-driven manner. We develop an efficient vision-language model and introduce additional optimization objectives during training to optimize for consistent feature representations. ConditionNET explicitly models the dependencies between actions, preconditions, and effects, leading to improved performance. We evaluate our model on two robotic datasets, one of which we collected for this paper, containing 406 successful and 138 failed teleoperated demonstrations of a Franka Emika Panda robot performing tasks like pouring and cleaning the counter. We show in our experiments that ConditionNET outperforms all baselines on both anomaly detection and phase prediction tasks. Furthermore, we implement an action monitoring system on a real robot to demonstrate the practical applicability of the learned preconditions and effects. Our results highlight the potential of ConditionNET for enhancing the reliability and adaptability of robots in real-world environments. The data is available on the project website: https://dsliwowski1.github.io/ConditionNET_page.

### ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills 
[[arxiv](https://arxiv.org/abs/2502.01143)] [[cool](https://papers.cool/arxiv/2502.01143)] [[pdf](https://arxiv.org/pdf/2502.01143)]
> **Authors**: Tairan He,Jiawei Gao,Wenli Xiao,Yuanhang Zhang,Zi Wang,Jiashun Wang,Zhengyi Luo,Guanqi He,Nikhil Sobanbab,Chaoyi Pan,Zeji Yi,Guannan Qu,Kris Kitani,Jessica Hodgins,Linxi "Jim" Fan,Yuke Zhu,Changliu Liu,Guanya Shi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Project website: https://agile.human2humanoid.com/
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习,系统与控制
- **Abstract**: Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.

### End-to-End Imitation Learning for Optimal Asteroid Proximity Operations 
[[arxiv](https://arxiv.org/abs/2502.01034)] [[cool](https://papers.cool/arxiv/2502.01034)] [[pdf](https://arxiv.org/pdf/2502.01034)]
> **Authors**: Patrick Quinn,George Nehma,Madhur Tiwari
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 7 pages, 8 figures. Submitted to the 2025 IEEE Aerospace Conference
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Controlling spacecraft near asteroids in deep space comes with many challenges. The delays involved necessitate heavy usage of limited onboard computation resources while fuel efficiency remains a priority to support the long loiter times needed for gathering data. Additionally, the difficulty of state determination due to the lack of traditional reference systems requires a guidance, navigation, and control (GNC) pipeline that ideally is both computationally and fuel-efficient, and that incorporates a robust state determination system. In this paper, we propose an end-to-end algorithm utilizing neural networks to generate near-optimal control commands from raw sensor data, as well as a hybrid model predictive control (MPC) guided imitation learning controller delivering improvements in computational efficiency over a traditional MPC controller.

### Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis 
[[arxiv](https://arxiv.org/abs/2502.00935)] [[cool](https://papers.cool/arxiv/2502.00935)] [[pdf](https://arxiv.org/pdf/2502.00935)]
> **Authors**: Kensuke Nakamura,Lasse Peters,Andrea Bajcsy
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 6 figures, 6 tables
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision-avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. In this work, our goal is to generalize safe robot controllers to prevent failures that are hard -- if not impossible -- to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag. We propose Latent Safety Filters, a latent-space generalization of HJ reachability that tractably operates directly on raw observation data (e.g., RGB images) by performing safety analysis in the latent embedding space of a generative world model. This transforms nuanced constraint specification to a classification problem in latent space and enables reasoning about dynamical consequences that are hard to simulate. In simulation and hardware experiments, we use Latent Safety Filters to safeguard arbitrary policies (from generative policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag or toppling cluttered objects.

### CAIMAN: Causal Action Influence Detection for Sample Efficient Loco-manipulation 
[[arxiv](https://arxiv.org/abs/2502.00835)] [[cool](https://papers.cool/arxiv/2502.00835)] [[pdf](https://arxiv.org/pdf/2502.00835)]
> **Authors**: Yuanchen Yuan,Jin Cheng,Núria Armengol Urpí,Stelian Coros
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Enabling legged robots to perform non-prehensile loco-manipulation with large and heavy objects is crucial for enhancing their versatility. However, this is a challenging task, often requiring sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured scenarios with obstacles. In this work, we present CAIMAN, a novel framework for learning loco-manipulation that relies solely on sparse task rewards. We leverage causal action influence to detect states where the robot is in control over other entities in the environment, and use this measure as an intrinsically motivated objective to enable sample-efficient learning. We employ a hierarchical control strategy, combining a low-level locomotion policy with a high-level policy that prioritizes task-relevant velocity commands. Through simulated and real-world experiments, including object manipulation with obstacles, we demonstrate the framework's superior sample efficiency, adaptability to diverse environments, and successful transfer to hardware without fine-tuning. The proposed approach paves the way for scalable, robust, and autonomous loco-manipulation in real-world applications.

### Strengthening Generative Robot Policies through Predictive World Modeling 
[[arxiv](https://arxiv.org/abs/2502.00622)] [[cool](https://papers.cool/arxiv/2502.00622)] [[pdf](https://arxiv.org/pdf/2502.00622)]
> **Authors**: Han Qi,Haocheng Yin,Yilun Du,Heng Yang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Website: https://computationalrobotics.seas.harvard.edu/GPC
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: We present generative predictive control (GPC), a learning control framework that (i) clones a generative diffusion-based policy from expert demonstrations, (ii) trains a predictive action-conditioned world model from both expert demonstrations and random explorations, and (iii) synthesizes an online planner that ranks and optimizes the action proposals from (i) by looking ahead into the future using the world model from (ii). Crucially, we show that conditional video diffusion allows learning (near) physics-accurate visual world models and enable robust visual foresight. Focusing on planar pushing with rich contact and collision, we show GPC dominates behavior cloning across state-based and vision-based, simulated and real-world experiments.

### VertiFormer: A Data-Efficient Multi-Task Transformer for Off-Road Robot Mobility 
[[arxiv](https://arxiv.org/abs/2502.00543)] [[cool](https://papers.cool/arxiv/2502.00543)] [[pdf](https://arxiv.org/pdf/2502.00543)]
> **Authors**: Mohammad Nazeri,Anuj Pokhrel,Alexandyr Card,Aniket Datar,Garrett Warnell,Xuesu Xiao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 9 figures, url: https://github.com/mhnazeri/VertiFormer
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Sophisticated learning architectures, e.g., Transformers, present a unique opportunity for robots to understand complex vehicle-terrain kinodynamic interactions for off-road mobility. While internet-scale data are available for Natural Language Processing (NLP) and Computer Vision (CV) tasks to train Transformers, real-world mobility data are difficult to acquire with physical robots navigating off-road terrain. Furthermore, training techniques specifically designed to process text and image data in NLP and CV may not apply to robot mobility. In this paper, we propose VertiFormer, a novel data-efficient multi-task Transformer model trained with only one hour of data to address such challenges of applying Transformer architectures for robot mobility on extremely rugged, vertically challenging, off-road terrain. Specifically, VertiFormer employs a new learnable masked modeling and next token prediction paradigm to predict the next pose, action, and terrain patch to enable a variety of off-road mobility tasks simultaneously, e.g., forward and inverse kinodynamics modeling. The non-autoregressive design mitigates computational bottlenecks and error propagation associated with autoregressive models. VertiFormer's unified modality representation also enhances learning of diverse temporal mappings and state representations, which, combined with multiple objective functions, further improves model generalization. Our experiments offer insights into effectively utilizing Transformers for off-road robot mobility with limited data and demonstrate our efficiently trained Transformer can facilitate multiple off-road mobility tasks onboard a physical mobile robot.

### FlexCloud: Direct, Modular Georeferencing and Drift-Correction of Point Cloud Maps 
[[arxiv](https://arxiv.org/abs/2502.00395)] [[cool](https://papers.cool/arxiv/2502.00395)] [[pdf](https://arxiv.org/pdf/2502.00395)]
> **Authors**: Maximilian Leitenstern,Marko Alten,Christian Bolea-Schaser,Dominik Kulmer,Marcel Weinmann,Markus Lienkamp
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted for publication at VEHITS 2025, Proceedings of the 11th International Conference on Vehicle Technology and Intelligent Transport Systems - VEHITS; 2025
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Current software stacks for real-world applications of autonomous driving leverage map information to ensure reliable localization, path planning, and motion prediction. An important field of research is the generation of point cloud maps, referring to the topic of simultaneous localization and mapping (SLAM). As most recent developments do not include global position data, the resulting point cloud maps suffer from internal distortion and missing georeferencing, preventing their use for map-based localization approaches. Therefore, we propose FlexCloud for an automatic georeferencing of point cloud maps created from SLAM. Our approach is designed to work modularly with different SLAM methods, utilizing only the generated local point cloud map and its odometry. Using the corresponding GNSS positions enables direct georeferencing without additional control points. By leveraging a 3D rubber-sheet transformation, we can correct distortions within the map caused by long-term drift while maintaining its structure. Our approach enables the creation of consistent, globally referenced point cloud maps from data collected by a mobile mapping system (MMS). The source code of our work is available at https://github.com/TUMFTM/FlexCloud.

### Simultaneous Estimation of Manipulation Skill and Hand Grasp Force from Forearm Ultrasound Images 
[[arxiv](https://arxiv.org/abs/2502.00275)] [[cool](https://papers.cool/arxiv/2502.00275)] [[pdf](https://arxiv.org/pdf/2502.00275)]
> **Authors**: Keshav Bimbraw,Srikar Nekkanti,Daniel B. Tiller II,Mihir Deshmukh,Berk Calli,Robert D. Howe,Haichong K. Zhang
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 30 pages, 52 references, 10 figures, 8 tables and 2 supplementary videos. Currently under review
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,新兴技术,人机交互
- **Abstract**: Accurate estimation of human hand configuration and the forces they exert is critical for effective teleoperation and skill transfer in robotic manipulation. A deeper understanding of human interactions with objects can further enhance teleoperation performance. To address this need, researchers have explored methods to capture and translate human manipulation skills and applied forces to robotic systems. Among these, biosignal-based approaches, particularly those using forearm ultrasound data, have shown significant potential for estimating hand movements and finger forces. In this study, we present a method for simultaneously estimating manipulation skills and applied hand force using forearm ultrasound data. Data collected from seven participants were used to train deep learning models for classifying manipulation skills and estimating grasp force. Our models achieved an average classification accuracy of 94.87 percent plus or minus 10.16 percent for manipulation skills and an average root mean square error (RMSE) of 0.51 plus or minus 0.19 Newtons for force estimation, as evaluated using five-fold cross-validation. These results highlight the effectiveness of forearm ultrasound in advancing human-machine interfacing and robotic teleoperation for complex manipulation tasks. This work enables new and effective possibilities for human-robot skill transfer and tele-manipulation, bridging the gap between human dexterity and robotic control.

### A Direct Semi-Exhaustive Search Method for Robust, Partial-to-Full Point Cloud Registration 
[[arxiv](https://arxiv.org/abs/2502.00115)] [[cool](https://papers.cool/arxiv/2502.00115)] [[pdf](https://arxiv.org/pdf/2502.00115)]
> **Authors**: Richard Cheng,Chavdar Papozov,Dan Helmick,Mark Tjersland
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: IROS 2024
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Point cloud registration refers to the problem of finding the rigid transformation that aligns two given point clouds, and is crucial for many applications in robotics and computer vision. The main insight of this paper is that we can directly optimize the point cloud registration problem without correspondences by utilizing an algorithmically simple, yet computationally complex, semi-exhaustive search approach that is very well-suited for parallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive Search (DSES), iterates over potential rotation matrices and efficiently computes the inlier-maximizing translation associated with each rotation. It then computes the optimal rigid transformation based on any desired distance metric by directly computing the error associated with each transformation candidate $\{R, t\}$. By leveraging the parallelism of modern GPUs, DSES outperforms state-of-the-art methods for partial-to-full point cloud registration on the simulated ModelNet40 benchmark and demonstrates high performance and robustness for pose estimation on a real-world robotics problem (https://youtu.be/q0q2-s2KSuA).

### Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach 
[[arxiv](https://arxiv.org/abs/2502.00114)] [[cool](https://papers.cool/arxiv/2502.00114)] [[pdf](https://arxiv.org/pdf/2502.00114)]
> **Authors**: Aaron Hao Tan,Angus Fung,Haitong Wang,Goldie Nejat
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 8 figures
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes.

### DISC: Dataset for Analyzing Driving Styles In Simulated Crashes for Mixed Autonomy 
[[arxiv](https://arxiv.org/abs/2502.00050)] [[cool](https://papers.cool/arxiv/2502.00050)] [[pdf](https://arxiv.org/pdf/2502.00050)]
> **Authors**: Sandip Sharan Senthil Kumar,Sandeep Thalapanane,Guru Nandhan Appiya Dilipkumar Peethambari,Sourang SriHari,Laura Zheng,Ming C. Lin
> **First submission**: 2025-01-28
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Handling pre-crash scenarios is still a major challenge for self-driving cars due to limited practical data and human-driving behavior datasets. We introduce DISC (Driving Styles In Simulated Crashes), one of the first datasets designed to capture various driving styles and behaviors in pre-crash scenarios for mixed autonomy analysis. DISC includes over 8 classes of driving styles/behaviors from hundreds of drivers navigating a simulated vehicle through a virtual city, encountering rare-event traffic scenarios. This dataset enables the classification of pre-crash human driving behaviors in unsafe conditions, supporting individualized trajectory prediction based on observed driving patterns. By utilizing a custom-designed VR-based in-house driving simulator, TRAVERSE, data was collected through a driver-centric study involving human drivers encountering twelve simulated accident scenarios. This dataset fills a critical gap in human-centric driving data for rare events involving interactions with autonomous vehicles. It enables autonomous systems to better react to human drivers and optimize trajectory prediction in mixed autonomy environments involving both human-driven and self-driving cars. In addition, individual driving behaviors are classified through a set of standardized questionnaires, carefully designed to identify and categorize driving behavior traits. We correlate data features with driving behaviors, showing that the simulated environment reflects real-world driving styles. DISC is the first dataset to capture how various driving styles respond to accident scenarios, offering significant potential to enhance autonomous vehicle safety and driving behavior analysis in mixed autonomy environments.

## 声音(cs.SD:Sound)

### Adapter-Based Multi-Agent AVSR Extension for Pre-Trained ASR Models 
[[arxiv](https://arxiv.org/abs/2502.01709)] [[cool](https://papers.cool/arxiv/2502.01709)] [[pdf](https://arxiv.org/pdf/2502.01709)]
> **Authors**: Christopher Simic,Korbinian Riedhammer,Tobias Bocklet
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at ICASSP 2025
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: We present an approach to Audio-Visual Speech Recognition that builds on a pre-trained Whisper model. To infuse visual information into this audio-only model, we extend it with an AV fusion module and LoRa adapters, one of the most up-to-date adapter approaches. One advantage of adapter-based approaches, is that only a relatively small number of parameters are trained, while the basic model remains unchanged. Common AVSR approaches train single models to handle several noise categories and noise levels simultaneously. Taking advantage of the lightweight nature of adapter approaches, we train noise-scenario-specific adapter-sets, each covering individual noise-categories or a specific noise-level range. The most suitable adapter-set is selected by previously classifying the noise-scenario. This enables our models to achieve an optimum coverage across different noise-categories and noise-levels, while training only a minimum number of parameters. Compared to a full fine-tuning approach with SOTA performance our models achieve almost comparable results over the majority of the tested noise-categories and noise-levels, with up to 88.5% less trainable parameters. Our approach can be extended by further noise-specific adapter-sets to cover additional noise scenarios. It is also possible to utilize the underlying powerful ASR model when no visual information is available, as it remains unchanged.

### Deep Active Speech Cancellation with Multi-Band Mamba Network 
[[arxiv](https://arxiv.org/abs/2502.01185)] [[cool](https://papers.cool/arxiv/2502.01185)] [[pdf](https://arxiv.org/pdf/2502.01185)]
> **Authors**: Yehuda Mishaly,Lior Wolf,Eliya Nachmani
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理,信号处理
- **Abstract**: We present a novel deep learning network for Active Speech Cancellation (ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively canceling both noise and speech signals. The proposed Multi-Band Mamba architecture segments input audio into distinct frequency bands, enabling precise anti-signal generation and improved phase alignment across frequencies. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Experimental results demonstrate substantial performance gains, achieving up to 7.2dB improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming existing methods. Audio samples are available at https://mishalydev.github.io/DeepASC-Demo

### Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.01152)] [[cool](https://papers.cool/arxiv/2502.01152)] [[pdf](https://arxiv.org/pdf/2502.01152)]
> **Authors**: Nanjun Zhou,Weilin Lin,Li Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 5 pages, 5 figures. This work has been accpeted by ICASSP 2025
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Backdoor attacks have posed a significant threat to the security of deep neural networks (DNNs). Despite considerable strides in developing defenses against backdoor attacks in the visual domain, the specialized defenses for the audio domain remain empty. Furthermore, the defenses adapted from the visual to audio domain demonstrate limited effectiveness. To fill this gap, we propose Gradient Norm-based FineTuning (GN-FT), a novel defense strategy against the attacks in the audio domain, based on the observation from the corresponding backdoored models. Specifically, we first empirically find that the backdoored neurons exhibit greater gradient values compared to other neurons, while clean neurons stay the lowest. On this basis, we fine-tune the backdoored model by incorporating the gradient norm regularization, aiming to weaken and reduce the backdoored neurons. We further approximate the loss computation for lower implementation costs. Extensive experiments on two speech recognition datasets across five models demonstrate the superior performance of our proposed method. To the best of our knowledge, this work is the first specialized and effective defense against backdoor attacks in the audio domain.

### Emotional Face-to-Speech 
[[arxiv](https://arxiv.org/abs/2502.01046)] [[cool](https://papers.cool/arxiv/2502.01046)] [[pdf](https://arxiv.org/pdf/2502.01046)]
> **Authors**: Jiaxin Ye,Boyuan Cao,Hongming Shan
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 声音,计算机视觉和模式识别,音频和语音处理
- **Abstract**: How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed emotional face-to-speech, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce DEmoFace, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos are shown at https://demoface-ai.github.io/.

### CycleGuardian: A Framework for Automatic RespiratorySound classification Based on Improved Deep clustering and Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.00734)] [[cool](https://papers.cool/arxiv/2502.00734)] [[pdf](https://arxiv.org/pdf/2502.00734)]
> **Authors**: Yun Chu,Qiuhao Wang,Enze Zhou,Ling Fu,Qian Liu,Gang Zheng
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,音频和语音处理
- **Abstract**: Auscultation plays a pivotal role in early respiratory and pulmonary disease diagnosis. Despite the emergence of deep learning-based methods for automatic respiratory sound classification post-Covid-19, limited datasets impede performance enhancement. Distinguishing between normal and abnormal respiratory sounds poses challenges due to the coexistence of normal respiratory components and noise components in both types. Moreover, different abnormal respiratory sounds exhibit similar anomalous features, hindering their differentiation. Besides, existing state-of-the-art models suffer from excessive parameter size, impeding deployment on resource-constrained mobile platforms. To address these issues, we design a lightweight network CycleGuardian and propose a framework based on an improved deep clustering and contrastive learning. We first generate a hybrid spectrogram for feature diversity and grouping spectrograms to facilitating intermittent abnormal sound capture.Then, CycleGuardian integrates a deep clustering module with a similarity-constrained clustering component to improve the ability to capture abnormal features and a contrastive learning module with group mixing for enhanced abnormal feature discernment. Multi-objective optimization enhances overall performance during training. In experiments we use the ICBHI2017 dataset, following the official split method and without any pre-trained weights, our method achieves Sp: 82.06 $\%$, Se: 44.47$\%$, and Score: 63.26$\%$ with a network model size of 38M, comparing to the current model, our method leads by nearly 7$\%$, achieving the current best performances. Additionally, we deploy the network on Android devices, showcasing a comprehensive intelligent respiratory sound auscultation system.

### AudioGenX: Explainability on Text-to-Audio Generative Models 
[[arxiv](https://arxiv.org/abs/2502.00459)] [[cool](https://papers.cool/arxiv/2502.00459)] [[pdf](https://arxiv.org/pdf/2502.00459)]
> **Authors**: Hyunju Kang,Geonhee Han,Yoonjae Jeong,Hogun Park
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 14 pages
- **标题**: None
- **领域**: 声音,人工智能,机器学习,音频和语音处理
- **Abstract**: Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.

### Do Audio-Visual Segmentation Models Truly Segment Sounding Objects? 
[[arxiv](https://arxiv.org/abs/2502.00358)] [[cool](https://papers.cool/arxiv/2502.00358)] [[pdf](https://arxiv.org/pdf/2502.00358)]
> **Authors**: Jia Li,Wenjie Zhao,Ziru Huang,Yunhui Guo,Yapeng Tian
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,机器学习,多媒体,音频和语音处理
- **Abstract**: Unlike traditional visual segmentation, audio-visual segmentation (AVS) requires the model not only to identify and segment objects but also to determine whether they are sound sources. Recent AVS approaches, leveraging transformer architectures and powerful foundation models like SAM, have achieved impressive performance on standard benchmarks. Yet, an important question remains: Do these models genuinely integrate audio-visual cues to segment sounding objects? In this paper, we systematically investigate this issue in the context of robust AVS. Our study reveals a fundamental bias in current methods: they tend to generate segmentation masks based predominantly on visual salience, irrespective of the audio context. This bias results in unreliable predictions when sounds are absent or irrelevant. To address this challenge, we introduce AVSBench-Robust, a comprehensive benchmark incorporating diverse negative audio scenarios including silence, ambient noise, and off-screen sounds. We also propose a simple yet effective approach combining balanced training with negative samples and classifier-guided similarity learning. Our extensive experiments show that state-of-theart AVS methods consistently fail under negative audio conditions, demonstrating the prevalence of visual bias. In contrast, our approach achieves remarkable improvements in both standard metrics and robustness measures, maintaining near-perfect false positive rates while preserving highquality segmentation performance.

### SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech Emotion Recognition 
[[arxiv](https://arxiv.org/abs/2502.00310)] [[cool](https://papers.cool/arxiv/2502.00310)] [[pdf](https://arxiv.org/pdf/2502.00310)]
> **Authors**: Alaa Nfissi,Wassim Bouachir,Nizar Bouguila,Brian Mishara
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Published in: IEEE Transactions on Affective Computing
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,音频和语音处理
- **Abstract**: In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition.

## 软件工程(cs.SE:Software Engineering)

### LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations 
[[arxiv](https://arxiv.org/abs/2502.02009)] [[cool](https://papers.cool/arxiv/2502.02009)] [[pdf](https://arxiv.org/pdf/2502.02009)]
> **Authors**: Ziyang Ye,Triet Huynh Minh Le,M. Ali Babar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能,密码学和安全,机器学习
- **Abstract**: Security misconfigurations in Container Orchestrators (COs) can pose serious threats to software systems. While Static Analysis Tools (SATs) can effectively detect these security vulnerabilities, the industry currently lacks automated solutions capable of fixing these misconfigurations. The emergence of Large Language Models (LLMs), with their proven capabilities in code understanding and generation, presents an opportunity to address this limitation. This study introduces LLMSecConfig, an innovative framework that bridges this gap by combining SATs with LLMs. Our approach leverages advanced prompting techniques and Retrieval-Augmented Generation (RAG) to automatically repair security misconfigurations while preserving operational functionality. Evaluation of 1,000 real-world Kubernetes configurations achieved a 94\% success rate while maintaining a low rate of introducing new misconfigurations. Our work makes a promising step towards automated container security management, reducing the manual effort required for configuration maintenance.

### SE Arena: Benchmarking Software Engineering Chatbots with Iterative Interactions 
[[arxiv](https://arxiv.org/abs/2502.01860)] [[cool](https://papers.cool/arxiv/2502.01860)] [[pdf](https://arxiv.org/pdf/2502.01860)]
> **Authors**: Zhimin Zhao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: https://huggingface.co/spaces/SE-Arena/Software-Engineering-Arena
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate SE-focused chatbots. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.

### Assessing Data Augmentation-Induced Bias in Training and Testing of Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.01825)] [[cool](https://papers.cool/arxiv/2502.01825)] [[pdf](https://arxiv.org/pdf/2502.01825)]
> **Authors**: Riddhi More,Jeremy S. Bradbury
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 4 pages
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Data augmentation has become a standard practice in software engineering to address limited or imbalanced data sets, particularly in specialized domains like test classification and bug detection where data can be scarce. Although techniques such as SMOTE and mutation-based augmentation are widely used in software testing and debugging applications, a rigorous understanding of how augmented training data impacts model bias is lacking. It is especially critical to consider bias in scenarios where augmented data sets are used not just in training but also in testing models. Through a comprehensive case study of flaky test classification, we demonstrate how to test for bias and understand the impact that the inclusion of augmented samples in testing sets can have on model evaluation.

### Agentic Bug Reproduction for Effective Automated Program Repair at Google 
[[arxiv](https://arxiv.org/abs/2502.01821)] [[cool](https://papers.cool/arxiv/2502.01821)] [[pdf](https://arxiv.org/pdf/2502.01821)]
> **Authors**: Runxiang Cheng,Michele Tufano,Jürgen Cito,José Cambronero,Pat Rondon,Renyao Wei,Aaron Sun,Satish Chandra
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Bug reports often lack sufficient detail for developers to reproduce and fix the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the bug is present and pass when it has been resolved, are crucial for debugging, but they are rarely included in bug reports, both in open-source and in industrial settings. Thus, automatically generating BRTs from bug reports has the potential to accelerate the debugging process and lower time to repair. This paper investigates automated BRT generation within an industry setting, specifically at Google, focusing on the challenges of a large-scale, proprietary codebase and considering real-world industry bugs extracted from Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT generation technique, LIBRO, and present our agent-based approach, BRT Agent, which makes use of a fine-tuned Large Language Model (LLM) for code editing. Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from Google's internal issue tracker. We further investigate the practical value of generated BRTs by integrating them with an Automated Program Repair (APR) system at Google. Our results show that providing BRTs to the APR system results in 30% more bugs with plausible fixes. Additionally, we introduce Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select the most promising fixes from all fixes generated by APR system. Our evaluation on EPR for Top-K and threshold-based fix selections demonstrates promising results and trade-offs. For example, EPR correctly selects a plausible fix from a pool of 20 candidates in 70% of cases, based on its top-1 ranking.

### Toward Neurosymbolic Program Comprehension 
[[arxiv](https://arxiv.org/abs/2502.01806)] [[cool](https://papers.cool/arxiv/2502.01806)] [[pdf](https://arxiv.org/pdf/2502.01806)]
> **Authors**: Alejandro Velasco,Aya Garryyeva,David N. Palacio,Antonio Mastropaolo,Denys Poshyvanyk
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their "black-box'' nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods--renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.

### ACECODER: Acing Coder RL via Automated Test-Case Synthesis 
[[arxiv](https://arxiv.org/abs/2502.01718)] [[cool](https://papers.cool/arxiv/2502.01718)] [[pdf](https://arxiv.org/pdf/2502.01718)]
> **Authors**: Huaye Zeng,Dongfu Jiang,Haozhe Wang,Ping Nie,Xiaotong Chen,Wenhu Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 1 figure, 8 tables
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学
- **Abstract**: Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.

### Process-Supervised Reinforcement Learning for Code Generation 
[[arxiv](https://arxiv.org/abs/2502.01715)] [[cool](https://papers.cool/arxiv/2502.01715)] [[pdf](https://arxiv.org/pdf/2502.01715)]
> **Authors**: Yufan Ye,Ting Zhang,Wenbin Jiang,Hua Huang
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Existing reinforcement learning strategies based on outcome supervision have proven effective in enhancing the performance of large language models(LLMs) for code generation. While reinforcement learning based on process supervision has shown great promise in handling multi-step reasoning tasks, its effectiveness in code generation remains largely underexplored and underjustified. The primary obstacle stems from the resource-intensive nature of constructing high-quality process-supervised data, which demands substantial human expertise and computational resources. In response to this challenge, we propose a "statement mutation/refactoring-compile and execution verification" strategy: mutating and refactoring code line-by-line through a teacher model, and utilizing compiler execution results to automatically label each line, resulting in line-by-line process-supervised data, which is pivotal for training a process-supervised reward model. The trained reward model is then integrated into the PRLCoder framework, followed by experimental validation on several benchmarks. Experimental results demonstrate that process-supervised reinforcement learning significantly surpasses methods relying solely on outcome supervision. Notably, in tackling complex code generation tasks, process-supervised reinforcement learning shows a clear advantage, ensuring both the integrity of the code generation process and the correctness of the generation results.

### The AI Agent Index 
[[arxiv](https://arxiv.org/abs/2502.01635)] [[cool](https://papers.cool/arxiv/2502.01635)] [[pdf](https://arxiv.org/pdf/2502.01635)]
> **Authors**: Stephen Casper,Luke Bailey,Rosco Hunter,Carson Ezell,Emma Cabalé,Michael Gerovitch,Stewart Slocum,Kevin Wei,Nikola Jurkovic,Ariba Khan,Phillip J. K. Christoffersen,A. Pinar Ozisik,Rakshit Trivedi,Dylan Hadfield-Menell,Noam Kolt
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accompanying website: https://aiagentindex.mit.edu/
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Leading AI developers and startups are increasingly deploying agentic AI systems that can plan and execute complex tasks with limited human involvement. However, there is currently no structured framework for documenting the technical components, intended uses, and safety features of agentic systems. To fill this gap, we introduce the AI Agent Index, the first public database to document information about currently deployed agentic AI systems. For each system that meets the criteria for inclusion in the index, we document the system's components (e.g., base model, reasoning implementation, tool use), application domains (e.g., computer use, software engineering), and risk management practices (e.g., evaluation results, guardrails), based on publicly available information and correspondence with developers. We find that while developers generally provide ample information regarding the capabilities and applications of agentic systems, they currently provide limited information regarding safety and risk management practices. The AI Agent Index is available online at https://aiagentindex.mit.edu/

### Learning to Generate Unit Tests for Automated Debugging 
[[arxiv](https://arxiv.org/abs/2502.01619)] [[cool](https://papers.cool/arxiv/2502.01619)] [[pdf](https://arxiv.org/pdf/2502.01619)]
> **Authors**: Archiki Prasad,Elias Stengel-Eskin,Justin Chih-Yao Chen,Zaid Khan,Mohit Bansal
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: First two authors contributed equally. Dataset and Code: https://github.com/archiki/UTGenDebug
- **标题**: None
- **领域**: 软件工程,人工智能,计算语言学,机器学习
- **Abstract**: Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language models (LLMs), motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and backtracks edits based on multiple generated UTs to avoid overfitting, and helps LLMs debug effectively. We show that UTGen outperforms other LLM-based baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5 32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17% and 12.35% (respectively) over other LLM-based UT generation baselines. Lastly, we demonstrate that UTGen is a better judge for code correctness, outperforming a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10 sampling using Qwen2.5 7B.

### Next Steps in LLM-Supported Java Verification 
[[arxiv](https://arxiv.org/abs/2502.01573)] [[cool](https://papers.cool/arxiv/2502.01573)] [[pdf](https://arxiv.org/pdf/2502.01573)]
> **Authors**: Samuel Teuber,Bernhard Beckert
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted to NSE 2025, 1st International Workshop on Neuro-Symbolic Software Engineering (ICSE Workshop), 6 pages, 3 figures
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习,计算机科学中的逻辑
- **Abstract**: Recent work has shown that Large Language Models (LLMs) are not only a suitable tool for code generation but also capable of generating annotation-based code specifications. Scaling these methodologies may allow us to deduce provable correctness guarantees for large-scale software systems. In comparison to other LLM tasks, the application field of deductive verification has the notable advantage of providing a rigorous toolset to check LLM-generated solutions. This short paper provides early results on how this rigorous toolset can be used to reliably elicit correct specification annotations from an unreliable LLM oracle.

### Prioritizing App Reviews for Developer Responses on Google Play 
[[arxiv](https://arxiv.org/abs/2502.01520)] [[cool](https://papers.cool/arxiv/2502.01520)] [[pdf](https://arxiv.org/pdf/2502.01520)]
> **Authors**: Mohsen Jafari,Forough Majidi,Abbas Heydarnoori
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 1 figure, 5 tables
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: The number of applications in Google Play has increased dramatically in recent years. On Google Play, users can write detailed reviews and rate apps, with these ratings significantly influencing app success and download numbers. Reviews often include notable information like feature requests, which are valuable for software maintenance. Users can update their reviews and ratings anytime. Studies indicate that apps with ratings below three stars are typically avoided by potential users. Since 2013, Google Play has allowed developers to respond to user reviews, helping resolve issues and potentially boosting overall ratings and download rates. However, responding to reviews is time-consuming, and only 13% to 18% of developers engage in this practice. To address this challenge, we propose a method to prioritize reviews based on response priority. We collected and preprocessed review data, extracted both textual and semantic features, and assessed their impact on the importance of responses. We labelled reviews as requiring a response or not and trained four different machine learning models to prioritize them. We evaluated the models performance using metrics such as F1-Score, Accuracy, Precision, and Recall. Our findings indicate that the XGBoost model is the most effective for prioritizing reviews needing a response.

### Analysis of Student-LLM Interaction in a Software Engineering Project 
[[arxiv](https://arxiv.org/abs/2502.01273)] [[cool](https://papers.cool/arxiv/2502.01273)] [[pdf](https://arxiv.org/pdf/2502.01273)]
> **Authors**: Agrawal Naman,Ridwan Shariffdeen,Guanlin Wang,Sanka Rasnayaka,Ganesh Neelakanta Iyer
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 8 pages
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs) are becoming increasingly competent across various domains, educators are showing a growing interest in integrating these LLMs into the learning process. Especially in software engineering, LLMs have demonstrated qualitatively better capabilities in code summarization, code generation, and debugging. Despite various research on LLMs for software engineering tasks in practice, limited research captures the benefits of LLMs for pedagogical advancements and their impact on the student learning process. To this extent, we analyze 126 undergraduate students' interaction with an AI assistant during a 13-week semester to understand the benefits of AI for software engineering learning. We analyze the conversations, code generated, code utilized, and the human intervention levels to integrate the code into the code base. Our findings suggest that students prefer ChatGPT over CoPilot. Our analysis also finds that ChatGPT generates responses with lower computational complexity compared to CoPilot. Furthermore, conversational-based interaction helps improve the quality of the code generated compared to auto-generated code. Early adoption of LLMs in software engineering is crucial to remain competitive in the rapidly developing landscape. Hence, the next generation of software engineers must acquire the necessary skills to interact with AI to improve productivity.

### ML-Dev-Bench: Comparative Analysis of AI Agents on ML development workflows 
[[arxiv](https://arxiv.org/abs/2502.00964)] [[cool](https://papers.cool/arxiv/2502.00964)] [[pdf](https://arxiv.org/pdf/2502.00964)]
> **Authors**: Harshith Padigela,Chintan Shah,Dinkar Juyal
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: In this report, we present ML-Dev-Bench, a benchmark aimed at testing agentic capabilities on applied Machine Learning development tasks. While existing benchmarks focus on isolated coding tasks or Kaggle-style competitions, ML-Dev-Bench tests agents' ability to handle the full complexity of ML development workflows. The benchmark assesses performance across critical aspects including dataset handling, model training, improving existing models, debugging, and API integration with popular ML tools. We evaluate three agents - ReAct, Openhands, and AIDE - on a diverse set of 30 tasks, providing insights into their strengths and limitations in handling practical ML development challenges. We open source the benchmark for the benefit of the community at \href{https://github.com/ml-dev-bench/ml-dev-bench}{https://github.com/ml-dev-bench/ml-dev-bench}.

### Position: More Rigorous Software Engineering Would Improve Reproducibility in Machine Learning Research 
[[arxiv](https://arxiv.org/abs/2502.00902)] [[cool](https://papers.cool/arxiv/2502.00902)] [[pdf](https://arxiv.org/pdf/2502.00902)]
> **Authors**: Moritz Wolter,Lokesh Veeramacheneni
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Source code available at https://github.com/BonnBytes/position_we_need_more_tests_in_ml
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Experimental verification and falsification of scholarly work are part of the scientific method's core. To improve the Machine Learning (ML)-communities' ability to verify results from prior work, we argue for more robust software engineering. We estimate the adoption of common engineering best practices by examining repository links from all recently accepted International Conference on Machine Learning (ICML), International Conference on Learning Representations (ICLR) and Neural Information Processing Systems (NeurIPS) papers as well as ICML papers over time. Based on the results, we recommend how we, as a community, can improve reproducibility in ML-research.

### Enhancing Code Consistency in AI Research with Large Language Models and Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.00611)] [[cool](https://papers.cool/arxiv/2502.00611)] [[pdf](https://arxiv.org/pdf/2502.00611)]
> **Authors**: Rajat Keshri,Arun George Zachariah,Michael Boone
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Ensuring that code accurately reflects the algorithms and methods described in research papers is critical for maintaining credibility and fostering trust in AI research. This paper presents a novel system designed to verify code implementations against the algorithms and methodologies outlined in corresponding research papers. Our system employs Retrieval-Augmented Generation to extract relevant details from both the research papers and code bases, followed by a structured comparison using Large Language Models. This approach improves the accuracy and comprehensiveness of code implementation verification while contributing to the transparency, explainability, and reproducibility of AI research. By automating the verification process, our system reduces manual effort, enhances research credibility, and ultimately advances the state of the art in code verification.

### CoDocBench: A Dataset for Code-Documentation Alignment in Software Maintenance 
[[arxiv](https://arxiv.org/abs/2502.00519)] [[cool](https://papers.cool/arxiv/2502.00519)] [[pdf](https://arxiv.org/pdf/2502.00519)]
> **Authors**: Kunal Pai,Premkumar Devanbu,Toufique Ahmed
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted at the 2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR) - Data and Tool Showcase Track
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: One of the central tasks in software maintenance is being able to understand and develop code changes. Thus, given a natural language description of the desired new operation of a function, an agent (human or AI) might be asked to generate the set of edits to that function to implement the desired new operation; likewise, given a set of edits to a function, an agent might be asked to generate a changed description, of that function's new workings. Thus, there is an incentive to train a neural model for change-related tasks. Motivated by this, we offer a new, "natural", large dataset of coupled changes to code and documentation mined from actual high-quality GitHub projects, where each sample represents a single commit where the code and the associated docstring were changed together. We present the methodology for gathering the dataset, and some sample, challenging (but realistic) tasks where our dataset provides opportunities for both learning and evaluation. We find that current models (specifically Llama-3.1 405B, Mixtral 8$\times$22B) do find these maintenance-related tasks challenging.

### How Do Model Export Formats Impact the Development of ML-Enabled Systems? A Case Study on Model Integration 
[[arxiv](https://arxiv.org/abs/2502.00429)] [[cool](https://papers.cool/arxiv/2502.00429)] [[pdf](https://arxiv.org/pdf/2502.00429)]
> **Authors**: Shreyas Kumar Parida,Ilias Gerostathopoulos,Justus Bogner
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted for publication at the International Conference onAIEngineering - Software Engineering forAI(CAIN'25, see https://conf.researchr.org/home/cain-2025)
- **标题**: None
- **领域**: 软件工程,机器学习
- **Abstract**: Machine learning (ML) models are often integrated into ML-enabled systems to provide software functionality that would otherwise be impossible. This integration requires the selection of an appropriate ML model export format, for which many options are available. These formats are crucial for ensuring a seamless integration, and choosing a suboptimal one can negatively impact system development. However, little evidence is available to guide practitioners during the export format selection. We therefore evaluated various model export formats regarding their impact on the development of ML-enabled systems from an integration perspective. Based on the results of a preliminary questionnaire survey (n=17), we designed an extensive embedded case study with two ML-enabled systems in three versions with different technologies. We then analyzed the effect of five popular export formats, namely ONNX, Pickle, TensorFlow's SavedModel, PyTorch's TorchScript, and Joblib. In total, we studied 30 units of analysis (2 systems x 3 tech stacks x 5 formats) and collected data via structured field notes. The holistic qualitative analysis of the results indicated that ONNX offered the most efficient integration and portability across most cases. SavedModel and TorchScript were very convenient to use in Python-based systems, but otherwise required workarounds (TorchScript more than SavedModel). SavedModel also allowed the easy incorporation of preprocessing logic into a single file, which made it scalable for complex deep learning use cases. Pickle and Joblib were the most challenging to integrate, even in Python-based systems. Regarding technical support, all model export formats had strong technical documentation and strong community support across platforms such as Stack Overflow and Reddit. Practitioners can use our findings to inform the selection of ML export formats suited to their context.

### OrcaLoca: An LLM Agent Framework for Software Issue Localization 
[[arxiv](https://arxiv.org/abs/2502.00350)] [[cool](https://papers.cool/arxiv/2502.00350)] [[pdf](https://arxiv.org/pdf/2502.00350)]
> **Authors**: Zhongming Yu,Hejia Zhang,Yujie Zhao,Hanxian Huang,Matrix Yao,Ke Ding,Jishen Zhao
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Simulating Rumor Spreading in Social Networks using LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.01450)] [[cool](https://papers.cool/arxiv/2502.01450)] [[pdf](https://arxiv.org/pdf/2502.01450)]
> **Authors**: Tianrui Hu,Dimitrios Liakopoulos,Xiwen Wei,Radu Marculescu,Neeraja J. Yadwadkar
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 7 pages, 8 figures
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.

### Israel-Hamas war through Telegram, Reddit and Twitter 
[[arxiv](https://arxiv.org/abs/2502.00060)] [[cool](https://papers.cool/arxiv/2502.00060)] [[pdf](https://arxiv.org/pdf/2502.00060)]
> **Authors**: Despoina Antonakaki,Sotiris Ioannidis
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能,机器学习
- **Abstract**: The Israeli-Palestinian conflict started on 7 October 2023, have resulted thus far to over 48,000 people killed including more than 17,000 children with a majority from Gaza, more than 30,000 people injured, over 10,000 missing, and over 1 million people displaced, fleeing conflict zones. The infrastructure damage includes the 87\% of housing units, 80\% of public buildings and 60\% of cropland 17 out of 36 hospitals, 68\% of road networks and 87\% of school buildings damaged. This conflict has as well launched an online discussion across various social media platforms. Telegram was no exception due to its encrypted communication and highly involved audience. The current study will cover an analysis of the related discussion in relation to different participants of the conflict and sentiment represented in those discussion. To this end, we prepared a dataset of 125K messages shared on channels in Telegram spanning from 23 October 2025 until today. Additionally, we apply the same analysis in two publicly available datasets from Twitter containing 2001 tweets and from Reddit containing 2M opinions. We apply a volume analysis across the three datasets, entity extraction and then proceed to BERT topic analysis in order to extract common themes or topics. Next, we apply sentiment analysis to analyze the emotional tone of the discussions. Our findings hint at polarized narratives as the hallmark of how political factions and outsiders mold public opinion. We also analyze the sentiment-topic prevalence relationship, detailing the trends that may show manipulation and attempts of propaganda by the involved parties. This will give a better understanding of the online discourse on the Israel-Palestine conflict and contribute to the knowledge on the dynamics of social media communication during geopolitical crises.

### Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring Polarization and Engagement in Simulated Social Networks 
[[arxiv](https://arxiv.org/abs/2502.00055)] [[cool](https://papers.cool/arxiv/2502.00055)] [[pdf](https://arxiv.org/pdf/2502.00055)]
> **Authors**: Ljubisa Bojic,Zorica Dodevska,Yashar Deldjoo,Nenad Pantelic
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-04
> **comment**: 8 pages, 2 figures
- **标题**: None
- **领域**: 社交和信息网络,人工智能,计算机与社会,人机交互,信息检索
- **Abstract**: Given the exponential advancement in AI technologies and the potential escalation of harmful effects from recommendation systems, it is crucial to simulate and evaluate these effects early on. Doing so can help prevent possible damage to both societies and technology companies. This paper introduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel simulation framework leveraging Large Language Models (LLMs) to explore the impacts of different content recommendation setups on user engagement and polarization in social networks. By creating diverse AI agents (AgentPrompts) with descriptive, static, and dynamic attributes, we assess their autonomous behaviour across three scenarios: Plurality, Balanced, and Similarity. Our findings reveal that the Similarity Scenario, which aligns content with user preferences, maximizes engagement while potentially fostering echo chambers. Conversely, the Plurality Scenario promotes diverse interactions but produces mixed engagement results. Our study emphasizes the need for a careful balance in recommender system designs to enhance user satisfaction while mitigating societal polarization. It underscores the unique value and challenges of incorporating LLMs into simulation environments. The benefits of RecSysLLMsP lie in its potential to calculate polarization effects, which is crucial for assessing societal impacts and determining user engagement levels with diverse recommender system setups. This advantage is essential for developing and maintaining a successful business model for social media companies. However, the study's limitations revolve around accurately emulating reality. Future efforts should validate the similarity in behaviour between real humans and AgentPrompts and establish metrics for measuring polarization scores.

### The Best Soules Basis for the Estimation of a Spectral Barycentre Network 
[[arxiv](https://arxiv.org/abs/2502.00038)] [[cool](https://papers.cool/arxiv/2502.00038)] [[pdf](https://arxiv.org/pdf/2502.00038)]
> **Authors**: François G. Meyer
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-04
> **comment**: 20 pages
- **标题**: None
- **领域**: 社交和信息网络,机器学习,数据分析、统计和概率,机器学习
- **Abstract**: The main contribution of this work is a fast algorithm to compute the barycentre of a set of networks based on a Laplacian spectral pseudo-distance. The core engine for the reconstruction of the barycentre is an algorithm that explores the large library of Soules bases, and returns a basis that yields a sparse approximation of the sample mean adjacency matrix. We prove that when the networks are random realizations of stochastic block models, then our algorithm reconstructs the population mean adjacency matrix. In addition to the theoretical analysis of the estimator of the barycentre network, we perform Monte Carlo simulations to validate the theoretical properties of the estimator. This work is significant because it opens the door to the design of new spectral-based network synthesis that have theoretical guarantees.

## 普通经济学(econ.GN:General Economics)

### A Comprehensive Review: Applicability of Deep Neural Networks in Business Decision Making and Market Prediction Investment 
[[arxiv](https://arxiv.org/abs/2502.00151)] [[cool](https://papers.cool/arxiv/2502.00151)] [[pdf](https://arxiv.org/pdf/2502.00151)]
> **Authors**: Viet Trinh
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 普通经济学,人工智能
- **Abstract**: Big data, both in its structured and unstructured formats, have brought in unforeseen challenges in economics and business. How to organize, classify, and then analyze such data to obtain meaningful insights are the ever-going research topics for business leaders and academic researchers. This paper studies recent applications of deep neural networks in decision making in economical business and investment; especially in risk management, portfolio optimization, and algorithmic trading. Set aside limitation in data privacy and cross-market analysis, the article establishes that deep neural networks have performed remarkably in financial classification and prediction. Moreover, the study suggests that by compositing multiple neural networks, spanning different data type modalities, a more robust, efficient, and scalable financial prediction framework can be constructed.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Privacy-Preserving Edge Speech Understanding with Tiny Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.01649)] [[cool](https://papers.cool/arxiv/2502.01649)] [[pdf](https://arxiv.org/pdf/2502.01649)]
> **Authors**: Afsara Benazir,Felix Xiaozhu Lin
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,机器学习,声音
- **Abstract**: Robust speech recognition systems rely on cloud service providers for inference. It needs to ensure that an untrustworthy provider cannot deduce the sensitive content in speech. Sanitization can be done on speech content keeping in mind that it has to avoid compromising transcription accuracy. Realizing the under utilized capabilities of tiny speech foundation models (FMs), for the first time, we propose a novel use: enhancing speech privacy on resource-constrained devices. We introduce XYZ, an edge/cloud privacy preserving speech inference engine that can filter sensitive entities without compromising transcript accuracy. We utilize a timestamp based on-device masking approach that utilizes a token to entity prediction model to filter sensitive entities. Our choice of mask strategically conceals parts of the input and hides sensitive data. The masked input is sent to a trusted cloud service or to a local hub to generate the masked output. The effectiveness of XYZ hinges on how well the entity time segments are masked. Our recovery is a confidence score based approach that chooses the best prediction between cloud and on-device model. We implement XYZ on a 64 bit Raspberry Pi 4B. Experiments show that our solution leads to robust speech recognition without forsaking privacy. XYZ with < 100 MB memory, achieves state-of-the-art (SOTA) speech transcription performance while filtering about 83% of private entities directly on-device. XYZ is 16x smaller in memory and 17x more compute efficient than prior privacy preserving speech frameworks and has a relative reduction in word error rate (WER) by 38.8-77.5% when compared to existing offline transcription services.

### mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition 
[[arxiv](https://arxiv.org/abs/2502.01547)] [[cool](https://papers.cool/arxiv/2502.01547)] [[pdf](https://arxiv.org/pdf/2502.01547)]
> **Authors**: Andrew Rouditchenko,Samuel Thomas,Hilde Kuehne,Rogerio Feris,James Glass
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,计算机视觉和模式识别,声音
- **Abstract**: Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions.

## 图像和视频处理(eess.IV:Image and Video Processing)

### ReMiDi: Reconstruction of Microstructure Using a Differentiable Diffusion MRI Simulator 
[[arxiv](https://arxiv.org/abs/2502.01988)] [[cool](https://papers.cool/arxiv/2502.01988)] [[pdf](https://arxiv.org/pdf/2502.01988)]
> **Authors**: Prathamesh Pradeep Khole,Zahra Kais Petiwala,Shri Prathaa Magesh,Ehsan Mirafzali,Utkarsh Gupta,Jing-Rebecca Li,Andrada Ianus,Razvan Marinescu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 15 pages, ~13 figures, 2 algorithms, 1 table
- **标题**: None
- **领域**: 图像和视频处理,图形,机器学习,医学物理
- **Abstract**: We propose ReMiDi, a novel method for inferring neuronal microstructure as arbitrary 3D meshes using a differentiable diffusion Magnetic Resonance Imaging (dMRI) simulator. We first implemented in PyTorch a differentiable dMRI simulator that simulates the forward diffusion process using a finite-element method on an input 3D microstructure mesh. To achieve significantly faster simulations, we solve the differential equation semi-analytically using a matrix formalism approach. Given a reference dMRI signal $S_{ref}$, we use the differentiable simulator to iteratively update the input mesh such that it matches $S_{ref}$ using gradient-based learning. Since directly optimizing the 3D coordinates of the vertices is challenging, particularly due to ill-posedness of the inverse problem, we instead optimize a lower-dimensional latent space representation of the mesh. The mesh is first encoded into spectral coefficients, which are further encoded into a latent $\textbf{z}$ using an auto-encoder, and are then decoded back into the true mesh. We present an end-to-end differentiable pipeline that simulates signals that can be tuned to match a reference signal by iteratively updating the latent representation $\textbf{z}$. We demonstrate the ability to reconstruct microstructures of arbitrary shapes represented by finite-element meshes, with a focus on axonal geometries found in the brain white matter, including bending, fanning and beading fibers. Our source code will be made available online.

### Layer Separation: Adjustable Joint Space Width Images Synthesis in Conventional Radiography 
[[arxiv](https://arxiv.org/abs/2502.01972)] [[cool](https://papers.cool/arxiv/2502.01972)] [[pdf](https://arxiv.org/pdf/2502.01972)]
> **Authors**: Haolin Wang,Yafei Ou,Prasoon Ambalathankandy,Gen Ota,Pengyu Dai,Masayuki Ikebe,Kenji Suzuki,Tamotsu Kamishima
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: :I.3.3; J.3; I.4.0
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Rheumatoid arthritis (RA) is a chronic autoimmune disease characterized by joint inflammation and progressive structural damage. Joint space width (JSW) is a critical indicator in conventional radiography for evaluating disease progression, which has become a prominent research topic in computer-aided diagnostic (CAD) systems. However, deep learning-based radiological CAD systems for JSW analysis face significant challenges in data quality, including data imbalance, limited variety, and annotation difficulties. This work introduced a challenging image synthesis scenario and proposed Layer Separation Networks (LSN) to accurately separate the soft tissue layer, the upper bone layer, and the lower bone layer in conventional radiographs of finger joints. Using these layers, the adjustable JSW images can be synthesized to address data quality challenges and achieve ground truth (GT) generation. Experimental results demonstrated that LSN-based synthetic images closely resemble real radiographs, and significantly enhanced the performance in downstream tasks. The code and dataset will be available.

### Efficient Brain Tumor Classification with Lightweight CNN Architecture: A Novel Approach 
[[arxiv](https://arxiv.org/abs/2502.01674)] [[cool](https://papers.cool/arxiv/2502.01674)] [[pdf](https://arxiv.org/pdf/2502.01674)]
> **Authors**: Priyam Ganguly,Akhilbaran Ghosh
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: Accepted in FMLDS 2024
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Brain tumor classification using MRI images is critical in medical diagnostics, where early and accurate detection significantly impacts patient outcomes. While recent advancements in deep learning (DL), particularly CNNs, have shown promise, many models struggle with balancing accuracy and computational efficiency and often lack robustness across diverse datasets. To address these challenges, we propose a novel model architecture integrating separable convolutions and squeeze and excitation (SE) blocks, designed to enhance feature extraction while maintaining computational efficiency. Our model further incorporates batch normalization and dropout to prevent overfitting, ensuring stable and reliable performance. The proposed model is lightweight because it uses separable convolutions, which reduce the number of parameters, and incorporates global average pooling instead of fully connected layers to minimize computational complexity while maintaining high accuracy. Our model does better than other models by about 0.5% to 1.0% in accuracy and 1.5% to 2.5% in loss reduction, as shown by many experiments. It has a validation accuracy of 99.22% and a test accuracy of 98.44%. These results highlight the model's ability to generalize effectively across different brain tumour types, offering a robust tools for clinical applications. Our work sets a new benchmark in the field, providing a foundation for future research in optimizing the accuracy and efficiency of DL models for medical image analysis.

### Assessing the use of Diffusion models for motion artifact correction in brain MRI 
[[arxiv](https://arxiv.org/abs/2502.01418)] [[cool](https://papers.cool/arxiv/2502.01418)] [[pdf](https://arxiv.org/pdf/2502.01418)]
> **Authors**: Paolo Angella,Vito Paolo Pastore,Matteo Santacesaria
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Accepted at IEEE International Symposium for Biomedical Imaging (ISBI) 2025
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习,数值分析
- **Abstract**: Magnetic Resonance Imaging generally requires long exposure times, while being sensitive to patient motion, resulting in artifacts in the acquired images, which may hinder their diagnostic relevance. Despite research efforts to decrease the acquisition time, and designing efficient acquisition sequences, motion artifacts are still a persistent problem, pushing toward the need for the development of automatic motion artifact correction techniques. Recently, diffusion models have been proposed as a solution for the task at hand. While diffusion models can produce high-quality reconstructions, they are also susceptible to hallucination, which poses risks in diagnostic applications. In this study, we critically evaluate the use of diffusion models for correcting motion artifacts in 2D brain MRI scans. Using a popular benchmark dataset, we compare a diffusion model-based approach with state-of-the-art methods consisting of Unets trained in a supervised fashion on motion-affected images to reconstruct ground truth motion-free images. Our findings reveal mixed results: diffusion models can produce accurate predictions or generate harmful hallucinations in this context, depending on data heterogeneity and the acquisition planes considered as input.

### Towards Robust and Generalizable Lensless Imaging with Modular Learned Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.01102)] [[cool](https://papers.cool/arxiv/2502.01102)] [[pdf](https://arxiv.org/pdf/2502.01102)]
> **Authors**: Eric Bezzam,Yohann Perron,Martin Vetterli
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 16 pages
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Lensless cameras disregard the conventional design that imaging should mimic the human eye. This is done by replacing the lens with a thin mask, and moving image formation to the digital post-processing. State-of-the-art lensless imaging techniques use learned approaches that combine physical modeling and neural networks. However, these approaches make simplifying modeling assumptions for ease of calibration and computation. Moreover, the generalizability of learned approaches to lensless measurements of new masks has not been studied. To this end, we utilize a modular learned reconstruction in which a key component is a pre-processor prior to image recovery. We theoretically demonstrate the pre-processor's necessity for standard image recovery techniques (Wiener filtering and iterative algorithms), and through extensive experiments show its effectiveness for multiple lensless imaging approaches and across datasets of different mask types (amplitude and phase). We also perform the first generalization benchmark across mask types to evaluate how well reconstructions trained with one system generalize to others. Our modular reconstruction enables us to use pre-trained components and transfer learning on new systems to cut down weeks of tedious measurements and training. As part of our work, we open-source four datasets, and software for measuring datasets and for training our modular reconstruction.

### FetDTIAlign: A Deep Learning Framework for Affine and Deformable Registration of Fetal Brain dMRI 
[[arxiv](https://arxiv.org/abs/2502.01057)] [[cool](https://papers.cool/arxiv/2502.01057)] [[pdf](https://arxiv.org/pdf/2502.01057)]
> **Authors**: Bo Li,Qi Zeng,Simon K. Warfield,Davood Karimi
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Under review
- **标题**: None
- **领域**: 图像和视频处理,人工智能
- **Abstract**: Diffusion MRI (dMRI) provides unique insights into fetal brain microstructure in utero. Longitudinal and cross-sectional fetal dMRI studies can reveal crucial neurodevelopmental changes but require precise spatial alignment across scans and subjects. This is challenging due to low data quality, rapid brain development, and limited anatomical landmarks. Existing registration methods, designed for high-quality adult data, struggle with these complexities. To address this, we introduce FetDTIAlign, a deep learning approach for fetal brain dMRI registration, enabling accurate affine and deformable alignment. FetDTIAlign features a dual-encoder architecture and iterative feature-based inference, reducing the impact of noise and low resolution. It optimizes network configurations and domain-specific features at each registration stage, enhancing both robustness and accuracy. We validated FetDTIAlign on data from 23 to 36 weeks gestation, covering 60 white matter tracts. It consistently outperformed two classical optimization-based methods and a deep learning pipeline, achieving superior anatomical correspondence. Further validation on external data from the Developing Human Connectome Project confirmed its generalizability across acquisition protocols. Our results demonstrate the feasibility of deep learning for fetal brain dMRI registration, providing a more accurate and reliable alternative to classical techniques. By enabling precise cross-subject and tract-specific analyses, FetDTIAlign supports new discoveries in early brain development.

### A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation 
[[arxiv](https://arxiv.org/abs/2502.00314)] [[cool](https://papers.cool/arxiv/2502.00314)] [[pdf](https://arxiv.org/pdf/2502.00314)]
> **Authors**: Moein Heidari,Ehsan Khodapanah Aghdam,Alexander Manzella,Daniel Hsu,Rebecca Scalabrino,Wenjin Chen,David J. Foran,Ilker Hacihaliloglu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: Accepted for presentation at the 2025 SPIE Medical Imaging Conference
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.

### Patch Triplet Similarity Purification for Guided Real-World Low-Dose CT Image Denoising 
[[arxiv](https://arxiv.org/abs/2502.00253)] [[cool](https://papers.cool/arxiv/2502.00253)] [[pdf](https://arxiv.org/pdf/2502.00253)]
> **Authors**: Junhao Long,Fengwei Yang,Juncheng Yan,Baoping Zhang,Chao Jin,Jian Yang,Changliang Zou,Jun Xu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Image denoising of low-dose computed tomography (LDCT) is an important problem for clinical diagnosis with reduced radiation exposure. Previous methods are mostly trained with pairs of synthetic or misaligned LDCT and normal-dose CT (NDCT) images. However, trained with synthetic noise or misaligned LDCT/NDCT image pairs, the denoising networks would suffer from blurry structure or motion artifacts. Since non-contrast CT (NCCT) images share the content characteristics to the corresponding NDCT images in a three-phase scan, they can potentially provide useful information for real-world LDCT image denoising. To exploit this aspect, in this paper, we propose to incorporate clean NCCT images as useful guidance for the learning of real-world LDCT image denoising networks. To alleviate the issue of spatial misalignment in training data, we design a new Patch Triplet Similarity Purification (PTSP) strategy to select highly similar patch (instead of image) triplets of LDCT, NDCT, and NCCT images for network training. Furthermore, we modify two image denoising transformers of SwinIR and HAT to accommodate the NCCT image guidance, by replacing vanilla self-attention with cross-attention. On our collected clinical dataset, the modified transformers trained with the data selected by our PTSP strategy show better performance than 15 comparison methods on real-world LDCT image denoising. Ablation studies validate the effectiveness of our NCCT image guidance and PTSP strategy. We will publicly release our data and code.

### Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study 
[[arxiv](https://arxiv.org/abs/2502.00146)] [[cool](https://papers.cool/arxiv/2502.00146)] [[pdf](https://arxiv.org/pdf/2502.00146)]
> **Authors**: Hassan Jahanandish,Shengtian Sang,Cynthia Xinran Li,Sulaiman Vesal,Indrani Bhattacharya,Jeong Hoon Lee,Richard Fan,Geoffrey A. Sonna,Mirabela Rusu
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target suspicious prostate lesions. This has led to artificial intelligence (AI) applications improving MRI-based detection of clinically significant prostate cancer (CsPCa). However, MRI-detected lesions must still be mapped to transrectal ultrasound (TRUS) images during biopsy, which results in missing CsPCa. This study systematically evaluates a multimodal AI framework integrating MRI and TRUS image sequences to enhance CsPCa identification. The study included 3110 patients from three cohorts across two institutions who underwent prostate biopsy. The proposed framework, based on the 3D UNet architecture, was evaluated on 1700 test cases, comparing performance to unimodal AI models that use either MRI or TRUS alone. Additionally, the proposed model was compared to radiologists in a cohort of 110 patients. The multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice (42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared to radiologists, the multimodal model showed higher specificity (88% vs. 78%) and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings demonstrate the potential of multimodal AI to improve CsPCa lesion targeting during biopsy and treatment planning, surpassing current unimodal models and radiologists; ultimately improving outcomes for prostate cancer patients.

### Advanced Assessment of Stroke in Retinal Fundus Imaging with Deep Multi-view Learning 
[[arxiv](https://arxiv.org/abs/2502.00079)] [[cool](https://papers.cool/arxiv/2502.00079)] [[pdf](https://arxiv.org/pdf/2502.00079)]
> **Authors**: Aysen Degerli,Mika Hilvo,Juha Pajula,Petri Huhtinen,Pekka Jäkälä
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Stroke is globally a major cause of mortality and morbidity, and hence accurate and rapid diagnosis of stroke is valuable. Retinal fundus imaging reveals the known markers of elevated stroke risk in the eyes, which are retinal venular widening, arteriolar narrowing, and increased tortuosity. In contrast to other imaging techniques used for stroke diagnosis, the acquisition of fundus images is easy, non-invasive, fast, and inexpensive. Therefore, in this study, we propose a multi-view stroke network (MVS-Net) to detect stroke and transient ischemic attack (TIA) using retinal fundus images. Contrary to existing studies, our study proposes for the first time a solution to discriminate stroke and TIA with deep multi-view learning by proposing an end-to-end deep network, consisting of multi-view inputs of fundus images captured from both right and left eyes. Accordingly, the proposed MVS-Net defines representative features from fundus images of both eyes and determines the relation within their macula-centered and optic nerve head-centered views. Experiments performed on a dataset collected from stroke and TIA patients, in addition to healthy controls, show that the proposed framework achieves an AUC score of 0.84 for stroke and TIA detection.

## 信号处理(eess.SP:Signal Processing)

### DRL-based Dolph-Tschebyscheff Beamforming in Downlink Transmission for Mobile Users 
[[arxiv](https://arxiv.org/abs/2502.01278)] [[cool](https://papers.cool/arxiv/2502.01278)] [[pdf](https://arxiv.org/pdf/2502.01278)]
> **Authors**: Nancy Nayak,Kin K. Leung,Lajos Hanzo
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: With the emergence of AI technologies in next-generation communication systems, machine learning plays a pivotal role due to its ability to address high-dimensional, non-stationary optimization problems within dynamic environments while maintaining computational efficiency. One such application is directional beamforming, achieved through learning-based blind beamforming techniques that utilize already existing radio frequency (RF) fingerprints of the user equipment obtained from the base stations and eliminate the need for additional hardware or channel and angle estimations. However, as the number of users and antenna dimensions increase, thereby expanding the problem's complexity, the learning process becomes increasingly challenging, and the performance of the learning-based method cannot match that of the optimal solution. In such a scenario, we propose a deep reinforcement learning-based blind beamforming technique using a learnable Dolph-Tschebyscheff antenna array that can change its beam pattern to accommodate mobile users. Our simulation results show that the proposed method can support data rates very close to the best possible values.

### Learned Bayesian Cramér-Rao Bound for Unknown Measurement Models Using Score Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.00724)] [[cool](https://papers.cool/arxiv/2502.00724)] [[pdf](https://arxiv.org/pdf/2502.00724)]
> **Authors**: Hai Victor Habi,Hagit Messer,Yoram Bresler
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 28 pages, 11 figures
- **标题**: None
- **领域**: 信号处理,人工智能,机器学习,机器学习
- **Abstract**: The Bayesian Cramér-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cramér-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise.

### Deep learning model for ECG reconstruction reveals the information content of ECG leads 
[[arxiv](https://arxiv.org/abs/2502.00559)] [[cool](https://papers.cool/arxiv/2502.00559)] [[pdf](https://arxiv.org/pdf/2502.00559)]
> **Authors**: Tomasz Gradowski,Teodor Buchner
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 8 figures
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: This study introduces a deep learning model based on the U-net architecture to reconstruct missing leads in electrocardiograms (ECGs). Using publicly available datasets, the model was trained to regenerate 12-lead ECG data from reduced lead configurations, demonstrating high accuracy in lead reconstruction. The results highlight the ability of the model to quantify the information content of each ECG lead and their inter-lead correlations. This has significant implications for optimizing lead selection in diagnostic scenarios, particularly in settings where full 12-lead ECGs are impractical. Additionally, the study provides insights into the physiological underpinnings of ECG signals and their propagation. The findings pave the way for advancements in telemedicine, portable ECG devices, and personalized cardiac diagnostics by reducing redundancy and enhancing signal interpretation.

## 系统与控制(eess.SY:Systems and Control)

### Model-Free Predictive Control: Introductory Algebraic Calculations, and a Comparison with HEOL and ANNs 
[[arxiv](https://arxiv.org/abs/2502.00443)] [[cool](https://papers.cool/arxiv/2502.00443)] [[pdf](https://arxiv.org/pdf/2502.00443)]
> **Authors**: Cédric Join,Emmanuel Delaleau,Michel Fliess
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: :49J99ACM Class:I.2.8
- **标题**: None
- **领域**: 系统与控制,人工智能
- **Abstract**: Model predictive control (MPC) is a popular control engineering practice, but requires a sound knowledge of the model. Model-free predictive control (MFPC), a burning issue today, also related to reinforcement learning (RL) in AI, is reformulated here via a linear differential equation with constant coefficients, thanks to a new perspective on optimal control combined with recent advances in the field of model-free control. It is replacing Dynamic Programming, the Hamilton-Jacobi-Bellman equation, and Pontryagin's Maximum Principle. The computing burden is low. The implementation is straightforward. Two nonlinear examples, a chemical reactor and a two tank system, are illustrating our approach. A comparison with the HEOL setting, where some expertise of the process model is needed, shows only a slight superiority of the later. A recent identification of the two tank system via a complex ANN architecture might indicate that a full modeling and the corresponding machine learning mechanism are not always necessary neither in control, nor, more generally, in AI.

### Differentiable Projection-based Learn to Optimize in Wireless Network-Part I: Convex Constrained (Non-)Convex Programming 
[[arxiv](https://arxiv.org/abs/2502.00053)] [[cool](https://papers.cool/arxiv/2502.00053)] [[pdf](https://arxiv.org/pdf/2502.00053)]
> **Authors**: Xiucheng Wang,Xuan Zhao,Nan Cheng
> **First submission**: 2025-01-29
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: This paper addresses a class of (non-)convex optimization problems subject to general convex constraints, which pose significant challenges for traditional methods due to their inherent non-convexity and diversity. Conventional convex optimization-based solvers often struggle to efficiently handle these problems in their most general form. While neural network (NN)-based approaches offer a promising alternative, ensuring the feasibility of NN-generated solutions and effectively training the NN remain key hurdles, largely because finite-capacity networks can produce infeasible outputs. To overcome these issues, we propose a projection-based method that projects any infeasible NN output onto the feasible domain, thus guaranteeing strict adherence to the constraints without compromising the NN's optimization capability. Furthermore, we derive the objective function values for both the raw NN outputs and their projected counterparts, along with the gradients of these values with respect to the NN parameters. This derivation enables label-free (unsupervised) training, reducing reliance on labeled data and improving scalability. Experimental results demonstrate that the proposed projection-based method consistently ensures feasibility.

## 优化与控制(math.OC:Optimization and Control)

### The Ball-Proximal (="Broximal") Point Method: a New Algorithm, Convergence Theory, and Applications 
[[arxiv](https://arxiv.org/abs/2502.02002)] [[cool](https://papers.cool/arxiv/2502.02002)] [[pdf](https://arxiv.org/pdf/2502.02002)]
> **Authors**: Kaja Gruntkowska,Hanmin Li,Aadi Rane,Peter Richtárik
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 44 pages, 3 figures
- **标题**: None
- **领域**: 优化与控制,机器学习,机器学习
- **Abstract**: Non-smooth and non-convex global optimization poses significant challenges across various applications, where standard gradient-based methods often struggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or Ball Point Method (BPM) for short - a novel algorithmic framework inspired by the classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we show, sheds new light on several foundational optimization paradigms and phenomena, including non-convex and non-smooth optimization, acceleration, smoothing, adaptive stepsize selection, and trust-region methods. At the core of BPM lies the ball-proximal ("broximal") operator, which arises from the classical proximal operator by replacing the quadratic distance penalty by a ball constraint. Surprisingly, and in sharp contrast with the sublinear rate of PPM in the nonsmooth convex regime, we prove that BPM converges linearly and in a finite number of steps in the same regime. Furthermore, by introducing the concept of ball-convexity, we prove that BPM retains the same global convergence guarantees under weaker assumptions, making it a powerful tool for a broader class of potentially non-convex optimization problems. Just like PPM plays the role of a conceptual method inspiring the development of practically efficient algorithms and algorithmic elements, e.g., gradient descent, adaptive step sizes, acceleration (Ahn & Sra, 2020), and "W" in AdamW (Zhuang et al., 2022), we believe that BPM should be understood in the same manner: as a blueprint and inspiration for further development.

### High-Dimensional Bayesian Optimization Using Both Random and Supervised Embeddings 
[[arxiv](https://arxiv.org/abs/2502.00854)] [[cool](https://papers.cool/arxiv/2502.00854)] [[pdf](https://arxiv.org/pdf/2502.00854)]
> **Authors**: Rémy Priem,Youssef Diouane,Nathalie Bartoli,Sylvain Dubreuil,Paul Saves
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: ef:AIAA Journal 2025 63:1, 162-173
- **标题**: None
- **领域**: 优化与控制,机器学习,机器学习
- **Abstract**: Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation.

### Mirror Descent Under Generalized Smoothness 
[[arxiv](https://arxiv.org/abs/2502.00753)] [[cool](https://papers.cool/arxiv/2502.00753)] [[pdf](https://arxiv.org/pdf/2502.00753)]
> **Authors**: Dingzhi Yu,Wei Jiang,Yuanyu Wan,Lijun Zhang
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 59 pages, 2 figures
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: Smoothness is crucial for attaining fast rates in first-order optimization. However, many optimization problems in modern machine learning involve non-smooth objectives. Recent studies relax the smoothness assumption by allowing the Lipschitz constant of the gradient to grow with respect to the gradient norm, which accommodates a broad range of objectives in practice. Despite this progress, existing generalizations of smoothness are restricted to Euclidean geometry with $\ell_2$-norm and only have theoretical guarantees for optimization in the Euclidean space. In this paper, we address this limitation by introducing a new $\ell*$-smoothness concept that measures the norm of Hessian in terms of a general norm and its dual, and establish convergence for mirror-descent-type algorithms, matching the rates under the classic smoothness. Notably, we propose a generalized self-bounding property that facilitates bounding the gradients via controlling suboptimality gaps, serving as a principal component for convergence analysis. Beyond deterministic optimization, we establish an anytime convergence for stochastic mirror descent based on a new bounded noise condition that encompasses the widely adopted bounded or affine noise assumptions.

### Uniform-in-time weak propagation of chaos for consensus-based optimization 
[[arxiv](https://arxiv.org/abs/2502.00582)] [[cool](https://papers.cool/arxiv/2502.00582)] [[pdf](https://arxiv.org/pdf/2502.00582)]
> **Authors**: Erhan Bayraktar,Ibrahim Ekren,Hongyi Zhou
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: keywords: Consensus-based optimization, Uniform-in-time propagation of chaos, Weak convergence, Sobolev spaces, Linearized Fokker-Planck equations
- **标题**: None
- **领域**: 优化与控制,机器学习,可能性
- **Abstract**: We study the uniform-in-time weak propagation of chaos for the consensus-based optimization (CBO) method on a bounded searching domain. We apply the methodology for studying long-time behaviors of interacting particle systems developed in the work of Delarue and Tse (ArXiv:2104.14973). Our work shows that the weak error has order $O(N^{-1})$ uniformly in time, where $N$ denotes the number of particles. The main strategy behind the proofs are the decomposition of the weak errors using the linearized Fokker-Planck equations and the exponential decay of their Sobolev norms. Consequently, our result leads to the joint convergence of the empirical distribution of the CBO particle system to the Dirac-delta distribution at the global minimizer in population size and running time in Wasserstein-type metrics.

### Distributed Primal-Dual Algorithms: Unification, Connections, and Insights 
[[arxiv](https://arxiv.org/abs/2502.00470)] [[cool](https://papers.cool/arxiv/2502.00470)] [[pdf](https://arxiv.org/pdf/2502.00470)]
> **Authors**: Runxiong Wu,Dong Liu,Xueqin Wang,Andi Wang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 15 pages, 4 figures, 1 table
- **标题**: None
- **领域**: 优化与控制,机器学习,机器学习
- **Abstract**: We study primal-dual algorithms for general empirical risk minimization problems in distributed settings, focusing on two prominent classes of algorithms. The first class is the communication-efficient distributed dual coordinate ascent (CoCoA), derived from the coordinate ascent method for solving the dual problem. The second class is the alternating direction method of multipliers (ADMM), including consensus ADMM, linearized ADMM, and proximal ADMM. We demonstrate that both classes of algorithms can be transformed into a unified update form that involves only primal and dual variables. This discovery reveals key connections between the two classes of algorithms: CoCoA can be interpreted as a special case of proximal ADMM for solving the dual problem, while consensus ADMM is closely related to a proximal ADMM algorithm. This discovery provides the insight that by adjusting the augmented Lagrangian parameter, we can easily enable the ADMM variants to outperform the CoCoA variants. We further explore linearized versions of ADMM and analyze the effects of tuning parameters on these ADMM variants in the distributed setting. Our theoretical findings are supported by extensive simulation studies and real-world data analysis.

### Provably-Stable Neural Network-Based Control of Nonlinear Systems 
[[arxiv](https://arxiv.org/abs/2502.00248)] [[cool](https://papers.cool/arxiv/2502.00248)] [[pdf](https://arxiv.org/pdf/2502.00248)]
> **Authors**: Anran Li,John P. Swensen,Mehdi Hosseinzadeh
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: ef:Engineering Applications of Artificial Intelligence, volume 138, pages 109252, year 2024
- **标题**: None
- **领域**: 优化与控制,机器学习,系统与控制
- **Abstract**: In recent years, Neural Networks (NNs) have been employed to control nonlinear systems due to their potential capability in dealing with situations that might be difficult for conventional nonlinear control schemes. However, to the best of our knowledge, the current literature on NN-based control lacks theoretical guarantees for stability and tracking performance. This precludes the application of NN-based control schemes to systems where stringent stability and performance guarantees are required. To address this gap, this paper proposes a systematic and comprehensive methodology to design provably-stable NN-based control schemes for affine nonlinear systems. Rigorous analysis is provided to show that the proposed approach guarantees stability of the closed-loop system with the NN in the loop. Also, it is shown that the resulting NN-based control scheme ensures that system states asymptotically converge to a neighborhood around the desired equilibrium point, with a tunable proximity threshold. The proposed methodology is validated and evaluated via simulation studies on an inverted pendulum and experimental studies on a Parrot Bebop 2 drone.

## 统计理论(math.ST:Statistics Theory)

### Minimax Optimality of Classical Scaling Under General Noise Conditions 
[[arxiv](https://arxiv.org/abs/2502.00947)] [[cool](https://papers.cool/arxiv/2502.00947)] [[pdf](https://arxiv.org/pdf/2502.00947)]
> **Authors**: Siddharth Vishwanath,Ery Arias-Castro
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 45 pages, 4 figures
- **标题**: None
- **领域**: 统计理论,机器学习,机器学习
- **Abstract**: We establish the consistency of classical scaling under a broad class of noise models, encompassing many commonly studied cases in literature. Our approach requires only finite fourth moments of the noise, significantly weakening standard assumptions. We derive convergence rates for classical scaling and establish matching minimax lower bounds, demonstrating that classical scaling achieves minimax optimality in recovering the true configuration even when the input dissimilarities are corrupted by noise.

## 地球物理学(physics.geo-ph:Geophysics)

### Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of Model Prediction and Scientific Understanding of Soil Organic Carbon 
[[arxiv](https://arxiv.org/abs/2502.00672)] [[cool](https://papers.cool/arxiv/2502.00672)] [[pdf](https://arxiv.org/pdf/2502.00672)]
> **Authors**: Haodi Xu,Joshua Fan,Feng Tao,Lifen Jiang,Fengqi You,Benjamin Z. Houlton,Ying Sun,Carla P. Gomes,Yiqi Luo
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 60 pages, 11 figures
- **标题**: None
- **领域**: 地球物理学,人工智能
- **Abstract**: Big data and the rapid development of artificial intelligence (AI) provide unprecedented opportunities to enhance our understanding of the global carbon cycle and other biogeochemical processes. However, retrieving mechanistic knowledge from big data remains a challenge. Here, we develop a Biogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a vectorized process-based soil carbon cycle model (i.e., Community Land Model version 5, CLM5) into a neural network (NN) structure to examine mechanisms governing soil organic carbon (SOC) storage from big data. BINN demonstrates high accuracy in retrieving biogeochemical parameter values from synthetic data in a parameter recovery experiment. We use BINN to predict six major processes regulating the soil carbon cycle (or components in process-based models) from 25,925 observed SOC profiles across the conterminous US and compared them with the same processes previously retrieved by a Bayesian inference-based PROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et al. 2020; 2023). The high agreement between the spatial patterns of the retrieved processes using the two approaches with an average correlation coefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge from big data. Additionally, the integration of neural networks and process-based models in BINN improves computational efficiency by more than 50 times over PRODA. We conclude that BINN is a transformative tool that harnesses the power of both AI and process-based modeling, facilitating new scientific discoveries while improving interpretability and accuracy of Earth system models.

## 医学物理(physics.med-ph:Medical Physics)

### Actor Critic with Experience Replay-based automatic treatment planning for prostate cancer intensity modulated radiotherapy 
[[arxiv](https://arxiv.org/abs/2502.00346)] [[cool](https://papers.cool/arxiv/2502.00346)] [[pdf](https://arxiv.org/pdf/2502.00346)]
> **Authors**: Md Mainul Abrar,Parvat Sapkota,Damon Sprouts,Xun Jia,Yujie Chi
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 27 Pages, 8 Figures, 4 Tables
- **标题**: None
- **领域**: 医学物理,人工智能,机器学习
- **Abstract**: Background: Real-time treatment planning in IMRT is challenging due to complex beam interactions. AI has improved automation, but existing models require large, high-quality datasets and lack universal applicability. Deep reinforcement learning (DRL) offers a promising alternative by mimicking human trial-and-error planning. Purpose: Develop a stochastic policy-based DRL agent for automatic treatment planning with efficient training, broad applicability, and robustness against adversarial attacks using Fast Gradient Sign Method (FGSM). Methods: Using the Actor-Critic with Experience Replay (ACER) architecture, the agent tunes treatment planning parameters (TPPs) in inverse planning. Training is based on prostate cancer IMRT cases, using dose-volume histograms (DVHs) as input. The model is trained on a single patient case, validated on two independent cases, and tested on 300+ plans across three datasets. Plan quality is assessed using ProKnow scores, and robustness is tested against adversarial attacks. Results: Despite training on a single case, the model generalizes well. Before ACER-based planning, the mean plan score was 6.20$\pm$1.84; after, 93.09% of cases achieved a perfect score of 9, with a mean of 8.93$\pm$0.27. The agent effectively prioritizes optimal TPP tuning and remains robust against adversarial attacks. Conclusions: The ACER-based DRL agent enables efficient, high-quality treatment planning in prostate cancer IMRT, demonstrating strong generalizability and robustness.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Probabilistic adaptation of language comprehension for individual speakers: Evidence from neural oscillations 
[[arxiv](https://arxiv.org/abs/2502.01299)] [[cool](https://papers.cool/arxiv/2502.01299)] [[pdf](https://arxiv.org/pdf/2502.01299)]
> **Authors**: Hanlin Wu,Xiaohui Rao,Zhenguang G. Cai
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,计算语言学
- **Abstract**: Listeners adapt language comprehension based on their mental representations of speakers, but how these representations are dynamically updated remains unclear. We investigated whether listeners probabilistically adapt their comprehension based on the likelihood of speakers producing stereotype-incongruent utterances. Our findings reveal two potential mechanisms: a speaker-general mechanism that adjusts overall expectations about speaker-content relationships, and a speaker-specific mechanism that updates individual speaker models. In two EEG experiments, participants heard speakers make stereotype-congruent or incongruent utterances, with incongruency base rate manipulated between blocks. In Experiment 1, speaker incongruency modulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations: incongruent utterances decreased oscillatory power in low base rate condition but increased it in high base rate condition. The theta effect varied with listeners' openness trait: less open participants showed theta increases to speaker-incongruencies, suggesting maintenance of speaker-specific information, while more open participants showed theta decreases, indicating flexible model updating. In Experiment 2, we dissociated base rate from the target speaker by manipulating the overall base rate using an alternative non-target speaker. Only the high-beta effect persisted, showing power decrease for speaker-incongruencies in low base rate condition but no effect in high base rate condition. The high-beta oscillations might reflect the speaker-general adjustment, while theta oscillations may index the speaker-specific model updating. These findings provide evidence for how language processing is shaped by social cognition in real time.

## 定量方法(q-bio.QM:Quantitative Methods)

### Blood Glucose Level Prediction in Type 1 Diabetes Using Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.00065)] [[cool](https://papers.cool/arxiv/2502.00065)] [[pdf](https://arxiv.org/pdf/2502.00065)]
> **Authors**: Soon Jynn Chu,Nalaka Amarasiri,Sandesh Giri,Priyata Kafle
> **First submission**: 2025-01-30
> **First announcement**: 2025-02-04
> **comment**: 15 pages, 7 figures. This work was accepted for CSCI 2024 conference
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Type 1 Diabetes is a chronic autoimmune condition in which the immune system attacks and destroys insulin-producing beta cells in the pancreas, resulting in little to no insulin production. Insulin helps glucose in your blood enter your muscle, fat, and liver cells so they can use it for energy or store it for later use. If insulin is insufficient, it causes sugar to build up in the blood and leads to serious health problems. People with Type 1 Diabetes need synthetic insulin every day. In diabetes management, continuous glucose monitoring is an important feature that provides near real-time blood glucose data. It is useful in deciding the synthetic insulin dose. In this research work, we used machine learning tools, deep neural networks, deep reinforcement learning, and voting and stacking regressors to predict blood glucose levels at 30-min time intervals using the latest DiaTrend dataset. Predicting blood glucose levels is useful in better diabetes management systems. The trained models were compared using several evaluation metrics. Our evaluation results demonstrate the performance of various models across different glycemic conditions for blood glucose prediction. The source codes of this work can be found in: https://github.com/soon-jynn-chu/t1d_bg_prediction

## 计算金融(q-fin.CP:Computational Finance)

### MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents 
[[arxiv](https://arxiv.org/abs/2502.00415)] [[cool](https://papers.cool/arxiv/2502.00415)] [[pdf](https://arxiv.org/pdf/2502.00415)]
> **Authors**: George Fatouros,Kostas Metaxas,John Soldatos,Manos Karathanassis
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: 25 pages, 7 figures, Under review at Financial Innovation (FIN)
- **标题**: None
- **领域**: 计算金融,人工智能,计算语言学,多代理系统,投资组合管理
- **Abstract**: MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\&P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\&P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.

## 一般财务(q-fin.GN:General Finance)

### Retail Market Analysis 
[[arxiv](https://arxiv.org/abs/2502.00024)] [[cool](https://papers.cool/arxiv/2502.00024)] [[pdf](https://arxiv.org/pdf/2502.00024)]
> **Authors**: Ke Yuan,Yaoxin Liu,Shriyesh Chandra,Rishav Roy
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 一般财务,机器学习
- **Abstract**: This project focuses on analyzing retail market trends using historical sales data, search trends, and customer reviews. By identifying the patterns and trending products, the analysis provides actionable insights for retailers to optimize inventory management and marketing strategies, ultimately enhancing customer satisfaction and maximizing revenue.

## 投资组合管理(q-fin.PM:Portfolio Management)

### Decision-informed Neural Networks with Large Language Model Integration for Portfolio Optimization 
[[arxiv](https://arxiv.org/abs/2502.00828)] [[cool](https://papers.cool/arxiv/2502.00828)] [[pdf](https://arxiv.org/pdf/2502.00828)]
> **Authors**: Yoontae Hwang,Yaxuan Kong,Stefan Zohren,Yongjae Lee
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: Submitted paper
- **标题**: None
- **领域**: 投资组合管理,人工智能,计算金融
- **Abstract**: This paper addresses the critical disconnect between prediction and decision quality in portfolio optimization by integrating Large Language Models (LLMs) with decision-focused learning. We demonstrate both theoretically and empirically that minimizing the prediction error alone leads to suboptimal portfolio decisions. We aim to exploit the representational power of LLMs for investment decisions. An attention mechanism processes asset relationships, temporal dependencies, and macro variables, which are then directly integrated into a portfolio optimization layer. This enables the model to capture complex market dynamics and align predictions with the decision objectives. Extensive experiments on S\&P100 and DOW30 datasets show that our model consistently outperforms state-of-the-art deep learning models. In addition, gradient-based analyses show that our model prioritizes the assets most crucial to decision making, thus mitigating the effects of prediction errors on portfolio performance. These findings underscore the value of integrating decision objectives into predictions for more robust and context-aware portfolio management.

### AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics 
[[arxiv](https://arxiv.org/abs/2502.00029)] [[cool](https://papers.cool/arxiv/2502.00029)] [[pdf](https://arxiv.org/pdf/2502.00029)]
> **Authors**: Kamer Ali Yuksel,Hassan Sawaf
> **First submission**: 2025-01-23
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 投资组合管理,人工智能,计算语言学,神经和进化计算,风险管理
- **Abstract**: Financial metrics like the Sharpe ratio are pivotal in evaluating investment performance by balancing risk and return. However, traditional metrics often struggle with robustness and generalization, particularly in dynamic and volatile market conditions. This paper introduces AlphaSharpe, a novel framework leveraging large language models (LLMs) to iteratively evolve and optimize financial metrics to discover enhanced risk-return metrics that outperform traditional approaches in robustness and correlation with future performance metrics by employing iterative crossover, mutation, and evaluation. Key contributions of this work include: (1) a novel use of LLMs to generate and refine financial metrics with implicit domain-specific knowledge, (2) a scoring mechanism to ensure that evolved metrics generalize effectively to unseen data, and (3) an empirical demonstration of 3x predictive power for future risk-returns, and 2x portfolio performance. Experimental results in a real-world dataset highlight the superiority of discovered metrics, making them highly relevant to portfolio managers and financial decision-makers. This framework not only addresses the limitations of existing metrics but also showcases the potential of LLMs in advancing financial analytics, paving the way for informed and robust investment strategies.

## 交易和市场微观结构(q-fin.TR:Trading and Market Microstructure)

### FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL Contest 2024 
[[arxiv](https://arxiv.org/abs/2502.01992)] [[cool](https://papers.cool/arxiv/2502.01992)] [[pdf](https://arxiv.org/pdf/2502.01992)]
> **Authors**: Arnav Grover
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Competition Track FinRL, ICAIF 2024
- **标题**: None
- **领域**: 交易和市场微观结构,机器学习
- **Abstract**: In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study proposes a novel prompt framework for fine-tuning large language models (LLM) with Reinforcement Learning from Market Feedback (RLMF). Our framework incorporates market-specific features and short-term price dynamics to generate more precise trading signals. Traditional LLMs, while competent in sentiment analysis, lack contextual alignment for financial market applications. To bridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom RLMF prompt design that integrates historical market data and reward-based feedback. Our evaluation shows that this RLMF-tuned framework outperforms baseline methods in signal consistency and achieving tighter trading outcomes; awarded as winner of Task II. You can find the code for this project on GitHub.

## 量子物理学(quant-ph:Quantum Physics)

### Adaptive Observation Cost Control for Variational Quantum Eigensolvers 
[[arxiv](https://arxiv.org/abs/2502.01704)] [[cool](https://papers.cool/arxiv/2502.01704)] [[pdf](https://arxiv.org/pdf/2502.01704)]
> **Authors**: Christopher J. Anders,Kim A. Nicoli,Bingting Wu,Naima Elosegui,Samuele Pedrielli,Lena Funcke,Karl Jansen,Stefan Kühn,Shinichi Nakajima
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 9 pages, 6 figures, 41st International Conference onMachineLearning(ICML 2024)
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: The objective to be minimized in the variational quantum eigensolver (VQE) has a restricted form, which allows a specialized sequential minimal optimization (SMO) that requires only a few observations in each iteration. However, the SMO iteration is still costly due to the observation noise -- one observation at a point typically requires averaging over hundreds to thousands of repeated quantum measurement shots for achieving a reasonable noise level. In this paper, we propose an adaptive cost control method, named subspace in confident region (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP) surrogate, and requires it to have low uncertainty over the subspace being updated, so that optimization in each iteration is performed with guaranteed accuracy. The adaptive cost control is performed by first setting the required accuracy according to the progress of the optimization, and then choosing the minimum number of measurement shots and their distribution such that the required accuracy is satisfied. We demonstrate that SubsCoRe significantly improves the efficiency of SMO, and outperforms the state-of-the-art methods.

### Quantum Machine Learning: A Hands-on Tutorial for Machine Learning Practitioners and Researchers 
[[arxiv](https://arxiv.org/abs/2502.01146)] [[cool](https://papers.cool/arxiv/2502.01146)] [[pdf](https://arxiv.org/pdf/2502.01146)]
> **Authors**: Yuxuan Du,Xinbiao Wang,Naixu Guo,Zhan Yu,Yang Qian,Kaining Zhang,Min-Hsiu Hsieh,Patrick Rebentrost,Dacheng Tao
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 260 pages; Comments are welcome
- **标题**: None
- **领域**: 量子物理学,人工智能,机器学习
- **Abstract**: This tutorial intends to introduce readers with a background in AI to quantum machine learning (QML) -- a rapidly evolving field that seeks to leverage the power of quantum computers to reshape the landscape of machine learning. For self-consistency, this tutorial covers foundational principles, representative QML algorithms, their potential applications, and critical aspects such as trainability, generalization, and computational complexity. In addition, practical code demonstrations are provided in https://qml-tutorial.github.io/ to illustrate real-world implementations and facilitate hands-on learning. Together, these elements offer readers a comprehensive overview of the latest advancements in QML. By bridging the gap between classical machine learning and quantum computing, this tutorial serves as a valuable resource for those looking to engage with QML and explore the forefront of AI in the quantum era.

### Online Learning of Pure States is as Hard as Mixed States 
[[arxiv](https://arxiv.org/abs/2502.00823)] [[cool](https://papers.cool/arxiv/2502.00823)] [[pdf](https://arxiv.org/pdf/2502.00823)]
> **Authors**: Maxime Meyer,Soumik Adhikary,Naixu Guo,Patrick Rebentrost
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 21 pages, 5 figures
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: Quantum state tomography, the task of learning an unknown quantum state, is a fundamental problem in quantum information. In standard settings, the complexity of this problem depends significantly on the type of quantum state that one is trying to learn, with pure states being substantially easier to learn than general mixed states. A natural question is whether this separation holds for any quantum state learning setting. In this work, we consider the online learning framework and prove the surprising result that learning pure states in this setting is as hard as learning mixed states. More specifically, we show that both classes share almost the same sequential fat-shattering dimension, leading to identical regret scaling under the $L_1$-loss. We also generalize previous results on full quantum state tomography in the online setting to learning only partially the density matrix, using smooth analysis.

### Super Quantum Mechanics 
[[arxiv](https://arxiv.org/abs/2502.00037)] [[cool](https://papers.cool/arxiv/2502.00037)] [[pdf](https://arxiv.org/pdf/2502.00037)]
> **Authors**: Mikhail Gennadievich Belov,Victor Victorovich Dubov,Vadim Konstantinovich Ivanov,Alexander Yurievich Maslov,Olga Vladimirovna Proshina,Vladislav Gennadievich Malyshkin
> **First submission**: 2025-01-25
> **First announcement**: 2025-02-04
> **comment**: The ML approach presented in arXiv:2407.04406 is extended to stationary and non-stationary quantum dynamics
- **标题**: None
- **领域**: 量子物理学,机器学习,数值分析
- **Abstract**: We introduce Super Quantum Mechanics (SQM) as a theory that considers states in Hilbert space subject to multiple quadratic constraints. Traditional quantum mechanics corresponds to a single quadratic constraint of wavefunction normalization. In its simplest form, SQM considers states in the form of unitary operators, where the quadratic constraints are conditions of unitarity. In this case, the stationary SQM problem is a quantum inverse problem with multiple applications in machine learning and artificial intelligence. The SQM stationary problem is equivalent to a new algebraic problem that we address in this paper. The SQM non-stationary problem considers the evolution of a quantum system, distinct from the explicit time dependence of the Hamiltonian, $H(t)$. Several options for the SQM dynamic equation are considered, and quantum circuits of 2D type are introduced, which transform one quantum system into another. Although no known physical process currently describes such dynamics, this approach naturally bridges direct and inverse quantum mechanics problems, allowing for the development of a new type of computer algorithm. Beyond computer modeling, the developed theory could be directly applied if or when a physical process capable of solving an inverse quantum problem in a single measurement act (analogous to wavefunction measurement in traditional quantum mechanics) is discovered in the future.

## 方法论(stat.ME:Methodology)

### Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices 
[[arxiv](https://arxiv.org/abs/2502.01512)] [[cool](https://papers.cool/arxiv/2502.01512)] [[pdf](https://arxiv.org/pdf/2502.01512)]
> **Authors**: Thibault de Surrel,Fabien Lotte,Sylvain Chevallier,Florian Yger
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,统计理论,机器学习
- **Abstract**: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite matrices, a key focus in information geometry. We introduced a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.

### Learning to Partially Defer for Sequences 
[[arxiv](https://arxiv.org/abs/2502.01459)] [[cool](https://papers.cool/arxiv/2502.01459)] [[pdf](https://arxiv.org/pdf/2502.01459)]
> **Authors**: Sahana Rayan,Ambuj Tewari
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,机器学习
- **Abstract**: In the Learning to Defer (L2D) framework, a prediction model can either make a prediction or defer it to an expert, as determined by a rejector. Current L2D methods train the rejector to decide whether to reject the entire prediction, which is not desirable when the model predicts long sequences. We present an L2D setting for sequence outputs where the system can defer specific outputs of the whole model prediction to an expert in an effort to interleave the expert and machine throughout the prediction. We propose two types of model-based post-hoc rejectors for pre-trained predictors: a token-level rejector, which defers specific token predictions to experts with next token prediction capabilities, and a one-time rejector for experts without such abilities, which defers the remaining sequence from a specific point onward. In the experiments, we also empirically demonstrate that such granular deferrals achieve better cost-accuracy tradeoffs than whole deferrals on Traveling salesman solvers and News summarization models.

### Optimizing Feature Selection in Causal Inference: A Three-Stage Computational Framework for Unbiased Estimation 
[[arxiv](https://arxiv.org/abs/2502.00501)] [[cool](https://papers.cool/arxiv/2502.00501)] [[pdf](https://arxiv.org/pdf/2502.00501)]
> **Authors**: Tianyu Yang,Md. Noor-E-Alam
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,人工智能,机器学习,机器学习
- **Abstract**: Feature selection is an important but challenging task in causal inference for obtaining unbiased estimates of causal quantities. Properly selected features in causal inference not only significantly reduce the time required to implement a matching algorithm but, more importantly, can also reduce the bias and variance when estimating causal quantities. When feature selection techniques are applied in causal inference, the crucial criterion is to select variables that, when used for matching, can achieve an unbiased and robust estimation of causal quantities. Recent research suggests that balancing only on treatment-associated variables introduces bias while balancing on spurious variables increases variance. To address this issue, we propose an enhanced three-stage framework that shows a significant improvement in selecting the desired subset of variables compared to the existing state-of-the-art feature selection framework for causal inference, resulting in lower bias and variance in estimating the causal quantity. We evaluated our proposed framework using a state-of-the-art synthetic data across various settings and observed superior performance within a feasible computation time, ensuring scalability for large-scale datasets. Finally, to demonstrate the applicability of our proposed methodology using large-scale real-world data, we evaluated an important US healthcare policy related to the opioid epidemic crisis: whether opioid use disorder has a causal relationship with suicidal behavior.

## 机器学习(stat.ML:Machine Learning)

### Theoretical and Practical Analysis of Fréchet Regression via Comparison Geometry 
[[arxiv](https://arxiv.org/abs/2502.01995)] [[cool](https://papers.cool/arxiv/2502.01995)] [[pdf](https://arxiv.org/pdf/2502.01995)]
> **Authors**: Masanari Kimura,Howard Bondell
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Fréchet regression extends classical regression methods to non-Euclidean metric spaces, enabling the analysis of data relationships on complex structures such as manifolds and graphs. This work establishes a rigorous theoretical analysis for Fréchet regression through the lens of comparison geometry which leads to important considerations for its use in practice. The analysis provides key results on the existence, uniqueness, and stability of the Fréchet mean, along with statistical guarantees for nonparametric regression, including exponential concentration bounds and convergence rates. Additionally, insights into angle stability reveal the interplay between curvature of the manifold and the behavior of the regression estimator in these non-Euclidean contexts. Empirical experiments validate the theoretical findings, demonstrating the effectiveness of proposed hyperbolic mappings, particularly for data with heteroscedasticity, and highlighting the practical usefulness of these results.

### Local minima of the empirical risk in high dimension: General theorems and convex examples 
[[arxiv](https://arxiv.org/abs/2502.01953)] [[cool](https://papers.cool/arxiv/2502.01953)] [[pdf](https://arxiv.org/pdf/2502.01953)]
> **Authors**: Kiana Asgari,Andrea Montanari,Basil Saeed
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 95 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We consider a general model for high-dimensional empirical risk minimization whereby the data $\mathbf{x}_i$ are $d$-dimensional isotropic Gaussian vectors, the model is parametrized by $\mathbfΘ\in\mathbb{R}^{d\times k}$, and the loss depends on the data via the projection $\mathbfΘ^\mathsf{T}\mathbf{x}_i$. This setting covers as special cases classical statistics methods (e.g. multinomial regression and other generalized linear models), but also two-layer fully connected neural networks with $k$ hidden neurons. We use the Kac-Rice formula from Gaussian process theory to derive a bound on the expected number of local minima of this empirical risk, under the proportional asymptotics in which $n,d\to\infty$, with $n\asymp d$. Via Markov's inequality, this bound allows to determine the positions of these minimizers (with exponential deviation bounds) and hence derive sharp asymptotics on the estimation and prediction error. In this paper, we apply our characterization to convex losses, where high-dimensional asymptotics were not (in general) rigorously established for $k\ge 2$. We show that our approach is tight and allows to prove previously conjectured results. In addition, we characterize the spectrum of the Hessian at the minimizer. A companion paper applies our general result to non-convex examples.

### Poisson Hierarchical Indian Buffet Processes for Within and Across Group Sharing of Latent Features-With Indications for Microbiome Species Sampling Models 
[[arxiv](https://arxiv.org/abs/2502.01919)] [[cool](https://papers.cool/arxiv/2502.01919)] [[pdf](https://arxiv.org/pdf/2502.01919)]
> **Authors**: Lancelot F. James,Juho Lee,Abhinav Pandey
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: experiments to be added
- **标题**: None
- **领域**: 机器学习,机器学习,可能性,统计理论
- **Abstract**: In this work, we present a comprehensive Bayesian posterior analysis of what we term Poisson Hierarchical Indian Buffet Processes, designed for complex random sparse count species sampling models that allow for the sharing of information across and within groups. This analysis covers a potentially infinite number of species and unknown parameters, which, within a Bayesian machine learning context, we are able to learn from as more information is sampled. To achieve our refined results, we employ a range of methodologies drawn from Bayesian latent feature models, random occupancy models, and excursion theory. Despite this complexity, our goal is to make our findings accessible to practitioners, including those who may not be familiar with these areas. To facilitate understanding, we adopt a pseudo-expository style that emphasizes clarity and practical utility. We aim to express our findings in a language that resonates with experts in microbiome and ecological studies, addressing gaps in modeling capabilities while acknowledging that we are not experts ourselves in these fields. This approach encourages the use of our models as basic components of more sophisticated frameworks employed by domain experts, embodying the spirit of the seminal work on the Dirichlet Process. Ultimately, our refined posterior analysis not only yields tractable computational procedures but also enables practical statistical implementation and provides a clear mapping to relevant quantities in microbiome analysis.

### Graph Canonical Correlation Analysis 
[[arxiv](https://arxiv.org/abs/2502.01780)] [[cool](https://papers.cool/arxiv/2502.01780)] [[pdf](https://arxiv.org/pdf/2502.01780)]
> **Authors**: Hongju Park,Shuyang Bai,Zhenyao Ye,Hwiyoung Lee,Tianzhou Ma,Shuo Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 40 pages, 3 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Canonical correlation analysis (CCA) is a widely used technique for estimating associations between two sets of multi-dimensional variables. Recent advancements in CCA methods have expanded their application to decipher the interactions of multiomics datasets, imaging-omics datasets, and more. However, conventional CCA methods are limited in their ability to incorporate structured patterns in the cross-correlation matrix, potentially leading to suboptimal estimations. To address this limitation, we propose the graph Canonical Correlation Analysis (gCCA) approach, which calculates canonical correlations based on the graph structure of the cross-correlation matrix between the two sets of variables. We develop computationally efficient algorithms for gCCA, and provide theoretical results for finite sample analysis of best subset selection and canonical correlation estimation by introducing concentration inequalities and stopping time rule based on martingale theories. Extensive simulations demonstrate that gCCA outperforms competing CCA methods. Additionally, we apply gCCA to a multiomics dataset of DNA methylation and RNA-seq transcriptomics, identifying both positively and negatively regulated gene expression pathways by DNA methylation pathways.

### Doubly Robust Monte Carlo Tree Search 
[[arxiv](https://arxiv.org/abs/2502.01672)] [[cool](https://papers.cool/arxiv/2502.01672)] [[pdf](https://arxiv.org/pdf/2502.01672)]
> **Authors**: Manqing Liu,Andrew L. Beam
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We present Doubly Robust Monte Carlo Tree Search (DR-MCTS), a novel algorithm that integrates Doubly Robust (DR) off-policy estimation into Monte Carlo Tree Search (MCTS) to enhance sample efficiency and decision quality in complex environments. Our approach introduces a hybrid estimator that combines MCTS rollouts with DR estimation, offering theoretical guarantees of unbiasedness and variance reduction under specified conditions. Empirical evaluations in Tic-Tac-Toe and the partially observable VirtualHome environment demonstrate DR-MCTS's superior performance over standard MCTS. In Tic-Tac-Toe, DR-MCTS achieves an 88% win rate compared to a 10% win rate for standard MCTS. In compound VirtualHome tasks, DR-MCTS attains a 20.7% success rate versus 10.3% for standard MCTS. Our scaling analysis reveals that DR-MCTS exhibits better sample efficiency, notably outperforming standard MCTS with larger language models while using a smaller model. These results underscore DR-MCTS's potential for efficient decision-making in complex, real-world scenarios where sample efficiency is paramount.

### Re-examining Double Descent and Scaling Laws under Norm-based Capacity via Deterministic Equivalence 
[[arxiv](https://arxiv.org/abs/2502.01585)] [[cool](https://papers.cool/arxiv/2502.01585)] [[pdf](https://arxiv.org/pdf/2502.01585)]
> **Authors**: Yichen Wang,Yudong Chen,Lorenzo Rosasco,Fanghui Liu
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 71 pages
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We investigate double descent and scaling laws in terms of weights rather than the number of parameters. Specifically, we analyze linear and random features models using the deterministic equivalence approach from random matrix theory. We precisely characterize how the weights norm concentrate around deterministic quantities and elucidate the relationship between the expected test error and the norm-based capacity (complexity). Our results rigorously answer whether double descent exists under norm-based capacity and reshape the corresponding scaling laws. Moreover, they prompt a rethinking of the data-parameter paradigm - from under-parameterized to over-parameterized regimes - by shifting the focus to norms (weights) rather than parameter count.

### Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery 
[[arxiv](https://arxiv.org/abs/2502.01583)] [[cool](https://papers.cool/arxiv/2502.01583)] [[pdf](https://arxiv.org/pdf/2502.01583)]
> **Authors**: Filip Kovačević,Yihan Zhang,Marco Mondelli
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,机器学习,可能性,统计理论
- **Abstract**: Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods that are routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace. The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery.

### Heterogeneous Treatment Effect in Time-to-Event Outcomes: Harnessing Censored Data with Recursively Imputed Trees 
[[arxiv](https://arxiv.org/abs/2502.01575)] [[cool](https://papers.cool/arxiv/2502.01575)] [[pdf](https://arxiv.org/pdf/2502.01575)]
> **Authors**: Tomer Meir,Uri Shalit,Malka Gorfine
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Tailoring treatments to individual needs is a central goal in fields such as medicine. A key step toward this goal is estimating Heterogeneous Treatment Effects (HTE) - the way treatments impact different subgroups. While crucial, HTE estimation is challenging with survival data, where time until an event (e.g., death) is key. Existing methods often assume complete observation, an assumption violated in survival data due to right-censoring, leading to bias and inefficiency. Cui et al. (2023) proposed a doubly-robust method for HTE estimation in survival data under no hidden confounders, combining a causal survival forest with an augmented inverse-censoring weighting estimator. However, we find it struggles under heavy censoring, which is common in rare-outcome problems such as Amyotrophic lateral sclerosis (ALS). Moreover, most current methods cannot handle instrumental variables, which are a crucial tool in the causal inference arsenal. We introduce Multiple Imputation for Survival Treatment Response (MISTR), a novel, general, and non-parametric method for estimating HTE in survival data. MISTR uses recursively imputed survival trees to handle censoring without directly modeling the censoring mechanism. Through extensive simulations and analysis of two real-world datasets-the AIDS Clinical Trials Group Protocol 175 and the Illinois unemployment dataset we show that MISTR outperforms prior methods under heavy censoring in the no-hidden-confounders setting, and extends to the instrumental variable setting. To our knowledge, MISTR is the first non-parametric approach for HTE estimation with unobserved confounders via instrumental variables.

### Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods 
[[arxiv](https://arxiv.org/abs/2502.01384)] [[cool](https://papers.cool/arxiv/2502.01384)] [[pdf](https://arxiv.org/pdf/2502.01384)]
> **Authors**: Oussama Zekri,Nicolas Boullé
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 23 pages, 4 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO

### Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization 
[[arxiv](https://arxiv.org/abs/2502.01347)] [[cool](https://papers.cool/arxiv/2502.01347)] [[pdf](https://arxiv.org/pdf/2502.01347)]
> **Authors**: Simone Bombari,Marco Mondelli
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Learning models have been shown to rely on spurious correlations between non-predictive features and the associated labels in the training data, with negative implications on robustness, bias and fairness. In this work, we provide a statistical characterization of this phenomenon for high-dimensional regression, when the data contains a predictive core feature $x$ and a spurious feature $y$. Specifically, we quantify the amount of spurious correlations $C$ learned via linear regression, in terms of the data covariance and the strength $λ$ of the ridge regularization. As a consequence, we first capture the simplicity of $y$ through the spectrum of its covariance, and its correlation with $x$ through the Schur complement of the full data covariance. Next, we prove a trade-off between $C$ and the in-distribution test loss $L$, by showing that the value of $λ$ that minimizes $L$ lies in an interval where $C$ is increasing. Finally, we investigate the effects of over-parameterization via the random features model, by showing its equivalence to regularized linear regression. Our theoretical results are supported by numerical experiments on Gaussian, Color-MNIST, and CIFAR-10 datasets.

### PtyGenography: using generative models for regularization of the phase retrieval problem 
[[arxiv](https://arxiv.org/abs/2502.01338)] [[cool](https://papers.cool/arxiv/2502.01338)] [[pdf](https://arxiv.org/pdf/2502.01338)]
> **Authors**: Selin Aslan,Tristan van Leeuwen,Allard Mosk,Palina Salanevich
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,机器学习,泛函分析,优化与控制
- **Abstract**: In phase retrieval and similar inverse problems, the stability of solutions across different noise levels is crucial for applications. One approach to promote it is using signal priors in a form of a generative model as a regularization, at the expense of introducing a bias in the reconstruction. In this paper, we explore and compare the reconstruction properties of classical and generative inverse problem formulations. We propose a new unified reconstruction approach that mitigates overfitting to the generative model for varying noise levels.

### Rational Gaussian wavelets and corresponding model driven neural networks 
[[arxiv](https://arxiv.org/abs/2502.01282)] [[cool](https://papers.cool/arxiv/2502.01282)] [[pdf](https://arxiv.org/pdf/2502.01282)]
> **Authors**: Attila Miklós Ámon,Kristian Fenech,Péter Kovács,Tamás Dózsa
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: Submitted to IEEE Transactions on Signal Processing, 2024 (under review)
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: In this paper we consider the continuous wavelet transform using Gaussian wavelets multiplied by an appropriate rational term. The zeros and poles of this rational modifier act as free parameters and their choice highly influences the shape of the mother wavelet. This allows the proposed construction to approximate signals with complex morphology using only a few wavelet coefficients. We show that the proposed rational Gaussian wavelets are admissible and provide numerical approximations of the wavelet coefficients using variable projection operators. In addition, we show how the proposed variable projection based rational Gaussian wavelet transform can be used in neural networks to obtain a highly interpretable feature learning layer. We demonstrate the effectiveness of the proposed scheme through a biomedical application, namely, the detection of ventricular ectopic beats (VEBs) in real ECG measurements.

### One-step full gradient suffices for low-rank fine-tuning, provably and efficiently 
[[arxiv](https://arxiv.org/abs/2502.01235)] [[cool](https://papers.cool/arxiv/2502.01235)] [[pdf](https://arxiv.org/pdf/2502.01235)]
> **Authors**: Yuanhe Zhang,Fanghui Liu,Yudong Chen
> **First submission**: 2025-02-03
> **First announcement**: 2025-02-04
> **comment**: 86 pages
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: This paper studies how to improve the performance of Low-Rank Adaption (LoRA) as guided by our theoretical analysis. Our first set of theoretical results show that for random initialization and linear models, \textit{i)} LoRA will align to the certain singular subspace of one-step gradient of full fine-tuning; \textit{ii)} preconditioners improve convergence in the high-rank case. These insights motivate us to focus on preconditioned LoRA using a specific spectral initialization strategy for aligning with certain subspaces. For both linear and nonlinear models, we prove that alignment and generalization guarantees can be directly achieved at initialization, and the subsequent linear convergence can be also built. Our analysis leads to the \emph{LoRA-One} algorithm (using \emph{One}-step gradient and preconditioning), a theoretically grounded algorithm that achieves significant empirical improvement over vanilla LoRA and its variants on several benchmarks. Our theoretical analysis, based on decoupling the learning dynamics and characterizing how spectral initialization contributes to feature learning, may be of independent interest for understanding matrix sensing and deep learning theory. The source code can be found in the https://github.com/YuanheZ/LoRA-One.

### Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and Guarantees 
[[arxiv](https://arxiv.org/abs/2502.01027)] [[cool](https://papers.cool/arxiv/2502.01027)] [[pdf](https://arxiv.org/pdf/2502.01027)]
> **Authors**: Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Learning-to-Defer (L2D) facilitates optimal task allocation between AI systems and decision-makers. Despite its potential, we show that current two-stage L2D frameworks are highly vulnerable to adversarial attacks, which can misdirect queries or overwhelm decision agents, significantly degrading system performance. This paper conducts the first comprehensive analysis of adversarial robustness in two-stage L2D frameworks. We introduce two novel attack strategies -- untargeted and targeted -- that exploit inherent structural vulnerabilities in these systems. To mitigate these threats, we propose SARD, a robust, convex, deferral algorithm rooted in Bayes and $(\mathcal{R},\mathcal{G})$-consistency. Our approach guarantees optimal task allocation under adversarial perturbations for all surrogates in the cross-entropy family. Extensive experiments on classification, regression, and multi-task benchmarks validate the robustness of SARD.

### HASSLE-free: A unified Framework for Sparse plus Low-Rank Matrix Decomposition for LLMs 
[[arxiv](https://arxiv.org/abs/2502.00899)] [[cool](https://papers.cool/arxiv/2502.00899)] [[pdf](https://arxiv.org/pdf/2502.00899)]
> **Authors**: Mehdi Makni,Kayhan Behdin,Zheng Xu,Natalia Ponomareva,Rahul Mazumder
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The impressive capabilities of large foundation models come at a cost of substantial computing resources to serve them. Compressing these pre-trained models is of practical interest as it can democratize deploying them to the machine learning community at large by lowering the costs associated with inference. A promising compression scheme is to decompose foundation models' dense weights into a sum of sparse plus low-rank matrices. In this paper, we design a unified framework coined HASSLE-free for (semi-structured) sparse plus low-rank matrix decomposition of foundation models. Our framework introduces the local layer-wise reconstruction error objective for this decomposition, we demonstrate that prior work solves a relaxation of this optimization problem; and we provide efficient and scalable methods to minimize the exact introduced optimization problem. HASSLE-free substantially outperforms state-of-the-art methods in terms of the introduced objective and a wide range of LLM evaluation benchmarks. For the Llama3-8B model with a 2:4 sparsity component plus a 64-rank component decomposition, a compression scheme for which recent work shows important inference acceleration on GPUs, HASSLE-free reduces the test perplexity by 12% for the WikiText-2 dataset and reduces the gap (compared to the dense model) of the average of eight popular zero-shot tasks by 15% compared to existing methods.

### Algorithmic Stability of Stochastic Gradient Descent with Momentum under Heavy-Tailed Noise 
[[arxiv](https://arxiv.org/abs/2502.00885)] [[cool](https://papers.cool/arxiv/2502.00885)] [[pdf](https://arxiv.org/pdf/2502.00885)]
> **Authors**: Thanh Dang,Melih Barsbey,A K M Rokonuzzaman Sonet,Mert Gurbuzbalaban,Umut Simsekli,Lingjiong Zhu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: 64 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,机器学习,优化与控制,可能性
- **Abstract**: Understanding the generalization properties of optimization algorithms under heavy-tailed noise has gained growing attention. However, the existing theoretical results mainly focus on stochastic gradient descent (SGD) and the analysis of heavy-tailed optimizers beyond SGD is still missing. In this work, we establish generalization bounds for SGD with momentum (SGDm) under heavy-tailed gradient noise. We first consider the continuous-time limit of SGDm, i.e., a Levy-driven stochastic differential equation (SDE), and establish quantitative Wasserstein algorithmic stability bounds for a class of potentially non-convex loss functions. Our bounds reveal a remarkable observation: For quadratic loss functions, we show that SGDm admits a worse generalization bound in the presence of heavy-tailed noise, indicating that the interaction of momentum and heavy tails can be harmful for generalization. We then extend our analysis to discrete-time and develop a uniform-in-time discretization error bound, which, to our knowledge, is the first result of its kind for SDEs with degenerate noise. This result shows that, with appropriately chosen step-sizes, the discrete dynamics retain the generalization properties of the limiting SDE. We illustrate our theory on both synthetic quadratic problems and neural networks.

### Error-quantified Conformal Inference for Time Series 
[[arxiv](https://arxiv.org/abs/2502.00818)] [[cool](https://papers.cool/arxiv/2502.00818)] [[pdf](https://arxiv.org/pdf/2502.00818)]
> **Authors**: Junxi Wu,Dongjian Hu,Yajie Bao,Shu-Tao Xia,Changliang Zou
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: ICLR 2025 camera version
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal inference provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a series of online conformal inference methods updated thresholds of prediction sets by performing online gradient descent on a sequence of quantile loss functions. A drawback of such methods is that they only use the information of revealed non-conformity scores via miscoverage indicators but ignore error quantification, namely the distance between the non-conformity score and the current threshold. To accurately leverage the dynamic of miscoverage error, we propose \textit{Error-quantified Conformal Inference} (ECI) by smoothing the quantile loss function. ECI introduces a continuous and adaptive feedback scale with the miscoverage error, rather than simple binary feedback in existing methods. We establish a long-term coverage guarantee for ECI under arbitrary dependence and distribution shift. The extensive experimental results show that ECI can achieve valid miscoverage control and output tighter prediction sets than other baselines.

### Orlicz-Sobolev Transport for Unbalanced Measures on a Graph 
[[arxiv](https://arxiv.org/abs/2502.00739)] [[cool](https://papers.cool/arxiv/2502.00739)] [[pdf](https://arxiv.org/pdf/2502.00739)]
> **Authors**: Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Moving beyond $L^p$ geometric structure, Orlicz-Wasserstein (OW) leverages a specific class of convex functions for Orlicz geometric structure. While OW remarkably helps to advance certain machine learning approaches, it has a high computational complexity due to its two-level optimization formula. Recently, Le et al. (2024) exploits graph structure to propose generalized Sobolev transport (GST), i.e., a scalable variant for OW. However, GST assumes that input measures have the same mass. Unlike optimal transport (OT), it is nontrivial to incorporate a mass constraint to extend GST for measures on a graph, possibly having different total mass. In this work, we propose to take a step back by considering the entropy partial transport (EPT) for nonnegative measures on a graph. By leveraging Caffarelli & McCann (2010)'s observations, EPT can be reformulated as a standard complete OT between two corresponding balanced measures. Consequently, we develop a novel EPT with Orlicz geometric structure, namely Orlicz-EPT, for unbalanced measures on a graph. Especially, by exploiting the dual EPT formulation and geometric structures of the graph-based Orlicz-Sobolev space, we derive a novel regularization to propose Orlicz-Sobolev transport (OST). The resulting distance can be efficiently computed by simply solving a univariate optimization problem, unlike the high-computational two-level optimization problem for Orlicz-EPT. Additionally, we derive geometric structures for the OST and draw its relations to other transport distances. We empirically show that OST is several-order faster than Orlicz-EPT. We further illustrate preliminary evidences on the advantages of OST for document classification, and several tasks in topological data analysis.

### Scalable Sobolev IPM for Probability Measures on a Graph 
[[arxiv](https://arxiv.org/abs/2502.00737)] [[cool](https://papers.cool/arxiv/2502.00737)] [[pdf](https://arxiv.org/pdf/2502.00737)]
> **Authors**: Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We investigate the Sobolev IPM problem for probability measures supported on a graph metric space. Sobolev IPM is an important instance of integral probability metrics (IPM), and is obtained by constraining a critic function within a unit ball defined by the Sobolev norm. In particular, it has been used to compare probability measures and is crucial for several theoretical works in machine learning. However, to our knowledge, there are no efficient algorithmic approaches to compute Sobolev IPM effectively, which hinders its practical applications. In this work, we establish a relation between Sobolev norm and weighted $L^p$-norm, and leverage it to propose a \emph{novel regularization} for Sobolev IPM. By exploiting the graph structure, we demonstrate that the regularized Sobolev IPM provides a \emph{closed-form} expression for fast computation. This advancement addresses long-standing computational challenges, and paves the way to apply Sobolev IPM for practical applications, even in large-scale settings. Additionally, the regularized Sobolev IPM is negative definite. Utilizing this property, we design positive-definite kernels upon the regularized Sobolev IPM, and provide preliminary evidences of their advantages on document classification and topological data analysis for measures on a graph.

### Sampling Binary Data by Denoising through Score Functions 
[[arxiv](https://arxiv.org/abs/2502.00557)] [[cool](https://papers.cool/arxiv/2502.00557)] [[pdf](https://arxiv.org/pdf/2502.00557)]
> **Authors**: Francis Bach,Saeed Saremi
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Gaussian smoothing combined with a probabilistic framework for denoising via the empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are the two key ingredients in the success of score-based generative models in Euclidean spaces. Smoothing holds the key for easing the problem of learning and sampling in high dimensions, denoising is needed for recovering the original signal, and TMF ties these together via the score function of noisy data. In this work, we extend this paradigm to the problem of learning and sampling the distribution of binary data on the Boolean hypercube by adopting Bernoulli noise, instead of Gaussian noise, as a smoothing device. We first derive a TMF-like expression for the optimal denoiser for the Hamming loss, where a score function naturally appears. Sampling noisy binary data is then achieved using a Langevin-like sampler which we theoretically analyze for different noise levels. At high Bernoulli noise levels sampling becomes easy, akin to log-concave sampling in Euclidean spaces. In addition, we extend the sequential multi-measurement sampling of Saremi et al. (2024) to the binary setting where we can bring the "effective noise" down by sampling multiple noisy measurements at a fixed noise level, without the need for continuous-time stochastic processes. We validate our formalism and theoretical findings by experiments on synthetic data and binarized images.

### Transition Transfer $Q$-Learning for Composite Markov Decision Processes 
[[arxiv](https://arxiv.org/abs/2502.00534)] [[cool](https://papers.cool/arxiv/2502.00534)] [[pdf](https://arxiv.org/pdf/2502.00534)]
> **Authors**: Jinhang Chai,Elynn Chen,Lin Yang
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: To bridge the gap between empirical success and theoretical understanding in transfer reinforcement learning (RL), we study a principled approach with provable performance guarantees. We introduce a novel composite MDP framework where high-dimensional transition dynamics are modeled as the sum of a low-rank component representing shared structure and a sparse component capturing task-specific variations. This relaxes the common assumption of purely low-rank transition models, allowing for more realistic scenarios where tasks share core dynamics but maintain individual variations. We introduce UCB-TQL (Upper Confidence Bound Transfer Q-Learning), designed for transfer RL scenarios where multiple tasks share core linear MDP dynamics but diverge along sparse dimensions. When applying UCB-TQL to a target task after training on a source task with sufficient trajectories, we achieve a regret bound of $\tilde{O}(\sqrt{eH^5N})$ that scales independently of the ambient dimension. Here, $N$ represents the number of trajectories in the target task, while $e$ quantifies the sparse differences between tasks. This result demonstrates substantial improvement over single task RL by effectively leveraging their structural similarities. Our theoretical analysis provides rigorous guarantees for how UCB-TQL simultaneously exploits shared dynamics while adapting to task-specific variations.

### Variance Reduction via Resampling and Experience Replay 
[[arxiv](https://arxiv.org/abs/2502.00520)] [[cool](https://papers.cool/arxiv/2502.00520)] [[pdf](https://arxiv.org/pdf/2502.00520)]
> **Authors**: Jiale Han,Xiaowu Dai,Yuhua Zhu
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Experience replay is a foundational technique in reinforcement learning that enhances learning stability by storing past experiences in a replay buffer and reusing them during training. Despite its practical success, its theoretical properties remain underexplored. In this paper, we present a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, providing rigorous variance reduction guarantees. We apply this framework to policy evaluation tasks using the Least-Squares Temporal Difference (LSTD) algorithm and a Partial Differential Equation (PDE)-based model-free algorithm, demonstrating significant improvements in stability and efficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we extend the framework to kernel ridge regression, showing that the experience replay-based method reduces the computational cost from the traditional $O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing variance. Extensive numerical experiments validate our theoretical findings, demonstrating the broad applicability and effectiveness of experience replay in diverse machine learning tasks.

### Decentralized Inference for Spatial Data Using Low-Rank Models 
[[arxiv](https://arxiv.org/abs/2502.00309)] [[cool](https://papers.cool/arxiv/2502.00309)] [[pdf](https://arxiv.org/pdf/2502.00309)]
> **Authors**: Jianwei Shi,Sameh Abdulah,Ying Sun,Marc G. Genton
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 84 pages
- **标题**: None
- **领域**: 机器学习,机器学习,计算,方法论
- **Abstract**: Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework.

### Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions 
[[arxiv](https://arxiv.org/abs/2502.00302)] [[cool](https://papers.cool/arxiv/2502.00302)] [[pdf](https://arxiv.org/pdf/2502.00302)]
> **Authors**: Yixuan He,Aaron Sandel,David Wipf,Mihai Cucuringu,John Mitani,Gesine Reinert
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,优化与控制,统计理论
- **Abstract**: How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.

### Learning Difference-of-Convex Regularizers for Inverse Problems: A Flexible Framework with Theoretical Guarantees 
[[arxiv](https://arxiv.org/abs/2502.00240)] [[cool](https://papers.cool/arxiv/2502.00240)] [[pdf](https://arxiv.org/pdf/2502.00240)]
> **Authors**: Yasi Zhang,Oscar Leong
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,图像和视频处理,优化与控制
- **Abstract**: Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}.

### Supervised Quadratic Feature Analysis: An Information Geometry Approach to Dimensionality Reduction 
[[arxiv](https://arxiv.org/abs/2502.00168)] [[cool](https://papers.cool/arxiv/2502.00168)] [[pdf](https://arxiv.org/pdf/2502.00168)]
> **Authors**: Daniel Herrera-Esposito,Johannes Burge
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-04
> **comment**: 18 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,机器学习,微分几何,统计理论
- **Abstract**: Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier.

## 其他论文

- [Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools](https://arxiv.org/abs/2502.01966)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,cs.DC,cs.CR,cs.SE in whitelist
- [LeaFi: Data Series Indexes on Steroids with Learned Filters](https://arxiv.org/abs/2502.01836)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Efficient Denial of Service Attack Detection in IoT using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2502.01835)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Designing Technologies for Value-based Mental Healthcare: Centering Clinicians' Perspectives on Outcomes Data Specification, Collection, and Use](https://arxiv.org/abs/2502.01829)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Relatively-Secure LLM-Based Steganography via Constrained Markov Decision Processes](https://arxiv.org/abs/2502.01827)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Firewalls to Secure Dynamic LLM Agentic Networks](https://arxiv.org/abs/2502.01822)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.CY in whitelist
- [Physics-Informed Surrogates for Temperature Prediction of Multi-Tracks in Laser Powder Bed Fusion](https://arxiv.org/abs/2502.01820)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Estimating Network Models using Neural Networks](https://arxiv.org/abs/2502.01810)
  - **标题**: None
  - **Filtered Reason**: none of stat.CO,stat.ML,cs.SI,econ.EM in whitelist
- [MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults](https://arxiv.org/abs/2502.01801)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Harmful Terms and Where to Find Them: Measuring and Modeling Unfavorable Financial Terms and Conditions in Shopping Websites at Scale](https://arxiv.org/abs/2502.01798)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Training Users Against Human and GPT-4 Generated Social Engineering Attacks](https://arxiv.org/abs/2502.01764)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Expected Return Symmetries](https://arxiv.org/abs/2502.01711)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA in whitelist
- [Imperfect Knowledge Management -- A Case Study in a Chilean Manufacturing Company](https://arxiv.org/abs/2502.01656)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY,cs.DB in whitelist
- [AI Load Dynamics--A Power Electronics Perspective](https://arxiv.org/abs/2502.01647)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF,cs.AR in whitelist
- [TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets](https://arxiv.org/abs/2502.01506)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,cs.CY in whitelist
- [The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI Collaboration](https://arxiv.org/abs/2502.01493)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Quantum Quandaries: Unraveling Encoding Vulnerabilities in Quantum Neural Networks](https://arxiv.org/abs/2502.01486)
  - **标题**: None
  - **Filtered Reason**: none of quant-ph,cs.ET in whitelist
- [Human-Agent Interaction in Synthetic Social Networks: A Framework for Studying Online Polarization](https://arxiv.org/abs/2502.01340)
  - **标题**: None
  - **Filtered Reason**: none of physics.soc-ph,cs.SI in whitelist
- [Neural Preconditioning Operator for Efficient PDE Solves](https://arxiv.org/abs/2502.01337)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [IBB: Fast Burrows-Wheeler Transform Construction for Length-Diverse DNA Data](https://arxiv.org/abs/2502.01327)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions](https://arxiv.org/abs/2502.01325)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [DietGlance: Dietary Monitoring and Personalized Analysis at a Glance with Knowledge-Empowered AI Assistant](https://arxiv.org/abs/2502.01317)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Expert-Generated Privacy Q&A Dataset for Conversational AI and User Study Insights](https://arxiv.org/abs/2502.01306)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Towards Autonomous Wood-Log Grasping with a Forestry Crane: Simulator and Benchmarking](https://arxiv.org/abs/2502.01304)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Augmented Knowledge Graph Querying leveraging LLMs](https://arxiv.org/abs/2502.01298)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [OCTOPINF: Workload-Aware Inference Serving for Edge Video Analytics](https://arxiv.org/abs/2502.01277)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Spectral Entanglement Fingerprinting: A Novel Framework for Ransomware Detection Using Cross-Frequency Anomalous Waveform Signatures](https://arxiv.org/abs/2502.01275)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Peering Behind the Shield: Guardrail Identification in Large Language Models](https://arxiv.org/abs/2502.01241)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Societal Attitudes Toward Service Robots: Adore, Abhor, Ignore, or Unsure?](https://arxiv.org/abs/2502.01231)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [How Good are Learned Cost Models, Really? Insights from Query Optimization Tasks](https://arxiv.org/abs/2502.01229)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Solgenia -- A Test Vessel Toward Energy-Efficient Autonomous Water Taxi Applications](https://arxiv.org/abs/2502.01207)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [PerfSeer: An Efficient and Accurate Deep Learning Models Performance Predictor](https://arxiv.org/abs/2502.01206)
  - **标题**: None
  - **Filtered Reason**: none of cs.PF in whitelist
- [On the impact of the parametrization of deep convolutional neural networks on post-training quantization](https://arxiv.org/abs/2502.01156)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs](https://arxiv.org/abs/2502.01071)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [DH-TRNG: A Dynamic Hybrid TRNG with Ultra-High Throughput and Area-Energy Efficiency](https://arxiv.org/abs/2502.01066)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using Newton-Raphson Method (HFASSON) and its application in CR-VANET](https://arxiv.org/abs/2502.01053)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [AutoDDG: Automated Dataset Description Generation using Large Language Models](https://arxiv.org/abs/2502.01050)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Simulating Application Behavior for Network Monitoring and Security](https://arxiv.org/abs/2502.01049)
  - **标题**: None
  - **Filtered Reason**: none of stat.AP,cs.NI in whitelist
- [TxnSails: Achieving Serializable Transaction Scheduling with Self-Adaptive Isolation Level Selection](https://arxiv.org/abs/2502.00991)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Detection of Distributed Denial of Service Attacks based on Machine Learning Algorithms](https://arxiv.org/abs/2502.00975)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [AI-Powered Spearphishing Cyber Attacks: Fact or Fiction?](https://arxiv.org/abs/2502.00961)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.CY in whitelist
- [ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation](https://arxiv.org/abs/2502.00893)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Data Fusion for Full-Range Response Reconstruction via Diffusion Models](https://arxiv.org/abs/2502.00795)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [AGNNCert: Defending Graph Neural Networks against Arbitrary Perturbations with Deterministic Certification](https://arxiv.org/abs/2502.00765)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Generative AI for Analyzing Participatory Rural Appraisal Data: An Exploratory Case Study in Gender Research](https://arxiv.org/abs/2502.00763)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs](https://arxiv.org/abs/2502.00722)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [REAL: Reinforcement Learning-Enabled xApps for Experimental Closed-Loop Optimization in O-RAN with OSC RIC and srsRAN](https://arxiv.org/abs/2502.00715)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large Language Models](https://arxiv.org/abs/2502.00709)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Leveraging LLMs for Dynamic IoT Systems Generation through Mixed-Initiative Interaction](https://arxiv.org/abs/2502.00689)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [A Flexible Precision Scaling Deep Neural Network Accelerator with Efficient Weight Combination](https://arxiv.org/abs/2502.00687)
  - **标题**: None
  - **Filtered Reason**: none of eess.SY,cs.AR in whitelist
- [Towards Robust Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2502.00653)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Integrating Cybersecurity Frameworks into IT Security: A Comprehensive Analysis of Threat Mitigation Strategies and Adaptive Technologies](https://arxiv.org/abs/2502.00651)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Constructing AI ethics narratives based on real-world data: Human-AI collaboration in data-driven visual storytelling](https://arxiv.org/abs/2502.00637)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Patterns and Purposes: A Cross-Journal Analysis of AI Tool Usage in Academic Writing](https://arxiv.org/abs/2502.00632)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations](https://arxiv.org/abs/2502.00626)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Less is More: Simplifying Network Traffic Classification Leveraging RFCs](https://arxiv.org/abs/2502.00586)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.NI in whitelist
- [DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D Visual-Inertial Navigation based on IMU-Vision-Net](https://arxiv.org/abs/2502.00575)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Engineering Educators' Perspectives on the Impact of Generative AI in Higher Education](https://arxiv.org/abs/2502.00569)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Do neonates hear what we measure? Assessing neonatal ward soundscapes at the neonates ears](https://arxiv.org/abs/2502.00565)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
- [Assessment of ChatGPT for Engineering Statics Analysis](https://arxiv.org/abs/2502.00562)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Position: Evaluating Generative AI Systems is a Social Science Measurement Challenge](https://arxiv.org/abs/2502.00561)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited Communication](https://arxiv.org/abs/2502.00558)
  - **标题**: None
  - **Filtered Reason**: none of cs.MA in whitelist
- [Graph Data Management and Graph Machine Learning: Synergies and Opportunities](https://arxiv.org/abs/2502.00529)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [A Novel Approach to the Initial Value Problem with a complete validated algorithm](https://arxiv.org/abs/2502.00503)
  - **标题**: None
  - **Filtered Reason**: none of math.NA,cs.SC in whitelist
- [The Societal Response to Potentially Sentient AI](https://arxiv.org/abs/2502.00388)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Towards a Supporting Framework for Neuro-Developmental Disorder: Considering Artificial Intelligence, Serious Games and Eye Tracking](https://arxiv.org/abs/2502.00381)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [GPU-Accelerated Modified Bessel Function of the Second Kind for Gaussian Processes](https://arxiv.org/abs/2502.00356)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [A Novel Approach to Translate Structural Aggregation Queries to MapReduce Code](https://arxiv.org/abs/2502.00343)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.DB in whitelist
- [SocratiQ: A Generative AI-Powered Learning Companion for Personalized Education and Broader Accessibility](https://arxiv.org/abs/2502.00341)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Agentic AI: Autonomy, Accountability, and the Algorithmic Society](https://arxiv.org/abs/2502.00289)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Late Breaking Results: Leveraging Approximate Computing for Carbon-Aware DNN Accelerators](https://arxiv.org/abs/2502.00286)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [How Generative AI supports human in conceptual design](https://arxiv.org/abs/2502.00283)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Can a Machine Feel Vibrations?: A Framework for Vibrotactile Sensation and Emotion Prediction via a Neural Network](https://arxiv.org/abs/2502.00268)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Enhancing Psychotherapeutic Alliance in College: When and How to Integrate Multimodal Large Language Models in Psychotherapy](https://arxiv.org/abs/2502.00229)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Team Size and Its Negative Impact on the Disruption Index](https://arxiv.org/abs/2502.00219)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [Asynchronous Fault-Tolerant Language Decidability for Runtime Verification of Distributed Systems](https://arxiv.org/abs/2502.00191)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Physics-informed Split Koopman Operators for Data-efficient Soft Robotic Simulation](https://arxiv.org/abs/2502.00162)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Open RAN Slicing with Quantum Optimization](https://arxiv.org/abs/2502.00142)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Decoding User Concerns in AI Health Chatbots: An Exploration of Security and Privacy in App Reviews](https://arxiv.org/abs/2502.00067)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.ET,cs.CR in whitelist
- [Digital Health Innovations for Screening and Mitigating Mental Health Impacts of Adverse Childhood Experiences: Narrative Review](https://arxiv.org/abs/2502.00066)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [GitHub Stargazers | Building Graph- and Edge-level Prediction Algorithms for Developer Social Networks](https://arxiv.org/abs/2502.00058)
  - **标题**: None
  - **Filtered Reason**: none of cs.SI in whitelist
- [Safeguarding the Future of Mobility: Cybersecurity Issues and Solutions for Infrastructure Associated with Electric Vehicle Charging](https://arxiv.org/abs/2502.00035)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [GNN-based Anchor Embedding for Exact Subgraph Matching](https://arxiv.org/abs/2502.00031)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB,cs.SI in whitelist
- [VRank: Enhancing Verilog Code Generation from Large Language Models via Self-Consistency](https://arxiv.org/abs/2502.00028)
  - **标题**: None
  - **Filtered Reason**: none of cs.PL,cs.AR in whitelist
- [Large Language Models for Education: ChemTAsk -- An Open-Source Paradigm for Automated Q&A in the Graduate Classroom](https://arxiv.org/abs/2502.00016)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Algorithmic Bias and the New Chicago School](https://arxiv.org/abs/2502.00014)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Lessons from complexity theory for AI governance](https://arxiv.org/abs/2502.00012)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [IntelliChain: An Integrated Framework for Enhanced Socratic Method Dialogue with LLMs and Knowledge Graphs](https://arxiv.org/abs/2502.00010)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship](https://arxiv.org/abs/2502.00009)
  - **标题**: None
  - **Filtered Reason**: none of econ.GN,cs.CY in whitelist
- [The Dead Internet Theory: A Survey on Artificial Interactions and the Future of Social Media](https://arxiv.org/abs/2502.00007)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Accelerating PageRank Algorithmic Tasks with a new Programmable Hardware Architecture](https://arxiv.org/abs/2502.00001)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
