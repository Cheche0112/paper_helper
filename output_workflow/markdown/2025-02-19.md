> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-19

共有343篇相关领域论文, 另有31篇其他

## 宇宙学和非银河系天体物理学(astro-ph.CO:Cosmology and Nongalactic Astrophysics)

### Learning the Universe: Learning to Optimize Cosmic Initial Conditions with Non-Differentiable Structure Formation Models 
[[arxiv](https://arxiv.org/abs/2502.13243)] [[cool](https://papers.cool/arxiv/2502.13243)] [[pdf](https://arxiv.org/pdf/2502.13243)]
> **Authors**: Ludvig Doeser,Metin Ata,Jens Jasche
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 pages, 13 figures
- **标题**: None
- **领域**: 宇宙学和非银河系天体物理学,星系天体物理学,机器学习
- **Abstract**: Making the most of next-generation galaxy clustering surveys requires overcoming challenges in complex, non-linear modelling to access the significant amount of information at smaller cosmological scales. Field-level inference has provided a unique opportunity beyond summary statistics to use all of the information of the galaxy distribution. However, addressing current challenges often necessitates numerical modelling that incorporates non-differentiable components, hindering the use of efficient gradient-based inference methods. In this paper, we introduce Learning the Universe by Learning to Optimize (LULO), a gradient-free framework for reconstructing the 3D cosmic initial conditions. Our approach advances deep learning to train an optimization algorithm capable of fitting state-of-the-art non-differentiable simulators to data at the field level. Importantly, the neural optimizer solely acts as a search engine in an iterative scheme, always maintaining full physics simulations in the loop, ensuring scalability and reliability. We demonstrate the method by accurately reconstructing initial conditions from $M_{200\mathrm{c}}$ halos identified in a dark matter-only $N$-body simulation with a spherical overdensity algorithm. The derived dark matter and halo overdensity fields exhibit $\geq80\%$ cross-correlation with the ground truth into the non-linear regime $k \sim 1h$ Mpc$^{-1}$. Additional cosmological tests reveal accurate recovery of the power spectra, bispectra, halo mass function, and velocities. With this work, we demonstrate a promising path forward to non-linear field-level inference surpassing the requirement of a differentiable physics model.

## 无序系统和神经网络(cond-mat.dis-nn:Disordered Systems and Neural Networks)

### Evidence of Replica Symmetry Breaking under the Nishimori conditions in epidemic inference on graphs 
[[arxiv](https://arxiv.org/abs/2502.13249)] [[cool](https://papers.cool/arxiv/2502.13249)] [[pdf](https://arxiv.org/pdf/2502.13249)]
> **Authors**: Alfredo Braunstein,Louise Budzynski,Matteo Mariani,Federico Ricci-Tersenghi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 17 pages, 7 figures
- **标题**: None
- **领域**: 无序系统和神经网络,统计力学,信息论,机器学习,物理与社会
- **Abstract**: In Bayesian inference, computing the posterior distribution from the data is typically a non-trivial problem, which usually requires approximations such as mean-field approaches or numerical methods, like the Monte Carlo Markov Chain. Being a high-dimensional distribution over a set of correlated variables, the posterior distribution can undergo the notorious replica symmetry breaking transition. When it happens, several mean-field methods and virtually every Monte Carlo scheme can not provide a reasonable approximation to the posterior and its marginals. Replica symmetry is believed to be guaranteed whenever the data is generated with known prior and likelihood distributions, namely under the so-called Nishimori conditions. In this paper, we break this belief, by providing a counter-example showing that, under the Nishimori conditions, replica symmetry breaking arises. Introducing a simple, geometrical model that can be thought of as a patient zero retrieval problem in a highly infectious regime of the epidemic Susceptible-Infectious model, we show that under the Nishimori conditions, there is evidence of replica symmetry breaking. We achieve this result by computing the instability of the replica symmetric cavity method toward the one step replica symmetry broken phase. The origin of this phenomenon -- replica symmetry breaking under the Nishimori conditions -- is likely due to the correlated disorder appearing in the epidemic models.

## 人工智能(cs.AI:Artificial Intelligence)

### Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation 
[[arxiv](https://arxiv.org/abs/2502.13392)] [[cool](https://papers.cool/arxiv/2502.13392)] [[pdf](https://arxiv.org/pdf/2502.13392)]
> **Authors**: Jim Dai,Manxi Wu,Zhanhao Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Pioneering companies such as Waymo have deployed robo-taxi services in several U.S. cities. These robo-taxis are electric vehicles, and their operations require the joint optimization of ride matching, vehicle repositioning, and charging scheduling in a stochastic environment. We model the operations of the ride-hailing system with robo-taxis as a discrete-time, average reward Markov Decision Process with infinite horizon. As the fleet size grows, the dispatching is challenging as the set of system state and the fleet dispatching action set grow exponentially with the number of vehicles. To address this, we introduce a scalable deep reinforcement learning algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the action space using atomic action decomposition. We evaluate our algorithm using real-world NYC for-hire vehicle data and we measure the performance using the long-run average reward achieved by the dispatching policy relative to a fluid-based reward upper bound. Our experiments demonstrate the superior performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct extensive numerical experiments to analyze the efficient allocation of charging facilities and assess the impact of vehicle range and charger speed on fleet performance.

### Reasoning with Reinforced Functional Token Tuning 
[[arxiv](https://arxiv.org/abs/2502.13389)] [[cool](https://papers.cool/arxiv/2502.13389)] [[pdf](https://arxiv.org/pdf/2502.13389)]
> **Authors**: Kongcheng Zhang,Qi Yao,Baisheng Lai,Jiaxing Huang,Wenkai Fang,Dacheng Tao,Mingli Song,Shunyu Liu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that empowers Large Language Models (LLMs) with self-play learn-to-reason capabilities. Unlike prior prompt-driven reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g., <analyze>, <verify>, <refine>) directly into the model vocabulary, enabling chain-of-thought construction with diverse human-like reasoning behaviors. Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning. Extensive experiments demonstrate the superiority of the proposed RFTT on mathematical benchmarks, significantly boosting Qwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to 60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently improves with more search rollouts at inference time. Our code is available at https://github.com/sastpg/RFTT.

### Reflection of Episodes: Learning to Play Game from Expert and Self Experiences 
[[arxiv](https://arxiv.org/abs/2502.13388)] [[cool](https://papers.cool/arxiv/2502.13388)] [[pdf](https://arxiv.org/pdf/2502.13388)]
> **Authors**: Xiaojie Xu,Zongyuan Li,Chang Lu,Runnan Qi,Yanan Ni,Lumin Jiang,Xiangbei Liu,Xuebo Zhang,Yongchun Fang,Kuihua Huang,Xian Guo,Zhanghua Wu,Zhenya Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: StarCraft II is a complex and dynamic real-time strategy (RTS) game environment, which is very suitable for artificial intelligence and reinforcement learning research. To address the problem of Large Language Model(LLM) learning in complex environments through self-reflection, we propose a Reflection of Episodes(ROE) framework based on expert experience and self-experience. This framework first obtains key information in the game through a keyframe selection method, then makes decisions based on expert experience and self-experience. After a game is completed, it reflects on the previous experience to obtain new self-experience. Finally, in the experiment, our method beat the robot under the Very Hard difficulty in TextStarCraft II. We analyze the data of the LLM in the process of the game in detail, verified its effectiveness.

### Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI 
[[arxiv](https://arxiv.org/abs/2502.13373)] [[cool](https://papers.cool/arxiv/2502.13373)] [[pdf](https://arxiv.org/pdf/2502.13373)]
> **Authors**: Swati Kar,Soumyabrata Dey,Mahesh K Banavar,Shahnewaz Karim Sakib
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's primary objectives include efficiently navigating the environment, reaching a target, and selectively engaging or evading an enemy. A reward function balances these goals while optimized hyperparameters enhance learning efficiency. Results show more than 80\% task completion rate, demonstrating effective decision-making. To enhance transparency, the jet's action choices are analyzed by comparing the rewards of the actual chosen action (factual action) with those of alternate actions (counterfactual actions), providing insights into the decision-making rationale. This study illustrates DRL's potential for multi-objective problem-solving with explainable AI. Project page is available at: \href{https://github.com/swatikar95/Autonomous-Fighter-Jet-Navigation-and-Combat}{Project GitHub Link}.

### Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13313)] [[cool](https://papers.cool/arxiv/2502.13313)] [[pdf](https://arxiv.org/pdf/2502.13313)]
> **Authors**: Soumi Das,Camila Kolling,Mohammad Aflah Khan,Mahsa Amani,Bishwamittra Ghosh,Qinyuan Wu,Till Speicher,Krishna P. Gummadi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: This is a work in progress. The draft may change in future
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: We study the inherent trade-offs in minimizing privacy risks and maximizing utility, while maintaining high computational efficiency, when fine-tuning large language models (LLMs). A number of recent works in privacy research have attempted to mitigate privacy risks posed by memorizing fine-tuning data by using differentially private training methods (e.g., DP), albeit at a significantly higher computational cost (inefficiency). In parallel, several works in systems research have focussed on developing (parameter) efficient fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether such efficient methods enhance or diminish privacy risks. In this paper, we investigate this gap and arrive at a surprising conclusion: efficient fine-tuning methods like LoRA mitigate privacy risks similar to private fine-tuning methods like DP. Our empirical finding directly contradicts prevailing wisdom that privacy and efficiency objectives are at odds during fine-tuning. Our finding is established by (a) carefully defining measures of privacy and utility that distinguish between memorizing sensitive and non-sensitive tokens in training and test datasets used in fine-tuning and (b) extensive evaluations using multiple open-source language models from Pythia, Gemma, and Llama families and different domain-specific datasets.

### Demonstrating specification gaming in reasoning models 
[[arxiv](https://arxiv.org/abs/2502.13295)] [[cool](https://papers.cool/arxiv/2502.13295)] [[pdf](https://arxiv.org/pdf/2502.13295)]
> **Authors**: Alexander Bondarenko,Denis Volk,Dmitrii Volkov,Jeffrey Ladish
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack. We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.

### Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment 
[[arxiv](https://arxiv.org/abs/2502.13170)] [[cool](https://papers.cool/arxiv/2502.13170)] [[pdf](https://arxiv.org/pdf/2502.13170)]
> **Authors**: Yuze Zhao,Tianyun Ji,Wenjun Feng,Zhenya Huang,Qi Liu,Zhiding Liu,Yixiao Ma,Kai Zhang,Enhong Chen
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: ICLR 2025 Poster;23 pages, 7 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs. We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at https://github.com/TnTWoW/code_reasoning.

### AIDE: AI-Driven Exploration in the Space of Code 
[[arxiv](https://arxiv.org/abs/2502.13138)] [[cool](https://papers.cool/arxiv/2502.13138)] [[pdf](https://arxiv.org/pdf/2502.13138)]
> **Authors**: Zhengyao Jiang,Dominik Schmidt,Dhruv Srikanth,Dixing Xu,Ian Kaplan,Deniss Jacenko,Yuxiang Wu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-and-error as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.

### Theorem Prover as a Judge for Synthetic Data Generation 
[[arxiv](https://arxiv.org/abs/2502.13137)] [[cool](https://papers.cool/arxiv/2502.13137)] [[pdf](https://arxiv.org/pdf/2502.13137)]
> **Authors**: Joshua Ong Jun Leang,Giwon Hong,Wenda Li,Shay B. Cohen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The demand for synthetic data in mathematical reasoning has increased due to its potential to enhance the mathematical capabilities of large language models (LLMs). However, ensuring the validity of intermediate reasoning steps remains a significant challenge, affecting data quality. While formal verification via theorem provers effectively validates LLM reasoning, the autoformalisation of mathematical proofs remains error-prone. In response, we introduce iterative autoformalisation, an approach that iteratively refines theorem prover formalisation to mitigate errors, thereby increasing the execution rate on the Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to rigorously assess LLM intermediate reasoning, effectively integrating autoformalisation with synthetic data generation. Finally, we present Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that replaces human annotation with theorem prover feedback in Reinforcement Learning from Human Feedback (RLHF). Across multiple LLMs, applying TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA.

### Rethinking Diverse Human Preference Learning through Principal Component Analysis 
[[arxiv](https://arxiv.org/abs/2502.13131)] [[cool](https://papers.cool/arxiv/2502.13131)] [[pdf](https://arxiv.org/pdf/2502.13131)]
> **Authors**: Feng Luo,Rui Yang,Hao Sun,Chunyuan Deng,Jiarui Yao,Jingyan Shen,Huan Zhang,Hanjie Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 14 pages
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.

### MatterChat: A Multi-Modal LLM for Material Science 
[[arxiv](https://arxiv.org/abs/2502.13107)] [[cool](https://papers.cool/arxiv/2502.13107)] [[pdf](https://arxiv.org/pdf/2502.13107)]
> **Authors**: Yingheng Tang,Wenbin Xu,Jie Cao,Jianzhu Ma,Weilu Gao,Steve Farrell,Benjamin Erichson,Michael W. Mahoney,Andy Nonaka,Zhi Yao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Understanding and predicting the properties of inorganic materials is crucial for accelerating advancements in materials science and driving applications in energy, electronics, and beyond. Integrating material structure data with language-based information through multi-modal large language models (LLMs) offers great potential to support these efforts by enhancing human-AI interaction. However, a key challenge lies in integrating atomic structures at full resolution into LLMs. In this work, we introduce MatterChat, a versatile structure-aware multi-modal LLM that unifies material structural data and textual inputs into a single cohesive model. MatterChat employs a bridging module to effectively align a pretrained machine learning interatomic potential with a pretrained LLM, reducing training costs and enhancing flexibility. Our results demonstrate that MatterChat significantly improves performance in material property prediction and human-AI interaction, surpassing general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in applications such as more advanced scientific reasoning and step-by-step material synthesis.

### Interactive Agents to Overcome Ambiguity in Software Engineering 
[[arxiv](https://arxiv.org/abs/2502.13069)] [[cool](https://papers.cool/arxiv/2502.13069)] [[pdf](https://arxiv.org/pdf/2502.13069)]
> **Authors**: Sanidhya Vijayvargiya,Xuhui Zhou,Akhila Yerukola,Maarten Sap,Graham Neubig
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 5 figures
- **标题**: None
- **领域**: 人工智能
- **Abstract**: AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements.

### AI-Assisted Decision Making with Human Learning 
[[arxiv](https://arxiv.org/abs/2502.13062)] [[cool](https://papers.cool/arxiv/2502.13062)] [[pdf](https://arxiv.org/pdf/2502.13062)]
> **Authors**: Gali Noti,Kate Donahue,Jon Kleinberg,Sigal Oren
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论,人机交互
- **Abstract**: AI systems increasingly support human decision-making. In many cases, despite the algorithm's superior performance, the final decision remains in human hands. For example, an AI may assist doctors in determining which diagnostic tests to run, but the doctor ultimately makes the diagnosis. This paper studies such AI-assisted decision-making settings, where the human learns through repeated interactions with the algorithm. In our framework, the algorithm -- designed to maximize decision accuracy according to its own model -- determines which features the human can consider. The human then makes a prediction based on their own less accurate model. We observe that the discrepancy between the algorithm's model and the human's model creates a fundamental tradeoff. Should the algorithm prioritize recommending more informative features, encouraging the human to recognize their importance, even if it results in less accurate predictions in the short term until learning occurs? Or is it preferable to forgo educating the human and instead select features that align more closely with their existing understanding, minimizing the immediate cost of learning? This tradeoff is shaped by the algorithm's time-discounted objective and the human's learning ability. Our results show that optimal feature selection has a surprisingly clean combinatorial characterization, reducible to a stationary sequence of feature subsets that is tractable to compute. As the algorithm becomes more "patient" or the human's learning improves, the algorithm increasingly selects more informative features, enhancing both prediction accuracy and the human's understanding. Notably, early investment in learning leads to the selection of more informative features than a later investment. We complement our analysis by showing that the impact of errors in the algorithm's knowledge is limited as it does not make the prediction directly.

### Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks 
[[arxiv](https://arxiv.org/abs/2502.13025)] [[cool](https://papers.cool/arxiv/2502.13025)] [[pdf](https://arxiv.org/pdf/2502.13025)]
> **Authors**: Markus J. Buehler
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,材料科学,计算语言学,机器学习
- **Abstract**: We present an agentic, autonomous graph expansion framework that iteratively structures and refines knowledge in situ. Unlike conventional knowledge graph construction methods relying on static extraction or single-pass learning, our approach couples a reasoning-native large language model with a continually updated graph representation. At each step, the system actively generates new concepts and relationships, merges them into a global graph, and formulates subsequent prompts based on its evolving structure. Through this feedback-driven loop, the model organizes information into a scale-free network characterized by hub formation, stable modularity, and bridging nodes that link disparate knowledge clusters. Over hundreds of iterations, new nodes and edges continue to appear without saturating, while centrality measures and shortest path distributions evolve to yield increasingly distributed connectivity. Our analysis reveals emergent patterns, such as the rise of highly connected 'hub' concepts and the shifting influence of 'bridge' nodes, indicating that agentic, self-reinforcing graph construction can yield open-ended, coherent knowledge structures. Applied to materials design problems, we present compositional reasoning experiments by extracting node-specific and synergy-level principles to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that transcend rote summarization and strengthen the framework's potential for open-ended scientific discovery. We discuss other applications in scientific discovery and outline future directions for enhancing scalability and interpretability.

### Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks 
[[arxiv](https://arxiv.org/abs/2502.13006)] [[cool](https://papers.cool/arxiv/2502.13006)] [[pdf](https://arxiv.org/pdf/2502.13006)]
> **Authors**: Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Automated Planning algorithms require a model of the domain that specifies the preconditions and effects of each action. Obtaining such a domain model is notoriously hard. Algorithms for learning domain models exist, yet it remains unclear whether learning a domain model and planning is an effective approach for numeric planning environments, i.e., where states include discrete and numeric state variables. In this work, we explore the benefits of learning a numeric domain model and compare it with alternative model-free solutions. As a case study, we use two tasks in Minecraft, a popular sandbox game that has been used as an AI challenge. First, we consider an offline learning setting, where a set of expert trajectories are available to learn from. This is the standard setting for learning domain models. We used the Numeric Safe Action Model Learning (NSAM) algorithm to learn a numeric domain model and solve new problems with the learned domain model and a numeric planner. We call this model-based solution NSAM_(+p), and compare it to several model-free Imitation Learning (IL) and Offline Reinforcement Learning (RL) algorithms. Empirical results show that some IL algorithms can learn faster to solve simple tasks, while NSAM_(+p) allows solving tasks that require long-term planning and enables generalizing to solve problems in larger environments. Then, we consider an online learning setting, where learning is done by moving an agent in the environment. For this setting, we introduce RAMP. In RAMP, observations collected during the agent's execution are used to simultaneously train an RL policy and learn a planning domain action model. This forms a positive feedback loop between the RL policy and the learned domain model. We demonstrate experimentally the benefits of using RAMP, showing that it finds more efficient plans and solves more problems than several RL baselines.

### You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations 
[[arxiv](https://arxiv.org/abs/2502.13001)] [[cool](https://papers.cool/arxiv/2502.13001)] [[pdf](https://arxiv.org/pdf/2502.13001)]
> **Authors**: Frederic Kirstein,Muneeb Khan,Jan Philip Wahle,Terry Ruas,Bela Gipp
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.

### Free Argumentative Exchanges for Explaining Image Classifiers 
[[arxiv](https://arxiv.org/abs/2502.12995)] [[cool](https://papers.cool/arxiv/2502.12995)] [[pdf](https://arxiv.org/pdf/2502.12995)]
> **Authors**: Avinash Kori,Antonio Rago,Francesca Toni
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 10 pages, 3 figures. To be published at AAMAS 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Deep learning models are powerful image classifiers but their opacity hinders their trustworthiness. Explanation methods for capturing the reasoning process within these classifiers faithfully and in a clear manner are scarce, due to their sheer complexity and size. We provide a solution for this problem by defining a novel method for explaining the outputs of image classifiers with debates between two agents, each arguing for a particular class. We obtain these debates as concrete instances of Free Argumentative eXchanges (FAXs), a novel argumentation-based multi-agent framework allowing agents to internalise opinions by other agents differently than originally stated. We define two metrics (consensus and persuasion rate) to assess the usefulness of FAXs as argumentative explanations for image classifiers. We then conduct a number of empirical experiments showing that FAXs perform well along these metrics as well as being more faithful to the image classifiers than conventional, non-argumentative explanation methods. All our implementations can be found at https://github.com/koriavinash1/FAX.

### Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger 
[[arxiv](https://arxiv.org/abs/2502.12961)] [[cool](https://papers.cool/arxiv/2502.12961)] [[pdf](https://arxiv.org/pdf/2502.12961)]
> **Authors**: Wenjun Li,Dexun Li,Kuicai Dong,Cong Zhang,Hao Zhang,Weiwen Liu,Yasheng Wang,Ruiming Tang,Yong Liu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs' internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks.

### Towards more Contextual Agents: An extractor-Generator Optimization Framework 
[[arxiv](https://arxiv.org/abs/2502.12926)] [[cool](https://papers.cool/arxiv/2502.12926)] [[pdf](https://arxiv.org/pdf/2502.12926)]
> **Authors**: Mourad Aouini,Jinan Loubani
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Language Model (LLM)-based agents have demonstrated remarkable success in solving complex tasks across a wide range of general-purpose applications. However, their performance often degrades in context-specific scenarios, such as specialized industries or research domains, where the absence of domain-relevant knowledge leads to imprecise or suboptimal outcomes. To address this challenge, our work introduces a systematic approach to enhance the contextual adaptability of LLM-based agents by optimizing their underlying prompts-critical components that govern agent behavior, roles, and interactions. Manually crafting optimized prompts for context-specific tasks is labor-intensive, error-prone, and lacks scalability. In this work, we introduce an Extractor-Generator framework designed to automate the optimization of contextual LLM-based agents. Our method operates through two key stages: (i) feature extraction from a dataset of gold-standard input-output examples, and (ii) prompt generation via a high-level optimization strategy that iteratively identifies underperforming cases and applies self-improvement techniques. This framework substantially improves prompt adaptability by enabling more precise generalization across diverse inputs, particularly in context-specific tasks where maintaining semantic consistency and minimizing error propagation are critical for reliable performance. Although developed with single-stage workflows in mind, the approach naturally extends to multi-stage workflows, offering broad applicability across various agent-based systems. Empirical evaluations demonstrate that our framework significantly enhances the performance of prompt-optimized agents, providing a structured and efficient approach to contextual LLM-based agents.

### Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.12876)] [[cool](https://papers.cool/arxiv/2502.12876)] [[pdf](https://arxiv.org/pdf/2502.12876)]
> **Authors**: Nandakishor M,Anjali M
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Creating personalized and adaptable conversational AI remains a key challenge. This paper introduces a Continuous Learning Conversational AI (CLCA) approach, implemented using A2C reinforcement learning, to move beyond static Large Language Models (LLMs). We use simulated sales dialogues, generated by LLMs, to train an A2C agent. This agent learns to optimize conversation strategies for personalization, focusing on engagement and delivering value. Our system architecture integrates reinforcement learning with LLMs for both data creation and response selection. This method offers a practical way to build personalized AI companions that evolve through continuous learning, advancing beyond traditional static LLM techniques.

### Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols 
[[arxiv](https://arxiv.org/abs/2502.12842)] [[cool](https://papers.cool/arxiv/2502.12842)] [[pdf](https://arxiv.org/pdf/2502.12842)]
> **Authors**: Kathrin Seßler,Arne Bewersdorff,Claudia Nerdel,Enkelejda Kasneci
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: This work has been submitted to the IJAIED for possible publication
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Effective feedback is essential for fostering students' success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feedback often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback quality of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent's performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student's work context. Qualitative analysis highlighted the LLM agent's limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that combining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators.

### Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research 
[[arxiv](https://arxiv.org/abs/2502.12669)] [[cool](https://papers.cool/arxiv/2502.12669)] [[pdf](https://arxiv.org/pdf/2502.12669)]
> **Authors**: Xiang Liu,Penglei Sun,Shuyan Chen,Longhan Zhang,Peijie Dong,Huajie You,Yongqi Zhang,Chang Yan,Xiaowen Chu,Tong-yi Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 23pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.

### RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts 
[[arxiv](https://arxiv.org/abs/2502.12589)] [[cool](https://papers.cool/arxiv/2502.12589)] [[pdf](https://arxiv.org/pdf/2502.12589)]
> **Authors**: Yu Zhang,Shujun Peng,Nengwu Wu,Xinhan Lin,Yang Hu,Jie Tang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recently, substantial advancements have been made in training language models to carry out step-by-step reasoning for solving intricate numerical reasoning tasks. Beyond the methods used to solve these problems, the structure and formulation of the problems themselves also play a crucial role in determining the performance of large language models. We observe that even small changes in the surface form of mathematical problems can have a profound impact on both the answer distribution and solve rate. This highlights the vulnerability of LLMs to surface-level variations, revealing its limited robustness when reasoning through complex problems. In this paper, we propose RM-PoT, a three-stage framework that integrates problem reformulation (RM), code-aided reasoning (PoT), and domain-aware few-shot learning to address these limitations. Our approach first reformulates the input problem into diverse surface forms to reduce structural bias, then retrieves five semantically aligned examples from a pre-constructed domain-specific question bank to provide contextual guidance, and finally generates executable Python code for precise computation.

### Exploring the Impact of Personality Traits on LLM Bias and Toxicity 
[[arxiv](https://arxiv.org/abs/2502.12566)] [[cool](https://papers.cool/arxiv/2502.12566)] [[pdf](https://arxiv.org/pdf/2502.12566)]
> **Authors**: Shuo Wang,Renhao Li,Xi Chen,Yulin Yuan,Derek F. Wong,Min Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: With the different roles that AI is expected to play in human life, imbuing large language models (LLMs) with different personalities has attracted increasing research interests. While the "personification" enhances human experiences of interactivity and adaptability of LLMs, it gives rise to critical concerns about content safety, particularly regarding bias, sentiment and toxicity of LLM generation. This study explores how assigning different personality traits to LLMs affects the toxicity and biases of their outputs. Leveraging the widely accepted HEXACO personality framework developed in social psychology, we design experimentally sound prompts to test three LLMs' performance on three toxic and bias benchmarks. The findings demonstrate the sensitivity of all three models to HEXACO personality traits and, more importantly, a consistent variation in the biases, negative sentiment and toxicity of their output. In particular, adjusting the levels of several personality traits can effectively reduce bias and toxicity in model performance, similar to humans' correlations between personality traits and toxic behaviors. The findings highlight the additional need to examine content safety besides the efficiency of training or fine-tuning methods for LLM personification. They also suggest a potential for the adjustment of personalities to be a simple and low-cost method to conduct controlled text generation.

## 计算语言学(cs.CL:Computation and Language)

### MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering 
[[arxiv](https://arxiv.org/abs/2502.13428)] [[cool](https://papers.cool/arxiv/2502.13428)] [[pdf](https://arxiv.org/pdf/2502.13428)]
> **Authors**: Guanming Xiong,Haochen Li,Wen Zhao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This study explores how to enhance the reasoning capabilities of large language models (LLMs) in knowledge base question answering (KBQA) by leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods are particularly challenging as these approaches require locating elements from knowledge bases and generating logical forms, demanding not only extensive annotated data but also strong reasoning capabilities. Although recent approaches leveraging LLMs as agents have demonstrated considerable potential, these studies are inherently constrained by their linear decision-making processes. To address this limitation, we propose a MCTS-based framework that enhances LLMs' reasoning capabilities through tree search methodology. We design a carefully designed step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our approach significantly outperforms linear decision-making methods, particularly in low-resource scenarios. Additionally, we contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision. Experimental results on the extended dataset demonstrate that our method achieves comparable performance to fully supervised models while using significantly less training data.

### TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition 
[[arxiv](https://arxiv.org/abs/2502.13422)] [[cool](https://papers.cool/arxiv/2502.13422)] [[pdf](https://arxiv.org/pdf/2502.13422)]
> **Authors**: Yuxiang Wang,Junhao Gan,Jianzhong Qi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,数据库
- **Abstract**: Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose TabSD, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. TabSD generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, SQL Verifier refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, SLQA and SEQA, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that TABSD outperforms the best-existing baseline models by 23.07%, 2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables.

### RLTHF: Targeted Human Feedback for LLM Alignment 
[[arxiv](https://arxiv.org/abs/2502.13417)] [[cool](https://papers.cool/arxiv/2502.13417)] [[pdf](https://arxiv.org/pdf/2502.13417)]
> **Authors**: Yifei Xu,Tusher Chakraborty,Emre Kıcıman,Bibek Aryal,Eduardo Rodrigues,Srinagesh Sharma,Roberto Estevao,Maria Angels de Luis Balaguer,Jessica Wolk,Rafael Padilha,Leonardo Nunes,Shobana Balakrishnan,Songwu Lu,Ranveer Chandra
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF's strategic data curation.

### Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13416)] [[cool](https://papers.cool/arxiv/2502.13416)] [[pdf](https://arxiv.org/pdf/2502.13416)]
> **Authors**: Ningke Li,Yahui Song,Kailong Wang,Yuekang Li,Ling Shi,Yi Liu,Haoyu Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 16 pages, under review. arXiv admin note: substantial text overlap with arXiv:2405.00648
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) face the challenge of hallucinations -- outputs that seem coherent but are actually incorrect. A particularly damaging type is fact-conflicting hallucination (FCH), where generated content contradicts established facts. Addressing FCH presents three main challenges: 1) Automatically constructing and maintaining large-scale benchmark datasets is difficult and resource-intensive; 2) Generating complex and efficient test cases that the LLM has not been trained on -- especially those involving intricate temporal features -- is challenging, yet crucial for eliciting hallucinations; and 3) Validating the reasoning behind LLM outputs is inherently difficult, particularly with complex logical relationships, as it requires transparency in the model's decision-making process. This paper presents Drowzee, an innovative end-to-end metamorphic testing framework that utilizes temporal logic to identify fact-conflicting hallucinations (FCH) in large language models (LLMs). Drowzee builds a comprehensive factual knowledge base by crawling sources like Wikipedia and uses automated temporal-logic reasoning to convert this knowledge into a large, extensible set of test cases with ground truth answers. LLMs are tested using these cases through template-based prompts, which require them to generate both answers and reasoning steps. To validate the reasoning, we propose two semantic-aware oracles that compare the semantic structure of LLM outputs to the ground truths. Across nine LLMs in nine different knowledge domains, experimental results show that Drowzee effectively identifies rates of non-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates of temporal-related hallucinations ranging from 16.7% to 39.2%.

### Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study 
[[arxiv](https://arxiv.org/abs/2502.13396)] [[cool](https://papers.cool/arxiv/2502.13396)] [[pdf](https://arxiv.org/pdf/2502.13396)]
> **Authors**: Wenwen Xie,Gray Gwizdz,Dongji Feng
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 5 pages, 5 tables, 1 figure
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While Large Language Models (LLMs) have emerged as promising tools for evaluating Natural Language Generation (NLG) tasks, their effectiveness is limited by their inability to appropriately weigh the importance of different topics, often overemphasizing minor details while undervaluing critical information, leading to misleading assessments. Our work proposes an efficient prompt design mechanism to address this specific limitation and provide a case study. Through strategic prompt engineering that incorporates explicit importance weighting mechanisms, we enhance using LLM-as-a-Judge ability to prioritize relevant information effectively, as demonstrated by an average improvement of 6% in the Human Alignment Rate (HAR) metric.

### MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification 
[[arxiv](https://arxiv.org/abs/2502.13383)] [[cool](https://papers.cool/arxiv/2502.13383)] [[pdf](https://arxiv.org/pdf/2502.13383)]
> **Authors**: Linzhuang Sun,Hao Liang,Jingxuan Wei,Bihui Yu,Tianpeng Li,Fan Yang,Zenan Zhou,Wentao Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别,机器学习
- **Abstract**: According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.

### Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor 
[[arxiv](https://arxiv.org/abs/2502.13374)] [[cool](https://papers.cool/arxiv/2502.13374)] [[pdf](https://arxiv.org/pdf/2502.13374)]
> **Authors**: Barys Liskavets,Shuvendu Roy,Maxim Ushakov,Mark Klibanov,Ali Etemad,Shane Luke
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rise of Large Language Models (LLMs) has led to significant interest in prompt compression, a technique aimed at reducing the length of input prompts while preserving critical information. However, the prominent approaches in prompt compression often require explicit questions or handcrafted templates for compression, limiting their generalizability. We propose Task-agnostic Prompt Compression (TPC), a novel framework that generalizes compression across tasks and domains without requiring input questions or templates. TPC generates a context-relevant task description using a task descriptor trained on a curated dataset of context and query pairs, and fine-tuned via reinforcement learning with a reward function designed to capture the most relevant information. The task descriptor is then utilized to compute the relevance of each sentence in the prompt to generate the compressed prompt. We introduce 3 model sizes (Base, Large, and Huge), where the largest model outperforms the existing state-of-the-art methods on LongBench and ZeroSCROLLS benchmarks, and our smallest model performs comparable to the existing solutions while being considerably smaller.

### Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval 
[[arxiv](https://arxiv.org/abs/2502.13369)] [[cool](https://papers.cool/arxiv/2502.13369)] [[pdf](https://arxiv.org/pdf/2502.13369)]
> **Authors**: Aditya Sharma,Luis Lara,Amal Zouaq,Christopher J. Pal
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.

### RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering 
[[arxiv](https://arxiv.org/abs/2502.13361)] [[cool](https://papers.cool/arxiv/2502.13361)] [[pdf](https://arxiv.org/pdf/2502.13361)]
> **Authors**: Sichu Liang,Linhai Zhang,Hongyu Zhu,Wenwen Wang,Yulan He,Deyu Zhou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.

### Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications 
[[arxiv](https://arxiv.org/abs/2502.13358)] [[cool](https://papers.cool/arxiv/2502.13358)] [[pdf](https://arxiv.org/pdf/2502.13358)]
> **Authors**: Yiming Zeng,Wanhao Yu,Zexin Li,Tao Ren,Yu Ma,Jinghan Cao,Xiyan Chen,Tingting Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have transformed natural language processing, yet they still struggle with direct text editing tasks that demand precise, context-aware modifications. While models like ChatGPT excel in text generation and analysis, their editing abilities often fall short, addressing only superficial issues rather than deeper structural or logical inconsistencies. In this work, we introduce a dual approach to enhance LLMs editing performance. First, we present InstrEditBench, a high-quality benchmark dataset comprising over 20,000 structured editing tasks spanning Wiki articles, LaTeX documents, code, and database Domain-specific Languages (DSL). InstrEditBench is generated using an innovative automated workflow that accurately identifies and evaluates targeted edits, ensuring that modifications adhere strictly to specified instructions without altering unrelated content. Second, we propose FineEdit, a specialized model trained on this curated benchmark. Experimental results demonstrate that FineEdit achieves significant improvements around {10\%} compared with Gemini on direct editing tasks, convincingly validating its effectiveness.

### Event Segmentation Applications in Large Language Model Enabled Automated Recall Assessments 
[[arxiv](https://arxiv.org/abs/2502.13349)] [[cool](https://papers.cool/arxiv/2502.13349)] [[pdf](https://arxiv.org/pdf/2502.13349)]
> **Authors**: Ryan A. Panela,Alex J. Barnett,Morgan D. Barense,Björn Herrmann
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 33 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Understanding how individuals perceive and recall information in their natural environments is critical to understanding potential failures in perception (e.g., sensory loss) and memory (e.g., dementia). Event segmentation, the process of identifying distinct events within dynamic environments, is central to how we perceive, encode, and recall experiences. This cognitive process not only influences moment-to-moment comprehension but also shapes event specific memory. Despite the importance of event segmentation and event memory, current research methodologies rely heavily on human judgements for assessing segmentation patterns and recall ability, which are subjective and time-consuming. A few approaches have been introduced to automate event segmentation and recall scoring, but validity with human responses and ease of implementation require further advancements. To address these concerns, we leverage Large Language Models (LLMs) to automate event segmentation and assess recall, employing chat completion and text-embedding models, respectively. We validated these models against human annotations and determined that LLMs can accurately identify event boundaries, and that human event segmentation is more consistent with LLMs than among humans themselves. Using this framework, we advanced an automated approach for recall assessments which revealed semantic similarity between segmented narrative events and participant recall can estimate recall performance. Our findings demonstrate that LLMs can effectively simulate human segmentation patterns and provide recall evaluations that are a scalable alternative to manual scoring. This research opens novel avenues for studying the intersection between perception, memory, and cognitive impairment using methodologies driven by artificial intelligence.

### Craw4LLM: Efficient Web Crawling for LLM Pretraining 
[[arxiv](https://arxiv.org/abs/2502.13347)] [[cool](https://papers.cool/arxiv/2502.13347)] [[pdf](https://arxiv.org/pdf/2502.13347)]
> **Authors**: Shi Yu,Zhiyuan Liu,Chenyan Xiong
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.

### Language Models are Few-Shot Graders 
[[arxiv](https://arxiv.org/abs/2502.13337)] [[cool](https://papers.cool/arxiv/2502.13337)] [[pdf](https://arxiv.org/pdf/2502.13337)]
> **Authors**: Chenyan Zhao,Mariana Silva,Seth Poulsen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.

### Language Models Can Predict Their Own Behavior 
[[arxiv](https://arxiv.org/abs/2502.13329)] [[cool](https://papers.cool/arxiv/2502.13329)] [[pdf](https://arxiv.org/pdf/2502.13329)]
> **Authors**: Dhananjay Ashok,Jonathan May
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated tokens. However, are there times when we can infer how the model will behave (e.g. abstain from answering a question) early in the computation, making generation unnecessary? We show that internal representation of input tokens alone can often precisely predict, not just the next token, but eventual behavior over the entire output sequence. We leverage this capacity and learn probes on internal states to create early warning (and exit) systems. Specifically, if the probes can confidently estimate the way the LM is going to behave, then the system will avoid generating tokens altogether and return the estimated behavior instead. On 27 text classification datasets spanning five different tasks, we apply this method to estimate the eventual answer of an LM under CoT prompting, reducing inference costs by 65% (average) while suffering an accuracy loss of no more than 1.4% (worst case). We demonstrate the potential of this method to pre-emptively identify when a model will abstain from answering a question, fail to follow output format specifications, or give a low-confidence response. We explore the limits of this capability, showing that probes generalize to unseen datasets, but perform worse when LM outputs are longer and struggle to predict properties that require access to knowledge that the models themselves lack. Encouragingly, performance scales with model size, suggesting applicability to the largest of models

### Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm 
[[arxiv](https://arxiv.org/abs/2502.13326)] [[cool](https://papers.cool/arxiv/2502.13326)] [[pdf](https://arxiv.org/pdf/2502.13326)]
> **Authors**: Vasudha Varadarajan,Syeda Mahwish,Xiaoran Liu,Julia Buffolino,Christian C. Luhmann,Ryan L. Boyd,H. Andrew Schwartz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic style of an individual talking about a recent decision they made. The participants then follow a classical decision-making experiment that captures their cognitive style, determined by how preferences change during a decision exercise. We find that language features, intended to capture cognitive style, can predict participants' decision style with moderate-to-high accuracy (AUC ~ 0.8), demonstrating that cognitive style can be partly captured and revealed by discourse patterns.

### Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare 
[[arxiv](https://arxiv.org/abs/2502.13319)] [[cool](https://papers.cool/arxiv/2502.13319)] [[pdf](https://arxiv.org/pdf/2502.13319)]
> **Authors**: Hiba Ahsan,Arnab Sen Sharma,Silvio Amir,David Bau,Byron C. Wallace
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We know from prior work that LLMs encode social biases, and that this manifests in clinical tasks. In this work we adopt tools from mechanistic interpretability to unveil sociodemographic representations and biases within LLMs in the context of healthcare. Specifically, we ask: Can we identify activations within LLMs that encode sociodemographic information (e.g., gender, race)? We find that gender information is highly localized in middle MLP layers and can be reliably manipulated at inference time via patching. Such interventions can surgically alter generated clinical vignettes for specific conditions, and also influence downstream clinical predictions which correlate with gender, e.g., patient risk of depression. We find that representation of patient race is somewhat more distributed, but can also be intervened upon, to a degree. To our knowledge, this is the first application of mechanistic interpretability methods to LLMs for healthcare.

### Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors 
[[arxiv](https://arxiv.org/abs/2502.13311)] [[cool](https://papers.cool/arxiv/2502.13311)] [[pdf](https://arxiv.org/pdf/2502.13311)]
> **Authors**: Jian Wang,Yinpei Dai,Yichi Zhang,Ziqiao Ma,Wenjie Li,Joyce Chai
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.

### Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations 
[[arxiv](https://arxiv.org/abs/2502.13310)] [[cool](https://papers.cool/arxiv/2502.13310)] [[pdf](https://arxiv.org/pdf/2502.13310)]
> **Authors**: Adib Mosharrof,Moghis Fereidouni,A. B. Siddique
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data. Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses. However, their task completion performance - measured by accurate execution of API calls - remains suboptimal, with the best models achieving only around 53% success in unseen domains. To improve task completion, we propose ZeroToD, a framework that incorporates a schema augmentation mechanism to enhance API call accuracy and overall task completion rates, particularly in out-of-domain settings. We also compare ZeroToD with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.

### Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via Prompt Chaining and Fine-Grained Feedback 
[[arxiv](https://arxiv.org/abs/2502.13298)] [[cool](https://papers.cool/arxiv/2502.13298)] [[pdf](https://arxiv.org/pdf/2502.13298)]
> **Authors**: Moghis Fereidouni,Md Sajid Ahmed,Adib Mosharrof,A. B. Siddique
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Task-oriented dialog (TOD) systems facilitate users in accomplishing complex, multi-turn tasks through natural language. While traditional approaches rely on extensive fine-tuning and annotated data for each domain, instruction-tuned large language models (LLMs) offer a more flexible alternative. However, LLMs struggle to reliably handle multi-turn task completion, particularly with accurately generating API calls and adapting to new domains without explicit demonstrations. To address these challenges, we propose RealTOD, a novel framework that enhances TOD systems through prompt chaining and fine-grained feedback mechanisms. Prompt chaining enables zero-shot domain adaptation via a two-stage prompting strategy, eliminating the need for human-curated demonstrations. Meanwhile, the fine-grained feedback mechanism improves task completion by verifying API calls against domain schemas and providing precise corrective feedback when errors are detected. We conduct extensive experiments on the SGD and BiTOD benchmarks using four LLMs. RealTOD improves API accuracy, surpassing AutoTOD by 37.74% on SGD and SimpleTOD by 11.26% on BiTOD. Human evaluations further confirm that LLMs integrated with RealTOD achieve superior task completion, fluency, and informativeness compared to existing methods.

### Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding 
[[arxiv](https://arxiv.org/abs/2502.13297)] [[cool](https://papers.cool/arxiv/2502.13297)] [[pdf](https://arxiv.org/pdf/2502.13297)]
> **Authors**: Yunpeng Xiao,Youpeng Zhao,Kai Shu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 12 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\% and 23.3\%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at https://github.com/24yearsoldstudent/Individual-NLU

### Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models 
[[arxiv](https://arxiv.org/abs/2502.13278)] [[cool](https://papers.cool/arxiv/2502.13278)] [[pdf](https://arxiv.org/pdf/2502.13278)]
> **Authors**: Sirisha Velampalli,Chandrashekar Muniyappa,Ashutosh Saxena
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: :68T50ACM Class:I.2.7; I.2.11
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Emojis are being frequently used in todays digital world to express from simple to complex thoughts more than ever before. Hence, they are also being used in sentiment analysis and targeted marketing campaigns. In this work, we performed sentiment analysis of Tweets as well as on emoji dataset from the Kaggle. Since tweets are sentences we have used Universal Sentence Encoder (USE) and Sentence Bidirectional Encoder Representations from Transformers (SBERT) end-to-end sentence embedding models to generate the embeddings which are used to train the Standard fully connected Neural Networks (NN), and LSTM NN models. We observe the text classification accuracy was almost the same for both the models around 98 percent. On the contrary, when the validation set was built using emojis that were not present in the training set then the accuracy of both the models reduced drastically to 70 percent. In addition, the models were also trained using the distributed training approach instead of a traditional singlethreaded model for better scalability. Using the distributed training approach, we were able to reduce the run-time by roughly 15% without compromising on accuracy. Finally, as part of explainable AI the Shap algorithm was used to explain the model behaviour and check for model biases for the given feature set.

### REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation 
[[arxiv](https://arxiv.org/abs/2502.13270)] [[cool](https://papers.cool/arxiv/2502.13270)] [[pdf](https://arxiv.org/pdf/2502.13270)]
> **Authors**: Dong-Ho Lee,Adyasha Maharana,Jay Pujara,Xiang Ren,Francesco Barbieri
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 20 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long-term, open-domain dialogue capabilities are essential for chatbots aiming to recall past interactions and demonstrate emotional intelligence (EI). Yet, most existing research relies on synthetic, LLM-generated data, leaving open questions about real-world conversational patterns. To address this gap, we introduce REALTALK, a 21-day corpus of authentic messaging app dialogues, providing a direct benchmark against genuine human interactions. We first conduct a dataset analysis, focusing on EI attributes and persona consistency to understand the unique challenges posed by real-world dialogues. By comparing with LLM-generated conversations, we highlight key differences, including diverse emotional expressions and variations in persona stability that synthetic dialogues often fail to capture. Building on these insights, we introduce two benchmark tasks: (1) persona simulation where a model continues a conversation on behalf of a specific user given prior dialogue context; and (2) memory probing where a model answers targeted questions requiring long-term memory of past interactions. Our findings reveal that models struggle to simulate a user solely from dialogue history, while fine-tuning on specific user chats improves persona emulation. Additionally, existing models face significant challenges in recalling and leveraging long-term context within real-world conversations.

### Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13260)] [[cool](https://papers.cool/arxiv/2502.13260)] [[pdf](https://arxiv.org/pdf/2502.13260)]
> **Authors**: Yingqian Cui,Pengfei He,Jingying Zeng,Hui Liu,Xianfeng Tang,Zhenwei Dai,Yan Han,Chen Luo,Jing Huang,Zhen Li,Suhang Wang,Yue Xing,Jiliang Tang,Qi He
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.

### HumT DumT: Measuring and controlling human-like language in LLMs 
[[arxiv](https://arxiv.org/abs/2502.13259)] [[cool](https://papers.cool/arxiv/2502.13259)] [[pdf](https://arxiv.org/pdf/2502.13259)]
> **Authors**: Myra Cheng,Sunny Yu,Dan Jurafsky
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to overreliance and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs. HumT also offers insights into the impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.

### Multilingual Language Model Pretraining using Machine-translated Data 
[[arxiv](https://arxiv.org/abs/2502.13252)] [[cool](https://papers.cool/arxiv/2502.13252)] [[pdf](https://arxiv.org/pdf/2502.13252)]
> **Authors**: Jiayi Wang,Yao Lu,Maurice Weber,Max Ryabinin,David Adelani,Yihong Chen,Raphael Tang,Pontus Stenetorp
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the quality and diversity of the available multilingual pretraining corpora. In this work, we find that machine-translated texts from a single high-quality source language can contribute significantly to the pretraining quality of multilingual LLMs. We translate FineWeb-Edu, a high-quality English web dataset, into nine languages, resulting in a 1.7-trillion-token dataset, which we call TransWebEdu and pretrain a 1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine non-English reasoning tasks, we show that TransWebLLM matches or outperforms state-of-the-art multilingual models trained using closed data, such as Llama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We demonstrate that adding less than 5% of TransWebEdu as domain-specific pretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian, Swahili, and Welsh understanding and commonsense reasoning tasks. To promote reproducibility, we release our corpus, models, and training pipeline under Open Source Initiative-approved licenses.

### Neural Attention Search 
[[arxiv](https://arxiv.org/abs/2502.13251)] [[cool](https://papers.cool/arxiv/2502.13251)] [[pdf](https://arxiv.org/pdf/2502.13251)]
> **Authors**: Difan Deng,Marius Lindauer
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

### Grounding LLM Reasoning with Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.13247)] [[cool](https://papers.cool/arxiv/2502.13247)] [[pdf](https://arxiv.org/pdf/2502.13247)]
> **Authors**: Alfonso Amayuelas,Joy Sain,Simerjot Kaur,Charese Smiley
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Knowledge Graphs (KGs) are valuable tools for representing relationships between entities in a structured format. Traditionally, these knowledge bases are queried to extract specific information. However, question-answering (QA) over such KGs poses a challenge due to the intrinsic complexity of natural language compared to the structured format and the size of these graphs. Despite these challenges, the structured nature of KGs can provide a solid foundation for grounding the outputs of Large Language Models (LLMs), offering organizations increased reliability and control. Recent advancements in LLMs have introduced reasoning methods at inference time to improve their performance and maximize their capabilities. In this work, we propose integrating these reasoning strategies with KGs to anchor every step or "thought" of the reasoning chains in KG data. Specifically, we evaluate both agentic and automated search methods across several reasoning strategies, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), using GRBench, a benchmark dataset for graph reasoning with domain-specific graphs. Our experiments demonstrate that this approach consistently outperforms baseline models, highlighting the benefits of grounding LLM reasoning processes in structured KG data.

### When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13246)] [[cool](https://papers.cool/arxiv/2502.13246)] [[pdf](https://arxiv.org/pdf/2502.13246)]
> **Authors**: Julia Mendelsohn,Ceren Budak
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会
- **Abstract**: Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. "water" or "vermin"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.

### SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering? 
[[arxiv](https://arxiv.org/abs/2502.13233)] [[cool](https://papers.cool/arxiv/2502.13233)] [[pdf](https://arxiv.org/pdf/2502.13233)]
> **Authors**: Yucheng Shi,Tianze Yang,Canyu Chen,Quanzheng Li,Tianming Liu,Xiang Li,Ninghao Liu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, three figures
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索,信息论
- **Abstract**: Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.

### Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation 
[[arxiv](https://arxiv.org/abs/2502.13207)] [[cool](https://papers.cool/arxiv/2502.13207)] [[pdf](https://arxiv.org/pdf/2502.13207)]
> **Authors**: Giorgio Franceschelli,Mirco Musolesi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习
- **Abstract**: Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.

### Private Text Generation by Seeding Large Language Model Prompts 
[[arxiv](https://arxiv.org/abs/2502.13193)] [[cool](https://papers.cool/arxiv/2502.13193)] [[pdf](https://arxiv.org/pdf/2502.13193)]
> **Authors**: Supriya Nagesh,Justin Y. Chen,Nina Mishra,Tal Wagner
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We explore how private synthetic text can be generated by suitably prompting a large language model (LLM). This addresses a challenge for organizations like hospitals, which hold sensitive text data like patient medical records, and wish to share it in order to train machine learning models for medical tasks, while preserving patient privacy. Methods that rely on training or finetuning a model may be out of reach, either due to API limits of third-party LLMs, or due to ethical and legal prohibitions on sharing the private data with the LLM itself. We propose Differentially Private Keyphrase Prompt Seeding (DP-KPS), a method that generates a private synthetic text corpus from a sensitive input corpus, by accessing an LLM only through privatized prompts. It is based on seeding the prompts with private samples from a distribution over phrase embeddings, thus capturing the input corpus while achieving requisite output diversity and maintaining differential privacy. We evaluate DP-KPS on downstream ML text classification tasks, and show that the corpora it generates preserve much of the predictive power of the original ones. Our findings offer hope that institutions can reap ML insights by privately sharing data with simple prompts and little compute.

### UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13141)] [[cool](https://papers.cool/arxiv/2502.13141)] [[pdf](https://arxiv.org/pdf/2502.13141)]
> **Authors**: Huawei Lin,Yingjie Lao,Tong Geng,Tan Yu,Weijie Zhao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security, Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger Attacks
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs. In this paper, departing from traditional deep learning attack paradigms, we explore their intrinsic relationship and collectively term them Prompt Trigger Attacks (PTA). This raises a key question: Can we determine if a prompt is benign or poisoned? To address this, we propose UniGuardian, the first unified defense mechanism designed to detect prompt injection, backdoor attacks, and adversarial attacks in LLMs. Additionally, we introduce a single-forward strategy to optimize the detection pipeline, enabling simultaneous attack detection and text generation within a single forward pass. Our experiments confirm that UniGuardian accurately and efficiently identifies malicious prompts in LLMs.

### Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning 
[[arxiv](https://arxiv.org/abs/2502.13127)] [[cool](https://papers.cool/arxiv/2502.13127)] [[pdf](https://arxiv.org/pdf/2502.13127)]
> **Authors**: Jingyang Lin,Andy Wong,Tian Xia,Shenghua He,Hui Wei,Mei Han,Jiebo Luo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 Pages, 6 Tables, 8 Figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in Large Language Models (LLMs) have enabled them to process increasingly longer sequences, ranging from 2K to 2M tokens and even beyond. However, simply extending the input sequence length does not necessarily lead to effective long-context understanding. In this study, we integrate Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate effective long-context understanding. To achieve this, we introduce LongFinanceQA, a synthetic dataset in the financial domain designed to improve long-context reasoning. Unlike existing long-context synthetic data, LongFinanceQA includes intermediate CoT reasoning before the final conclusion, which encourages LLMs to perform explicit reasoning, improving accuracy and interpretability in long-context understanding. To generate synthetic CoT reasoning, we propose Property-driven Agentic Inference (PAI), an agentic framework that simulates human-like reasoning steps, including property extraction, retrieval, and summarization. We evaluate PAI's reasoning capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark, outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's financial subset.

### RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises 
[[arxiv](https://arxiv.org/abs/2502.13125)] [[cool](https://papers.cool/arxiv/2502.13125)] [[pdf](https://arxiv.org/pdf/2502.13125)]
> **Authors**: Zenan Zhai,Hao Li,Xudong Han,Zhenxuan Zhang,Yixuan Zhang,Timothy Baldwin,Haonan Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in large language models (LLMs) have shown that they can answer questions requiring complex reasoning. However, their ability to identify and respond to text containing logical fallacies or deliberately misleading premises remains less studied. To address this gap, we introduce RuozhiBench, a bilingual dataset comprising 677 carefully curated questions that contain various forms of deceptive reasoning, meticulously crafted through extensive human effort and expert review. In a comprehensive evaluation of 17 LLMs from 5 Series over RuozhiBench using both open-ended and two-choice formats, we conduct extensive analyses on evaluation protocols and result patterns. Despite their high scores on conventional benchmarks, these models showed limited ability to detect and reason correctly about logical fallacies, with even the best-performing model, Claude-3-haiku, achieving only 62% accuracy compared to the human of more than 90%.

### NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions 
[[arxiv](https://arxiv.org/abs/2502.13124)] [[cool](https://papers.cool/arxiv/2502.13124)] [[pdf](https://arxiv.org/pdf/2502.13124)]
> **Authors**: Weizhe Yuan,Jane Yu,Song Jiang,Karthik Padthe,Yang Li,Dong Wang,Ilia Kulikov,Kyunghyun Cho,Yuandong Tian,Jason E Weston,Xian Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Dataset at https://huggingface.co/datasets/facebook/natural_reasoning
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Scaling reasoning capabilities beyond traditional domains such as math and coding is hindered by the lack of diverse and high-quality questions. To overcome this limitation, we introduce a scalable approach for generating diverse and challenging reasoning questions, accompanied by reference answers. We present NaturalReasoning, a comprehensive dataset comprising 2.8 million questions that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. We demonstrate the utility of the questions in NaturalReasoning through knowledge distillation experiments which show that NaturalReasoning can effectively elicit and transfer reasoning capabilities from a strong teacher model. Furthermore, we demonstrate that NaturalReasoning is also effective for unsupervised self-training using external reward models or self-rewarding.

### Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context 
[[arxiv](https://arxiv.org/abs/2502.13120)] [[cool](https://papers.cool/arxiv/2502.13120)] [[pdf](https://arxiv.org/pdf/2502.13120)]
> **Authors**: Marion Bartl,Thomas Brendan Murphy,Susan Leavy
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 9 pages, 7 figures, submitted to ACL 2025 (ARR February 2025 cycle)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Gender-inclusive language is often used with the aim of ensuring that all individuals, regardless of gender, can be associated with certain concepts. While psycholinguistic studies have examined its effects in relation to human cognition, it remains unclear how Large Language Models (LLMs) process gender-inclusive language. Given that commercial LLMs are gaining an increasingly strong foothold in everyday applications, it is crucial to examine whether LLMs in fact interpret gender-inclusive language neutrally, because the language they generate has the potential to influence the language of their users. This study examines whether LLM-generated coreferent terms align with a given gender expression or reflect model biases. Adapting psycholinguistic methods from French to English and German, we find that in English, LLMs generally maintain the antecedent's gender but exhibit underlying masculine bias. In German, this bias is much stronger, overriding all tested gender-neutralization strategies.

### STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13119)] [[cool](https://papers.cool/arxiv/2502.13119)] [[pdf](https://arxiv.org/pdf/2502.13119)]
> **Authors**: Narun Raman,Taylor Lundy,Thiago Amin,Jesse Perla,Kevin Leyton-Brown
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 pages, 11 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: How should one judge whether a given large language model (LLM) can reliably perform economic reasoning? Most existing LLM benchmarks focus on specific applications and fail to present the model with a rich variety of economic tasks. A notable exception is Raman et al. [2024], who offer an approach for comprehensively benchmarking strategic decision-making; however, this approach fails to address the non-strategic settings prevalent in microeconomics, such as supply-and-demand analysis. We address this gap by taxonomizing microeconomic reasoning into $58$ distinct elements, focusing on the logic of supply and demand, each grounded in up to $10$ distinct domains, $5$ perspectives, and $3$ types. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub auto-STEER, which generates a set of questions by adapting handwritten templates to target new domains and perspectives. Because it offers an automated way of generating fresh questions, auto-STEER mitigates the risk that LLMs will be trained to over-fit evaluation benchmarks; we thus hope that it will serve as a useful tool both for evaluating and fine-tuning models for years to come. We demonstrate the usefulness of our benchmark via a case study on $27$ LLMs, ranging from small open-source models to the current state of the art. We examined each model's ability to solve microeconomic problems across our whole taxonomy and present the results across a range of prompting strategies and scoring metrics.

### Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization 
[[arxiv](https://arxiv.org/abs/2502.13108)] [[cool](https://papers.cool/arxiv/2502.13108)] [[pdf](https://arxiv.org/pdf/2502.13108)]
> **Authors**: Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Amit Agarwal,Bhargava Kumar,Srikant Panda,Tejaswini Kumar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings. We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.

### Text2World: Benchmarking Large Language Models for Symbolic World Model Generation 
[[arxiv](https://arxiv.org/abs/2502.13092)] [[cool](https://papers.cool/arxiv/2502.13092)] [[pdf](https://arxiv.org/pdf/2502.13092)]
> **Authors**: Mengkang Hu,Tianxing Chen,Yude Zou,Yuheng Lei,Qiguang Chen,Ming Li,Yao Mu,Hongyuan Zhang,Wenqi Shao,Ping Luo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Project page: https://text-to-world.github.io/
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at https://text-to-world.github.io/.

### KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based Portraits 
[[arxiv](https://arxiv.org/abs/2502.13076)] [[cool](https://papers.cool/arxiv/2502.13076)] [[pdf](https://arxiv.org/pdf/2502.13076)]
> **Authors**: Xin Xia,Yujin Wang,Jun Zhou,Guisheng Zhong,Linning Cai,Chen Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Patent analysis highly relies on concise and interpretable document representations, referred to as patent portraits. Keyphrases, both present and absent, are ideal candidates for patent portraits due to their brevity, representativeness, and clarity. In this paper, we introduce KAPPA, an integrated framework designed to construct keyphrase-based patent portraits and enhance patent analysis. KAPPA operates in two phases: patent portrait construction and portrait-based analysis. To ensure effective portrait construction, we propose a semantic-calibrated keyphrase generation paradigm that integrates pre-trained language models with a prompt-based hierarchical decoding strategy to leverage the multi-level structural characteristics of patents. For portrait-based analysis, we develop a comprehensive framework that employs keyphrase-based patent portraits to enable efficient and accurate patent analysis. Extensive experiments on benchmark datasets of keyphrase generation, the proposed model achieves significant improvements compared to state-of-the-art baselines. Further experiments conducted on real-world patent applications demonstrate that our keyphrase-based portraits effectively capture domain-specific knowledge and enrich semantic representation for patent analysis tasks.

### Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity 
[[arxiv](https://arxiv.org/abs/2502.13063)] [[cool](https://papers.cool/arxiv/2502.13063)] [[pdf](https://arxiv.org/pdf/2502.13063)]
> **Authors**: Yuri Kuratov,Mikhail Arkhipov,Aydar Bulatov,Mikhail Burtsev
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.

### Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection 
[[arxiv](https://arxiv.org/abs/2502.13061)] [[cool](https://papers.cool/arxiv/2502.13061)] [[pdf](https://arxiv.org/pdf/2502.13061)]
> **Authors**: Jingbiao Mei,Jinghong Chen,Guangyu Yang,Weizhe Lin,Bill Byrne
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Preprint. Under Review
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While large multimodal models have shown strong generalization across various tasks, they exhibit poor generalization to hateful meme detection due to the dynamic nature of memes tied to emerging social trends and breaking news. Recent work further highlights the limitations of conventional supervised fine-tuning for large multimodal models in this context. To address these challenges, we propose Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a novel two-stage fine-tuning framework designed to improve both in-domain accuracy and cross-domain generalization. Experimental results on six widely used meme classification datasets demonstrate that LMM-RGCL achieves state-of-the-art performance, outperforming agent-based systems such as VPD-PALI-X-55B. Furthermore, our method effectively generalizes to out-of-domain memes under low-resource settings, surpassing models like GPT-4o.

### SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13059)] [[cool](https://papers.cool/arxiv/2502.13059)] [[pdf](https://arxiv.org/pdf/2502.13059)]
> **Authors**: Xianfu Cheng,Wei Zhang,Shiwei Zhang,Jian Yang,Xiangyuan Guan,Xianjie Wu,Xiang Li,Ge Zhang,Jiaheng Liu,Yuying Mai,Yutao Zeng,Zhoufutu Wen,Ke Jin,Baorui Wang,Weixiao Zhou,Yunhong Lu,Tongliang Li,Wenhao Huang,Zhoujun Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (e.g. common and domain-specific knowledge). In this work, we introduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate the factuality ability of MLLMs to answer natural language short questions. SimpleVQA is characterized by six key features: it covers multiple tasks and multiple scenarios, ensures high quality and challenging queries, maintains static and timeless reference answers, and is straightforward to evaluate. Our approach involves categorizing visual question-answering items into 9 different tasks around objective events or common knowledge and situating these within 9 topics. Rigorous quality control processes are implemented to guarantee high-quality, concise, and clear answers, facilitating evaluation with minimal variance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a comprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into their image comprehension and text generation abilities by identifying and analyzing error cases.

### AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks 
[[arxiv](https://arxiv.org/abs/2502.13053)] [[cool](https://papers.cool/arxiv/2502.13053)] [[pdf](https://arxiv.org/pdf/2502.13053)]
> **Authors**: Yurun Chen,Xueyu Hu,Keting Yin,Juncheng Li,Shengyu Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As researchers continuously optimize AI agents to perform tasks more effectively within operating systems, they often neglect to address the critical need for enabling these agents to identify "impostors" within the system. Through an analysis of the agents' operating environment, we identified a potential threat: attackers can disguise their attack methods as environmental elements, injecting active disturbances into the agents' execution process, thereby disrupting their decision-making. We define this type of attack as Active Environment Injection Attack (AEIA). Based on this, we propose AEIA-MN, an active environment injection attack scheme that exploits interaction vulnerabilities in the mobile operating system to evaluate the robustness of MLLM-based agents against such threats. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% in the AndroidWorld benchmark.

### Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction 
[[arxiv](https://arxiv.org/abs/2502.13044)] [[cool](https://papers.cool/arxiv/2502.13044)] [[pdf](https://arxiv.org/pdf/2502.13044)]
> **Authors**: Nils Constantin Hellwig,Jakob Fehle,Udo Kruschwitz,Christian Wolff
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks.

### Natural Language Generation from Visual Sequences: Challenges and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.13034)] [[cool](https://papers.cool/arxiv/2502.13034)] [[pdf](https://arxiv.org/pdf/2502.13034)]
> **Authors**: Aditya K Surikuchi,Raquel Fernández,Sandro Pezzelle
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: The ability to use natural language to talk about visual content is at the core of human intelligence and a crucial feature of any artificial intelligence system. Various studies have focused on generating text for single images. In contrast, comparatively little attention has been paid to exhaustively analyzing and advancing work on multiple-image vision-to-text settings. In this position paper, we claim that any task dealing with temporally ordered sequences of multiple images or frames is an instance of a broader, more general problem involving the understanding of intricate relationships between the visual content and the corresponding text. We comprehensively analyze five tasks that are instances of this problem and argue that they pose a common set of challenges and share similarities in terms of modeling and evaluation approaches. Based on the insights from these various aspects and stages of multi-image-to-text generation, we highlight several open questions and suggest future research directions. We believe that these directions can advance the understanding of complex phenomena in this domain and the development of better models.

### HPSS: Heuristic Prompting Strategy Search for LLM Evaluators 
[[arxiv](https://arxiv.org/abs/2502.13031)] [[cool](https://papers.cool/arxiv/2502.13031)] [[pdf](https://arxiv.org/pdf/2502.13031)]
> **Authors**: Bosi Wen,Pei Ke,Yufei Sun,Cunxiang Wang,Xiaotao Gu,Jinfeng Zhou,Jie Tang,Hongning Wang,Minlie Huang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 32 pages, 10 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Since the adoption of large language models (LLMs) for text evaluation has become increasingly prevalent in the field of natural language processing (NLP), a series of existing works attempt to optimize the prompts for LLM evaluators to improve their alignment with human judgment. However, their efforts are limited to optimizing individual factors of evaluation prompts, such as evaluation criteria or output formats, neglecting the combinatorial impact of multiple factors, which leads to insufficient optimization of the evaluation pipeline. Nevertheless, identifying well-behaved prompting strategies for adjusting multiple factors requires extensive enumeration. To this end, we comprehensively integrate 8 key factors for evaluation prompts and propose a novel automatic prompting strategy optimization method called Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm, HPSS conducts an iterative search to find well-behaved prompting strategies for LLM evaluators. A heuristic function is employed to guide the search process, enhancing the performance of our algorithm. Extensive experiments across four evaluation tasks demonstrate the effectiveness of HPSS, consistently outperforming both human-designed evaluation prompts and existing automatic prompt optimization methods.

### Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.13019)] [[cool](https://papers.cool/arxiv/2502.13019)] [[pdf](https://arxiv.org/pdf/2502.13019)]
> **Authors**: Sha Li,Naren Ramakrishnan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 14 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the remarkable capabilities of Large Language Models (LLMs) in various NLP tasks, they remain vulnerable to hallucinations due to their limited parametric knowledge and lack of domain-specific expertise. Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating external document retrieval to augment the knowledge base of LLMs. In this approach, RAG retrieves document chunks from an external corpus in response to a query, which are then used as context for the downstream language model to generate an answer. However, these retrieved knowledge sources often include irrelevant or erroneous information, undermining the effectiveness of RAG in downstream tasks. To overcome this limitation, we introduce a compact, efficient, and pluggable module designed to refine external knowledge sources before feeding them to the generator. The module reconstructs retrieved content by extracting the most relevant and supportive information and reorganising it into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritises critical knowledge and aligns it with the generator's preferences. This method enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.

### Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge 
[[arxiv](https://arxiv.org/abs/2502.13010)] [[cool](https://papers.cool/arxiv/2502.13010)] [[pdf](https://arxiv.org/pdf/2502.13010)]
> **Authors**: Mohammad Reza Rezaei,Reza Saadati Fard,Jayson Parker,Rahul G. Krishnan,Milad Lankarany
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,多代理系统
- **Abstract**: Large Language Models (LLMs) have significantly advanced medical question-answering by leveraging extensive clinical data and medical literature. However, the rapid evolution of medical knowledge and the labor-intensive process of manually updating domain-specific resources pose challenges to the reliability of these systems. To address this, we introduce Adaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates the construction and continuous updating of medical knowledge graphs, integrates reasoning, and retrieves current external evidence, such as PubMed and WikiSearch. By dynamically linking new findings and complex medical concepts, AMG-RAG not only improves accuracy but also enhances interpretability in medical queries. Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of 66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to 100 times larger. Notably, these improvements are achieved without increasing computational overhead, highlighting the critical role of automated knowledge graph generation and external evidence retrieval in delivering up-to-date, trustworthy medical insights.

### Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation 
[[arxiv](https://arxiv.org/abs/2502.13004)] [[cool](https://papers.cool/arxiv/2502.13004)] [[pdf](https://arxiv.org/pdf/2502.13004)]
> **Authors**: Wafaa Wardah,Tuğçe Melike Koçak Büyüktaş,Kirill Shchegelskiy,Sebastian Möller,Robert P. Spang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Objective speech quality models aim to predict human-perceived speech quality using automated methods. However, cross-lingual generalization remains a major challenge, as Mean Opinion Scores (MOS) vary across languages due to linguistic, perceptual, and dataset-specific differences. A model trained primarily on English data may struggle to generalize to languages with different phonetic, tonal, and prosodic characteristics, leading to inconsistencies in objective assessments. This study investigates the cross-lingual performance of two speech quality models: NISQA, a CNN-based model, and a Transformer-based Audio Spectrogram Transformer (AST) model. Both models were trained exclusively on English datasets containing over 49,000 speech samples and subsequently evaluated on speech in German, French, Mandarin, Swedish, and Dutch. We analyze model performance using Pearson Correlation Coefficient (PCC) and Root Mean Square Error (RMSE) across five speech quality dimensions: coloration, discontinuity, loudness, noise, and MOS. Our findings show that while AST achieves a more stable cross-lingual performance, both models exhibit noticeable biases. Notably, Mandarin speech quality predictions correlate highly with human MOS scores, whereas Swedish and Dutch present greater prediction challenges. Discontinuities remain difficult to model across all languages. These results highlight the need for more balanced multilingual datasets and architecture-specific adaptations to improve cross-lingual generalization.

### B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability 
[[arxiv](https://arxiv.org/abs/2502.12992)] [[cool](https://papers.cool/arxiv/2502.12992)] [[pdf](https://arxiv.org/pdf/2502.12992)]
> **Authors**: Yifan Wang,Sukrut Rao,Ji-Ung Lee,Mayank Jobanputra,Vera Demberg
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 20 pages, 15 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings. Our code is available at https://anonymous.4open.science/r/bcos_lm.

### Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs 
[[arxiv](https://arxiv.org/abs/2502.12988)] [[cool](https://papers.cool/arxiv/2502.12988)] [[pdf](https://arxiv.org/pdf/2502.12988)]
> **Authors**: Zixiao Wang,Duzhen Zhang,Ishita Agrawal,Shen Gao,Le Song,Xiuying Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 19 pages, 3 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character's responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought processes of a character. Using Lu Xun, a renowned Chinese writer, as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun's internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope that this work inspires future research on deep character persona simulation LLM.

### Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs 
[[arxiv](https://arxiv.org/abs/2502.12982)] [[cool](https://papers.cool/arxiv/2502.12982)] [[pdf](https://arxiv.org/pdf/2502.12982)]
> **Authors**: Longxu Dou,Qian Liu,Fan Zhou,Changyu Chen,Zili Wang,Ziqi Jin,Zichen Liu,Tongyao Zhu,Cunxiao Du,Penghui Yang,Haonan Wang,Jiaheng Liu,Yongchi Zhao,Xiachong Feng,Xin Mao,Man Tsung Yeung,Kunat Pipatanakul,Fajri Koto,Min Si Thu,Hynek Kydlíček,Zeyi Liu,Qunshu Lin,Sittipong Sripaisarnmongkol,Kridtaphad Sae-Khow,Nirattisai Thongchim, et al. (16 additional authors not shown)
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 49 pages, 16 figures. Technical Report of Sailor2: https://sea-sailor.github.io/blog/sailor2/
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Sailor2 is a family of cutting-edge multilingual language models for South-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit diverse applications. Building on Qwen2.5, Sailor2 undergoes continuous pre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to support 13 SEA languages while retaining proficiency in Chinese and English. Sailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA languages. We also deliver a comprehensive cookbook on how to develop the multilingual model in an efficient manner, including five key aspects: data curation, pre-training, post-training, model customization and evaluation. We hope that Sailor2 model (Apache 2.0 license) will drive language development in the SEA region, and Sailor2 cookbook will inspire researchers to build more inclusive LLMs for other under-served languages.

### Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking 
[[arxiv](https://arxiv.org/abs/2502.12970)] [[cool](https://papers.cool/arxiv/2502.12970)] [[pdf](https://arxiv.org/pdf/2502.12970)]
> **Authors**: Junda Zhu,Lingyong Yan,Shuaiqiang Wang,Dawei Yin,Lei Sha
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that R2D effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks.

### A Survey of Text Classification Under Class Distribution Shift 
[[arxiv](https://arxiv.org/abs/2502.12965)] [[cool](https://papers.cool/arxiv/2502.12965)] [[pdf](https://arxiv.org/pdf/2502.12965)]
> **Authors**: Adriana Valentina Costache,Silviu Florin Gheorghe,Eduard Gabriel Poesina,Paul Irofti,Radu Tudor Ionescu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: The basic underlying assumption of machine learning (ML) models is that the training and test data are sampled from the same distribution. However, in daily practice, this assumption is often broken, i.e.~the distribution of the test data changes over time, which hinders the application of conventional ML models. One domain where the distribution shift naturally occurs is text classification, since people always find new topics to discuss. To this end, we survey research articles studying open-set text classification and related tasks. We divide the methods in this area based on the constraints that define the kind of distribution shift and the corresponding problem formulation, i.e.~learning with the Universum, zero-shot learning, and open-set learning. We next discuss the predominant mitigation approaches for each problem setup. Finally, we identify several future work directions, aiming to push the boundaries beyond the state of the art. Interestingly, we find that continual learning can solve many of the issues caused by the shifting class distribution. We maintain a list of relevant papers at https://github.com/Eduard6421/Open-Set-Survey.

### Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs 
[[arxiv](https://arxiv.org/abs/2502.12964)] [[cool](https://papers.cool/arxiv/2502.12964)] [[pdf](https://arxiv.org/pdf/2502.12964)]
> **Authors**: Adi Simhi,Itay Itzhak,Fazl Barez,Gabriel Stanovsky,Yonatan Belinkov
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: :I.2.7
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) often generate outputs that lack grounding in real-world facts, a phenomenon known as hallucinations. Prior research has associated hallucinations with model uncertainty, leveraging this relationship for hallucination detection and mitigation. In this paper, we challenge the underlying assumption that all hallucinations are associated with uncertainty. Using knowledge detection and uncertainty measurement methods, we demonstrate that models can hallucinate with high certainty even when they have the correct knowledge. We further show that high-certainty hallucinations are consistent across models and datasets, distinctive enough to be singled out, and challenge existing mitigation methods. Our findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong .

### Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing 
[[arxiv](https://arxiv.org/abs/2502.12962)] [[cool](https://papers.cool/arxiv/2502.12962)] [[pdf](https://arxiv.org/pdf/2502.12962)]
> **Authors**: Xiaoju Ye,Zhichun Wang,Jingyuan Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Limited by the context window size of Large Language Models(LLMs), handling various tasks with input tokens exceeding the upper limit has been challenging, whether it is a simple direct retrieval task or a complex multi-hop reasoning task. Although various methods have been proposed to enhance the long-context processing capabilities of LLMs, they either incur substantial post-training costs, or require additional tool modules(e.g.,RAG), or have not shown significant improvement in realistic tasks. Our work observes the correlation between the attention distribution and generated answers across each layer, and establishes the attention allocation aligns with retrieval-augmented capabilities through experiments. Drawing on the above insights, we propose a novel method InfiniRetri that leverages the LLMs's own attention information to enable accurate retrieval across inputs of infinitely length. Our evaluations indicate that InfiniRetri achieves 100% accuracy in the Needle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model, surpassing other method or larger models and setting a new state-of-the-art(SOTA). Moreover, our method achieves significant performance improvements on real-world benchmarks, with a maximum 288% improvement. In addition, InfiniRetri can be applied to any Transformer-based LLMs without additional training and substantially reduces inference latency and compute overhead in long texts. In summary, our comprehensive studies show InfiniRetri's potential for practical applications and creates a paradigm for retrievaling information using LLMs own capabilities under infinite-length tokens. Code will be released in link.

### AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages 
[[arxiv](https://arxiv.org/abs/2502.12959)] [[cool](https://papers.cool/arxiv/2502.12959)] [[pdf](https://arxiv.org/pdf/2502.12959)]
> **Authors**: Steve Bakos,Félix Gaschi,David Guzmán,Riddhi More,Kelly Chutong Li,En-Shiun Annie Lee
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 24 pages, 2 figures, to be published in Proceedings of NAACL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces AlignFreeze, a method that freezes either the layers' lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most detrimental to the lower ones. Freezing the lower layers can prevent performance degradation. Particularly, AlignFreeze improves Part-of-Speech (PoS) tagging performances in languages where full realignment fails: with XLM-R, it provides improvements of more than one standard deviation in accuracy in seven more languages than full realignment.

### Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text 
[[arxiv](https://arxiv.org/abs/2502.12953)] [[cool](https://papers.cool/arxiv/2502.12953)] [[pdf](https://arxiv.org/pdf/2502.12953)]
> **Authors**: Andrei Jarca,Florinel Alin Croitoru,Radu Tudor Ionescu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Masked language modeling has become a widely adopted unsupervised technique to pre-train language models. However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.

### Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models 
[[arxiv](https://arxiv.org/abs/2502.12947)] [[cool](https://papers.cool/arxiv/2502.12947)] [[pdf](https://arxiv.org/pdf/2502.12947)]
> **Authors**: Gyeongman Kim,Gyouk Chu,Eunho Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years. However, their high memory requirements prevent their use in resource-constrained environments. While knowledge distillation (KD) has been a proven method for model compression, its application to MoE teacher models remains underexplored. Through our investigation, we discover that non-activated experts in MoE models possess valuable knowledge that benefits student models. We further demonstrate that existing KD methods are not optimal for compressing MoE models, as they fail to leverage this knowledge effectively. To address this, we propose two intuitive MoE-specific KD methods for the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR), both designed to effectively extract knowledge from all experts. Specifically, KA augments knowledge by sampling experts multiple times, while SAR uses all experts and adjusts the expert weights through router training to provide optimal knowledge. Extensive experiments show that our methods outperform conventional KD methods, demonstrating their effectiveness for MoE teacher models.

### LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation 
[[arxiv](https://arxiv.org/abs/2502.12945)] [[cool](https://papers.cool/arxiv/2502.12945)] [[pdf](https://arxiv.org/pdf/2502.12945)]
> **Authors**: Junchen Fu,Xuri Ge,Kaiwen Zheng,Ioannis Arapakis,Xin Xin,Joemon M. Jose
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored. In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies.

### Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning in Low-Resource Languages 
[[arxiv](https://arxiv.org/abs/2502.12932)] [[cool](https://papers.cool/arxiv/2502.12932)] [[pdf](https://arxiv.org/pdf/2502.12932)]
> **Authors**: Salsabila Zahirah Pranida,Rifo Ahmad Genadi,Fajri Koto
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 18 pages total: 8 pages of main body, 6 pages of appendix. 4 figures in main body, 6 figures in appendix. Submitted to ARR on February 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quantifying reasoning capability in low-resource languages remains a challenge in NLP due to data scarcity and limited access to annotators. While LLM-assisted dataset construction has proven useful for medium- and high-resource languages, its effectiveness in low-resource languages, particularly for commonsense reasoning, is still unclear. In this paper, we compare three dataset creation strategies: (1) LLM-assisted dataset generation, (2) machine translation, and (3) human-written data by native speakers, to build a culturally nuanced story comprehension dataset. We focus on Javanese and Sundanese, two major local languages in Indonesia, and evaluate the effectiveness of open-weight and closed-weight LLMs in assisting dataset creation through extensive manual validation. To assess the utility of synthetic data, we fine-tune language models on classification and generation tasks using this data and evaluate performance on a human-written test set. Our findings indicate that LLM-assisted data creation outperforms machine translation.

### Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts 
[[arxiv](https://arxiv.org/abs/2502.12928)] [[cool](https://papers.cool/arxiv/2502.12928)] [[pdf](https://arxiv.org/pdf/2502.12928)]
> **Authors**: Leiyu Pan,Zhenpeng Su,Minxuan Lv,Yizhe Xiong,Xiangwen Zhang,Zijia Lin,Hui Chen,Jungong Han,Guiguang Ding,Cheng Luo,Di Zhang,Kun Gai,Deyi Xiong
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models have demonstrated exceptional performance across a wide range of tasks. However, dense models usually suffer from sparse activation, where many activation values tend towards zero (i.e., being inactivated). We argue that this could restrict the efficient exploration of model representation space. To mitigate this issue, we propose Finedeep, a deep-layered fine-grained expert architecture for dense models. Our framework partitions the feed-forward neural network layers of traditional dense models into small experts, arranges them across multiple sub-layers. A novel routing mechanism is proposed to determine each expert's contribution. We conduct extensive experiments across various model sizes, demonstrating that our approach significantly outperforms traditional dense architectures in terms of perplexity and benchmark performance while maintaining a comparable number of parameters and floating-point operations. Moreover, we find that Finedeep achieves optimal results when balancing depth and width, specifically by adjusting the number of expert sub-layers and the number of experts per sub-layer. Empirical results confirm that Finedeep effectively alleviates sparse activation and efficiently utilizes representation capacity in dense models.

### SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems 
[[arxiv](https://arxiv.org/abs/2502.12927)] [[cool](https://papers.cool/arxiv/2502.12927)] [[pdf](https://arxiv.org/pdf/2502.12927)]
> **Authors**: Mike Zhang,Amalie Pernille Dilling,Léon Gondelman,Niels Erik Ruan Lyngdorf,Euan D. Lindsay,Johannes Bjerva
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce Synthetic Educational Feedback Loops (SEFL), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In SEFL, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-->student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that SEFL-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal SEFL's potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.

### Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data 
[[arxiv](https://arxiv.org/abs/2502.12924)] [[cool](https://papers.cool/arxiv/2502.12924)] [[pdf](https://arxiv.org/pdf/2502.12924)]
> **Authors**: Maite Heredia,Gorka Labaka,Jeremy Barnes,Aitor Soroa
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Code-switching (CS) is still a critical challenge in Natural Language Processing (NLP). Current Large Language Models (LLMs) struggle to interpret and generate code-switched text, primarily due to the scarcity of large-scale CS datasets for training. This paper presents a novel methodology to generate CS data using LLMs, and test it on the English-Spanish language pair. We propose back-translating natural CS sentences into monolingual English, and using the resulting parallel corpus to fine-tune LLMs to turn monolingual sentences into CS. Unlike previous approaches to CS generation, our methodology uses natural CS data as a starting point, allowing models to learn its natural distribution beyond grammatical patterns. We thoroughly analyse the models' performance through a study on human preferences, a qualitative error analysis and an evaluation with popular automatic metrics. Results show that our methodology generates fluent code-switched text, expanding research opportunities in CS communication, and that traditional metrics do not correlate with human judgement when assessing the quality of the generated CS data. We release our code and generated dataset under a CC-BY-NC-SA license.

### On-Device LLMs for Home Assistant: Dual Role in Intent Detection and Response Generation 
[[arxiv](https://arxiv.org/abs/2502.12923)] [[cool](https://papers.cool/arxiv/2502.12923)] [[pdf](https://arxiv.org/pdf/2502.12923)]
> **Authors**: Rune Birkmose,Nathan Mørkeberg Reece,Esben Hofstedt Norvin,Johannes Bjerva,Mike Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper investigates whether Large Language Models (LLMs), fine-tuned on synthetic but domain-representative data, can perform the twofold task of (i) slot and intent detection and (ii) natural language response generation for a smart home assistant, while running solely on resource-limited, CPU-only edge hardware. We fine-tune LLMs to produce both JSON action calls and text responses. Our experiments show that 16-bit and 8-bit quantized variants preserve high accuracy on slot and intent detection and maintain strong semantic coherence in generated text, while the 4-bit model, while retaining generative fluency, suffers a noticeable drop in device-service classification accuracy. Further evaluations on noisy human (non-synthetic) prompts and out-of-domain intents confirm the models' generalization ability, obtaining around 80--86\% accuracy. While the average inference time is 5--6 seconds per query -- acceptable for one-shot commands but suboptimal for multi-turn dialogue -- our results affirm that an on-device LLM can effectively unify command interpretation and flexible response generation for home automation without relying on specialized hardware.

### Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison 
[[arxiv](https://arxiv.org/abs/2502.12921)] [[cool](https://papers.cool/arxiv/2502.12921)] [[pdf](https://arxiv.org/pdf/2502.12921)]
> **Authors**: George-Kirollos Saad,Scott Sanner
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.

### Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation 
[[arxiv](https://arxiv.org/abs/2502.12911)] [[cool](https://papers.cool/arxiv/2502.12911)] [[pdf](https://arxiv.org/pdf/2502.12911)]
> **Authors**: Zheng Yuan,Hao Chen,Zijin Hong,Qinggang Zhang,Feiran Huang,Xiao Huang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,数据库
- **Abstract**: Generating SQLs from user queries is a long-standing challenge, where the accuracy of initial schema linking significantly impacts subsequent SQL generation performance. However, current schema linking models still struggle with missing relevant schema elements or an excess of redundant ones. A crucial reason for this is that commonly used metrics, recall and precision, fail to capture relevant element missing and thus cannot reflect actual schema linking performance. Motivated by this, we propose an enhanced schema linking metric by introducing a restricted missing indicator. Accordingly, we introduce Knapsack optimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent designed to prevent the missing of relevant schema elements while minimizing the inclusion of redundant ones. KaSLA employs a hierarchical linking strategy that first identifies the optimal table linking and subsequently links columns within the selected table to reduce linking candidate space. In each linking process, it utilize a knapsack optimization approach to link potentially relevant elements while accounting for a limited tolerance of potential redundant ones.With this optimization, KaSLA-1.6B achieves superior schema linking results compared to large-scale LLMs, including deepseek-v3 with state-of-the-art (SOTA) schema linking method. Extensive experiments on Spider and BIRD benchmarks verify that KaSLA can significantly improve the SQL generation performance of SOTA text-to-SQL models by substituting their schema linking processes.

### Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements 
[[arxiv](https://arxiv.org/abs/2502.12904)] [[cool](https://papers.cool/arxiv/2502.12904)] [[pdf](https://arxiv.org/pdf/2502.12904)]
> **Authors**: Shu Yang,Shenzhe Zhu,Zeyu Wu,Keyu Wang,Junchi Yao,Junchao Wu,Lijie Hu,Mengdi Li,Derek F. Wong,Di Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in dynamic, real-world scenarios. Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job postings, social media, and news, categorized into 5 major fraud types. Unlike previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to assess LLMs' resistance to fraud at different stages, including credibility building, urgency creation, and emotional manipulation. Furthermore, we evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM provides general decision-making assistance, and 2. Role-play, where the model assumes a specific persona, widely used in real-world agent-based interactions. Our evaluation reveals the significant challenges in defending against fraud and phishing inducement, especially in role-play settings and fake job postings. Additionally, we observe a substantial performance gap between Chinese and English, underscoring the need for improved multilingual fraud detection capabilities.

### Soundwave: Less is More for Speech-Text Alignment in LLMs 
[[arxiv](https://arxiv.org/abs/2502.12900)] [[cool](https://papers.cool/arxiv/2502.12900)] [[pdf](https://arxiv.org/pdf/2502.12900)]
> **Authors**: Yuhao Zhang,Zhiheng Liu,Fan Bu,Ruiyu Zhang,Benyou Wang,Haizhou Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,声音
- **Abstract**: Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.

### None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks 
[[arxiv](https://arxiv.org/abs/2502.12896)] [[cool](https://papers.cool/arxiv/2502.12896)] [[pdf](https://arxiv.org/pdf/2502.12896)]
> **Authors**: Eva Sánchez Salido,Julio Gonzalo,Guillermo Marco
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.

### Multilingual European Language Models: Benchmarking Approaches and Challenges 
[[arxiv](https://arxiv.org/abs/2502.12895)] [[cool](https://papers.cool/arxiv/2502.12895)] [[pdf](https://arxiv.org/pdf/2502.12895)]
> **Authors**: Fabio Barth,Georg Rehm
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.

### H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking 
[[arxiv](https://arxiv.org/abs/2502.12893)] [[cool](https://papers.cool/arxiv/2502.12893)] [[pdf](https://arxiv.org/pdf/2502.12893)]
> **Authors**: Martin Kuo,Jianyi Zhang,Aolin Ding,Qinsi Wang,Louis DiValentin,Yujia Bao,Wei Wei,Hai Li,Yiran Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Website: https://maliciouseducator.org/
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Reasoning Models (LRMs) have recently extended their powerful reasoning capabilities to safety checks-using chain-of-thought reasoning to decide whether a request should be answered. While this new approach offers a promising route for balancing model utility and safety, its robustness remains underexplored. To address this gap, we introduce Malicious-Educator, a benchmark that disguises extremely dangerous or malicious requests beneath seemingly legitimate educational prompts. Our experiments reveal severe security flaws in popular commercial-grade LRMs, including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1 model initially maintains a high refusal rate of about 98%, subsequent model updates significantly compromise its safety; and attackers can easily extract criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any additional tricks. To further highlight these vulnerabilities, we propose Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method that leverages the model's own displayed intermediate reasoning to jailbreak its safety reasoning mechanism. Under H-CoT, refusal rates sharply decline-dropping from 98% to below 2%-and, in some instances, even transform initially cautious tones into ones that are willing to provide harmful content. We hope these findings underscore the urgent need for more robust safety mechanisms to preserve the benefits of advanced reasoning capabilities without compromising ethical standards.

### Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030? 
[[arxiv](https://arxiv.org/abs/2502.12886)] [[cool](https://papers.cool/arxiv/2502.12886)] [[pdf](https://arxiv.org/pdf/2502.12886)]
> **Authors**: Georg Rehm,Annika Grützner-Zahn,Fabio Barth
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) demonstrate unprecedented capabilities and define the state of the art for almost all natural language processing (NLP) tasks and also for essentially all Language Technology (LT) applications. LLMs can only be trained for languages for which a sufficient amount of pre-training data is available, effectively excluding many languages that are typically characterised as under-resourced. However, there is both circumstantial and empirical evidence that multilingual LLMs, which have been trained using data sets that cover multiple languages (including under-resourced ones), do exhibit strong capabilities for some of these under-resourced languages. Eventually, this approach may have the potential to be a technological off-ramp for those under-resourced languages for which "native" LLMs, and LLM-based technologies, cannot be developed due to a lack of training data. This paper, which concentrates on European languages, examines this idea, analyses the current situation in terms of technology support and summarises related work. The article concludes by focusing on the key open questions that need to be answered for the approach to be put into practice in a systematic way.

### How desirable is alignment between LLMs and linguistically diverse human users? 
[[arxiv](https://arxiv.org/abs/2502.12884)] [[cool](https://papers.cool/arxiv/2502.12884)] [[pdf](https://arxiv.org/pdf/2502.12884)]
> **Authors**: Pia Knoeferle,Sebastian Möller,Dorothea Kolossa,Veronika Solopova,Georg Rehm
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We discuss how desirable it is that Large Language Models (LLMs) be able to adapt or align their language behavior with users who may be diverse in their language use. User diversity may come about among others due to i) age differences; ii) gender characteristics, and/or iii) multilingual experience, and associated differences in language processing and use. We consider potential consequences for usability, communication, and LLM development.

### PAFT: Prompt-Agnostic Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.12859)] [[cool](https://papers.cool/arxiv/2502.12859)] [[pdf](https://arxiv.org/pdf/2502.12859)]
> **Authors**: Chenxing Wei,Yao Shu,Mingwen Ou,Ying Tiffany He,Fei Richard Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 20 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.

### Rejected Dialects: Biases Against African American Language in Reward Models 
[[arxiv](https://arxiv.org/abs/2502.12858)] [[cool](https://papers.cool/arxiv/2502.12858)] [[pdf](https://arxiv.org/pdf/2502.12858)]
> **Authors**: Joel Mire,Zubin Trivadi Aysola,Daniel Chechelnitsky,Nicholas Deas,Chrysoula Zerva,Maarten Sap
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted to NAACL Findings 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Preference alignment via reward models helps build safe, helpful, and reliable large language models (LLMs). However, subjectivity in preference judgments and the lack of representative sampling in preference data collection can introduce new biases, hindering reward models' fairness and equity. In this work, we introduce a framework for evaluating dialect biases in reward models and conduct a case study on biases against African American Language (AAL) through several experiments comparing reward model preferences and behavior on paired White Mainstream English (WME) and both machine-translated and human-written AAL corpora. We show that reward models are less aligned with human preferences when processing AAL texts vs. WME ones (-4\% accuracy on average), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and steer conversations toward WME, even when prompted with AAL texts. Our findings provide a targeted analysis of anti-AAL biases at a relatively understudied stage in LLM development, highlighting representational harms and ethical questions about the desired behavior of LLMs concerning AAL.

### Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models 
[[arxiv](https://arxiv.org/abs/2502.12855)] [[cool](https://papers.cool/arxiv/2502.12855)] [[pdf](https://arxiv.org/pdf/2502.12855)]
> **Authors**: Neeraj Gangwar,Suma P Bhat,Nickvash Kani
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: While large models pre-trained on high-quality data exhibit excellent performance across various reasoning tasks, including mathematical reasoning (e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical reasoning remains a challenging problem. Common approaches to address this challenge include knowledge distillation, where smaller student models learn from large pre-trained teacher models, and data augmentation, such as rephrasing questions. Despite these efforts, smaller models struggle with arithmetic computations, leading to errors in mathematical reasoning. In this work, we focus on leveraging a programmatically generated arithmetic dataset to enhance the reasoning capabilities of smaller models. We investigate two key approaches to incorporate this dataset -- (1) intermediate fine-tuning, where a model is fine-tuned on the arithmetic dataset before being trained on a reasoning dataset, and (2) integrating the arithmetic dataset into the instruction-tuning mixture, allowing the model to learn arithmetic skills alongside general instruction-following abilities. Our experiments on multiple reasoning benchmarks demonstrate that incorporating an arithmetic dataset, whether through targeted fine-tuning or within the instruction-tuning mixture, enhances the models' arithmetic capabilities, which in turn improves their mathematical reasoning performance.

### S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.12853)] [[cool](https://papers.cool/arxiv/2502.12853)] [[pdf](https://arxiv.org/pdf/2502.12853)]
> **Authors**: Ruotian Ma,Peisong Wang,Cheng Liu,Xingyan Liu,Jiaqi Chen,Bang Zhang,Xin Zhou,Nan Du,Jia Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S$^2$R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\% to 81.6\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S$^2$R. Our code and data are available at https://github.com/NineAbyss/S2R.

### MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching 
[[arxiv](https://arxiv.org/abs/2502.12852)] [[cool](https://papers.cool/arxiv/2502.12852)] [[pdf](https://arxiv.org/pdf/2502.12852)]
> **Authors**: Fabian David Schmidt,Florian Schneider,Chris Biemann,Goran Glavaš
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.

### MeMo: Towards Language Models with Associative Memory Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.12851)] [[cool](https://papers.cool/arxiv/2502.12851)] [[pdf](https://arxiv.org/pdf/2502.12851)]
> **Authors**: Fabio Massimo Zanzotto,Elena Sofia Ruzzetti,Giancarlo A. Xompero,Leonardo Ranaldi,Davide Venditti,Federico Ranaldi,Cristina Giannone,Andrea Favalli,Raniero Romagnoli
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: :I.2.7; I.2.6; I.2.4
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that memorization precedes learning. We introduce MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented with the MeMo architecture, showing the memorization power of the one-layer and the multi-layer configurations.

### An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation 
[[arxiv](https://arxiv.org/abs/2502.12836)] [[cool](https://papers.cool/arxiv/2502.12836)] [[pdf](https://arxiv.org/pdf/2502.12836)]
> **Authors**: Mohammad Feli,Iman Azimi,Pasi Liljeberg,Amir M. Rahmani
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.

### Subword models struggle with word learning, but surprisal hides it 
[[arxiv](https://arxiv.org/abs/2502.12835)] [[cool](https://papers.cool/arxiv/2502.12835)] [[pdf](https://arxiv.org/pdf/2502.12835)]
> **Authors**: Bastian Bunzeck,Sina Zarrieß
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 12 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We study word learning in subword and character language models with the psycholinguistic lexical decision task. While subword LMs struggle to discern words and non-words with high accuracy, character LMs solve this task easily and consistently. Furthermore, when comparing word learning and syntactic learning, both processes are separable in character LM where word learning predates syntactic learning, whereas these processes are simultaneous in subword LM. This raises questions about the adequacy of subword LMs for modeling language acquisition and positions character LMs as a viable alternative.

### KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan 
[[arxiv](https://arxiv.org/abs/2502.12829)] [[cool](https://papers.cool/arxiv/2502.12829)] [[pdf](https://arxiv.org/pdf/2502.12829)]
> **Authors**: Mukhammed Togmanov,Nurdaulet Mukhituly,Diana Turmakhan,Jonibek Mansurov,Maiya Goloburda,Akhmed Sakip,Zhuohan Xie,Yuxia Wang,Bekassyl Syzdykov,Nurkhan Laiyk,Alham Fikri Aji,Ekaterina Kochmar,Preslav Nakov,Fajri Koto
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.

### Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12825)] [[cool](https://papers.cool/arxiv/2502.12825)] [[pdf](https://arxiv.org/pdf/2502.12825)]
> **Authors**: Rubing Li,João Sedoc,Arun Sundararajan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy.

### Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12821)] [[cool](https://papers.cool/arxiv/2502.12821)] [[pdf](https://arxiv.org/pdf/2502.12821)]
> **Authors**: Elena Stringli,Maria Lymperaiou,Giorgos Filandrianos,Giorgos Stamou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Inverse tasks can uncover potential reasoning gaps as Large Language Models (LLMs) scale up. In this work, we explore the redefinition task, in which we assign alternative values to well-known physical constants and units of measure, prompting LLMs to respond accordingly. Our findings show that not only does model performance degrade with scale, but its false confidence also rises. Moreover, while factors such as prompting strategies or response formatting are influential, they do not preclude LLMs from anchoring to memorized values.

### Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12813)] [[cool](https://papers.cool/arxiv/2502.12813)] [[pdf](https://arxiv.org/pdf/2502.12813)]
> **Authors**: Adnan Ahmad,Stefan Hillmann,Sebastian Möller
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: In this study, we explore the application of Large Language Models (LLMs) for generating synthetic users and simulating user conversations with a task-oriented dialogue system and present detailed results and their analysis. We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success. We employ two proprietary LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives. We perform a detailed analysis of the user profiles generated by LLMs to assess the diversity, consistency, and potential biases inherent in these LLM-generated user simulations. We find that GPT-o1 generates more heterogeneous user distribution across most user attributes, while GPT-4o generates more skewed user attributes. The generated set of user profiles are then utilized to simulate dialogue sessions by interacting with a task-oriented dialogue system.

### Towards Text-Image Interleaved Retrieval 
[[arxiv](https://arxiv.org/abs/2502.12799)] [[cool](https://papers.cool/arxiv/2502.12799)] [[pdf](https://arxiv.org/pdf/2502.12799)]
> **Authors**: Xin Zhang,Ziqi Dai,Yongqi Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Jun Yu,Wenjie Li,Min Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 16 pages, 14 figures
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别,信息检索
- **Abstract**: Current multimodal information retrieval studies mainly focus on single-image inputs, which limits real-world applications involving multiple images and text-image interleaved content. In this work, we introduce the text-image interleaved retrieval (TIIR) task, where the query and document are interleaved text-image sequences, and the model is required to understand the semantics from the interleaved context for effective retrieval. We construct a TIIR benchmark based on naturally interleaved wikiHow tutorials, where a specific pipeline is designed to generate interleaved queries. To explore the task, we adapt several off-the-shelf retrievers and build a dense baseline by interleaved multimodal large language model (MLLM). We then propose a novel Matryoshka Multimodal Embedder (MME), which compresses the number of visual tokens at different granularity, to address the challenge of excessive visual tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption of existing models does not consistently yield effective results. Our MME achieves significant improvements over the baseline by substantially fewer visual tokens. We provide extensive analysis and will release the dataset and code to facilitate future research.

### Commonsense Reasoning in Arab Culture 
[[arxiv](https://arxiv.org/abs/2502.12788)] [[cool](https://papers.cool/arxiv/2502.12788)] [[pdf](https://arxiv.org/pdf/2502.12788)]
> **Authors**: Abdelrahman Sadallah,Junior Cedric Tonga,Khalid Almubarak,Saeed Almheiri,Farah Atif,Chatrine Qwaider,Karima Kadaoui,Sara Shatnawi,Yaser Alesh,Fajri Koto
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce \datasetname, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. \datasetname spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.

### Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach 
[[arxiv](https://arxiv.org/abs/2502.12771)] [[cool](https://papers.cool/arxiv/2502.12771)] [[pdf](https://arxiv.org/pdf/2502.12771)]
> **Authors**: Danny Dongyeop Han,Yunju Cho,Jiook Cha,Jay-Yoon Lee
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,神经元和认知
- **Abstract**: Self-supervised language and audio models effectively predict brain responses to speech. However, traditional prediction models rely on linear mappings from unimodal features, despite the complex integration of auditory signals with linguistic and semantic information across widespread brain networks during speech comprehension. Here, we introduce a nonlinear, multimodal prediction model that combines audio and linguistic features from pre-trained models (e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in prediction performance (unnormalized and normalized correlation) over traditional unimodal linear models, as well as a 7.7% and 14.4% improvement, respectively, over prior state-of-the-art models. These improvements represent a major step towards future robust in-silico testing and improved decoding performance. They also reveal how auditory and semantic information are fused in motor, somatosensory, and higher-level semantic regions, aligning with existing neurolinguistic theories. Overall, our work highlights the often neglected potential of nonlinear and multimodal approaches to brain modeling, paving the way for future studies to embrace these strategies in naturalistic neurolinguistics research.

### How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild 
[[arxiv](https://arxiv.org/abs/2502.12769)] [[cool](https://papers.cool/arxiv/2502.12769)] [[pdf](https://arxiv.org/pdf/2502.12769)]
> **Authors**: Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.

### R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs 
[[arxiv](https://arxiv.org/abs/2502.12767)] [[cool](https://papers.cool/arxiv/2502.12767)] [[pdf](https://arxiv.org/pdf/2502.12767)]
> **Authors**: Sumin Jo,Junseong Choi,Jiho Kim,Edward Choi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference.

### Efficient Machine Translation Corpus Generation: Integrating Human-in-the-Loop Post-Editing with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12755)] [[cool](https://papers.cool/arxiv/2502.12755)] [[pdf](https://arxiv.org/pdf/2502.12755)]
> **Authors**: Kamer Ali Yuksel,Ahmet Gunduz,Abdul Baseet Anees,Hassan Sawaf
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,人机交互
- **Abstract**: This paper introduces an advanced methodology for machine translation (MT) corpus generation, integrating semi-automated, human-in-the-loop post-editing with large language models (LLMs) to enhance efficiency and translation quality. Building upon previous work that utilized real-time training of a custom MT quality estimation metric, this system incorporates novel LLM features such as Enhanced Translation Synthesis and Assisted Annotation Analysis, which improve initial translation hypotheses and quality assessments, respectively. Additionally, the system employs LLM-Driven Pseudo Labeling and a Translation Recommendation System to reduce human annotator workload in specific contexts. These improvements not only retain the original benefits of cost reduction and enhanced post-edit quality but also open new avenues for leveraging cutting-edge LLM advancements. The project's source code is available for community use, promoting collaborative developments in the field. The demo video can be accessed here.

### MediaMind: Revolutionizing Media Monitoring using Agentification 
[[arxiv](https://arxiv.org/abs/2502.12745)] [[cool](https://papers.cool/arxiv/2502.12745)] [[pdf](https://arxiv.org/pdf/2502.12745)]
> **Authors**: Ahmet Gunduz,Kamer Ali Yuksel,Hassan Sawaf
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: In an era of rapid technological advancements, agentification of software tools has emerged as a critical innovation, enabling systems to function autonomously and adaptively. This paper introduces MediaMind as a case study to demonstrate the agentification process, highlighting how existing software can be transformed into intelligent agents capable of independent decision-making and dynamic interaction. Developed by aiXplain, MediaMind leverages agent-based architecture to autonomously monitor, analyze, and provide insights from multilingual media content in real time. The focus of this paper is on the technical methodologies and design principles behind agentifying MediaMind, showcasing how agentification enhances adaptability, efficiency, and responsiveness. Through detailed case studies and practical examples, we illustrate how the agentification of MediaMind empowers organizations to streamline workflows, optimize decision-making, and respond to evolving trends. This work underscores the broader potential of agentification to revolutionize software tools across various domains.

### Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation 
[[arxiv](https://arxiv.org/abs/2502.12744)] [[cool](https://papers.cool/arxiv/2502.12744)] [[pdf](https://arxiv.org/pdf/2502.12744)]
> **Authors**: Yong Zhang,Bingyuan Zhang,Zhitao Li,Ming Li,Ning Cheng,Minchuan Chen,Tao Wei,Jun Ma,Shaojun Wang,Jing Xiao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted by the 50th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rapid advancement of large language models (LLMs) has significantly enhanced their reasoning abilities, enabling increasingly complex tasks. However, these capabilities often diminish in smaller, more computationally efficient models like GPT-2. Recent research shows that reasoning distillation can help small models acquire reasoning capabilities, but most existing methods focus primarily on improving teacher-generated reasoning paths. Our observations reveal that small models can generate high-quality reasoning paths during sampling, even without chain-of-thought prompting, though these paths are often latent due to their low probability under standard decoding strategies. To address this, we propose Self-Enhanced Reasoning Training (SERT), which activates and leverages latent reasoning capabilities in small models through self-training on filtered, self-generated reasoning paths under zero-shot conditions. Experiments using OpenAI's GPT-3.5 as the teacher model and GPT-2 models as the student models demonstrate that SERT enhances the reasoning abilities of small models, improving their performance in reasoning distillation.

### "I know myself better, but not really greatly": Using LLMs to Detect and Explain LLM-Generated Texts 
[[arxiv](https://arxiv.org/abs/2502.12743)] [[cool](https://papers.cool/arxiv/2502.12743)] [[pdf](https://arxiv.org/pdf/2502.12743)]
> **Authors**: Jiazhou Ji,Jie Guo,Weidong Qiu,Zheng Huang,Yang Xu,Xinru Lu,Xiaoyu Jiang,Ruizhe Li,Shujun Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated impressive capabilities in generating human-like texts, but the potential misuse of such LLM-generated texts raises the need to distinguish between human-generated and LLM-generated content. This paper explores the detection and explanation capabilities of LLM-based detectors of LLM-generated texts, in the context of a binary classification task (human-generated texts vs LLM-generated texts) and a ternary classification task (human-generated texts, LLM-generated texts, and undecided). By evaluating on six close/open-source LLMs with different sizes, our findings reveal that while self-detection consistently outperforms cross-detection, i.e., LLMs can detect texts generated by themselves more accurately than those generated by other LLMs, the performance of self-detection is still far from ideal, indicating that further improvements are needed. We also show that extending the binary to the ternary classification task with a new class "Undecided" can enhance both detection accuracy and explanation quality, with improvements being statistically significant and consistent across all LLMs. We finally conducted comprehensive qualitative and quantitative analyses on the explanation errors, which are categorized into three types: reliance on inaccurate features (the most frequent error), hallucinations, and incorrect reasoning. These findings with our human-annotated dataset emphasize the need for further research into improving both self-detection and self-explanation, particularly to address overfitting issues that may hinder generalization.

### Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation 
[[arxiv](https://arxiv.org/abs/2502.12737)] [[cool](https://papers.cool/arxiv/2502.12737)] [[pdf](https://arxiv.org/pdf/2502.12737)]
> **Authors**: Shengxiang Gao,Jey Han Lau,Jianzhong Qi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 17 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements at test time,we introduce SG-KBQA: a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. It uses the richer semantics and awareness of the knowledge base structure provided by schema contexts to enhance generalizability. We show that SG-KBQA achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings. Our source code is available at https://github.com/gaosx2000/SG_KBQA.

### Translate Smart, not Hard: Cascaded Translation Systems with Quality-Aware Deferral 
[[arxiv](https://arxiv.org/abs/2502.12701)] [[cool](https://papers.cool/arxiv/2502.12701)] [[pdf](https://arxiv.org/pdf/2502.12701)]
> **Authors**: António Farinhas,Nuno M. Guerreiro,Sweta Agrawal,Ricardo Rei,André F. T. Martins
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Larger models often outperform smaller ones but come with high computational costs. Cascading offers a potential solution. By default, it uses smaller models and defers only some instances to larger, more powerful models. However, designing effective deferral rules remains a challenge. In this paper, we propose a simple yet effective approach for machine translation, using existing quality estimation (QE) metrics as deferral rules. We show that QE-based deferral allows a cascaded system to match the performance of a larger model while invoking it for a small fraction (30% to 50%) of the examples, significantly reducing computational costs. We validate this approach through both automatic and human evaluation.

### Multi-Novelty: Improve the Diversity and Novelty of Contents Generated by Large Language Models via inference-time Multi-Views Brainstorming 
[[arxiv](https://arxiv.org/abs/2502.12700)] [[cool](https://papers.cool/arxiv/2502.12700)] [[pdf](https://arxiv.org/pdf/2502.12700)]
> **Authors**: Arash Lagzian,Srinivas Anumasa,Dianbo Liu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) demonstrate remarkable proficiency in generating accurate and fluent text. However, they often struggle with diversity and novelty, leading to repetitive or overly deterministic responses. These limitations stem from constraints in training data, including gaps in specific knowledge domains, outdated information, and an over-reliance on textual sources. Such shortcomings reduce their effectiveness in tasks requiring creativity, multi-perspective reasoning, and exploratory thinking, such as LLM based AI scientist agents and creative artist agents . To address this challenge, we introduce inference-time multi-view brainstorming method, a novel approach that enriches input prompts with diverse perspectives derived from both textual and visual sources, which we refere to as "Multi-Novelty". By incorporating additional contextual information as diverse starting point for chain of thoughts, this method enhances the variety and creativity of generated outputs. Importantly, our approach is model-agnostic, requiring no architectural modifications and being compatible with both open-source and proprietary LLMs.

### Speech-FT: A Fine-tuning Strategy for Enhancing Speech Representation Models Without Compromising Generalization Ability 
[[arxiv](https://arxiv.org/abs/2502.12672)] [[cool](https://papers.cool/arxiv/2502.12672)] [[pdf](https://arxiv.org/pdf/2502.12672)]
> **Authors**: Tzu-Quan Lin,Wei-Ping Huang,Hao Tang,Hung-yi Lee
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Speech representation models are highly effective at extracting general features for various tasks. While fine-tuning can enhance these representations for specific applications, it often compromises their generalization ability. To address this challenge, we propose Speech-FT, a fine-tuning strategy for speech representation models that leverages model merging to preserve generalization ability while still benefiting from fine-tuning. Speech-FT is effective across different fine-tuning scenarios and is compatible with various types of speech representation models, providing a versatile solution. Speech-FT offers an efficient and practical approach to further improving general speech representations after pre-training.

### Baichuan-M1: Pushing the Medical Capability of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.12671)] [[cool](https://papers.cool/arxiv/2502.12671)] [[pdf](https://arxiv.org/pdf/2502.12671)]
> **Authors**: Bingning Wang,Haizhou Zhao,Huozhi Zhou,Liang Song,Mingyu Xu,Wei Cheng,Xiangrong Zeng,Yupeng Zhang,Yuqi Huo,Zecheng Wang,Zhengyun Zhao,Da Pan,Fan Yang,Fei Kou,Fei Li,Fuzhong Chen,Guosheng Dong,Han Liu,Hongda Zhang,Jin He,Jinjie Yang,Kangxi Wu,Kegeng Wu,Lei Su,Linlin Niu, et al. (18 additional authors not shown)
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 33 pages, technical report
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The current generation of large language models (LLMs) is typically designed for broad, general-purpose applications, while domain-specific LLMs, especially in vertical fields like medicine, remain relatively scarce. In particular, the development of highly efficient and practical LLMs for the medical domain is challenging due to the complexity of medical knowledge and the limited availability of high-quality data. To bridge this gap, we introduce Baichuan-M1, a series of large language models specifically optimized for medical applications. Unlike traditional approaches that simply continue pretraining on existing models or apply post-training to a general base model, Baichuan-M1 is trained from scratch with a dedicated focus on enhancing medical capabilities. Our model is trained on 20 trillion tokens and incorporates a range of effective training methods that strike a balance between general capabilities and medical expertise. As a result, Baichuan-M1 not only performs strongly across general domains such as mathematics and coding but also excels in specialized medical fields. We have open-sourced Baichuan-M1-14B, a mini version of our model, which can be accessed through the following links.

### Evaluation of Best-of-N Sampling Strategies for Language Model Alignment 
[[arxiv](https://arxiv.org/abs/2502.12668)] [[cool](https://papers.cool/arxiv/2502.12668)] [[pdf](https://arxiv.org/pdf/2502.12668)]
> **Authors**: Yuki Ichihara,Yuu Jinnai,Tetsuro Morimura,Kaito Ariu,Kenshi Abe,Mitsuki Sakamoto,Eiji Uchibe
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ef:Transactions onMachineLearningResearch (TMLR), 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Since the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value can lead to a compromise of its performance on the true objective. Previous work proposes Regularized BoN sampling (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates reward hacking and empirically (Jinnai et al., 2024). However, Jinnai et al. (2024) introduce RBoN based on a heuristic and they lack the analysis of why such regularization strategy improves the performance of BoN sampling. The aim of this study is to analyze the effect of BoN sampling on regularization strategies. Using the regularization strategies corresponds to robust optimization, which maximizes the worst case over a set of possible perturbations in the proxy reward. Although the theoretical guarantees are not directly applicable to RBoN, RBoN corresponds to a practical implementation. This paper proposes an extension of the RBoN framework, called Stochastic RBoN sampling (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN in proxy reward. We then perform an empirical evaluation using the AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward. In addition, we also propose another simple RBoN method, the Sentence Length Regularized BoN, which has a better performance in the experiment as compared to the previous methods.

### A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization 
[[arxiv](https://arxiv.org/abs/2502.12665)] [[cool](https://papers.cool/arxiv/2502.12665)] [[pdf](https://arxiv.org/pdf/2502.12665)]
> **Authors**: Junhui He,Junna Xing,Nan Wang,Rui Xu,Shangyu Wu,Peng Zhou,Qiang Liu,Chun Jason Xue,Qingan Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \times$.

### Demystifying Multilingual Chain-of-Thought in Process Reward Modeling 
[[arxiv](https://arxiv.org/abs/2502.12663)] [[cool](https://papers.cool/arxiv/2502.12663)] [[pdf](https://arxiv.org/pdf/2502.12663)]
> **Authors**: Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are designed to perform a wide range of tasks. To improve their ability to solve complex problems requiring multi-step reasoning, recent research leverages process reward modeling to provide fine-grained feedback at each step of the reasoning process for reinforcement learning (RL), but it predominantly focuses on English. In this paper, we tackle the critical challenge of extending process reward models (PRMs) to multilingual settings. To achieve this, we train multilingual PRMs on a dataset spanning seven languages, which is translated from English. Through comprehensive evaluations on two widely used reasoning benchmarks across 11 languages, we demonstrate that multilingual PRMs not only improve average accuracy but also reduce early-stage reasoning errors. Furthermore, our results highlight the sensitivity of multilingual PRMs to both the number of training languages and the volume of English data, while also uncovering the benefits arising from more candidate responses and trainable parameters. This work opens promising avenues for robust multilingual applications in complex, multi-step reasoning tasks. In addition, we release the code to foster research along this line.

### R.R.: Unveiling LLM Training Privacy through Recollection and Ranking 
[[arxiv](https://arxiv.org/abs/2502.12658)] [[cool](https://papers.cool/arxiv/2502.12658)] [[pdf](https://arxiv.org/pdf/2502.12658)]
> **Authors**: Wenlong Meng,Zhenyuan Guo,Lenan Wu,Chen Gong,Wenyan Liu,Weixian Li,Chengkun Wei,Wenzhi Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 13 pages, 9 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLM's training data remains challenging. In this paper, we propose R.R. (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the R.R. achieves better PII identical performance compared to baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release the replicate package of R.R. at a link.

### One Size doesn't Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction 
[[arxiv](https://arxiv.org/abs/2502.12633)] [[cool](https://papers.cool/arxiv/2502.12633)] [[pdf](https://arxiv.org/pdf/2502.12633)]
> **Authors**: Ben Liu,Jihan Zhang,Fangquan Lin,Xu Jia,Min Peng
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \textbf{P}erson\textbf{A}lized \textbf{C}onversational tutoring ag\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students' learning styles based on the Felder and Silverman learning style model, aligning with each student's persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students' comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.

### Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions 
[[arxiv](https://arxiv.org/abs/2502.12616)] [[cool](https://papers.cool/arxiv/2502.12616)] [[pdf](https://arxiv.org/pdf/2502.12616)]
> **Authors**: Leonardo Ranaldi,Marco Valentino,Alexander Polonsky,Andrè Freitas
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).

### Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction 
[[arxiv](https://arxiv.org/abs/2502.12614)] [[cool](https://papers.cool/arxiv/2502.12614)] [[pdf](https://arxiv.org/pdf/2502.12614)]
> **Authors**: Lu Yang,Jiajia Li,En Ci,Lefei Zhang,Zuchao Li,Ping Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted to NAACL-main 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations simultaneously, the inclusion of irrelevant relations introduces decision complexity and impacts extraction accuracy. Therefore, for multi-relation extraction, we propose LDNet, which incorporates multi-aspect relation modeling and a label drop mechanism. By assigning different relations to different levels for understanding and decision-making, we reduce decision confusion. Additionally, the label drop mechanism effectively mitigates the impact of irrelevant relations. Experiments show that LDNet outperforms or achieves competitive performance with state-of-the-art systems on 9 tasks, 33 datasets, in both single-modal and multi-modal, few-shot and zero-shot settings.\footnote{https://github.com/Lu-Yang666/LDNet}

### Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection 
[[arxiv](https://arxiv.org/abs/2502.12611)] [[cool](https://papers.cool/arxiv/2502.12611)] [[pdf](https://arxiv.org/pdf/2502.12611)]
> **Authors**: Jiatao Li,Xiaojun Wan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rise of Large Language Models (LLMs) necessitates accurate AI-generated text detection. However, current approaches largely overlook the influence of author characteristics. We investigate how sociolinguistic attributes-gender, CEFR proficiency, academic field, and language environment-impact state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our results reveal significant biases: CEFR proficiency and language environment consistently affected detector accuracy, while gender and academic field showed detector-dependent effects. These findings highlight the crucial need for socially aware AI text detection to avoid unfairly penalizing specific demographic groups. We offer novel empirical evidence, a robust statistical framework, and actionable insights for developing more equitable and reliable detection systems in real-world, out-of-domain contexts. This work paves the way for future research on bias mitigation, inclusive evaluation benchmarks, and socially responsible LLM detectors.

### COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation 
[[arxiv](https://arxiv.org/abs/2502.12601)] [[cool](https://papers.cool/arxiv/2502.12601)] [[pdf](https://arxiv.org/pdf/2502.12601)]
> **Authors**: Sean Wang,Yicheng Jiang,Yuxin Tang,Lu Cheng,Hanjie Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Uncertainty Quantification (UQ) for Natural Language Generation (NLG) is crucial for assessing the performance of Large Language Models (LLMs), as it reveals confidence in predictions, identifies failure modes, and gauges output reliability. Conformal Prediction (CP), a model-agnostic method that generates prediction sets with a specified error rate, has been adopted for UQ in classification tasks, where the size of the prediction set indicates the model's uncertainty. However, when adapting CP to NLG, the sampling-based method for generating candidate outputs cannot guarantee the inclusion of the ground truth, limiting its applicability across a wide range of error rates. To address this, we propose \ourmethod, a method that explicitly adds the ground truth to the candidate outputs and uses logit scores to measure nonconformity. Our experiments with six LLMs on four NLG tasks show that \ourmethod outperforms baseline methods in calibrating error rates and empirical cover rates, offering accurate UQ across a wide range of user-specified error rates.

### Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge Expansion 
[[arxiv](https://arxiv.org/abs/2502.12598)] [[cool](https://papers.cool/arxiv/2502.12598)] [[pdf](https://arxiv.org/pdf/2502.12598)]
> **Authors**: Mingyang Wang,Alisa Stoll,Lukas Lange,Heike Adel,Hinrich Schütze,Jannik Strötgen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Adapting large language models (LLMs) to new and diverse knowledge is essential for their lasting effectiveness in real-world applications. This survey provides an overview of state-of-the-art methods for expanding the knowledge of LLMs, focusing on integrating various knowledge types, including factual information, domain expertise, language proficiency, and user preferences. We explore techniques, such as continual learning, model editing, and retrieval-based explicit adaptation, while discussing challenges like knowledge consistency and scalability. Designed as a guide for researchers and practitioners, this survey sheds light on opportunities for advancing LLMs as adaptable and robust knowledge systems.

### PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery 
[[arxiv](https://arxiv.org/abs/2502.12594)] [[cool](https://papers.cool/arxiv/2502.12594)] [[pdf](https://arxiv.org/pdf/2502.12594)]
> **Authors**: Bowei He,Lihao Yin,Hui-Ling Zhen,Xiaokun Zhang,Mingxuan Yuan,Chen Ma
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some instruction data irrelevant to model capability recovery may introduce negative effects. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions where model capabilities are most severely compromised within a certain recovery data budget. Our approach first applies manifold learning and spectral clustering to group recovery data in the semantic space, revealing capability-specific instruction sets. We then adaptively allocate the data budget to different clusters based on the degrees of model capability degradation. In each cluster, we prioritize data samples where model performance has declined dramatically. To mitigate potential negative transfer, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the original post-training data.

### LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data 
[[arxiv](https://arxiv.org/abs/2502.12583)] [[cool](https://papers.cool/arxiv/2502.12583)] [[pdf](https://arxiv.org/pdf/2502.12583)]
> **Authors**: Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Shengjie Ma,Aofan Liu,Hui Xiong,Jian Guo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Despite the growing development of long-context large language models (LLMs), data-centric approaches relying on synthetic data have been hindered by issues related to faithfulness, which limit their effectiveness in enhancing model performance on tasks such as long-context reasoning and question answering (QA). These challenges are often exacerbated by misinformation caused by lack of verification, reasoning without attribution, and potential knowledge conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. By integrating ground truth and citation-based reasoning prompts, we eliminate distractions and improve the accuracy of reasoning chains, thus mitigating the need for costly verification processes. We open-source two synthesized datasets, LongFaith-SFT and LongFaith-PO, which systematically address multiple dimensions of faithfulness, including verified reasoning, attribution, and contextual grounding. Extensive experiments on multi-hop reasoning datasets and LongBench demonstrate that models fine-tuned on these datasets significantly improve performance. Our ablation studies highlight the scalability and adaptability of the LongFaith pipeline, showcasing its broad applicability in developing long-context LLMs.

### A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification 
[[arxiv](https://arxiv.org/abs/2502.12576)] [[cool](https://papers.cool/arxiv/2502.12576)] [[pdf](https://arxiv.org/pdf/2502.12576)]
> **Authors**: Geetanjali Bihani,Julia Rayz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, 2 figures. Accepted for publication in the Proceedings of the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and ExplainableAI. NAFIPS'2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: With the advent of social media, children are becoming increasingly vulnerable to the risk of grooming in online settings. Detecting grooming instances in an online conversation poses a significant challenge as the interactions are not necessarily sexually explicit, since the predators take time to build trust and a relationship with their victim. Moreover, predators evade detection using indirect and coded language. While previous studies have fine-tuned Transformers to automatically identify grooming in chat conversations, they overlook the impact of coded and indirect language on model predictions, and how these align with human perceptions of grooming. In this paper, we address this gap and evaluate bi-encoders on the task of classifying different degrees of grooming risk in chat contexts, for three different participant groups, i.e. law enforcement officers, real victims, and decoys. Using a fuzzy-theoretic framework, we map human assessments of grooming behaviors to estimate the actual degree of grooming risk. Our analysis reveals that fine-tuned models fail to tag instances where the predator uses indirect speech pathways and coded language to evade detection. Further, we find that such instances are characterized by a higher presence of out-of-vocabulary (OOV) words in samples, causing the model to misclassify. Our findings highlight the need for more robust models to identify coded language from noisy chat inputs in grooming contexts.

### A Cognitive Writing Perspective for Constrained Long-Form Text Generation 
[[arxiv](https://arxiv.org/abs/2502.12568)] [[cool](https://papers.cool/arxiv/2502.12568)] [[pdf](https://arxiv.org/pdf/2502.12568)]
> **Authors**: Kaiyang Wan,Honglin Mu,Rui Hao,Haoran Luo,Tianle Gu,Xiuying Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.

### Self Iterative Label Refinement via Robust Unlabeled Learning 
[[arxiv](https://arxiv.org/abs/2502.12565)] [[cool](https://papers.cool/arxiv/2502.12565)] [[pdf](https://arxiv.org/pdf/2502.12565)]
> **Authors**: Hikaru Asano,Tadashi Kozuno,Yukino Baba
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 17 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1).

### Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory 
[[arxiv](https://arxiv.org/abs/2502.12563)] [[cool](https://papers.cool/arxiv/2502.12563)] [[pdf](https://arxiv.org/pdf/2502.12563)]
> **Authors**: Geetanjali Bihani,Tatiana Ringenberg,Julia Rayz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 9 pages, 2 figures. Accepted for publication in the Proceedings of the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and ExplainableAI. NAFIPS'2024
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Encoding implicit language presents a challenge for language models, especially in high-risk domains where maintaining high precision is important. Automated detection of online child grooming is one such critical domain, where predators manipulate victims using a combination of explicit and implicit language to convey harmful intentions. While recent studies have shown the potential of Transformer language models like SBERT for preemptive grooming detection, they primarily depend on surface-level features and approximate real victim grooming processes using vigilante and law enforcement conversations. The question of whether these features and approximations are reasonable has not been addressed thus far. In this paper, we address this gap and study whether SBERT can effectively discern varying degrees of grooming risk inherent in conversations, and evaluate its results across different participant groups. Our analysis reveals that while fine-tuning aids language models in learning to assign grooming scores, they show high variance in predictions, especially for contexts containing higher degrees of grooming risk. These errors appear in cases that 1) utilize indirect speech pathways to manipulate victims and 2) lack sexually explicit content. This finding underscores the necessity for robust modeling of indirect speech acts by language models, particularly those employed by predators.

### SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings 
[[arxiv](https://arxiv.org/abs/2502.12562)] [[cool](https://papers.cool/arxiv/2502.12562)] [[pdf](https://arxiv.org/pdf/2502.12562)]
> **Authors**: Weikai Lu,Hao Peng,Huiping Zhuang,Cen Chen,Ziqian Zeng
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,密码学和安全,多媒体
- **Abstract**: Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.

### How does a Language-Specific Tokenizer affect LLMs? 
[[arxiv](https://arxiv.org/abs/2502.12560)] [[cool](https://papers.cool/arxiv/2502.12560)] [[pdf](https://arxiv.org/pdf/2502.12560)]
> **Authors**: Jean Seo,Jaeyoon Kim,SungJoo Byun,Hyopil Shin
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The necessity of language-specific tokenizers intuitively appears crucial for effective natural language processing, yet empirical analyses on their significance and underlying reasons are lacking. This study explores how language-specific tokenizers influence the behavior of Large Language Models predominantly trained with English text data, through the case study of Korean. The research unfolds in two main stages: (1) the development of a Korean-specific extended tokenizer and (2) experiments to compare models with the basic tokenizer and the extended tokenizer through various Next Token Prediction tasks. Our in-depth analysis reveals that the extended tokenizer decreases confidence in incorrect predictions during generation and reduces cross-entropy in complex tasks, indicating a tendency to produce less nonsensical outputs. Consequently, the extended tokenizer provides stability during generation, potentially leading to higher performance in downstream tasks.

## 密码学和安全(cs.CR:Cryptography and Security)

### A Survey of Anomaly Detection in Cyber-Physical Systems 
[[arxiv](https://arxiv.org/abs/2502.13256)] [[cool](https://papers.cool/arxiv/2502.13256)] [[pdf](https://arxiv.org/pdf/2502.13256)]
> **Authors**: Danial Abshari,Meera Sridhar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: In our increasingly interconnected world, Cyber-Physical Systems (CPS) play a crucial role in industries like healthcare, transportation, and manufacturing by combining physical processes with computing power. These systems, however, face many challenges, especially regarding security and system faults. Anomalies in CPS may indicate unexpected problems, from sensor malfunctions to cyber-attacks, and must be detected to prevent failures that can cause harm or disrupt services. This paper provides an overview of the different ways researchers have approached anomaly detection in CPS. We categorize and compare methods like machine learning, deep learning, mathematical models, invariant, and hybrid techniques. Our goal is to help readers understand the strengths and weaknesses of these methods and how they can be used to create safer, more reliable CPS. By identifying the gaps in current solutions, we aim to encourage future research that will make CPS more secure and adaptive in our increasingly automated world.

### Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks 
[[arxiv](https://arxiv.org/abs/2502.13175)] [[cool](https://papers.cool/arxiv/2502.13175)] [[pdf](https://arxiv.org/pdf/2502.13175)]
> **Authors**: Wenpeng Xing,Minghao Li,Mohan Li,Meng Han
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器人技术
- **Abstract**: Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.

### Unveiling Privacy Risks in LLM Agent Memory 
[[arxiv](https://arxiv.org/abs/2502.13172)] [[cool](https://papers.cool/arxiv/2502.13172)] [[pdf](https://arxiv.org/pdf/2502.13172)]
> **Authors**: Bo Wang,Weiyi He,Pengfei He,Shenglai Zeng,Zhen Xiang,Yue Xing,Jiliang Tang
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: Under review
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.

### Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection 
[[arxiv](https://arxiv.org/abs/2502.13171)] [[cool](https://papers.cool/arxiv/2502.13171)] [[pdf](https://arxiv.org/pdf/2502.13171)]
> **Authors**: Muhammad Fahad Zia,Sri Harish Kalidass
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: IEEE Intelligent Cybersecurity Conference (ICSC2024)
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: Phishing is the most prevalent type of cyber-attack today and is recognized as the leading source of data breaches with significant consequences for both individuals and corporations. Web-based phishing attacks are the most frequent with vectors such as social media posts and emails containing links to phishing URLs that once clicked on render host systems vulnerable to more sinister attacks. Research efforts to detect phishing URLs have involved the use of supervised learning techniques that use large amounts of data to train models and have high computational requirements. They also involve analysis of features derived from vectors including email contents thus affecting user privacy. Additionally, they suffer from a lack of resilience against evolution of threats especially with the advent of generative AI techniques to bypass these systems as with AI-generated phishing URLs. Unsupervised methods such as clustering techniques have also been used in phishing detection in the past, however, they are at times unscalable due to the use of pair-wise comparisons. They also lack high detection rates while detecting phishing campaigns. In this paper, we propose an unsupervised learning approach that is not only fast but scalable, as it does not involve pair-wise comparisons. It is able to detect entire campaigns at a time with a high detection rate while preserving user privacy; this includes the recent surge of campaigns with targeted phishing URLs generated by malicious entities using generative AI techniques.

### SmartLLM: Smart Contract Auditing using Custom Generative AI 
[[arxiv](https://arxiv.org/abs/2502.13167)] [[cool](https://papers.cool/arxiv/2502.13167)] [[pdf](https://arxiv.org/pdf/2502.13167)]
> **Authors**: Jun Kevin,Pujianto Yugopuspito
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Smart contracts are essential to decentralized finance (DeFi) and blockchain ecosystems but are increasingly vulnerable to exploits due to coding errors and complex attack vectors. Traditional static analysis tools and existing vulnerability detection methods often fail to address these challenges comprehensively, leading to high false-positive rates and an inability to detect dynamic vulnerabilities. This paper introduces SmartLLM, a novel approach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented Generation (RAG) to enhance the accuracy and efficiency of smart contract auditing. By integrating domain-specific knowledge from ERC standards and employing advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM achieves superior performance compared to static analysis tools like Mythril and Slither, as well as zero-shot large language model (LLM) prompting methods such as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of 100% and an accuracy score of 70%, highlighting the model's robustness in identifying vulnerabilities, including reentrancy and access control issues. This research advances smart contract security by offering a scalable and effective auditing solution, supporting the secure adoption of decentralized applications.

### ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs 
[[arxiv](https://arxiv.org/abs/2502.13162)] [[cool](https://papers.cool/arxiv/2502.13162)] [[pdf](https://arxiv.org/pdf/2502.13162)]
> **Authors**: Ziyi Ni,Hao Wang,Huacan Wang
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) have achieved remarkable success in various domains but remain vulnerable to adversarial jailbreak attacks. Existing prompt-defense strategies, including parameter-modifying and parameter-free approaches, face limitations in adaptability, interpretability, and customization, constraining their effectiveness against evolving threats. To address these challenges, we propose ShieldLearner, a novel paradigm that mimics human learning in defense. Through trial and error, it autonomously distills attack signatures into a Pattern Atlas and synthesizes defense heuristics into a Meta-analysis Framework, enabling systematic and interpretable threat detection. Furthermore, we introduce Adaptive Adversarial Augmentation to generate adversarial variations of successfully defended prompts, enabling continuous self-improvement without model retraining. In addition to standard benchmarks, we create a hard test set by curating adversarial prompts from the Wildjailbreak dataset, emphasizing more concealed malicious intent. Experimental results show that ShieldLearner achieves a significantly higher defense success rate than existing baselines on both conventional and hard test sets, while also operating with lower computational overhead, making it a practical and efficient solution for real-world adversarial defense.

### LAMD: Context-driven Android Malware Detection and Classification with LLMs 
[[arxiv](https://arxiv.org/abs/2502.13055)] [[cool](https://papers.cool/arxiv/2502.13055)] [[pdf](https://arxiv.org/pdf/2502.13055)]
> **Authors**: Xingzhi Qian,Xinran Zheng,Yiling He,Shuo Yang,Lorenzo Cavallaro
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,机器学习
- **Abstract**: The rapid growth of mobile applications has escalated Android malware threats. Although there are numerous detection methods, they often struggle with evolving attacks, dataset biases, and limited explainability. Large Language Models (LLMs) offer a promising alternative with their zero-shot inference and reasoning capabilities. However, applying LLMs to Android malware detection presents two key challenges: (1)the extensive support code in Android applications, often spanning thousands of classes, exceeds LLMs' context limits and obscures malicious behavior within benign functionality; (2)the structural complexity and interdependencies of Android applications surpass LLMs' sequence-based reasoning, fragmenting code analysis and hindering malicious intent inference. To address these challenges, we propose LAMD, a practical context-driven framework to enable LLM-based Android malware detection. LAMD integrates key context extraction to isolate security-critical code regions and construct program structures, then applies tier-wise code reasoning to analyze application behavior progressively, from low-level instructions to high-level semantics, providing final prediction and explanation. A well-designed factual consistency verification mechanism is equipped to mitigate LLM hallucinations from the first tier. Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes.

### Does Training with Synthetic Data Truly Protect Privacy? 
[[arxiv](https://arxiv.org/abs/2502.12976)] [[cool](https://papers.cool/arxiv/2502.12976)] [[pdf](https://arxiv.org/pdf/2502.12976)]
> **Authors**: Yunpeng Zhao,Jie Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted to ICLR 2025
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: As synthetic data becomes increasingly popular in machine learning tasks, numerous methods--without formal differential privacy guarantees--use synthetic data for training. These methods often claim, either explicitly or implicitly, to protect the privacy of the original training data. In this work, we explore four different training paradigms: coreset selection, dataset distillation, data-free knowledge distillation, and synthetic data generated from diffusion models. While all these methods utilize synthetic data for training, they lead to vastly different conclusions regarding privacy preservation. We caution that empirical approaches to preserving data privacy require careful and rigorous evaluation; otherwise, they risk providing a false sense of privacy.

### Preventing the Popular Item Embedding Based Attack in Federated Recommendations 
[[arxiv](https://arxiv.org/abs/2502.12958)] [[cool](https://papers.cool/arxiv/2502.12958)] [[pdf](https://arxiv.org/pdf/2502.12958)]
> **Authors**: Jun Zhang,Huan Li,Dazhong Rong,Yan Zhao,Ke Chen,Lidan Shou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted at ICDE 2024, Extension
- **标题**: None
- **领域**: 密码学和安全,数据库,机器学习
- **Abstract**: Privacy concerns have led to the rise of federated recommender systems (FRS), which can create personalized models across distributed clients. However, FRS is vulnerable to poisoning attacks, where malicious users manipulate gradients to promote their target items intentionally. Existing attacks against FRS have limitations, as they depend on specific models and prior knowledge, restricting their real-world applicability. In our exploration of practical FRS vulnerabilities, we devise a model-agnostic and prior-knowledge-free attack, named PIECK (Popular Item Embedding based Attack). The core module of PIECK is popular item mining, which leverages embedding changes during FRS training to effectively identify the popular items. Built upon the core module, PIECK branches into two diverse solutions: The PIECKIPE solution employs an item popularity enhancement module, which aligns the embeddings of targeted items with the mined popular items to increase item exposure. The PIECKUEA further enhances the robustness of the attack by using a user embedding approximation module, which approximates private user embeddings using mined popular items. Upon identifying PIECK, we evaluate existing federated defense methods and find them ineffective against PIECK, as poisonous gradients inevitably overwhelm the cold target items. We then propose a novel defense method by introducing two regularization terms during user training, which constrain item popularity enhancement and user embedding approximation while preserving FRS performance. We evaluate PIECK and its defense across two base models, three real datasets, four top-tier attacks, and six general defense methods, affirming the efficacy of both PIECK and its defense.

### Malware Detection based on API calls 
[[arxiv](https://arxiv.org/abs/2502.12863)] [[cool](https://papers.cool/arxiv/2502.12863)] [[pdf](https://arxiv.org/pdf/2502.12863)]
> **Authors**: Christofer Fellicious,Manuel Bischof,Kevin Mayer,Dorian Eikenberg,Stefan Hausotte,Hans P. Reiser,Michael Granitzer
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Malware attacks pose a significant threat in today's interconnected digital landscape, causing billions of dollars in damages. Detecting and identifying families as early as possible provides an edge in protecting against such malware. We explore a lightweight, order-invariant approach to detecting and mitigating malware threats: analyzing API calls without regard to their sequence. We publish a public dataset of over three hundred thousand samples and their function call parameters for this task, annotated with labels indicating benign or malicious activity. The complete dataset is above 550GB uncompressed in size. We leverage machine learning algorithms, such as random forests, and conduct behavioral analysis by examining patterns and anomalies in API call sequences. By investigating how the function calls occur regardless of their order, we can identify discriminating features that can help us identify malware early on. The models we've developed are not only effective but also efficient. They are lightweight and can run on any machine with minimal performance overhead, while still achieving an impressive F1-Score of over 85\%. We also empirically show that we only need a subset of the function call sequence, specifically calls to the ntdll.dll library, to identify malware. Our research demonstrates the efficacy of this approach through empirical evaluations, underscoring its accuracy and scalability. The code is open source and available at Github along with the dataset on Zenodo.

### RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.12794)] [[cool](https://papers.cool/arxiv/2502.12794)] [[pdf](https://arxiv.org/pdf/2502.12794)]
> **Authors**: Tanqiu Jiang,Changjiang Li,Fenglong Ma,Ting Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Published in ICLR 2025
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别,机器学习
- **Abstract**: Differentially private diffusion models (DPDMs) harness the remarkable generative capabilities of diffusion models while enforcing differential privacy (DP) for sensitive data. However, existing DPDM training approaches often suffer from significant utility loss, large memory footprint, and expensive inference cost, impeding their practical uses. To overcome such limitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a novel approach that integrates retrieval augmented generation (RAG) into DPDM training. Specifically, RAPID leverages available public data to build a knowledge base of sample trajectories; when training the diffusion model on private data, RAPID computes the early sampling steps as queries, retrieves similar trajectories from the knowledge base as surrogates, and focuses on training the later sampling steps in a differentially private manner. Extensive evaluation using benchmark datasets and models demonstrates that, with the same privacy guarantee, RAPID significantly outperforms state-of-the-art approaches by large margins in generative quality, memory footprint, and inference cost, suggesting that retrieval-augmented DP training represents a promising direction for developing future privacy-preserving generative models. The code is available at: https://github.com/TanqiuJiang/RAPID

### Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training 
[[arxiv](https://arxiv.org/abs/2502.12734)] [[cool](https://papers.cool/arxiv/2502.12734)] [[pdf](https://arxiv.org/pdf/2502.12734)]
> **Authors**: Yuanfan Li,Zhaohan Zhang,Chengzhengxu Li,Chao Shen,Xiaoming Liu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Submitted to ACL 2025, Preprint, Under review
- **标题**: None
- **领域**: 密码学和安全,计算语言学
- **Abstract**: Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 9 text perturbation strategies and 5 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches.

### TREND: A Whitespace Replacement Information Hiding Method 
[[arxiv](https://arxiv.org/abs/2502.12710)] [[cool](https://papers.cool/arxiv/2502.12710)] [[pdf](https://arxiv.org/pdf/2502.12710)]
> **Authors**: Malte Hellmeier,Hendrik Norkowski,Ernst-Christoph Schrewe,Haydar Qarawlus,Falk Howar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,软件工程
- **Abstract**: Large Language Models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and a text generated by an LLM has become almost impossible. Information hiding techniques such as digital watermarking or steganography can help by embedding information inside text without being noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or do not work on pure, unformatted text. In this paper, we introduce a novel method for information hiding termed TREND, which is able to conceal any byte-encoded sequence within a cover text. The proposed method is implemented as a multi-platform library using the Kotlin programming language, accompanied by a command-line tool and a web interface provided as examples of usage. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without increasing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. Our experimental benchmark comparison on a dataset of one million Wikipedia articles compares ten algorithms from literature and practice. It proves the robustness of our proposed method in various applications while remaining imperceptible to humans. We discuss the limitations of limited embedding capacity and further robustness, which guide implications for future work.

### Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach 
[[arxiv](https://arxiv.org/abs/2502.12630)] [[cool](https://papers.cool/arxiv/2502.12630)] [[pdf](https://arxiv.org/pdf/2502.12630)]
> **Authors**: Tvrtko Sternak,Davor Runje,Dorian Granoša,Chi Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: This paper presents a novel approach to evaluating the security of large language models (LLMs) against prompt leakage-the exposure of system-level prompts or proprietary configurations. We define prompt leakage as a critical threat to secure LLM deployment and introduce a framework for testing the robustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we implement a multi-agent system where cooperative agents are tasked with probing and exploiting the target LLM to elicit its prompt. Guided by traditional definitions of security in cryptography, we further define a prompt leakage-safe system as one in which an attacker cannot distinguish between two agents: one initialized with an original prompt and the other with a prompt stripped of all sensitive information. In a safe system, the agents' outputs will be indistinguishable to the attacker, ensuring that sensitive information remains secure. This cryptographically inspired framework provides a rigorous standard for evaluating and designing secure LLMs. This work establishes a systematic methodology for adversarial testing of prompt leakage, bridging the gap between automated threat modeling and practical LLM security. You can find the implementation of our prompt leakage probing on GitHub.

### DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent 
[[arxiv](https://arxiv.org/abs/2502.12575)] [[cool](https://papers.cool/arxiv/2502.12575)] [[pdf](https://arxiv.org/pdf/2502.12575)]
> **Authors**: Pengyu Zhu,Zhenhong Zhou,Yuanhe Zhang,Shilinlu Yan,Kun Wang,Sen Su
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities. However, backdoor attacks are typically detectable by safety audits that analyze the reasoning process of agents. To this end, we propose a novel backdoor implantation strategy called \textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}. Specifically, we introduce dynamic encryption, which maps the backdoor into benign content, effectively circumventing safety audits. To enhance stealthiness, we further decompose the backdoor into multiple sub-backdoor fragments. Based on these advancements, backdoors are allowed to bypass safety audits significantly. Additionally, we present AgentBackdoorEval, a dataset designed for the comprehensive evaluation of agent backdoor attacks. Experimental results across multiple datasets demonstrate that our method achieves an attack success rate nearing 100\% while maintaining a detection rate of 0\%, illustrating its effectiveness in evading safety audits. Our findings highlight the limitations of existing safety mechanisms in detecting advanced attacks, underscoring the urgent need for more robust defenses against backdoor threats. Code and data are available at https://github.com/whfeLingYu/DemonAgent.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework 
[[arxiv](https://arxiv.org/abs/2502.13407)] [[cool](https://papers.cool/arxiv/2502.13407)] [[pdf](https://arxiv.org/pdf/2502.13407)]
> **Authors**: Ziyuan Liu,Ruifei Zhu,Long Gao,Yuanxiu Zhou,Jingyu Ma,Yuantao Gu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 14 pages, 9 figures. Submitted to IEEE Transactions on Geoscience and Remote Sensing (TGRS)
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at https://github.com/circleLZY/MTKD-CD.

### MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping 
[[arxiv](https://arxiv.org/abs/2502.13399)] [[cool](https://papers.cool/arxiv/2502.13399)] [[pdf](https://arxiv.org/pdf/2502.13399)]
> **Authors**: Hossein Zaremehrjerdi,Lisa Coffey,Talukder Jubery,Huyu Liu,Jon Turkus,Kyle Linders,James C. Schnable,Patrick S. Schnable,Baskar Ganapathysubramanian
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: :68T07; 68U10
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Quantifying the variation in yield component traits of maize (Zea mays L.), which together determine the overall productivity of this globally important crop, plays a critical role in plant genetics research, plant breeding, and the development of improved farming practices. Grain yield per acre is calculated by multiplying the number of plants per acre, ears per plant, number of kernels per ear, and the average kernel weight. The number of kernels per ear is determined by the number of kernel rows per ear multiplied by the number of kernels per row. Traditional manual methods for measuring these two traits are time-consuming, limiting large-scale data collection. Recent automation efforts using image processing and deep learning encounter challenges such as high annotation costs and uncertain generalizability. We tackle these issues by exploring Large Vision Models for zero-shot, annotation-free maize kernel segmentation. By using an open-source large vision model, the Segment Anything Model (SAM), we segment individual kernels in RGB images of maize ears and apply a graph-based algorithm to calculate the number of kernels per row. Our approach successfully identifies the number of kernels per row across a wide range of maize ears, showing the potential of zero-shot learning with foundation vision models combined with image processing techniques to improve automation and reduce subjectivity in agronomic data collection. All our code is open-sourced to make these affordable phenotyping methods accessible to everyone.

### SNN-Driven Multimodal Human Action Recognition via Event Camera and Skeleton Data Fusion 
[[arxiv](https://arxiv.org/abs/2502.13385)] [[cool](https://papers.cool/arxiv/2502.13385)] [[pdf](https://arxiv.org/pdf/2502.13385)]
> **Authors**: Naichuan Zheng,Hailun Xia
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.

### Pretrained Image-Text Models are Secretly Video Captioners 
[[arxiv](https://arxiv.org/abs/2502.13363)] [[cool](https://papers.cool/arxiv/2502.13363)] [[pdf](https://arxiv.org/pdf/2502.13363)]
> **Authors**: Chunhui Zhang,Yiren Jian,Zhongyu Ouyang,Soroush Vosoughi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted to the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL 2025). The first two authors contributed equally and were listed in random order
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Developing video captioning models is computationally expensive. The dynamic nature of video also complicates the design of multimodal models that can effectively caption these sequences. However, we find that by using minimal computational resources and without complex modifications to address video dynamics, an image-based model can be repurposed to outperform several specialised video captioning systems. Our adapted model demonstrates top tier performance on major benchmarks, ranking 2nd on MSRVTT and MSVD, and 3rd on VATEX. We transform it into a competitive video captioner by post training a typical image captioning model BLIP2 with only 6,000 video text pairs and simply concatenating frames (significantly fewer data than other methods), which use 2.5 to 144 million pairs. From a resource optimization perspective, this video captioning study focuses on three fundamental factors: optimizing model scale, maximizing data efficiency, and incorporating reinforcement learning. This extensive study demonstrates that a lightweight, image based adaptation strategy can rival state-of-the-art video captioning systems, offering a practical solution for low-resource scenarios.

### MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching 
[[arxiv](https://arxiv.org/abs/2502.13234)] [[cool](https://papers.cool/arxiv/2502.13234)] [[pdf](https://arxiv.org/pdf/2502.13234)]
> **Authors**: Yen-Siang Wu,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Project page: https://www.csie.ntu.edu.tw/~b09902097/motionmatcher/
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Text-to-video (T2V) diffusion models have shown promising capabilities in synthesizing realistic videos from input text prompts. However, the input text description alone provides limited control over the precise objects movements and camera framing. In this work, we tackle the motion customization problem, where a reference video is provided as motion guidance. While most existing methods choose to fine-tune pre-trained diffusion models to reconstruct the frame differences of the reference video, we observe that such strategy suffer from content leakage from the reference video, and they cannot capture complex motion accurately. To address this issue, we propose MotionMatcher, a motion customization framework that fine-tunes the pre-trained T2V diffusion model at the feature level. Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features to fine-tune diffusion models, ensuring precise motion learning. For the sake of memory efficiency and accessibility, we utilize a pre-trained T2V diffusion model, which contains considerable prior knowledge about video motion, to compute these motion features. In our experiments, we demonstrate state-of-the-art motion customization performances, validating the design of our framework.

### Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.13146)] [[cool](https://papers.cool/arxiv/2502.13146)] [[pdf](https://arxiv.org/pdf/2502.13146)]
> **Authors**: Shuo Xing,Yuping Wang,Peiran Li,Ruizheng Bai,Yueqi Wang,Chengxuan Qian,Huaxiu Yao,Zhengzhong Tu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in https://github.com/taco-group/Re-Align.

### Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation 
[[arxiv](https://arxiv.org/abs/2502.13145)] [[cool](https://papers.cool/arxiv/2502.13145)] [[pdf](https://arxiv.org/pdf/2502.13145)]
> **Authors**: Bencheng Liao,Hongyuan Tao,Qian Zhang,Tianheng Cheng,Yingyue Li,Haoran Yin,Wenyu Liu,Xinggang Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Code andmodelare available at https://github.com/hustvl/mmMamba
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba

### AV-Flow: Transforming Text to Audio-Visual Human-like Interactions 
[[arxiv](https://arxiv.org/abs/2502.13133)] [[cool](https://papers.cool/arxiv/2502.13133)] [[pdf](https://arxiv.org/pdf/2502.13133)]
> **Authors**: Aggelina Chatziagapi,Louis-Philippe Morency,Hongyu Gong,Michael Zollhoefer,Dimitris Samaras,Alexander Richard
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations, AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars. Project page: https://aggelinacha.github.io/AV-Flow/

### Magma: A Foundation Model for Multimodal AI Agents 
[[arxiv](https://arxiv.org/abs/2502.13130)] [[cool](https://papers.cool/arxiv/2502.13130)] [[pdf](https://arxiv.org/pdf/2502.13130)]
> **Authors**: Jianwei Yang,Reuben Tan,Qianhui Wu,Ruijie Zheng,Baolin Peng,Yongyuan Liang,Yu Gu,Mu Cai,Seonghyeon Ye,Joel Jang,Yuquan Deng,Lars Liden,Jianfeng Gao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 29 pages, 16 figures, technical report from MSR
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,人机交互,机器学习,机器人技术
- **Abstract**: We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.

### Understanding and Rectifying Safety Perception Distortion in VLMs 
[[arxiv](https://arxiv.org/abs/2502.13095)] [[cool](https://papers.cool/arxiv/2502.13095)] [[pdf](https://arxiv.org/pdf/2502.13095)]
> **Authors**: Xiaohan Zou,Jian Kang,George Kesidis,Lu Lin
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学,机器学习
- **Abstract**: Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a "safer" direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component, ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that ShiftDC significantly enhances alignment performance on safety benchmarks without impairing model utility.

### Enhancing Power Grid Inspections with Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.13037)] [[cool](https://papers.cool/arxiv/2502.13037)] [[pdf](https://arxiv.org/pdf/2502.13037)]
> **Authors**: Diogo Lavado,Ricardo Santos,Andre Coelho,Joao Santos,Alessandra Micheletti,Claudia Soares
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Ensuring the safety and reliability of power grids is critical as global energy demands continue to rise. Traditional inspection methods, such as manual observations or helicopter surveys, are resource-intensive and lack scalability. This paper explores the use of 3D computer vision to automate power grid inspections, utilizing the TS40K dataset -- a high-density, annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic segmentation, our approach addresses challenges like class imbalance and noisy data to enhance the detection of critical grid components such as power lines and towers. The benchmark results indicate significant performance improvements, with IoU scores reaching 95.53% for the detection of power lines using transformer-based models. Our findings illustrate the potential for integrating ML into grid maintenance workflows, increasing efficiency and enabling proactive risk management strategies.

### A deep learning framework for efficient pathology image analysis 
[[arxiv](https://arxiv.org/abs/2502.13027)] [[cool](https://papers.cool/arxiv/2502.13027)] [[pdf](https://arxiv.org/pdf/2502.13027)]
> **Authors**: Peter Neidlinger,Tim Lenz,Sebastian Foersch,Chiara M. L. Loeffler,Jan Clusmann,Marco Gustav,Lawrence A. Shaktah,Rupert Langer,Bastian Dislich,Lisa A. Boardman,Amy J. French,Ellen L. Goode,Andrea Gsur,Stefanie Brezina,Marc J. Gunter,Robert Steinfelder,Hans-Michael Behrens,Christoph Röcken,Tabitha Harrison,Ulrike Peters,Amanda I. Phipps,Giuseppe Curigliano,Nicola Fusco,Antonio Marra,Michael Hoffmeister, et al. (2 additional authors not shown)
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Artificial intelligence (AI) has transformed digital pathology by enabling biomarker prediction from high-resolution whole slide images (WSIs). However, current methods are computationally inefficient, processing thousands of redundant tiles per WSI and requiring complex aggregator models. We introduce EAGLE (Efficient Approach for Guided Local Examination), a deep learning framework that emulates pathologists by selectively analyzing informative regions. EAGLE incorporates two foundation models: CHIEF for efficient tile selection and Virchow2 for extracting high-quality features. Benchmarking was conducted against leading slide- and tile-level foundation models across 31 tasks from four cancer types, spanning morphology, biomarker prediction and prognosis. EAGLE outperformed state-of-the-art foundation models by up to 23% and achieved the highest AUROC overall. It processed a slide in 2.27 seconds, reducing computational time by more than 99% compared to existing models. This efficiency enables real-time workflows, allows pathologists to validate all tiles which are used by the model during analysis, and eliminates dependence on high-performance computing, making AI-powered pathology more accessible. By reliably identifying meaningful regions and minimizing artifacts, EAGLE provides robust and interpretable outputs, supporting rapid slide searches, integration into multi-omics pipelines and emerging clinical foundation models.

### Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms 
[[arxiv](https://arxiv.org/abs/2502.13023)] [[cool](https://papers.cool/arxiv/2502.13023)] [[pdf](https://arxiv.org/pdf/2502.13023)]
> **Authors**: Kangning Cui,Rongkun Zhu,Manqi Wang,Wei Tang,Gregory D. Larsen,Victor P. Pauca,Sarra Alqahtani,Fan Yang,David Segurado,David Lutz,Jean-Michel Morel,Miles R. Silman
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 8 figures, 4 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Palms are ecologically and economically indicators of tropical forest health, biodiversity, and human impact that support local economies and global forest product supply chains. While palm detection in plantations is well-studied, efforts to map naturally occurring palms in dense forests remain limited by overlapping crowns, uneven shading, and heterogeneous landscapes. We develop PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline for detecting and localizing palms in dense tropical forests using large orthomosaic images. Orthomosaics are created from thousands of aerial images and spanning several to hundreds of gigabytes. Our contributions are threefold. First, we construct a large UAV-derived orthomosaic dataset collected across 21 ecologically diverse sites in western Ecuador, annotated with 8,830 bounding boxes and 5,026 palm center points. Second, we evaluate multiple state-of-the-art object detectors based on efficiency and performance, integrating zero-shot SAM 2 as the segmentation backbone, and refining the results for precise geographic mapping. Third, we apply calibration methods to align confidence scores with IoU and explore saliency maps for feature explainability. Though optimized for palms, PRISM is adaptable for identifying other natural objects, such as eastern white pines. Future work will explore transfer learning for lower-resolution datasets (0.5 to 1m).

### Mean of Means: Human Localization with Calibration-free and Unconstrained Camera Settings (extended version) 
[[arxiv](https://arxiv.org/abs/2502.13017)] [[cool](https://papers.cool/arxiv/2502.13017)] [[pdf](https://arxiv.org/pdf/2502.13017)]
> **Authors**: Tianyi Zhang,Wengyu Zhang,Xulu Zhang,Jiaxin Wu,Xiao-Yong Wei,Jiannong Cao,Qing Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: arXiv admin note: substantial text overlap with arXiv:2407.20870
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: Accurate human localization is crucial for various applications, especially in the Metaverse era. Existing high precision solutions rely on expensive, tag-dependent hardware, while vision-based methods offer a cheaper, tag-free alternative. However, current vision solutions based on stereo vision face limitations due to rigid perspective transformation principles and error propagation in multi-stage SVD solvers. These solutions also require multiple high-resolution cameras with strict setup constraints.To address these limitations, we propose a probabilistic approach that considers all points on the human body as observations generated by a distribution centered around the body's geometric center. This enables us to improve sampling significantly, increasing the number of samples for each point of interest from hundreds to billions. By modeling the relation between the means of the distributions of world coordinates and pixel coordinates, leveraging the Central Limit Theorem, we ensure normality and facilitate the learning process. Experimental results demonstrate human localization accuracy of 96\% within a 0.3$m$ range and nearly 100\% accuracy within a 0.5$m$ range, achieved at a low cost of only 10 USD using two web cameras with a resolution of 640$\times$480 pixels.

### Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for Improved Text-Based Learning for LGE Detection 
[[arxiv](https://arxiv.org/abs/2502.12948)] [[cool](https://papers.cool/arxiv/2502.12948)] [[pdf](https://arxiv.org/pdf/2502.12948)]
> **Authors**: Athira J Jacob,Puneet Sharma,Daniel Rueckert
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Poster at Workshop on LargeLanguageModels and GenerativeAIfor Health at AAAI 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Detection of hyperenhancement from cardiac LGE MRI images is a complex task requiring significant clinical expertise. Although deep learning-based models have shown promising results for the task, they require large amounts of data with fine-grained annotations. Clinical reports generated for cardiac MR studies contain rich, clinically relevant information, including the location, extent and etiology of any scars present. Although recently developed CLIP-based training enables pretraining models with image-text pairs, it requires large amounts of data and further finetuning strategies on downstream tasks. In this study, we use various strategies rooted in domain knowledge to train a model for LGE detection solely using text from clinical reports, on a relatively small clinical cohort of 965 patients. We improve performance through the use of synthetic data augmentation, by systematically creating scar images and associated text. In addition, we standardize the orientation of the images in an anatomy-informed way to enable better alignment of spatial and text features. We also use a captioning loss to enable fine-grained supervision and explore the effect of pretraining of the vision encoder on performance. Finally, ablation studies are carried out to elucidate the contributions of each design component to the overall performance of the model.

### Contrast-Unity for Partially-Supervised Temporal Sentence Grounding 
[[arxiv](https://arxiv.org/abs/2502.12917)] [[cool](https://papers.cool/arxiv/2502.12917)] [[pdf](https://arxiv.org/pdf/2502.12917)]
> **Authors**: Haicheng Wang,Chen Ju,Weixiong Lin,Chaofan Ma,Shuai Xiao,Ya Zhang,Yanfeng Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted by ICASSP 2025.The first two authors share the same contribution. arXiv admin note: text overlap with arXiv:2302.09850
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Temporal sentence grounding aims to detect event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great results but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation costs, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip is available during training. To make full use of partial labels, we specially design one contrast-unity framework, with the two-stage goal of implicit-explicit progressive grounding. In the implicit stage, we align event-query representations at fine granularity using comprehensive quadruple contrastive learning: event-query gather, event-background separation, intra-cluster compactness and inter-cluster separability. Then, high-quality representations bring acceptable grounding pseudo-labels. In the explicit stage, to explicitly optimize grounding objectives, we train one fully-supervised model using obtained pseudo-labels for grounding refinement and denoising. Extensive experiments and thoroughly ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision, as well as our superior performance.

### CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image 
[[arxiv](https://arxiv.org/abs/2502.12894)] [[cool](https://papers.cool/arxiv/2502.12894)] [[pdf](https://arxiv.org/pdf/2502.12894)]
> **Authors**: Kaixin Yao,Longwen Zhang,Xinhao Yan,Yan Zeng,Qixuan Zhang,Lan Xu,Wei Yang,Jiayuan Gu,Jingyi Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Project Page: https://sites.google.com/view/cast4
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.

### Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models 
[[arxiv](https://arxiv.org/abs/2502.12892)] [[cool](https://papers.cool/arxiv/2502.12892)] [[pdf](https://arxiv.org/pdf/2502.12892)]
> **Authors**: Thomas Fel,Ekdeep Singh Lubana,Jacob S. Prince,Matthew Kowal,Victor Boutin,Isabel Papadimitriou,Binxu Wang,Martin Wattenberg,Demba Ba,Talia Konkle
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine learning interpretability, enabling the unsupervised decomposition of model representations into a dictionary of abstract, human-interpretable concepts. However, we reveal a fundamental limitation: existing SAEs exhibit severe instability, as identical models trained on similar datasets can produce sharply different dictionaries, undermining their reliability as an interpretability tool. To address this issue, we draw inspiration from the Archetypal Analysis framework introduced by Cutler & Breiman (1994) and present Archetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex hull of data. This geometric anchoring significantly enhances the stability of inferred dictionaries, and their mildly relaxed variants RA-SAEs further match state-of-the-art reconstruction abilities. To rigorously assess dictionary quality learned by SAEs, we introduce two new benchmarks that test (i) plausibility, if dictionaries recover "true" classification directions and (ii) identifiability, if dictionaries disentangle synthetic concept mixtures. Across all evaluations, RA-SAEs consistently yield more structured representations while uncovering novel, semantically meaningful concepts in large-scale vision models.

### Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations 
[[arxiv](https://arxiv.org/abs/2502.12801)] [[cool](https://papers.cool/arxiv/2502.12801)] [[pdf](https://arxiv.org/pdf/2502.12801)]
> **Authors**: Hinrich Rahlfs,Markus Hüllebrand,Sebastian Schmitter,Christoph Strecker,Andreas Harloff,Anja Hennemuth
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Presented at MICAD 2024 Conference
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose a novel approach that uses sparse annotations from clinical studies to train a 3D segmentation of the carotid artery wall. We use a centerline annotation to sample perpendicular cross-sections of the carotid artery and use an adversarial 2D network to segment them. These annotations are then transformed into 3D pseudo-labels for training of a 3D convolutional neural network, circumventing the creation of manual 3D masks. For pseudo-label creation in the bifurcation area we propose the use of cross-sections perpendicular to the bifurcation axis and show that this enhances segmentation performance. Different sampling distances had a lesser impact. The proposed method allows for efficient training of 3D segmentation, offering potential improvements in the assessment of carotid artery stenosis and allowing the extraction of 3D biomarkers such as plaque volume.

### Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation Mechanism for Spiking Neural Networks in 3D cloud 
[[arxiv](https://arxiv.org/abs/2502.12791)] [[cool](https://papers.cool/arxiv/2502.12791)] [[pdf](https://arxiv.org/pdf/2502.12791)]
> **Authors**: Jian Song,Boxuan Zheng,Xiangfei Yang,Donglin Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Due to the similar characteristics between event-based visual data and point clouds, recent studies have emerged that treat event data as event clouds to learn based on point cloud analysis. Additionally, some works approach point clouds from the perspective of event vision, employing Spiking Neural Network (SNN) due to their asynchronous nature. However, these contributions are often domain-specific, making it difficult to extend their applicability to other intersecting fields. Moreover, while SNN-based visual tasks have seen significant growth, the conventional timestep-wise iterative activation strategy largely limits their real-world applications by large timesteps, resulting in significant delays and increased computational costs. Although some innovative methods achieve good performance with short timesteps (<10), few have fundamentally restructured the update strategy of spiking neurons to completely overcome the limitations of timesteps. In response to these concerns, we propose a novel and general activation strategy for spiking neurons called Activation-wise Membrane Potential Propagation (AMP2). This approach extends the concept of timesteps from a manually crafted parameter within the activation function to any existing network structure. In experiments on common point cloud tasks (classification, object, and scene segmentation) and event cloud tasks (action recognition), we found that AMP2 stabilizes SNN training, maintains competitive performance, and reduces latency compared to the traditional timestep-wise activation paradigm.

### 3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces 
[[arxiv](https://arxiv.org/abs/2502.12742)] [[cool](https://papers.cool/arxiv/2502.12742)] [[pdf](https://arxiv.org/pdf/2502.12742)]
> **Authors**: Fabian Bongratz,Yitong Li,Sama Elbaroudy,Christian Wachinger
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted by Information Processing in Medical Imaging (IPMI) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. To achieve this, we leverage a Brownian bridge process which allows for direct structured mapping between shape contours and medical images. Specifically, we adapt the concept of the Brownian bridge diffusion model to 3D and extend it to embrace various complementary shape representations. Our experiments demonstrate significant improvements in the geometric accuracy of reconstructed structures compared to previous voxel-based approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding high variation in non-target structures like the skull. Finally, we highlight the capability of our approach to simulate cortical atrophy at the sub-voxel level. Our code is available at https://github.com/ai-med/Cor2Vox.

### Spiking Vision Transformer with Saccadic Attention 
[[arxiv](https://arxiv.org/abs/2502.12677)] [[cool](https://papers.cool/arxiv/2502.12677)] [[pdf](https://arxiv.org/pdf/2502.12677)]
> **Authors**: Shuai Wang,Malu Zhang,Dehao Zhang,Ammar Belatreche,Yichen Xiao,Yu Liang,Yimeng Shan,Qian Sun,Enqi Zhang,Yang Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counterparts. Here, we first analyze why SNN-based ViTs suffer from limited performance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected visual areas at each timestep and significantly enhances whole scene understanding through temporal interactions. Building on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.

### Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.12635)] [[cool](https://papers.cool/arxiv/2502.12635)] [[pdf](https://arxiv.org/pdf/2502.12635)]
> **Authors**: Yunhao Gou,Hansi Yang,Zhili Liu,Kai Chen,Yihan Zeng,Lanqing Hong,Zhenguo Li,Qun Liu,James T. Kwok,Yu Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models (MLLMs) but it is hindered by corrupted datasets containing hallucinated content, incorrect responses, and poor OCR quality. While prior works focus on dataset refinement through high-quality data collection or rule-based filtering, they are costly or limited to specific types of corruption. To deeply understand how corrupted data affects MLLMs, in this paper, we systematically investigate this issue and find that while corrupted data degrades the performance of MLLMs, its effects are largely superficial in that the performance of MLLMs can be largely restored by either disabling a small subset of parameters or post-training with a small amount of clean data. Additionally, corrupted MLLMs exhibit improved ability to distinguish clean samples from corrupted ones, enabling the dataset cleaning without external help. Based on those insights, we propose a corruption-robust training paradigm combining self-validation and post-training, which significantly outperforms existing corruption mitigation strategies.

### MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation 
[[arxiv](https://arxiv.org/abs/2502.12632)] [[cool](https://papers.cool/arxiv/2502.12632)] [[pdf](https://arxiv.org/pdf/2502.12632)]
> **Authors**: Sihyun Yu,Meera Hahn,Dan Kondratyuk,Jinwoo Shin,Agrim Gupta,José Lezama,Irfan Essa,David Ross,Jonathan Huang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: preprint. 26 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Diffusion models are successful for synthesizing high-quality videos but are limited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained footage (e.g. over minutes) still remains an open research question. In this paper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers), a new diffusion model specialized for long video generation. MALT Diffusion (or just MALT) handles long videos by subdividing them into short segments and doing segment-level autoregressive generation. To achieve this, we first propose recurrent attention layers that encode multiple segments into a compact memory latent vector; by maintaining this memory vector over time, MALT is able to condition on it and continuously generate new footage based on a long temporal context. We also present several training techniques that enable the model to generate frames over a long horizon with consistent quality and minimal degradation. We validate the effectiveness of MALT through experiments on long video benchmarks. We first perform extensive analysis of MALT in long-contextual understanding capability and stability using popular long video benchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video generation on UCF-101, outperforming the previous state-of-the-art of 648.4. Finally, we explore MALT's capabilities in a text-to-video generation setting and show that it can produce long videos compared with recent techniques for long text-to-video generation.

### DAMamba: Vision State Space Model with Dynamic Adaptive Scan 
[[arxiv](https://arxiv.org/abs/2502.12627)] [[cool](https://papers.cool/arxiv/2502.12627)] [[pdf](https://arxiv.org/pdf/2502.12627)]
> **Authors**: Tanzhe Li,Caoshuo Li,Jiayi Lyu,Hongjuan Pei,Baochang Zhang,Taisong Jin,Rongrong Ji
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: State space models (SSMs) have recently garnered significant attention in computer vision. However, due to the unique characteristics of image data, adapting SSMs from natural language processing to computer vision has not outperformed the state-of-the-art convolutional neural networks (CNNs) and Vision Transformers (ViTs). Existing vision SSMs primarily leverage manually designed scans to flatten image patches into sequences locally or globally. This approach disrupts the original semantic spatial adjacency of the image and lacks flexibility, making it difficult to capture complex image structures. To address this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven method that adaptively allocates scanning orders and regions. This enables more flexible modeling capabilities while maintaining linear computational complexity and global modeling capacity. Based on DAS, we further propose the vision backbone DAMamba, which significantly outperforms current state-of-the-art vision Mamba models in vision tasks such as image classification, object detection, instance segmentation, and semantic segmentation. Notably, it surpasses some of the latest state-of-the-art CNNs and ViTs. Code will be available at https://github.com/ltzovo/DAMamba.

### S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images 
[[arxiv](https://arxiv.org/abs/2502.12604)] [[cool](https://papers.cool/arxiv/2502.12604)] [[pdf](https://arxiv.org/pdf/2502.12604)]
> **Authors**: Lei Ding,Xibing Zuo,Danfeng Hong,Haitao Guo,Jun Lu,Zhihui Gong,Lorenzo Bruzzone
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images remains a difficult challenge due to the inherent spatio-temporal complexity within data, and the heterogeneity arising from different imaging sensors. Inspired by recent advancements in Visual Foundation Models (VFMs) and Contrastive Learning (CL) methodologies, this research aims to develop CL methodologies to translate implicit knowledge in VFM into change representations, thus eliminating the need for explicit supervision. To this end, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both homogeneous and multimodal RS images. Differently from existing CL methodologies that typically focus on learning multi-temporal similarities, we introduce a novel triplet learning strategy that explicitly models temporal differences, which are crucial to the CD task. Furthermore, random spatial and spectral perturbations are introduced during the training to enhance robustness to temporal noise. In addition, a grid sparsity regularization is defined to suppress insignificant changes, and an IoU-matching algorithm is developed to refine the CD results. Experiments on four benchmark CD datasets demonstrate that the proposed S2C learning framework achieves significant improvements in accuracy, surpassing current state-of-the-art by over 31\%, 9\%, 23\%, and 15\%, respectively. It also demonstrates robustness and sample efficiency, suitable for training and adaptation of various Visual Foundation Models (VFMs) or backbone neural networks. The relevant code will be available at: github.com/DingLei14/S2C.

### CutPaste&Find: Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base 
[[arxiv](https://arxiv.org/abs/2502.12591)] [[cool](https://papers.cool/arxiv/2502.12591)] [[pdf](https://arxiv.org/pdf/2502.12591)]
> **Authors**: Cong-Duy Nguyen,Xiaobao Wu,Duc Anh Vu,Shuai Zhao,Thong Nguyen,Anh Tuan Luu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal reasoning capabilities, but they remain susceptible to hallucination, particularly object hallucination where non-existent objects or incorrect attributes are fabricated in generated descriptions. Existing detection methods achieve strong performance but rely heavily on expensive API calls and iterative LVLM-based validation, making them impractical for large-scale or offline use. To address these limitations, we propose CutPaste\&Find, a lightweight and training-free framework for detecting hallucinations in LVLM-generated outputs. Our approach leverages off-the-shelf visual and linguistic modules to perform multi-step verification efficiently without requiring LVLM inference. At the core of our framework is a Visual-aid Knowledge Base that encodes rich entity-attribute relationships and associated image representations. We introduce a scaling factor to refine similarity scores, mitigating the issue of suboptimal alignment values even for ground-truth image-text pairs. Comprehensive evaluations on benchmark datasets, including POPE and R-Bench, demonstrate that CutPaste\&Find achieves competitive hallucination detection performance while being significantly more efficient and cost-effective than previous methods.

### GVTNet: Graph Vision Transformer For Face Super-Resolution 
[[arxiv](https://arxiv.org/abs/2502.12570)] [[cool](https://papers.cool/arxiv/2502.12570)] [[pdf](https://arxiv.org/pdf/2502.12570)]
> **Authors**: Chao Yang,Yong Fan,Cheng Lu,Minghao Yuan,Zhijing Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advances in face super-resolution research have utilized the Transformer architecture. This method processes the input image into a series of small patches. However, because of the strong correlation between different facial components in facial images. When it comes to super-resolution of low-resolution images, existing algorithms cannot handle the relationships between patches well, resulting in distorted facial components in the super-resolution results. To solve the problem, we propose a transformer architecture based on graph neural networks called graph vision transformer network. We treat each patch as a graph node and establish an adjacency matrix based on the information between patches. In this way, the patch only interacts between neighboring patches, further processing the relationship of facial components. Quantitative and visualization experiments have underscored the superiority of our algorithm over state-of-the-art techniques. Through detailed comparisons, we have demonstrated that our algorithm possesses more advanced super-resolution capabilities, particularly in enhancing facial components. The PyTorch code is available at https://github.com/continueyang/GVTNet

### MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos 
[[arxiv](https://arxiv.org/abs/2502.12558)] [[cool](https://papers.cool/arxiv/2502.12558)] [[pdf](https://arxiv.org/pdf/2502.12558)]
> **Authors**: Huaying Yuan,Jian Ni,Yueze Wang,Junjie Zhou,Zhengyang Liang,Zheng Liu,Zhao Cao,Zhicheng Dou,Ji-Rong Wen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Retrieval augmented generation (RAG) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multimodal large language models (MLLMs) to generate high-quality answers in a cost-effective way. In this work, we present MomentSeeker, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key advantages. First, it incorporates long videos of over 500 seconds on average, making it the first benchmark specialized for long-video moment retrieval. Second, it covers a wide range of task categories (including Moment Search, Caption Alignment, Image-conditioned Moment Search, and Video-conditioned Moment Search) and diverse application scenarios (e.g., sports, movies, cartoons, and ego), making it a comprehensive tool for assessing retrieval models' general LVMR performance. Additionally, the evaluation tasks are carefully curated through human annotation, ensuring the reliability of assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic data, which demonstrates strong performance on our benchmark. We perform extensive experiments with various popular multimodal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods. Our created resources will be shared with community to advance future research in this field.

### Spatiotemporal Multi-Camera Calibration using Freely Moving People 
[[arxiv](https://arxiv.org/abs/2502.12546)] [[cool](https://papers.cool/arxiv/2502.12546)] [[pdf](https://arxiv.org/pdf/2502.12546)]
> **Authors**: Sang-Eun Lee,Ko Nishino,Shohei Nobuhara
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We propose a novel method for spatiotemporal multi-camera calibration using freely moving people in multiview videos. Since calibrating multiple cameras and finding matches across their views are inherently interdependent, performing both in a unified framework poses a significant challenge. We address these issues as a single registration problem of matching two sets of 3D points, leveraging human motion in dynamic multi-person scenes. To this end, we utilize 3D human poses obtained from an off-the-shelf monocular 3D human pose estimator and transform them into 3D points on a unit sphere, to solve the rotation, time offset, and the association alternatingly. We employ a probabilistic approach that can jointly solve both problems of aligning spatiotemporal data and establishing correspondences through soft assignment between two views. The translation is determined by applying coplanarity constraints. The pairwise registration results are integrated into a multiview setup, and then a nonlinear optimization method is used to improve the accuracy of the camera poses, temporal offsets, and multi-person associations. Extensive experiments on synthetic and real data demonstrate the effectiveness and flexibility of the proposed method as a practical marker-free calibration tool.

### When Segmentation Meets Hyperspectral Image: New Paradigm for Hyperspectral Image Classification 
[[arxiv](https://arxiv.org/abs/2502.12541)] [[cool](https://papers.cool/arxiv/2502.12541)] [[pdf](https://arxiv.org/pdf/2502.12541)]
> **Authors**: Weilian Zhou,Weixuan Xie,Sei-ichiro Kamata,Man Sing Wong,Huiying,Hou,Haipeng Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Hyperspectral image (HSI) classification is a cornerstone of remote sensing, enabling precise material and land-cover identification through rich spectral information. While deep learning has driven significant progress in this task, small patch-based classifiers, which account for over 90% of the progress, face limitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach considers a limited receptive field, resulting in insufficient spatial structural information critical for object-level identification and noise-like misclassifications even within uniform regions; (2) undefined optimal patch sizes lead to coarse label predictions, which degrade performance; and (3) a lack of multi-shape awareness around objects. To address these challenges, we draw inspiration from large-scale image segmentation techniques, which excel at handling object boundaries-a capability essential for semantic labeling in HSI classification. However, their application remains under-explored in this task due to (1) the prevailing notion that larger patch sizes degrade performance, (2) the extensive unlabeled regions in HSI groundtruth, and (3) the misalignment of input shapes between HSI data and segmentation models. Thus, in this study, we propose a novel paradigm and baseline, HSIseg, for HSI classification that leverages segmentation techniques combined with a novel Dynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We also introduce an intuitive progressive learning framework with adaptive pseudo-labeling to iteratively incorporate unlabeled regions into the training process, thereby advancing the application of segmentation techniques. Additionally, we incorporate auxiliary data through multi-source data collaboration, promoting better feature interaction. Validated on five public HSI datasets, our proposal outperforms state-of-the-art methods.

## 计算机与社会(cs.CY:Computers and Society)

### Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing 
[[arxiv](https://arxiv.org/abs/2502.12838)] [[cool](https://papers.cool/arxiv/2502.12838)] [[pdf](https://arxiv.org/pdf/2502.12838)]
> **Authors**: Berk Yilmaz,Huthaifa I. Ashqar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,计算语言学
- **Abstract**: The recent advances in large language models (LLMs) have revolutionized industries such as finance, marketing, and customer service by enabling sophisticated natural language processing tasks. However, the broad adoption of LLMs brings significant challenges, particularly in the form of social biases that can be embedded within their outputs. Biases related to gender, age, and other sensitive attributes can lead to unfair treatment, raising ethical concerns and risking both company reputation and customer trust. This study examined bias in finance-related marketing slogans generated by LLMs (i.e., ChatGPT) by prompting tailored ads targeting five demographic categories: gender, marital status, age, income level, and education level. A total of 1,700 slogans were generated for 17 unique demographic groups, and key terms were categorized into four thematic groups: empowerment, financial, benefits and features, and personalization. Bias was systematically assessed using relative bias calculations and statistically tested with the Kolmogorov-Smirnov (KS) test against general slogans generated for any individual. Results revealed that marketing slogans are not neutral; rather, they emphasize different themes based on demographic factors. Women, younger individuals, low-income earners, and those with lower education levels receive more distinct messaging compared to older, higher-income, and highly educated individuals. This underscores the need to consider demographic-based biases in AI-generated marketing strategies and their broader societal implications. The findings of this study provide a roadmap for developing more equitable AI systems, highlighting the need for ongoing bias detection and mitigation efforts in LLMs.

### The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1 
[[arxiv](https://arxiv.org/abs/2502.12659)] [[cool](https://papers.cool/arxiv/2502.12659)] [[pdf](https://arxiv.org/pdf/2502.12659)]
> **Authors**: Kaiwen Zhou,Chengzhi Liu,Xuandong Zhao,Shreedhar Jangam,Jayanth Srinivasa,Gaowen Liu,Dawn Song,Xin Eric Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap.

### LLM Safety for Children 
[[arxiv](https://arxiv.org/abs/2502.12552)] [[cool](https://papers.cool/arxiv/2502.12552)] [[pdf](https://arxiv.org/pdf/2502.12552)]
> **Authors**: Prasanjit Rath,Hari Shrawgi,Parag Agrawal,Sandipan Dandapat
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children's lives such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children often overlooked by standard safety evaluations and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM powered applications. Additionally we develop Child User Models that reflect the varied personalities and interests of children informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state of the art LLMs. Our observations reveal significant safety gaps in LLMs particularly in categories harmful to children but not adults

## 数据库(cs.DB:Databases)

### LLM-Powered Proactive Data Systems 
[[arxiv](https://arxiv.org/abs/2502.13016)] [[cool](https://papers.cool/arxiv/2502.13016)] [[pdf](https://arxiv.org/pdf/2502.13016)]
> **Authors**: Sepanta Zeighami,Yiming Lin,Shreya Shankar,Aditya Parameswaran
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ef:IEEE Data Engineering Bulletin March 2025
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: With the power of LLMs, we now have the ability to query data that was previously impossible to query, including text, images, and video. However, despite this enormous potential, most present-day data systems that leverage LLMs are reactive, reflecting our community's desire to map LLMs to known abstractions. Most data systems treat LLMs as an opaque black box that operates on user inputs and data as is, optimizing them much like any other approximate, expensive UDFs, in conjunction with other relational operators. Such data systems do as they are told, but fail to understand and leverage what the LLM is being asked to do (i.e. the underlying operations, which may be error-prone), the data the LLM is operating on (e.g., long, complex documents), or what the user really needs. They don't take advantage of the characteristics of the operations and/or the data at hand, or ensure correctness of results when there are imprecisions and ambiguities. We argue that data systems instead need to be proactive: they need to be given more agency -- armed with the power of LLMs -- to understand and rework the user inputs and the data and to make decisions on how the operations and the data should be represented and processed. By allowing the data system to parse, rewrite, and decompose user inputs and data, or to interact with the user in ways that go beyond the standard single-shot query-result paradigm, the data system is able to address user needs more efficiently and effectively. These new capabilities lead to a rich design space where the data system takes more initiative: they are empowered to perform optimization based on the transformation operations, data characteristics, and user intent. We discuss various successful examples of how this framework has been and can be applied in real-world tasks, and present future directions for this ambitious research agenda.

### Personalized Top-k Set Queries Over Predicted Scores 
[[arxiv](https://arxiv.org/abs/2502.12998)] [[cool](https://papers.cool/arxiv/2502.12998)] [[pdf](https://arxiv.org/pdf/2502.12998)]
> **Authors**: Sohrab Namazi Nia,Subhodeep Ghosh,Senjuti Basu Roy,Sihem Amer-Yahia
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能,机器学习
- **Abstract**: This work studies the applicability of expensive external oracles such as large language models in answering top-k queries over predicted scores. Such scores are incurred by user-defined functions to answer personalized queries over multi-modal data. We propose a generic computational framework that handles arbitrary set-based scoring functions, as long as the functions could be decomposed into constructs, each of which sent to an oracle (in our case an LLM) to predict partial scores. At a given point in time, the framework assumes a set of responses and their partial predicted scores, and it maintains a collection of possible sets that are likely to be the true top-k. Since calling oracles is costly, our framework judiciously identifies the next construct, i.e., the next best question to ask the oracle so as to maximize the likelihood of identifying the true top-k. We present a principled probabilistic model that quantifies that likelihood. We study efficiency opportunities in designing algorithms. We run an evaluation with three large scale datasets, scoring functions, and baselines. Experiments indicate the efficacy of our framework, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring result accuracy. Scalability experiments further indicate that our framework could be used in large-scale applications.

### Graph Neural Networks for Databases: A Survey 
[[arxiv](https://arxiv.org/abs/2502.12908)] [[cool](https://papers.cool/arxiv/2502.12908)] [[pdf](https://arxiv.org/pdf/2502.12908)]
> **Authors**: Ziming Li,Youhuan Li,Yuyu Luo,Guoliang Li,Chuxu Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: A survey focus on GNNs and databases. 9 pages, 4 figures
- **标题**: None
- **领域**: 数据库,人工智能
- **Abstract**: Graph neural networks (GNNs) are powerful deep learning models for graph-structured data, demonstrating remarkable success across diverse domains. Recently, the database (DB) community has increasingly recognized the potentiality of GNNs, prompting a surge of researches focusing on improving database systems through GNN-based approaches. However, despite notable advances, There is a lack of a comprehensive review and understanding of how GNNs could improve DB systems. Therefore, this survey aims to bridge this gap by providing a structured and in-depth overview of GNNs for DB systems. Specifically, we propose a new taxonomy that classifies existing methods into two key categories: (1) Relational Databases, which includes tasks like performance prediction, query optimization, and text-to-SQL, and (2) Graph Databases, addressing challenges like efficient graph query processing and graph similarity computation. We systematically review key methods in each category, highlighting their contributions and practical implications. Finally, we suggest promising avenues for integrating GNNs into Database systems.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Edge-Colored Clustering in Hypergraphs: Beyond Minimizing Unsatisfied Edges 
[[arxiv](https://arxiv.org/abs/2502.13000)] [[cool](https://papers.cool/arxiv/2502.13000)] [[pdf](https://arxiv.org/pdf/2502.13000)]
> **Authors**: Alex Crane,Thomas Stanley,Blair D. Sullivan,Nate Veldt
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,离散数学,机器学习
- **Abstract**: We consider a framework for clustering edge-colored hypergraphs, where the goal is to cluster (equivalently, to color) objects based on the primary type of multiway interactions they participate in. One well-studied objective is to color nodes to minimize the number of unsatisfied hyperedges -- those containing one or more nodes whose color does not match the hyperedge color. We motivate and present advances for several directions that extend beyond this minimization problem. We first provide new algorithms for maximizing satisfied edges, which is the same at optimality but is much more challenging to approximate, with all prior work restricted to graphs. We develop the first approximation algorithm for hypergraphs, and then refine it to improve the best-known approximation factor for graphs. We then introduce new objective functions that incorporate notions of balance and fairness, and provide new hardness results, approximations, and fixed-parameter tractability results.

### Approximate Tree Completion and Learning-Augmented Algorithms for Metric Minimum Spanning Trees 
[[arxiv](https://arxiv.org/abs/2502.12993)] [[cool](https://papers.cool/arxiv/2502.12993)] [[pdf](https://arxiv.org/pdf/2502.12993)]
> **Authors**: Nate Veldt,Thomas Stanley,Benjamin W. Priest,Trevor Steil,Keita Iwabuchi,T. S. Jayram,Geoffrey Sanders
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,离散数学,机器学习
- **Abstract**: Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric space is a fundamental primitive for hierarchical clustering and many other ML tasks, but this takes $Ω(n^2)$ time to even approximate. We introduce a framework for metric MSTs that first (1) finds a forest of disconnected components using practical heuristics, and then (2) finds a small weight set of edges to connect disjoint components of the forest into a spanning tree. We prove that optimally solving the second step still takes $Ω(n^2)$ time, but we provide a subquadratic 2.62-approximation algorithm. In the spirit of learning-augmented algorithms, we then show that if the forest found in step (1) overlaps with an optimal MST, we can approximate the original MST problem in subquadratic time, where the approximation factor depends on a measure of overlap. In practice, we find nearly optimal spanning trees for a wide range of metrics, while being orders of magnitude faster than exact algorithms.

## 图形(cs.GR:Graphics)

### MoVer: Motion Verification for Motion Graphics Animations 
[[arxiv](https://arxiv.org/abs/2502.13372)] [[cool](https://papers.cool/arxiv/2502.13372)] [[pdf](https://arxiv.org/pdf/2502.13372)]
> **Authors**: Jiaju Ma,Maneesh Agrawala
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all of spatio-temporal properties described in the prompt. We introduce MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. We identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). We implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. We then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, our pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation. To evaluate our pipeline, we build a synthetic dataset of 5600 text prompts paired with ground truth MoVer verification programs. We find that while our LLM-based pipeline is able to automatically generate a correct motion graphics animation for 58.8% of the test prompts without any iteration, this number raises to 93.6% with up to 50 correction iterations. Project website: https://mover-dsl.github.io/

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Envious Explore and Exploit 
[[arxiv](https://arxiv.org/abs/2502.12798)] [[cool](https://papers.cool/arxiv/2502.12798)] [[pdf](https://arxiv.org/pdf/2502.12798)]
> **Authors**: Omer Ben-Porat,Yotam Gafni,Or Markovetzki
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,人工智能,机器学习
- **Abstract**: Explore-and-exploit tradeoffs play a key role in recommendation systems (RSs), aiming at serving users better by learning from previous interactions. Despite their commercial success, the societal effects of explore-and-exploit mechanisms are not well understood, especially regarding the utility discrepancy they generate between different users. In this work, we measure such discrepancy using the economic notion of envy. We present a multi-armed bandit-like model in which every round consists of several sessions, and rewards are realized once per round. We call the latter property reward consistency, and show that the RS can leverage this property for better societal outcomes. On the downside, doing so also generates envy, as late-to-arrive users enjoy the information gathered by early-to-arrive users. We examine the generated envy under several arrival order mechanisms and virtually any anonymous algorithm, i.e., any algorithm that treats all similar users similarly without leveraging their identities. We provide tight envy bounds on uniform arrival and upper bound the envy for nudged arrival, in which the RS can affect the order of arrival by nudging its users. Furthermore, we study the efficiency-fairness trade-off by devising an algorithm that allows constant envy and approximates the optimal welfare in restricted settings. Finally, we validate our theoretical results empirically using simulations.

## 人机交互(cs.HC:Human-Computer Interaction)

### Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance 
[[arxiv](https://arxiv.org/abs/2502.13321)] [[cool](https://papers.cool/arxiv/2502.13321)] [[pdf](https://arxiv.org/pdf/2502.13321)]
> **Authors**: Tejas Srinivasan,Jesse Thomason
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学
- **Abstract**: Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration.

### Talking About the Assumption in the Room 
[[arxiv](https://arxiv.org/abs/2502.13268)] [[cool](https://papers.cool/arxiv/2502.13268)] [[pdf](https://arxiv.org/pdf/2502.13268)]
> **Authors**: Ramaravind Kommiya Mothilal,Faisal M. Lalani,Syed Ishtiaque Ahmed,Shion Guha,Sharifa Sultana
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 19 pages without references, single-column, preprint for conference
- **标题**: None
- **领域**: 人机交互,机器学习
- **Abstract**: The reference to assumptions in how practitioners use or interact with machine learning (ML) systems is ubiquitous in HCI and responsible ML discourse. However, what remains unclear from prior works is the conceptualization of assumptions and how practitioners identify and handle assumptions throughout their workflows. This leads to confusion about what assumptions are and what needs to be done with them. We use the concept of an argument from Informal Logic, a branch of Philosophy, to offer a new perspective to understand and explicate the confusions surrounding assumptions. Through semi-structured interviews with 22 ML practitioners, we find what contributes most to these confusions is how independently assumptions are constructed, how reactively and reflectively they are handled, and how nebulously they are recorded. Our study brings the peripheral discussion of assumptions in ML to the center and presents recommendations for practitioners to better think about and work with assumptions.

### Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents 
[[arxiv](https://arxiv.org/abs/2502.13012)] [[cool](https://papers.cool/arxiv/2502.13012)] [[pdf](https://arxiv.org/pdf/2502.13012)]
> **Authors**: Chaoran Chen,Bingsheng Yao,Ruishi Zou,Wenyue Hua,Weimin Lyu,Toby Jia-Jun Li,Dakuo Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.

### UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design 
[[arxiv](https://arxiv.org/abs/2502.12561)] [[cool](https://papers.cool/arxiv/2502.12561)] [[pdf](https://arxiv.org/pdf/2502.12561)]
> **Authors**: Yuxuan Lu,Bingsheng Yao,Hansu Gu,Jing Huang,Jessie Wang,Laurence Li,Jiri Gesi,Qi He,Toby Jia-Jun Li,Dakuo Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.

## 信息检索(cs.IR:Information Retrieval)

### G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable Recommendation 
[[arxiv](https://arxiv.org/abs/2502.12586)] [[cool](https://papers.cool/arxiv/2502.12586)] [[pdf](https://arxiv.org/pdf/2502.12586)]
> **Authors**: Yuhan Li,Xinni Zhang,Linhao Luo,Heng Chang,Yuxiang Ren,Irwin King,Jia Li
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted by WWW 2025, research track
- **标题**: None
- **领域**: 信息检索,计算语言学
- **Abstract**: Explainable recommendation has demonstrated significant advantages in informing users about the logic behind recommendations, thereby increasing system transparency, effectiveness, and trustworthiness. To provide personalized and interpretable explanations, existing works often combine the generation capabilities of large language models (LLMs) with collaborative filtering (CF) information. CF information extracted from the user-item interaction graph captures the user behaviors and preferences, which is crucial for providing informative explanations. However, due to the complexity of graph structure, effectively extracting the CF information from graphs still remains a challenge. Moreover, existing methods often struggle with the integration of extracted CF information with LLMs due to its implicit representation and the modality gap between graph structures and natural language explanations. To address these challenges, we propose G-Refer, a framework using graph retrieval-augmented large language models (LLMs) for explainable recommendation. Specifically, we first employ a hybrid graph retrieval mechanism to retrieve explicit CF signals from both structural and semantic perspectives. The retrieved CF information is explicitly formulated as human-understandable text by the proposed graph translation and accounts for the explanations generated by LLMs. To bridge the modality gap, we introduce knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of LLMs to process and utilize the retrieved CF information to generate explanations. Extensive experiments show that G-Refer achieves superior performance compared with existing methods in both explainability and stability. Codes and data are available at https://github.com/Yuhan1i/G-Refer.

## 机器学习(cs.LG:Machine Learning)

### : Generalizing Large Language Models for Multi-property Molecule Optimization 
[[arxiv](https://arxiv.org/abs/2502.13398)] [[cool](https://papers.cool/arxiv/2502.13398)] [[pdf](https://arxiv.org/pdf/2502.13398)]
> **Authors**: Vishal Dey,Xiao Hu,Xia Ning
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Vishal Dey and Xiao Hu contributed equally to this paper
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,化学物理,定量方法
- **Abstract**: Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce $\mathtt{MoMUInstruct}$, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging $\mathtt{MoMUInstruct}$, we develop $\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that $\mathtt{GeLLM^3O}$s consistently outperform state-of-the-art baselines. $\mathtt{GeLLM^3O}$s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of $\mathtt{GeLLM^3O}$s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. $\mathtt{MoMUInstruct}$, models, and code are accessible through https://github.com/ninglab/GeLLMO.

### Flow-based generative models as iterative algorithms in probability space 
[[arxiv](https://arxiv.org/abs/2502.13394)] [[cool](https://papers.cool/arxiv/2502.13394)] [[pdf](https://arxiv.org/pdf/2502.13394)]
> **Authors**: Yao Xie,Xiuyuan Cheng
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: Generative AI (GenAI) has revolutionized data-driven modeling by enabling the synthesis of high-dimensional data across various applications, including image generation, language modeling, biomedical signal processing, and anomaly detection. Flow-based generative models provide a powerful framework for capturing complex probability distributions, offering exact likelihood estimation, efficient sampling, and deterministic transformations between distributions. These models leverage invertible mappings governed by Ordinary Differential Equations (ODEs), enabling precise density estimation and likelihood evaluation. This tutorial presents an intuitive mathematical framework for flow-based generative models, formulating them as neural network-based representations of continuous probability densities. We explore key theoretical principles, including the Wasserstein metric, gradient flows, and density evolution governed by ODEs, to establish convergence guarantees and bridge empirical advancements with theoretical insights. By providing a rigorous yet accessible treatment, we aim to equip researchers and practitioners with the necessary tools to effectively apply flow-based generative models in signal processing and machine learning.

### Quantum Recurrent Neural Networks with Encoder-Decoder for Time-Dependent Partial Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.13370)] [[cool](https://papers.cool/arxiv/2502.13370)] [[pdf](https://arxiv.org/pdf/2502.13370)]
> **Authors**: Yuan Chen,Abdul Khaliq,Khaled M. Furati
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析,量子物理学
- **Abstract**: Nonlinear time-dependent partial differential equations are essential in modeling complex phenomena across diverse fields, yet they pose significant challenges due to their computational complexity, especially in higher dimensions. This study explores Quantum Recurrent Neural Networks within an encoder-decoder framework, integrating Variational Quantum Circuits into Gated Recurrent Units and Long Short-Term Memory networks. Using this architecture, the model efficiently compresses high-dimensional spatiotemporal data into a compact latent space, facilitating more efficient temporal evolution. We evaluate the algorithms on the Hamilton-Jacobi-Bellman equation, Burgers' equation, the Gray-Scott reaction-diffusion system, and the three dimensional Michaelis-Menten reaction-diffusion equation. The results demonstrate the superior performance of the quantum-based algorithms in capturing nonlinear dynamics, handling high-dimensional spaces, and providing stable solutions, highlighting their potential as an innovative tool in solving challenging and complex systems.

### K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction 
[[arxiv](https://arxiv.org/abs/2502.13344)] [[cool](https://papers.cool/arxiv/2502.13344)] [[pdf](https://arxiv.org/pdf/2502.13344)]
> **Authors**: Tassallah Abdullahi,Ioanna Gemou,Nihal V. Nayak,Ghulam Murtaza,Stephen H. Bach,Carsten Eickhoff,Ritambhara Singh
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,生物分子
- **Abstract**: Drug discovery is a complex and time-intensive process that requires identifying and validating new therapeutic candidates. Computational approaches using large-scale biomedical knowledge graphs (KGs) offer a promising solution to accelerate this process. However, extracting meaningful insights from large-scale KGs remains challenging due to the complexity of graph traversal. Existing subgraph-based methods are tailored to graph neural networks (GNNs), making them incompatible with other models, such as large language models (LLMs). We introduce K-Paths, a retrieval framework that extracts structured, diverse, and biologically meaningful paths from KGs. Integrating these paths enables LLMs and GNNs to effectively predict unobserved drug-drug and drug-disease interactions. Unlike traditional path-ranking approaches, K-Paths retrieves and transforms paths into a structured format that LLMs can directly process, facilitating explainable reasoning. K-Paths employs a diversity-aware adaptation of Yen's algorithm to retrieve the K shortest loopless paths between entities in an interaction query, prioritizing biologically relevant and diverse relationships. Our experiments on benchmark datasets show that K-Paths improves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on drug repurposing and 13.42 points on interaction severity prediction. We also show that Llama 70B achieves F1-score gains of 6.18 and 8.46 points, respectively. K-Paths also improves the supervised training efficiency of EmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining strong predictive performance. Beyond its scalability and efficiency, K-Paths uniquely bridges the gap between KGs and LLMs, providing explainable rationales for predicted interactions. These capabilities show that K-Paths is a valuable tool for efficient data-driven drug discovery.

### How Expressive are Knowledge Graph Foundation Models? 
[[arxiv](https://arxiv.org/abs/2502.13339)] [[cool](https://papers.cool/arxiv/2502.13339)] [[pdf](https://arxiv.org/pdf/2502.13339)]
> **Authors**: Xingyue Huang,Pablo Barceló,Michael M. Bronstein,İsmail İlkan Ceylan,Mikhail Galkin,Juan L Reutter,Miguel Romero Orth
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits the model's expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.

### VUS: Effective and Efficient Accuracy Measures for Time-Series Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.13318)] [[cool](https://papers.cool/arxiv/2502.13318)] [[pdf](https://arxiv.org/pdf/2502.13318)]
> **Authors**: Paul Boniol,Ashwin K. Krishna,Marine Bruel,Qinghua Liu,Mingyi Huang,Themis Palpanas,Ruey S. Tsay,Aaron Elmore,Michael J. Franklin,John Paparrizos
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Anomaly detection (AD) is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. In contrast to other domains where AD mainly focuses on point-based anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with range-based anomalies (i.e., outliers spanning multiple observations). Nevertheless, it is common to use traditional point-based information retrieval measures, such as Precision, Recall, and F-score, to assess the quality of methods by thresholding the anomaly score to mark each point as an anomaly or not. However, mapping discrete labels into continuous data introduces unavoidable shortcomings, complicating the evaluation of range-based anomalies. Notably, the choice of evaluation measure may significantly bias the experimental outcome. Despite over six decades of attention, there has never been a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures. This paper extensively evaluates quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. Motivated by this observation, we first extend the AUC-based measures to account for range-based anomalies. Then, we introduce a new family of parameter-free and threshold-independent measures, Volume Under the Surface (VUS), to evaluate methods while varying parameters. We also introduce two optimized implementations for VUS that reduce significantly the execution time of the initial implementation. Our findings demonstrate that our four measures are significantly more robust in assessing the quality of time-series AD methods.

### A Label-Free Heterophily-Guided Approach for Unsupervised Graph Fraud Detection 
[[arxiv](https://arxiv.org/abs/2502.13308)] [[cool](https://papers.cool/arxiv/2502.13308)] [[pdf](https://arxiv.org/pdf/2502.13308)]
> **Authors**: Junjun Pan,Yixin Liu,Xin Zheng,Yizhen Zheng,Alan Wee-Chung Liew,Fuyi Li,Shirui Pan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 9 pages, 3 figures. Accepted by AAAI 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph fraud detection (GFD) has rapidly advanced in protecting online services by identifying malicious fraudsters. Recent supervised GFD research highlights that heterophilic connections between fraudsters and users can greatly impact detection performance, since fraudsters tend to camouflage themselves by building more connections to benign users. Despite the promising performance of supervised GFD methods, the reliance on labels limits their applications to unsupervised scenarios; Additionally, accurately capturing complex and diverse heterophily patterns without labels poses a further challenge. To fill the gap, we propose a Heterophily-guided Unsupervised Graph fraud dEtection approach (HUGE) for unsupervised GFD, which contains two essential components: a heterophily estimation module and an alignment-based fraud detection module. In the heterophily estimation module, we design a novel label-free heterophily metric called HALO, which captures the critical graph properties for GFD, enabling its outstanding ability to estimate heterophily from node attributes. In the alignment-based fraud detection module, we develop a joint MLP-GNN architecture with ranking loss and asymmetric alignment loss. The ranking loss aligns the predicted fraud score with the relative order of HALO, providing an extra robustness guarantee by comparing heterophily among non-adjacent nodes. Moreover, the asymmetric alignment loss effectively utilizes structural information while alleviating the feature-smooth effects of GNNs. Extensive experiments on 6 datasets demonstrate that HUGE significantly outperforms competitors, showcasing its effectiveness and robustness.

### Application of Context-dependent Interpretation of Biosignals Recognition to Control a Bionic Multifunctional Hand Prosthesis 
[[arxiv](https://arxiv.org/abs/2502.13301)] [[cool](https://papers.cool/arxiv/2502.13301)] [[pdf](https://arxiv.org/pdf/2502.13301)]
> **Authors**: Pawel Trajdos,Marek Kurzynski
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ef:Biocybernetics and Biomedical Engineering, 44,2024, 161-182
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The paper presents an original method for controlling a surface-electromyography-driven (sEMG) prosthesis. A context-dependent recognition system is proposed in which the same class of sEMG signals may have a different interpretation, depending on the context. This allowed the repertoire of performed movements to be increased. The proposed structure of the context-dependent recognition system includes unambiguously defined decision sequences covering the overall action of the prosthesis, i.e. the so-called boxes. Because the boxes are mutually isolated environments, each box has its own interpretation of the recognition result, as well as a separate local-recognition-task-focused classifier. Due to the freedom to assign contextual meanings to classes of biosignals, the construction procedure of the classifier can be optimised in terms of the local classification quality in a given box or the classification quality of the entire system. In the paper, two optimisation problems are formulated, differing in the adopted constraints on optimisation variables, with the methods of solving the problems based on an exhaustive search and an evolutionary algorithm, being developed. Experimental studies were conducted using signals from 1 able-bodied person with simulation of amputation and 10 volunteers with transradial amputations. The study compared the classical recognition system and the context-dependent system for various classifier models. An unusual testing strategy was adopted in the research, taking into account the specificity of the considered recognition task, with two original quality measures resulting from this scheme then being applied. The results obtained confirm the hypothesis that the application of the context-dependent classifier led to an improvement in classification quality.

### Prediction of Clinical Complication Onset using Neural Point Processes 
[[arxiv](https://arxiv.org/abs/2502.13290)] [[cool](https://papers.cool/arxiv/2502.13290)] [[pdf](https://arxiv.org/pdf/2502.13290)]
> **Authors**: Sachini Weerasekara,Sagar Kamarthi,Jacqueline Isaacs
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Predicting medical events in advance within critical care settings is paramount for patient outcomes and resource management. Utilizing predictive models, healthcare providers can anticipate issues such as cardiac arrest, sepsis, or respiratory failure before they manifest. Recently, there has been a surge in research focusing on forecasting adverse medical event onsets prior to clinical manifestation using machine learning. However, while these models provide temporal prognostic predictions for the occurrence of a specific adverse event of interest within defined time intervals, their interpretability often remains a challenge. In this work, we explore the applicability of neural temporal point processes in the context of adverse event onset prediction, with the aim of explaining clinical pathways and providing interpretable insights. Our experiments span six state-of-the-art neural point processes and six critical care datasets, each focusing on the onset of distinct adverse events. This work represents a novel application class of neural temporal point processes in event prediction.

### Multiple Distribution Shift -- Aerial (MDS-A): A Dataset for Test-Time Error Detection and Model Adaptation 
[[arxiv](https://arxiv.org/abs/2502.13289)] [[cool](https://papers.cool/arxiv/2502.13289)] [[pdf](https://arxiv.org/pdf/2502.13289)]
> **Authors**: Noel Ngu,Aditya Taparia,Gerardo I. Simari,Mario Leiva,Jack Corcoran,Ransalu Senanayake,Paulo Shakarian,Nathaniel D. Bastian
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Machine learning models assume that training and test samples are drawn from the same distribution. As such, significant differences between training and test distributions often lead to degradations in performance. We introduce Multiple Distribution Shift -- Aerial (MDS-A) -- a collection of inter-related datasets of the same aerial domain that are perturbed in different ways to better characterize the effects of out-of-distribution performance. Specifically, MDS-A is a set of simulated aerial datasets collected under different weather conditions. We include six datasets under different simulated weather conditions along with six baseline object-detection models, as well as several test datasets that are a mix of weather conditions that we show have significant differences from the training data. In this paper, we present characterizations of MDS-A, provide performance results for the baseline machine learning models (on both their specific training datasets and the test data), as well as results of the baselines after employing recent knowledge-engineering error-detection techniques (EDR) thought to improve out-of-distribution performance. The dataset is available at https://lab-v2.github.io/mdsa-dataset-website.

### Breaking the bonds of generative artificial intelligence by minimizing the maximum entropy 
[[arxiv](https://arxiv.org/abs/2502.13287)] [[cool](https://papers.cool/arxiv/2502.13287)] [[pdf](https://arxiv.org/pdf/2502.13287)]
> **Authors**: Mattia Miotto,Lorenzo Monacelli
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,统计力学,信息论
- **Abstract**: The emergence of generative artificial intelligence (GenAI), comprising large language models, text-to-image generators, and AI algorithms for medical drug and material design, had a transformative impact on society. However, despite an initial exponential growth surpassing Moore's law, progress is now plateauing, suggesting we are approaching the limits of current technology. Indeed, these models are notoriously data-hungry, prone to overfitting, and challenging to direct during the generative process, hampering their effective professional employment. To cope with these limitations, we propose a paradigm shift in GenAI by introducing an ab initio method based on the minimal maximum entropy principle. Our approach does not fit the data. Instead, it compresses information in the training set by finding a latent representation parameterized by arbitrary nonlinear functions, such as neural networks. The result is a general physics-driven model, which is data-efficient, resistant to overfitting, and flexible, permitting to control and influence the generative process. Benchmarking shows that our method outperforms variational autoencoders (VAEs) with similar neural architectures, particularly on undersampled datasets. We demonstrate the methods effectiveness in generating images, even with limited training data, and its unprecedented capability to customize the generation process a posteriori without the need of any fine-tuning or retraining.

### Benefits of Early Stopping in Gradient Descent for Overparameterized Logistic Regression 
[[arxiv](https://arxiv.org/abs/2502.13283)] [[cool](https://papers.cool/arxiv/2502.13283)] [[pdf](https://arxiv.org/pdf/2502.13283)]
> **Authors**: Jingfeng Wu,Peter Bartlett,Matus Telgarsky,Bin Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In overparameterized logistic regression, gradient descent (GD) iterates diverge in norm while converging in direction to the maximum $\ell_2$-margin solution -- a phenomenon known as the implicit bias of GD. This work investigates additional regularization effects induced by early stopping in well-specified high-dimensional logistic regression. We first demonstrate that the excess logistic risk vanishes for early-stopped GD but diverges to infinity for GD iterates at convergence. This suggests that early-stopped GD is well-calibrated, whereas asymptotic GD is statistically inconsistent. Second, we show that to attain a small excess zero-one risk, polynomially many samples are sufficient for early-stopped GD, while exponentially many samples are necessary for any interpolating estimator, including asymptotic GD. This separation underscores the statistical benefits of early stopping in the overparameterized regime. Finally, we establish nonasymptotic bounds on the norm and angular differences between early-stopped GD and $\ell_2$-regularized empirical risk minimizer, thereby connecting the implicit regularization of GD with explicit $\ell_2$-regularization.

### Value Gradient Sampler: Sampling as Sequential Decision Making 
[[arxiv](https://arxiv.org/abs/2502.13280)] [[cool](https://papers.cool/arxiv/2502.13280)] [[pdf](https://arxiv.org/pdf/2502.13280)]
> **Authors**: Sangwoong Yoon,Himchan Hwang,Hyeokju Jeong,Dong Kyu Shin,Che-Sang Park,Sehee Kwon,Frank Chongwoo Park
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Code: https://github.com/swyoon/value-gradient-sampler/
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We propose the Value Gradient Sampler (VGS), a trainable sampler based on the interpretation of sampling as discrete-time sequential decision-making. VGS generates samples from a given unnormalized density (i.e., energy) by drifting and diffusing randomly initialized particles. In VGS, finding the optimal drift is equivalent to solving an optimal control problem where the cost is the upper bound of the KL divergence between the target density and the samples. We employ value-based dynamic programming to solve this optimal control problem, which gives the gradient of the value function as the optimal drift vector. The connection to sequential decision making allows VGS to leverage extensively studied techniques in reinforcement learning, making VGS a fast, adaptive, and accurate sampler that achieves competitive results in various sampling benchmarks. Furthermore, VGS can replace MCMC in contrastive divergence training of energy-based models. We demonstrate the effectiveness of VGS in training accurate energy-based models in industrial anomaly detection applications.

### HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views 
[[arxiv](https://arxiv.org/abs/2502.13277)] [[cool](https://papers.cool/arxiv/2502.13277)] [[pdf](https://arxiv.org/pdf/2502.13277)]
> **Authors**: Khaled Mohammed Saifuddin,Shihao Ji,Esra Akbas
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advancements in Graph Contrastive Learning (GCL) have demonstrated remarkable effectiveness in improving graph representations. However, relying on predefined augmentations (e.g., node dropping, edge perturbation, attribute masking) may result in the loss of task-relevant information and a lack of adaptability to diverse input data. Furthermore, the selection of negative samples remains rarely explored. In this paper, we introduce HyperGCL, a novel multimodal GCL framework from a hypergraph perspective. HyperGCL constructs three distinct hypergraph views by jointly utilizing the input graph's structure and attributes, enabling a comprehensive integration of multiple modalities in contrastive learning. A learnable adaptive topology augmentation technique enhances these views by preserving important relations and filtering out noise. View-specific encoders capture essential characteristics from each view, while a network-aware contrastive loss leverages the underlying topology to define positive and negative samples effectively. Extensive experiments on benchmark datasets demonstrate that HyperGCL achieves state-of-the-art node classification performance.

### A Machine Learning Approach That Beats Large Rubik's Cubes 
[[arxiv](https://arxiv.org/abs/2502.13266)] [[cool](https://papers.cool/arxiv/2502.13266)] [[pdf](https://arxiv.org/pdf/2502.13266)]
> **Authors**: Alexander Chervov,Kirill Khoruzhii,Nikita Bukhal,Jalal Naghiyev,Vladislav Zamkovoy,Ivan Koltsov,Lyudmila Cheldieva,Arsenii Sychev,Arsenii Lenin,Mark Obozov,Egor Urvanov,Alexey Romanov
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 12 pages, 3 tables, 3 figures
- **标题**: None
- **领域**: 机器学习,离散数学
- **Abstract**: The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Additionally, our solution is more than 26 times faster in solving 3x3x3 Rubik's cubes while requiring up to 18.5 times less model training time than the most efficient state-of-the-art competitor.

### Random Forest Autoencoders for Guided Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.13257)] [[cool](https://papers.cool/arxiv/2502.13257)] [[pdf](https://arxiv.org/pdf/2502.13257)]
> **Authors**: Adrien Aumon,Shuang Ni,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Decades of research have produced robust methods for unsupervised data visualization, yet supervised visualization$\unicode{x2013}$where expert labels guide representations$\unicode{x2013}$remains underexplored, as most supervised approaches prioritize classification over visualization. Recently, RF-PHATE, a diffusion-based manifold learning method leveraging random forests and information geometry, marked significant progress in supervised visualization. However, its lack of an explicit mapping function limits scalability and prevents application to unseen data, posing challenges for large datasets and label-scarce scenarios. To overcome these limitations, we introduce Random Forest Autoencoders (RF-AE), a neural network-based framework for out-of-sample kernel extension that combines the flexibility of autoencoders with the supervised learning strengths of random forests and the geometry captured by RF-PHATE. RF-AE enables efficient out-of-sample supervised visualization and outperforms existing methods, including RF-PHATE's standard kernel extension, in both accuracy and interpretability. Additionally, RF-AE is robust to the choice of hyper-parameters and generalizes to any kernel-based dimensionality reduction method.

### Conformal Prediction as Bayesian Quadrature 
[[arxiv](https://arxiv.org/abs/2502.13228)] [[cool](https://papers.cool/arxiv/2502.13228)] [[pdf](https://arxiv.org/pdf/2502.13228)]
> **Authors**: Jake C. Snell,Thomas L. Griffiths
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 16 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: As machine learning-based prediction systems are increasingly used in high-stakes situations, it is important to understand how such predictive models will perform upon deployment. Distribution-free uncertainty quantification techniques such as conformal prediction provide guarantees about the loss black-box models will incur even when the details of the models are hidden. However, such methods are based on frequentist probability, which unduly limits their applicability. We revisit the central aspects of conformal prediction from a Bayesian perspective and thereby illuminate the shortcomings of frequentist guarantees. We propose a practical alternative based on Bayesian quadrature that provides interpretable guarantees and offers a richer representation of the likely range of losses to be observed at test time.

### Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations 
[[arxiv](https://arxiv.org/abs/2502.13221)] [[cool](https://papers.cool/arxiv/2502.13221)] [[pdf](https://arxiv.org/pdf/2502.13221)]
> **Authors**: Lee Cohen,Jack Hsieh,Connie Hong,Judy Hanwen Shen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机与社会,计算机科学与博弈论
- **Abstract**: In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and giving some candidates an unfair advantage. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a ``two-ticket'' scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. We further generalize this approach to an $n$-ticket scheme and prove that hiring outcomes converge to a fixed, group-independent decision, eliminating disparities arising from differential LLM access. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.

### The impact of conformer quality on learned representations of molecular conformer ensembles 
[[arxiv](https://arxiv.org/abs/2502.13220)] [[cool](https://papers.cool/arxiv/2502.13220)] [[pdf](https://arxiv.org/pdf/2502.13220)]
> **Authors**: Keir Adams,Connor W. Coley
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,化学物理
- **Abstract**: Training machine learning models to predict properties of molecular conformer ensembles is an increasingly popular strategy to accelerate the conformational analysis of drug-like small molecules, reactive organic substrates, and homogeneous catalysts. For high-throughput analyses especially, trained surrogate models can help circumvent traditional approaches to conformational analysis that rely on expensive conformer searches and geometry optimizations. Here, we question how the performance of surrogate models for predicting 3D conformer-dependent properties (of a single, active conformer) is affected by the quality of the 3D conformers used as their input. How well do lower-quality conformers inform the prediction of properties of higher-quality conformers? Does the fidelity of geometry optimization matter when encoding random conformers? For models that encode sets of conformers, how does the presence of the active conformer that induces the target property affect model accuracy? How do predictions from a surrogate model compare to estimating the properties from cheap ensembles themselves? We explore these questions in the context of predicting Sterimol parameters of conformer ensembles optimized with density functional theory. Although answers will be case-specific, our analyses provide a valuable perspective on 3D representation learning models and raise practical considerations regarding when conformer quality matters.

### Learning To Explore With Predictive World Model Via Self-Supervised Learning 
[[arxiv](https://arxiv.org/abs/2502.13200)] [[cool](https://papers.cool/arxiv/2502.13200)] [[pdf](https://arxiv.org/pdf/2502.13200)]
> **Authors**: Alana Santana,Paula P. Costa,Esther L. Colombini
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Autonomous artificial agents must be able to learn behaviors in complex environments without humans to design tasks and rewards. Designing these functions for each environment is not feasible, thus, motivating the development of intrinsic reward functions. In this paper, we propose using several cognitive elements that have been neglected for a long time to build an internal world model for an intrinsically motivated agent. Our agent performs satisfactory iterations with the environment, learning complex behaviors without needing previously designed reward functions. We used 18 Atari games to evaluate what cognitive skills emerge in games that require reactive and deliberative behaviors. Our results show superior performance compared to the state-of-the-art in many test cases with dense and sparse rewards.

### Enhancing Machine Learning Performance through Intelligent Data Quality Assessment: An Unsupervised Data-centric Framework 
[[arxiv](https://arxiv.org/abs/2502.13198)] [[cool](https://papers.cool/arxiv/2502.13198)] [[pdf](https://arxiv.org/pdf/2502.13198)]
> **Authors**: Manal Rahal,Bestoun S. Ahmed,Gergely Szabados,Torgny Fornstedt,Jorgen Samuelsson
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 42 pages
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Poor data quality limits the advantageous power of Machine Learning (ML) and weakens high-performing ML software systems. Nowadays, data are more prone to the risk of poor quality due to their increasing volume and complexity. Therefore, tedious and time-consuming work goes into data preparation and improvement before moving further in the ML pipeline. To address this challenge, we propose an intelligent data-centric evaluation framework that can identify high-quality data and improve the performance of an ML system. The proposed framework combines the curation of quality measurements and unsupervised learning to distinguish high- and low-quality data. The framework is designed to integrate flexible and general-purpose methods so that it is deployed in various domains and applications. To validate the outcomes of the designed framework, we implemented it in a real-world use case from the field of analytical chemistry, where it is tested on three datasets of anti-sense oligonucleotides. A domain expert is consulted to identify the relevant quality measurements and evaluate the outcomes of the framework. The results show that the quality-centric data evaluation framework identifies the characteristics of high-quality data that guide the conduct of efficient laboratory experiments and consequently improve the performance of the ML system.

### On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis 
[[arxiv](https://arxiv.org/abs/2502.13191)] [[cool](https://papers.cool/arxiv/2502.13191)] [[pdf](https://arxiv.org/pdf/2502.13191)]
> **Authors**: Junyi Guan,Abhijith Sharma,Chong Tian,Salem Lahlou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 13 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at https://anonymous.4open.science/r/MIA_SNN-3610.

### Application of machine learning algorithm in temperature field reconstruction 
[[arxiv](https://arxiv.org/abs/2502.13190)] [[cool](https://papers.cool/arxiv/2502.13190)] [[pdf](https://arxiv.org/pdf/2502.13190)]
> **Authors**: Qianyu He,Huaiwei Sun,Yubo Li,Zhiwen You,Qiming Zheng,Yinghan Huang,Sipeng Zhu,Fengyu Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,流体动力学
- **Abstract**: This study focuses on the stratification patterns and dynamic evolution of reservoir water temperatures, aiming to estimate and reconstruct the temperature field using limited and noisy local measurement data. Due to complex measurement environments and technical limitations, obtaining complete temperature information for reservoirs is highly challenging. Therefore, accurately reconstructing the temperature field from a small number of local data points has become a critical scientific issue. To address this, the study employs Proper Orthogonal Decomposition (POD) and sparse representation methods to reconstruct the temperature field based on temperature data from a limited number of local measurement points. The results indicate that satisfactory reconstruction can be achieved when the number of POD basis functions is set to 2 and the number of measurement points is 10. Under different water intake depths, the reconstruction errors of both POD and sparse representation methods remain stable at around 0.15, fully validating the effectiveness of these methods in reconstructing the temperature field based on limited local temperature data. Additionally, the study further explores the distribution characteristics of reconstruction errors for POD and sparse representation methods under different water level intervals, analyzing the optimal measurement point layout scheme and potential limitations of the reconstruction methods in this case. This research not only effectively reduces measurement costs and computational resource consumption but also provides a new technical approach for reservoir temperature analysis, holding significant theoretical and practical importance.

### MoBA: Mixture of Block Attention for Long-Context LLMs 
[[arxiv](https://arxiv.org/abs/2502.13189)] [[cool](https://papers.cool/arxiv/2502.13189)] [[pdf](https://arxiv.org/pdf/2502.13189)]
> **Authors**: Enzhe Lu,Zhejun Jiang,Jingyuan Liu,Yulun Du,Tao Jiang,Chao Hong,Shaowei Liu,Weiran He,Enming Yuan,Yuzhi Wang,Zhiqi Huang,Huan Yuan,Suting Xu,Xinran Xu,Guokun Lai,Yanru Chen,Huabin Zheng,Junjie Yan,Jianlin Su,Yuxin Wu,Neo Y. Zhang,Zhilin Yang,Xinyu Zhou,Mingxing Zhang,Jiezhong Qiu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.

### A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.13187)] [[cool](https://papers.cool/arxiv/2502.13187)] [[pdf](https://arxiv.org/pdf/2502.13187)]
> **Authors**: Longchao Da,Justin Turnau,Thirulogasankar Pranav Kutralingam,Alvaro Velasquez,Paulo Shakarian,Hua Wei
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 19 pages, 6 figures, 5 tables
- **标题**: None
- **领域**: 机器学习,人工智能,机器人技术
- **Abstract**: Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a repository to include the most up-to-date sim-to-real research work to help domain researchers.

### RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals 
[[arxiv](https://arxiv.org/abs/2502.13181)] [[cool](https://papers.cool/arxiv/2502.13181)] [[pdf](https://arxiv.org/pdf/2502.13181)]
> **Authors**: Jaemu Heo,Eldor Fozilov,Hyunmin Song,Taehwan Kim
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model relations between elements of a sequence in parallel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong performance in processing sequential data, the size of their parameters is considerably larger when compared to other architectures such as RNN and CNN based models. Therefore, several approaches have explored parameter sharing and recurrence in Transformer models to address their computational demands. However, such methods struggle to maintain high performance compared to the original transformer model. To address this challenge, we propose our novel approach, RingFormer, which employs one Transformer layer that processes input repeatedly in a circular, ring-like manner, while utilizing low-rank matrices to generate input-dependent level signals. This allows us to reduce the model parameters substantially while maintaining high performance in a variety of tasks such as translation and image classification, as validated in the experiments.

### Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.13180)] [[cool](https://papers.cool/arxiv/2502.13180)] [[pdf](https://arxiv.org/pdf/2502.13180)]
> **Authors**: Hongxu Wang,Zhu Sun,Yingpeng Du,Lu Zhang,Tiantian He,Yew-Soon Ong
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recommender systems (RSs) play a crucial role in shaping our digital interactions, influencing how we access and engage with information across various domains. Traditional research has predominantly centered on maximizing recommendation accuracy, often leading to unintended side effects such as echo chambers and constrained user experiences. Drawing inspiration from autonomous driving, we introduce a novel framework that categorizes RS autonomy into five distinct levels, ranging from basic rule-based accuracy-driven systems to behavior-aware, uncertain multi-objective RSs - where users may have varying needs, such as accuracy, diversity, and fairness. In response, we propose an approach that dynamically identifies and optimizes multiple objectives based on individual user preferences, fostering more ethical and intelligent user-centric recommendations. To navigate the uncertainty inherent in multi-objective RSs, we develop a Bayesian optimization (BO) framework that captures personalized trade-offs between different objectives while accounting for their uncertain interdependencies. Furthermore, we introduce an orthogonal meta-learning paradigm to enhance BO efficiency and effectiveness by leveraging shared knowledge across similar tasks and mitigating conflicts among objectives through the discovery of orthogonal information. Finally, extensive empirical evaluations demonstrate the effectiveness of our method in optimizing uncertain multi-objectives for individual users, paving the way for more adaptive and user-focused RSs.

### PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.13179)] [[cool](https://papers.cool/arxiv/2502.13179)] [[pdf](https://arxiv.org/pdf/2502.13179)]
> **Authors**: Jiaqi Zhao,Miao Zhang,Ming Wang,Yuzhang Shang,Kaihao Zhang,Weili Guan,Yaowei Wang,Min Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 20 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at https://github.com/zjq0455/PTQ1.61.

### Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis 
[[arxiv](https://arxiv.org/abs/2502.13178)] [[cool](https://papers.cool/arxiv/2502.13178)] [[pdf](https://arxiv.org/pdf/2502.13178)]
> **Authors**: Jiaqi Zhao,Ming Wang,Miao Zhang,Yuzhang Shang,Xuebo Liu,Yaowei Wang,Min Zhang,Liqiang Nie
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 17 pages, 3 fugures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation metrics.Through comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.

### KL Penalty Control via Perturbation for Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.13177)] [[cool](https://papers.cool/arxiv/2502.13177)] [[pdf](https://arxiv.org/pdf/2502.13177)]
> **Authors**: Sangkyu Lee,Janghoon Han,Hosung Song,Stanley Jungkyu Choi,Honglak Lee,Youngjae Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Preprint; Under review
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\varepsilon$-Direct Preference Optimization ($\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $β$ for each preference pair. Specifically, $\varepsilon$-DPO adaptively controls $β$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $β$ during training by simply reusing the logit of the current policy and the reference policy. Experimental results show that $\varepsilon$-DPO outperforms existing direct alignment algorithms and KL penalty relaxation methods on general chatbot benchmarks, highlighting the significance of adaptive KL penalty relaxation at the instance-level in DPO.

### BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference 
[[arxiv](https://arxiv.org/abs/2502.13176)] [[cool](https://papers.cool/arxiv/2502.13176)] [[pdf](https://arxiv.org/pdf/2502.13176)]
> **Authors**: Ahmed Burak Gulhan,Krishna Teja Chitty-Venkata,Murali Emani,Mahmut Kandemir,Venkatram Vishwanath
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches) are essential for reducing time complexity. However, they result in a linear increase in GPU memory as the context length grows. While recent work explores KV-cache eviction and compression policies to reduce memory usage, they often consider uniform KV-caches across all attention heads, leading to suboptimal performance. We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache. Our empirical analysis demonstrates that not all KV-caches are equally critical for LLM performance. Using a one-time profiling approach, BaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our method on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels.

### Generative Topology Optimization: Exploring Diverse Solutions in Structural Design 
[[arxiv](https://arxiv.org/abs/2502.13174)] [[cool](https://papers.cool/arxiv/2502.13174)] [[pdf](https://arxiv.org/pdf/2502.13174)]
> **Authors**: Andreas Radler,Eric Volkmann,Johannes Brandstetter,Arturs Berzins
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: 11 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,材料科学,人工智能,计算机视觉和模式识别
- **Abstract**: Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Generative Topology Optimization (GenTO) - a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate GenTO on 2D and 3D TO problems. Our results demonstrate that GenTO produces more diverse solutions than any prior method while maintaining near-optimality and being an order of magnitude faster due to inherent parallelism. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.

### Thinking Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.13173)] [[cool](https://papers.cool/arxiv/2502.13173)] [[pdf](https://arxiv.org/pdf/2502.13173)]
> **Authors**: Wang Yang,Hongye Jin,Jingfeng Yang,Vipin Chaudhary,Xiaotian Han
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.

### Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions 
[[arxiv](https://arxiv.org/abs/2502.13135)] [[cool](https://papers.cool/arxiv/2502.13135)] [[pdf](https://arxiv.org/pdf/2502.13135)]
> **Authors**: Taedong Yun,Eric Yang,Mustafa Safdari,Jong Ha Lee,Vaishnavi Vinod Kumar,S. Sara Mahdavi,Jonathan Amar,Derek Peyton,Reut Aharony,Andreas Michaelides,Logan Schneider,Isaac Galatzer-Levy,Yugang Jia,John Canny,Arthur Gretton,Maja Matarić
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.

### Learning to Defer for Causal Discovery with Imperfect Experts 
[[arxiv](https://arxiv.org/abs/2502.13132)] [[cool](https://papers.cool/arxiv/2502.13132)] [[pdf](https://arxiv.org/pdf/2502.13132)]
> **Authors**: Oscar Clivio,Divyat Mahajan,Perouz Taslakian,Sara Magliacane,Ioannis Mitliagkas,Valentina Zantedeschi,Alexandre Drouin
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Integrating expert knowledge, e.g. from large language models, into causal discovery algorithms can be challenging when the knowledge is not guaranteed to be correct. Expert recommendations may contradict data-driven results, and their reliability can vary significantly depending on the domain or specific query. Existing methods based on soft constraints or inconsistencies in predicted causal relationships fail to account for these variations in expertise. To remedy this, we propose L2D-CD, a method for gauging the correctness of expert recommendations and optimally combining them with data-driven causal discovery results. By adapting learning-to-defer (L2D) algorithms for pairwise causal discovery (CD), we learn a deferral function that selects whether to rely on classical causal discovery methods using numerical data or expert recommendations based on textual meta-data. We evaluate L2D-CD on the canonical Tübingen pairs dataset and demonstrate its superior performance compared to both the causal discovery method and the expert used in isolation. Moreover, our approach identifies domains where the expert's performance is strong or weak. Finally, we outline a strategy for generalizing this approach to causal discovery on graphs with more than two variables, paving the way for further research in this area.

### Near-Optimal Private Learning in Linear Contextual Bandits 
[[arxiv](https://arxiv.org/abs/2502.13115)] [[cool](https://papers.cool/arxiv/2502.13115)] [[pdf](https://arxiv.org/pdf/2502.13115)]
> **Authors**: Fan Chen,Jiachun Li,Alexander Rakhlin,David Simchi-Levi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,统计理论,机器学习
- **Abstract**: We analyze the problem of private learning in generalized linear contextual bandits. Our approach is based on a novel method of re-weighted regression, yielding an efficient algorithm with regret of order $\sqrt{T}+\frac{1}α$ and $\sqrt{T}/α$ in the joint and local model of $α$-privacy, respectively. Further, we provide near-optimal private procedures that achieve dimension-independent rates in private linear models and linear contextual bandits. In particular, our results imply that joint privacy is almost "for free" in all the settings we consider, partially addressing the open problem posed by Azize and Basu (2024).

### Constrained Online Convex Optimization with Polyak Feasibility Steps 
[[arxiv](https://arxiv.org/abs/2502.13112)] [[cool](https://papers.cool/arxiv/2502.13112)] [[pdf](https://arxiv.org/pdf/2502.13112)]
> **Authors**: Spencer Hutchinson,Mahnoosh Alizadeh
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 20 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: In this work, we study online convex optimization with a fixed constraint function $g : \mathbb{R}^d \rightarrow \mathbb{R}$. Prior work on this problem has shown $O(\sqrt{T})$ regret and cumulative constraint satisfaction $\sum_{t=1}^{T} g(x_t) \leq 0$, while only accessing the constraint value and subgradient at the played actions $g(x_t), \partial g(x_t)$. Using the same constraint information, we show a stronger guarantee of anytime constraint satisfaction $g(x_t) \leq 0 \ \forall t \in [T]$, and matching $O(\sqrt{T})$ regret guarantees. These contributions are thanks to our approach of using Polyak feasibility steps to ensure constraint satisfaction, without sacrificing regret. Specifically, after each step of online gradient descent, our algorithm applies a subgradient descent step on the constraint function where the step-size is chosen according to the celebrated Polyak step-size. We further validate this approach with numerical experiments.

### MLPs at the EOC: Dynamics of Feature Learning 
[[arxiv](https://arxiv.org/abs/2502.13110)] [[cool](https://papers.cool/arxiv/2502.13110)] [[pdf](https://arxiv.org/pdf/2502.13110)]
> **Authors**: Dávid Terjék
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Since infinitely wide neural networks in the kernel regime are random feature models, the success of contemporary deep learning lies in the rich regime, where a satisfying theory should explain not only the convergence of gradient descent but the learning of features along the way. Such a theory should also cover phenomena observed by practicioners including the Edge of Stability (EOS) and the catapult mechanism. For a practically relevant theory in the limit, neural network parameterizations have to efficiently reproduce limiting behavior as width and depth are scaled up. While widthwise scaling is mostly settled, depthwise scaling is solved only at initialization by the Edge of Chaos (EOC). During training, scaling up depth is either done by inversely scaling the learning rate or adding residual connections. We propose $(1)$ the Normalized Update Parameterization ($ν$P) to solve this issue by growing hidden layer sizes depthwise inducing the regularized evolution of preactivations, $(2)$ a hypothetical explanation for feature learning via the cosine of new and cumulative parameter updates and $(3)$ a geometry-aware learning rate schedule that is able to prolong the catapult phase indefinitely. We support our hypotheses and demonstrate the usefulness of $ν$P and the learning rate schedule by empirical evidence.

### Enhanced uncertainty quantification variational autoencoders for the solution of Bayesian inverse problems 
[[arxiv](https://arxiv.org/abs/2502.13105)] [[cool](https://papers.cool/arxiv/2502.13105)] [[pdf](https://arxiv.org/pdf/2502.13105)]
> **Authors**: Andrea Tonini,Luca Dede'
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: Among other uses, neural networks are a powerful tool for solving deterministic and Bayesian inverse problems in real-time. In the Bayesian framework, variational autoencoders, a specialized type of neural network, enable the estimation of model parameters and their distribution based on observational data allowing to perform real-time inverse uncertainty quantification. In this work, we build upon existing research [Goh, H. et al., Proceedings of Machine Learning Research, 2022] by proposing a novel loss function to train variational autoencoders for Bayesian inverse problems. When the forward map is affine, we provide a theoretical proof of the convergence of the latent states of variational autoencoders to the posterior distribution of the model parameters. We validate this theoretical result through numerical tests and we compare the proposed variational autoencoder with the existing one in the literature. Finally, we test the proposed variational autoencoder on the Laplace equation.

### tn4ml: Tensor Network Training and Customization for Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.13090)] [[cool](https://papers.cool/arxiv/2502.13090)] [[pdf](https://arxiv.org/pdf/2502.13090)]
> **Authors**: Ema Puljak,Sergio Sanchez-Ramirez,Sergi Masot-Llima,Jofre Vallès-Muns,Artur Garcia-Saez,Maurizio Pierini
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数学软件,量子物理学
- **Abstract**: Tensor Networks have emerged as a prominent alternative to neural networks for addressing Machine Learning challenges in foundational sciences, paving the way for their applications to real-life problems. This paper introduces tn4ml, a novel library designed to seamlessly integrate Tensor Networks into optimization pipelines for Machine Learning tasks. Inspired by existing Machine Learning frameworks, the library offers a user-friendly structure with modules for data embedding, objective function definition, and model training using diverse optimization strategies. We demonstrate its versatility through two examples: supervised learning on tabular data and unsupervised learning on an image dataset. Additionally, we analyze how customizing the parts of the Machine Learning pipeline for Tensor Networks influences performance metrics.

### BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression Classification 
[[arxiv](https://arxiv.org/abs/2502.13080)] [[cool](https://papers.cool/arxiv/2502.13080)] [[pdf](https://arxiv.org/pdf/2502.13080)]
> **Authors**: Bich-Chung Phan,Thanh Ma,Huu-Hoa Nguyen,Thanh-Nghi Do
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 16 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Gene expression classification is a pivotal yet challenging task in bioinformatics, primarily due to the high dimensionality of genomic data and the risk of overfitting. To bridge this gap, we propose BOLIMES, a novel feature selection algorithm designed to enhance gene expression classification by systematically refining the feature subset. Unlike conventional methods that rely solely on statistical ranking or classifier-specific selection, we integrate the robustness of Boruta with the interpretability of LIME, ensuring that only the most relevant and influential genes are retained. BOLIMES first employs Boruta to filter out non-informative genes by comparing each feature against its randomized counterpart, thus preserving valuable information. It then uses LIME to rank the remaining genes based on their local importance to the classifier. Finally, an iterative classification evaluation determines the optimal feature subset by selecting the number of genes that maximizes predictive accuracy. By combining exhaustive feature selection with interpretability-driven refinement, our solution effectively balances dimensionality reduction with high classification performance, offering a powerful solution for high-dimensional gene expression analysis.

### $k$-Graph: A Graph Embedding for Interpretable Time Series Clustering 
[[arxiv](https://arxiv.org/abs/2502.13049)] [[cool](https://papers.cool/arxiv/2502.13049)] [[pdf](https://arxiv.org/pdf/2502.13049)]
> **Authors**: Paul Boniol,Donato Tiano,Angela Bonifati,Themis Palpanas
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series clustering poses a significant challenge with diverse applications across domains. A prominent drawback of existing solutions lies in their limited interpretability, often confined to presenting users with centroids. In addressing this gap, our work presents $k$-Graph, an unsupervised method explicitly crafted to augment interpretability in time series clustering. Leveraging a graph representation of time series subsequences, $k$-Graph constructs multiple graph representations based on different subsequence lengths. This feature accommodates variable-length time series without requiring users to predetermine subsequence lengths. Our experimental results reveal that $k$-Graph outperforms current state-of-the-art time series clustering algorithms in accuracy, while providing users with meaningful explanations and interpretations of the clustering outcomes.

### Fragility-aware Classification for Understanding Risk and Improving Generalization 
[[arxiv](https://arxiv.org/abs/2502.13024)] [[cool](https://papers.cool/arxiv/2502.13024)] [[pdf](https://arxiv.org/pdf/2502.13024)]
> **Authors**: Chen Yang,Zheng Cui,Daniel Zhuoyu Long,Jin Qi,Ruohan Zhan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models.

### Efficient and Sharp Off-Policy Learning under Unobserved Confounding 
[[arxiv](https://arxiv.org/abs/2502.13022)] [[cool](https://papers.cool/arxiv/2502.13022)] [[pdf](https://arxiv.org/pdf/2502.13022)]
> **Authors**: Konstantin Hess,Dennis Frauen,Valentyn Melnychuk,Stefan Feuerriegel
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We develop a novel method for personalized off-policy learning in scenarios with unobserved confounding. Thereby, we address a key limitation of standard policy learning: standard policy learning assumes unconfoundedness, meaning that no unobserved factors influence both treatment assignment and outcomes. However, this assumption is often violated, because of which standard policy learning produces biased estimates and thus leads to policies that can be harmful. To address this limitation, we employ causal sensitivity analysis and derive a statistically efficient estimator for a sharp bound on the value function under unobserved confounding. Our estimator has three advantages: (1) Unlike existing works, our estimator avoids unstable minimax optimization based on inverse propensity weighted outcomes. (2) Our estimator is statistically efficient. (3) We prove that our estimator leads to the optimal confounding-robust policy. Finally, we extend our theory to the related task of policy improvement under unobserved confounding, i.e., when a baseline policy such as the standard of care is available. We show in experiments with synthetic and real-world data that our method outperforms simple plug-in approaches and existing baselines. Our method is highly relevant for decision-making where unobserved confounding can be problematic, such as in healthcare and public policy.

### Ensemble Kalman filter in latent space using a variational autoencoder pair 
[[arxiv](https://arxiv.org/abs/2502.12987)] [[cool](https://papers.cool/arxiv/2502.12987)] [[pdf](https://arxiv.org/pdf/2502.12987)]
> **Authors**: Ivo Pasmans,Yumeng Chen,Tobias Sebastian Finn,Marc Bocquet,Alberto Carrassi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,大气和海洋物理
- **Abstract**: Popular (ensemble) Kalman filter data assimilation (DA) approaches assume that the errors in both the a priori estimate of the state and those in the observations are Gaussian. For constrained variables, e.g. sea ice concentration or stress, such an assumption does not hold. The variational autoencoder (VAE) is a machine learning (ML) technique that allows to map an arbitrary distribution to/from a latent space in which the distribution is supposedly closer to a Gaussian. We propose a novel hybrid DA-ML approach in which VAEs are incorporated in the DA procedure. Specifically, we introduce a variant of the popular ensemble transform Kalman filter (ETKF) in which the analysis is applied in the latent space of a single VAE or a pair of VAEs. In twin experiments with a simple circular model, whereby the circle represents an underlying submanifold to be respected, we find that the use of a VAE ensures that a posteri ensemble members lie close to the manifold containing the truth. Furthermore, online updating of the VAE is necessary and achievable when this manifold varies in time, i.e. when it is non-stationary. We demonstrate that introducing an additional second latent space for the observational innovations improves robustness against detrimental effects of non-Gaussianity and bias in the observational errors but it slightly lessens the performance if observational errors are strictly Gaussian.

### Towards Variational Flow Matching on General Geometries 
[[arxiv](https://arxiv.org/abs/2502.12981)] [[cool](https://papers.cool/arxiv/2502.12981)] [[pdf](https://arxiv.org/pdf/2502.12981)]
> **Authors**: Olga Zaghen,Floor Eijkelboom,Alison Pouplin,Erik J. Bekkers
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,微分几何
- **Abstract**: We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an extension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian distributions for generative modeling on structured manifolds. We derive a variational objective for probability flows on manifolds with closed-form geodesics, making RG-VFM comparable - though fundamentally different to Riemannian Flow Matching (RFM) in this geometric setting. Experiments on a checkerboard dataset wrapped on the sphere demonstrate that RG-VFM captures geometric structure more effectively than Euclidean VFM and baseline methods, establishing it as a robust framework for manifold-aware generative modeling.

### Electron flow matching for generative reaction mechanism prediction obeying conservation laws 
[[arxiv](https://arxiv.org/abs/2502.12979)] [[cool](https://papers.cool/arxiv/2502.12979)] [[pdf](https://arxiv.org/pdf/2502.12979)]
> **Authors**: Joonyoung F. Joung,Mun Hong Fong,Nicholas Casetti,Jordan P. Liles,Ne S. Dassanayake,Connor W. Coley
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Central to our understanding of chemical reactivity is the principle of mass conservation, which is fundamental for ensuring physical consistency, balancing equations, and guiding reaction design. However, data-driven computational models for tasks such as reaction product prediction rarely abide by this most basic constraint. In this work, we recast the problem of reaction prediction as a problem of electron redistribution using the modern deep generative framework of flow matching. Our model, FlowER, overcomes limitations inherent in previous approaches by enforcing exact mass conservation, thereby resolving hallucinatory failure modes, recovering mechanistic reaction sequences for unseen substrate scaffolds, and generalizing effectively to out-of-domain reaction classes with extremely data-efficient fine-tuning. FlowER additionally enables estimation of thermodynamic or kinetic feasibility and manifests a degree of chemical intuition in reaction prediction tasks. This inherently interpretable framework represents a significant step in bridging the gap between predictive accuracy and mechanistic understanding in data-driven reaction outcome prediction.

### Guaranteed Conditional Diffusion: 3D Block-based Models for Scientific Data Compression 
[[arxiv](https://arxiv.org/abs/2502.12951)] [[cool](https://papers.cool/arxiv/2502.12951)] [[pdf](https://arxiv.org/pdf/2502.12951)]
> **Authors**: Jaemoon Lee,Xiao Li,Liangji Zhu,Sanjay Ranka,Anand Rangarajan
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper proposes a new compression paradigm -- Guaranteed Conditional Diffusion with Tensor Correction (GCDTC) -- for lossy scientific data compression. The framework is based on recent conditional diffusion (CD) generative models, and it consists of a conditional diffusion model, tensor correction, and error guarantee. Our diffusion model is a mixture of 3D conditioning and 2D denoising U-Net. The approach leverages a 3D block-based compressing module to address spatiotemporal correlations in structured scientific data. Then, the reverse diffusion process for 2D spatial data is conditioned on the ``slices'' of content latent variables produced by the compressing module. After training, the denoising decoder reconstructs the data with zero noise and content latent variables, and thus it is entirely deterministic. The reconstructed outputs of the CD model are further post-processed by our tensor correction and error guarantee steps to control and ensure a maximum error distortion, which is an inevitable requirement in lossy scientific data compression. Our experiments involving two datasets generated by climate and chemical combustion simulations show that our framework outperforms standard convolutional autoencoders and yields competitive compression quality with an existing scientific data compression algorithm.

### Efficient Learning Under Density Shift in Incremental Settings Using Cramér-Rao-Based Regularization 
[[arxiv](https://arxiv.org/abs/2502.12949)] [[cool](https://papers.cool/arxiv/2502.12949)] [[pdf](https://arxiv.org/pdf/2502.12949)]
> **Authors**: Behraj Khan,Behroz Mirza,Nouman Durrani,Tahir Syed
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The continuous surge in data volume and velocity is often dealt with using data orchestration and distributed processing approaches, abstracting away the machine learning challenges that exist at the algorithmic level. With growing interest in automating the learning loop, training with data that arrive in a sequence rather than in the classical in-memory training data form will face a machine learning challenge because of evolving feature distributions across batches of training data biasing the cross-validation step (\cite{sugiyama2012machine}). This work takes a distributed density estimation angle to the problem where data are temporally distributed. It processes data in batches and allows a neural network to treat a batch as training data. The method accumulates knowledge about the data density via posterior probability absorption using the Fisher Information Matrix, which contains information about the local optimization gradients for the batch. This is then used as a regularizer for the loss in the following batch, and therefore the density estimate for the entire dataset constructively gets more robust to the non-iid distribution shift. This needs the presence of a pair of batches in memory at a time, so the space cost is not a function of the size of the complete, distributed dataset. We proposed a novel regularization-based approach Covariate Shift Correction $C^{2}A$ that leverages Fisher information and Kullback-Leibler divergence to adapt to both natural and sequential covariate shift caused by dataset fragmentation. $C^{2}A$ achieves $19\%$ accuracy at maximum against state-of-the-art methods.

### Performance of Zero-Shot Time Series Foundation Models on Cloud Data 
[[arxiv](https://arxiv.org/abs/2502.12944)] [[cool](https://papers.cool/arxiv/2502.12944)] [[pdf](https://arxiv.org/pdf/2502.12944)]
> **Authors**: William Toner,Thomas L. Lee,Artjom Joosen,Rajkarn Singh,Martin Asenov
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 5 pages, Preprint
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. FMs are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including cloud data. In this work we investigate this claim, exploring the effectiveness of FMs on cloud data. We demonstrate that many well-known FMs fail to generate meaningful or accurate zero-shot forecasts in this setting. We support this claim empirically, showing that FMs are outperformed consistently by simple linear baselines. We also illustrate a number of interesting pathologies, including instances where FMs suddenly output seemingly erratic, random-looking forecasts. Our results suggest a widespread failure of FMs to model cloud data.

### Tuning Algorithmic and Architectural Hyperparameters in Graph-Based Semi-Supervised Learning with Provable Guarantees 
[[arxiv](https://arxiv.org/abs/2502.12937)] [[cool](https://papers.cool/arxiv/2502.12937)] [[pdf](https://arxiv.org/pdf/2502.12937)]
> **Authors**: Ally Yalei Du,Eric Huang,Dravyansh Sharma
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 31 pages (11 pages main body), 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph-based semi-supervised learning is a powerful paradigm in machine learning for modeling and exploiting the underlying graph structure that captures the relationship between labeled and unlabeled data. A large number of classical as well as modern deep learning based algorithms have been proposed for this problem, often having tunable hyperparameters. We initiate a formal study of tuning algorithm hyperparameters from parameterized algorithm families for this problem. We obtain novel $O(\log n)$ pseudo-dimension upper bounds for hyperparameter selection in three classical label propagation-based algorithm families, where $n$ is the number of nodes, implying bounds on the amount of data needed for learning provably good parameters. We further provide matching $Ω(\log n)$ pseudo-dimension lower bounds, thus asymptotically characterizing the learning-theoretic complexity of the parameter tuning problem. We extend our study to selecting architectural hyperparameters in modern graph neural networks. We bound the Rademacher complexity for tuning the self-loop weighting in recently proposed Simplified Graph Convolution (SGC) networks. We further propose a tunable architecture that interpolates graph convolutional neural networks (GCN) and graph attention networks (GAT) in every layer, and provide Rademacher complexity bounds for tuning the interpolation coefficient.

### Universal Embedding Function for Traffic Classification via QUIC Domain Recognition Pretraining: A Transfer Learning Success 
[[arxiv](https://arxiv.org/abs/2502.12930)] [[cool](https://papers.cool/arxiv/2502.12930)] [[pdf](https://arxiv.org/pdf/2502.12930)]
> **Authors**: Jan Luxemburk,Karel Hynek,Richard Plný,Tomáš Čejka
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,网络和互联网架构
- **Abstract**: Encrypted traffic classification (TC) methods must adapt to new protocols and extensions as well as to advancements in other machine learning fields. In this paper, we follow a transfer learning setup best known from computer vision. We first pretrain an embedding model on a complex task with a large number of classes and then transfer it to five well-known TC datasets. The pretraining task is recognition of SNI domains in encrypted QUIC traffic, which in itself is a problem for network monitoring due to the growing adoption of TLS Encrypted Client Hello. Our training pipeline -- featuring a disjoint class setup, ArcFace loss function, and a modern deep learning architecture -- aims to produce universal embeddings applicable across tasks. The proposed solution, based on nearest neighbors search in the embedding space, surpasses SOTA performance on four of the five TC datasets. A comparison with a baseline method utilizing raw packet sequences revealed unexpected findings with potential implications for the broader TC field. We published the model architecture, trained weights, and transfer learning experiments.

### Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options 
[[arxiv](https://arxiv.org/abs/2502.12929)] [[cool](https://papers.cool/arxiv/2502.12929)] [[pdf](https://arxiv.org/pdf/2502.12929)]
> **Authors**: Lakshmi Nair,Ian Trase,Mark Kim
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Github code: https://github.com/flagshippioneering/Flow-of-Options
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.

### Lightweight Online Adaption for Time Series Foundation Model Forecasts 
[[arxiv](https://arxiv.org/abs/2502.12920)] [[cool](https://papers.cool/arxiv/2502.12920)] [[pdf](https://arxiv.org/pdf/2502.12920)]
> **Authors**: Thomas L. Lee,William Toner,Rajkarn Singh,Artjom Joosem,Martin Asenov
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, Preprint
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Foundation models (FMs) have emerged as a promising approach for time series forecasting. While effective, FMs typically remain fixed during deployment due to the high computational costs of learning them online. Consequently, deployed FMs fail to adapt their forecasts to current data characteristics, despite the availability of online feedback from newly arriving data. This raises the question of whether FM performance can be enhanced by the efficient usage of this feedback. We propose AdapTS to answer this question. AdapTS is a lightweight mechanism for the online adaption of FM forecasts in response to online feedback. AdapTS consists of two parts: a) the AdapTS-Forecaster which is used to learn the current data distribution; and b) the AdapTS-Weighter which is used to combine the forecasts of the FM and the AdapTS-Forecaster. We evaluate the performance of AdapTS in conjunction with several recent FMs across a suite of standard time series datasets. In all of our experiments we find that using AdapTS improves performance. This work demonstrates how efficient usage of online feedback can be used to improve FM forecasts.

### A Smooth Transition Between Induction and Deduction: Fast Abductive Learning Based on Probabilistic Symbol Perception 
[[arxiv](https://arxiv.org/abs/2502.12919)] [[cool](https://papers.cool/arxiv/2502.12919)] [[pdf](https://arxiv.org/pdf/2502.12919)]
> **Authors**: Lin-Han Jia,Si-Yu Han,Lan-Zhe Guo,Zhi Zhou,Zhao-Long Li,Yu-Feng Li,Zhi-Hua Zhou
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Abductive learning (ABL) that integrates strengths of machine learning and logical reasoning to improve the learning generalization, has been recently shown effective. However, its efficiency is affected by the transition between numerical induction and symbolical deduction, leading to high computational costs in the worst-case scenario. Efforts on this issue remain to be limited. In this paper, we identified three reasons why previous optimization algorithms for ABL were not effective: insufficient utilization of prediction, symbol relationships, and accumulated experience in successful abductive processes, resulting in redundant calculations to the knowledge base. To address these challenges, we introduce an optimization algorithm named as Probabilistic Symbol Perception (PSP), which makes a smooth transition between induction and deduction and keeps the correctness of ABL unchanged. We leverage probability as a bridge and present an efficient data structure, achieving the transfer from a continuous probability sequence to discrete Boolean sequences with low computational complexity. Experiments demonstrate the promising results.

### GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning 
[[arxiv](https://arxiv.org/abs/2502.12913)] [[cool](https://papers.cool/arxiv/2502.12913)] [[pdf](https://arxiv.org/pdf/2502.12913)]
> **Authors**: Sifan Zhou,Shuo Wang,Zhihang Yuan,Mingjia Shi,Yuzhang Shang,Dawei Yang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to BF16-based fine-tuning while significantly reducing 1.85x memory usage. Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.

### Probabilistic neural operators for functional uncertainty quantification 
[[arxiv](https://arxiv.org/abs/2502.12902)] [[cool](https://papers.cool/arxiv/2502.12902)] [[pdf](https://arxiv.org/pdf/2502.12902)]
> **Authors**: Christopher Bülte,Philipp Scholl,Gitta Kutyniok
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Neural operators aim to approximate the solution operator of a system of differential equations purely from data. They have shown immense success in modeling complex dynamical systems across various domains. However, the occurrence of uncertainties inherent in both model and data has so far rarely been taken into account\textemdash{}a critical limitation in complex, chaotic systems such as weather forecasting. In this paper, we introduce the probabilistic neural operator (PNO), a framework for learning probability distributions over the output function space of neural operators. PNO extends neural operators with generative modeling based on strictly proper scoring rules, integrating uncertainty information directly into the training process. We provide a theoretical justification for the approach and demonstrate improved performance in quantifying uncertainty across different domains and with respect to different baselines. Furthermore, PNO requires minimal adjustment to existing architectures, shows improved performance for most probabilistic prediction tasks, and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. Implementing our approach into large-scale models for physical applications can lead to improvements in corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models.

### The Relationship Between Head Injury and Alzheimer's Disease: A Causal Analysis with Bayesian Networks 
[[arxiv](https://arxiv.org/abs/2502.12898)] [[cool](https://papers.cool/arxiv/2502.12898)] [[pdf](https://arxiv.org/pdf/2502.12898)]
> **Authors**: Andrei Lixandru
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study examines the potential causal relationship between head injury and the risk of developing Alzheimer's disease (AD) using Bayesian networks and regression models. Using a dataset of 2,149 patients, we analyze key medical history variables, including head injury history, memory complaints, cardiovascular disease, and diabetes. Logistic regression results suggest an odds ratio of 0.88 for head injury, indicating a potential but statistically insignificant protective effect against AD. In contrast, memory complaints exhibit a strong association with AD, with an odds ratio of 4.59. Linear regression analysis further confirms the lack of statistical significance for head injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive importance of memory complaints. These findings highlight the complex interplay of medical history factors in AD risk assessment and underscore the need for further research utilizing larger datasets and advanced causal modeling techniques.

### Testing for Causal Fairness 
[[arxiv](https://arxiv.org/abs/2502.12874)] [[cool](https://papers.cool/arxiv/2502.12874)] [[pdf](https://arxiv.org/pdf/2502.12874)]
> **Authors**: Jiarun Fu,LiZhong Ding,Pengqi Li,Qiuning Wei,Yurong Cheng,Xu Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Causality is widely used in fairness analysis to prevent discrimination on sensitive attributes, such as genders in career recruitment and races in crime prediction. However, the current data-based Potential Outcomes Framework (POF) often leads to untrustworthy fairness analysis results when handling high-dimensional data. To address this, we introduce a distribution-based POF that transform fairness analysis into Distributional Closeness Testing (DCT) by intervening on sensitive attributes. We define counterfactual closeness fairness as the null hypothesis of DCT, where a sensitive attribute is considered fair if its factual and counterfactual potential outcome distributions are sufficiently close. We introduce the Norm-Adaptive Maximum Mean Discrepancy Treatment Effect (N-TE) as a statistic for measuring distributional closeness and apply DCT using the empirical estimator of NTE, referred to Counterfactual Fairness-CLOseness Testing ($\textrm{CF-CLOT}$). To ensure the trustworthiness of testing results, we establish the testing consistency of N-TE through rigorous theoretical analysis. $\textrm{CF-CLOT}$ demonstrates sensitivity in fairness analysis through the flexibility of the closeness parameter $ε$. Unfair sensitive attributes have been successfully tested by $\textrm{CF-CLOT}$ in extensive experiments across various real-world scenarios, which validate the consistency of the testing.

### Leveraging Intermediate Representations for Better Out-of-Distribution Detection 
[[arxiv](https://arxiv.org/abs/2502.12849)] [[cool](https://papers.cool/arxiv/2502.12849)] [[pdf](https://arxiv.org/pdf/2502.12849)]
> **Authors**: Gianluca Guglielmo,Marc Masana
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Code is available at https://github.com/gigug/LIR
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: In real-world applications, machine learning models must reliably detect Out-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD detection methods often rely on analyzing the logits or the embeddings of the penultimate layer of a neural network. However, little work has been conducted on the exploitation of the rich information encoded in intermediate layers. To address this, we analyze the discriminative power of intermediate layers and show that they can positively be used for OoD detection. Therefore, we propose to regularize intermediate layers with an energy-based contrastive loss, and by grouping multiple layers in a single aggregated response. We demonstrate that intermediate layer activations improves OoD detection performance by running a comprehensive evaluation across multiple datasets.

### MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts 
[[arxiv](https://arxiv.org/abs/2502.12845)] [[cool](https://papers.cool/arxiv/2502.12845)] [[pdf](https://arxiv.org/pdf/2502.12845)]
> **Authors**: Nian Ran,Yue Wang,Richard Allmendinger
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Molecular design plays a critical role in advancing fields such as drug discovery, materials science, and chemical engineering. This work introduces the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel framework that combines domain-specific knowledge with the adaptability of Large Language Models to optimize molecular properties across multiple objectives. Leveraging in-context learning and multi-objective optimization, MOLLM achieves superior efficiency, innovation, and performance, significantly surpassing state-of-the-art (SOTA) methods. Recognizing the substantial impact of initial populations on evolutionary algorithms, we categorize them into three types: best initial, worst initial, and random initial, to ensure the initial molecules are the same for each method across experiments. Our results demonstrate that MOLLM consistently outperforms SOTA models in all of our experiments. We also provide extensive ablation studies to evaluate the superiority of our components.

### An improved wind power prediction via a novel wind ramp identification algorithm 
[[arxiv](https://arxiv.org/abs/2502.12807)] [[cool](https://papers.cool/arxiv/2502.12807)] [[pdf](https://arxiv.org/pdf/2502.12807)]
> **Authors**: Yifan Xu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Authors: Yifan Xu Abstract: Conventional wind power prediction methods often struggle to provide accurate and reliable predictions in the presence of sudden changes in wind speed and power output. To address this challenge, this study proposes an integrated algorithm that combines a wind speed mutation identification algorithm, an optimized similar period matching algorithm and a wind power prediction algorithm. By exploiting the convergence properties of meteorological events, the method significantly improves the accuracy of wind power prediction under sudden meteorological changes. Firstly, a novel adaptive model based on variational mode decomposition, the VMD-IC model, is developed for identifying and labelling key turning points in the historical wind power data, representing abrupt meteorological environments. At the same time, this paper proposes Ramp Factor (RF) indicators and wind speed similarity coefficient to optimize the definition algorithm of the current wind power ramp event (WPRE). After innovating the definition of climbing and denoising algorithm, this paper uses the Informer deep learning algorithm to output the first two models as well as multimodal data such as NWP numerical weather forecasts to achieve accurate wind forecasts. The experimental results of the ablation study confirm the effectiveness and reliability of the proposed wind slope identification method. Compared with existing methods, the proposed model exhibits excellent performance and provides valuable guidance for the safe and cost-effective operation of power systems.

### PPGF: Probability Pattern-Guided Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.12802)] [[cool](https://papers.cool/arxiv/2502.12802)] [[pdf](https://arxiv.org/pdf/2502.12802)]
> **Authors**: Yanru Sun,Zongxia Xie,Haoyu Xing,Hualong Yu,Qinghua Hu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Time series forecasting (TSF) is an essential branch of machine learning with various applications. Most methods for TSF focus on constructing different networks to extract better information and improve performance. However, practical application data contain different internal mechanisms, resulting in a mixture of multiple patterns. That is, the model's ability to fit different patterns is different and generates different errors. In order to solve this problem, we propose an end-to-end framework, namely probability pattern-guided time series forecasting (PPGF). PPGF reformulates the TSF problem as a forecasting task guided by probabilistic pattern classification. Firstly, we propose the grouping strategy to approach forecasting problems as classification and alleviate the impact of data imbalance on classification. Secondly, we predict in the corresponding class interval to guarantee the consistency of classification and forecasting. In addition, True Class Probability (TCP) is introduced to pay more attention to the difficult samples to improve the classification accuracy. Detailedly, PPGF classifies the different patterns to determine which one the target value may belong to and estimates it accurately in the corresponding interval. To demonstrate the effectiveness of the proposed framework, we conduct extensive experiments on real-world datasets, and PPGF achieves significant performance improvements over several baseline methods. Furthermore, the effectiveness of TCP and the necessity of consistency between classification and forecasting are proved in the experiments. All data and codes are available online: https://github.com/syrGitHub/PPGF.

### Learning Counterfactually Fair Models via Improved Generation with Neural Causal Models 
[[arxiv](https://arxiv.org/abs/2502.12796)] [[cool](https://papers.cool/arxiv/2502.12796)] [[pdf](https://arxiv.org/pdf/2502.12796)]
> **Authors**: Krishn Vishwas Kher,Aditya Varun V,Shantanu Das,SakethaNath Jagarlapudi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 9 pages, 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: One of the main concerns while deploying machine learning models in real-world applications is fairness. Counterfactual fairness has emerged as an intuitive and natural definition of fairness. However, existing methodologies for enforcing counterfactual fairness seem to have two limitations: (i) generating counterfactual samples faithful to the underlying causal graph, and (ii) as we argue in this paper, existing regularizers are mere proxies and do not directly enforce the exact definition of counterfactual fairness. In this work, our aim is to mitigate both issues. Firstly, we propose employing Neural Causal Models (NCMs) for generating the counterfactual samples. For implementing the abduction step in NCMs, the posteriors of the exogenous variables need to be estimated given a counterfactual query, as they are not readily available. As a consequence, $\mathcal{L}_3$ consistency with respect to the underlying causal graph cannot be guaranteed in practice due to the estimation errors involved. To mitigate this issue, we propose a novel kernel least squares loss term that enforces the $\mathcal{L}_3$ constraints explicitly. Thus, we obtain an improved counterfactual generation suitable for the counterfactual fairness task. Secondly, we propose a new MMD-based regularizer term that explicitly enforces the counterfactual fairness conditions into the base model while training. We show an improved trade-off between counterfactual fairness and generalization over existing baselines on synthetic and benchmark datasets.

### Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models 
[[arxiv](https://arxiv.org/abs/2502.12776)] [[cool](https://papers.cool/arxiv/2502.12776)] [[pdf](https://arxiv.org/pdf/2502.12776)]
> **Authors**: Daiki Chijiwa,Taku Hasegawa,Kyosuke Nishida,Kuniko Saito,Susumu Takeuchi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: While foundation models have been exploited for various expert tasks through fine-tuning, any foundation model will become outdated due to its old knowledge or limited capability. Thus the underlying foundation model should be eventually replaced by new ones, which leads to repeated cost of fine-tuning these new models. Existing work addresses this problem by inference-time tuning, i.e., modifying the output probabilities from the new foundation model with the outputs from the old foundation model and its fine-tuned model, which involves an additional overhead in inference by the latter two models. In this paper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT), that reduces the inference overhead by its nature, based on the reformulation of fine-tuning as the reward maximization. Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss function as in fine-tuning. During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization. Experimental results, covering both vision and language models, demonstrate that the PRT-trained model can achieve comparable accuracy to the existing work of inference-time tuning, with less inference cost.

### One-bit Compressed Sensing using Generative Models 
[[arxiv](https://arxiv.org/abs/2502.12762)] [[cool](https://papers.cool/arxiv/2502.12762)] [[pdf](https://arxiv.org/pdf/2502.12762)]
> **Authors**: Swatantra Kafle,Geethu Joseph,Pramod K. Varshney
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: This paper addresses the classical problem of one-bit compressed sensing using a deep learning-based reconstruction algorithm that leverages a trained generative model to enhance the signal reconstruction performance. The generator, a pre-trained neural network, learns to map from a low-dimensional latent space to a higher-dimensional set of sparse vectors. This generator is then used to reconstruct sparse vectors from their one-bit measurements by searching over its range. The presented algorithm provides an excellent reconstruction performance because the generative model can learn additional structural information about the signal beyond sparsity. Furthermore, we provide theoretical guarantees on the reconstruction accuracy and sample complexity of the algorithm. Through numerical experiments using three publicly available image datasets, MNIST, Fashion-MNIST, and Omniglot, we demonstrate the superior performance of the algorithm compared to other existing algorithms and show that our algorithm can recover both the amplitude and the direction of the signal from one-bit measurements.

### Navigating Demand Uncertainty in Container Shipping: Deep Reinforcement Learning for Enabling Adaptive and Feasible Master Stowage Planning 
[[arxiv](https://arxiv.org/abs/2502.12756)] [[cool](https://papers.cool/arxiv/2502.12756)] [[pdf](https://arxiv.org/pdf/2502.12756)]
> **Authors**: Jaike van Twiller,Yossiri Adulyasak,Erick Delage,Djordje Grbic,Rune Møller Jensen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: This paper is currently under review for IJCAI 2025
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: Reinforcement learning (RL) has shown promise in solving various combinatorial optimization problems. However, conventional RL faces challenges when dealing with real-world constraints, especially when action space feasibility is explicit and dependent on the corresponding state or trajectory. In this work, we focus on using RL in container shipping, often considered the cornerstone of global trade, by dealing with the critical challenge of master stowage planning. The main objective is to maximize cargo revenue and minimize operational costs while navigating demand uncertainty and various complex operational constraints, namely vessel capacity and stability, which must be dynamically updated along the vessel's voyage. To address this problem, we implement a deep reinforcement learning framework with feasibility projection to solve the master stowage planning problem (MPP) under demand uncertainty. The experimental results show that our architecture efficiently finds adaptive, feasible solutions for this multi-stage stochastic optimization problem, outperforming traditional mixed-integer programming and RL with feasibility regularization. Our AI-driven decision-support policy enables adaptive and feasible planning under uncertainty, optimizing operational efficiency and capacity utilization while contributing to sustainable and resilient global supply chains.

### Architect of the Bits World: Masked Autoregressive Modeling for Circuit Generation Guided by Truth Table 
[[arxiv](https://arxiv.org/abs/2502.12751)] [[cool](https://papers.cool/arxiv/2502.12751)] [[pdf](https://arxiv.org/pdf/2502.12751)]
> **Authors**: Haoyuan Wu,Haisheng Zheng,Shoubo Hu,Zhuolun He,Bei Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Logic synthesis, a critical stage in electronic design automation (EDA), optimizes gate-level circuits to minimize power consumption and area occupancy in integrated circuits (ICs). Traditional logic synthesis tools rely on human-designed heuristics, often yielding suboptimal results. Although differentiable architecture search (DAS) has shown promise in generating circuits from truth tables, it faces challenges such as high computational complexity, convergence to local optima, and extensive hyperparameter tuning. Consequently, we propose a novel approach integrating conditional generative models with DAS for circuit generation. Our approach first introduces CircuitVQ, a circuit tokenizer trained based on our Circuit AutoEncoder We then develop CircuitAR, a masked autoregressive model leveraging CircuitVQ as the tokenizer. CircuitAR can generate preliminary circuit structures from truth tables, which guide DAS in producing functionally equivalent circuits. Notably, we observe the scalability and emergent capability in generating complex circuit structures of our CircuitAR models. Extensive experiments also show the superior performance of our method. This research bridges the gap between probabilistic generative models and precise circuit generation, offering a robust solution for logic synthesis.

### Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment 
[[arxiv](https://arxiv.org/abs/2502.12732)] [[cool](https://papers.cool/arxiv/2502.12732)] [[pdf](https://arxiv.org/pdf/2502.12732)]
> **Authors**: Haoyuan Wu,Haisheng Zheng,Yuan Pu,Bei Yu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Understanding the structure and function of circuits is crucial for electronic design automation (EDA). Circuits can be formulated as And-Inverter graphs (AIGs), enabling efficient implementation of representation learning through graph neural networks (GNNs). Masked modeling paradigms have been proven effective in graph representation learning. However, masking augmentation to original circuits will destroy their logical equivalence, which is unsuitable for circuit representation learning. Moreover, existing masked modeling paradigms often prioritize structural information at the expense of abstract information such as circuit function. To address these limitations, we introduce MGVGA, a novel constrained masked modeling paradigm incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA). Specifically, MGM preserves logical equivalence by masking gates in the latent space rather than in the original circuits, subsequently reconstructing the attributes of these masked gates. Meanwhile, large language models (LLMs) have demonstrated an excellent understanding of the Verilog code functionality. Building upon this capability, VGA performs masking operations on original circuits and reconstructs masked gates under the constraints of equivalent Verilog codes, enabling GNNs to learn circuit functions from LLMs. We evaluate MGVGA on various logic synthesis tasks for EDA and show the superior performance of MGVGA compared to previous state-of-the-art methods. Our code is available at https://github.com/wuhy68/MGVGA.

### Learning the symmetric group: large from small 
[[arxiv](https://arxiv.org/abs/2502.12717)] [[cool](https://papers.cool/arxiv/2502.12717)] [[pdf](https://arxiv.org/pdf/2502.12717)]
> **Authors**: Max Petschack,Alexandr Garbali,Jan de Gier
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,组合学,表征论
- **Abstract**: Machine learning explorations can make significant inroads into solving difficult problems in pure mathematics. One advantage of this approach is that mathematical datasets do not suffer from noise, but a challenge is the amount of data required to train these models and that this data can be computationally expensive to generate. Key challenges further comprise difficulty in a posteriori interpretation of statistical models and the implementation of deep and abstract mathematical problems. We propose a method for scalable tasks, by which models trained on simpler versions of a task can then generalize to the full task. Specifically, we demonstrate that a transformer neural-network trained on predicting permutations from words formed by general transpositions in the symmetric group $S_{10}$ can generalize to the symmetric group $S_{25}$ with near 100\% accuracy. We also show that $S_{10}$ generalizes to $S_{16}$ with similar performance if we only use adjacent transpositions. We employ identity augmentation as a key tool to manage variable word lengths, and partitioned windows for training on adjacent transpositions. Finally we compare variations of the method used and discuss potential challenges with extending the method to other tasks.

### CausalMan: A physics-based simulator for large-scale causality 
[[arxiv](https://arxiv.org/abs/2502.12707)] [[cool](https://papers.cool/arxiv/2502.12707)] [[pdf](https://arxiv.org/pdf/2502.12707)]
> **Authors**: Nicholas Tagliapietra,Juergen Luettin,Lavdim Halilaj,Moritz Willig,Tim Pychynski,Kristian Kersting
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: A comprehensive understanding of causality is critical for navigating and operating within today's complex real-world systems. The absence of realistic causal models with known data generating processes complicates fair benchmarking. In this paper, we present the CausalMan simulator, modeled after a real-world production line. The simulator features a diverse range of linear and non-linear mechanisms and challenging-to-predict behaviors, such as discrete mode changes. We demonstrate the inadequacy of many state-of-the-art approaches and analyze the significant differences in their performance and tractability, both in terms of runtime and memory complexity. As a contribution, we will release the CausalMan large-scale simulator. We present two derived datasets, and perform an extensive evaluation of both.

### Scalable Model Merging with Progressive Layer-wise Distillation 
[[arxiv](https://arxiv.org/abs/2502.12706)] [[cool](https://papers.cool/arxiv/2502.12706)] [[pdf](https://arxiv.org/pdf/2502.12706)]
> **Authors**: Jing Xu,Jiazheng Li,Jingzhao Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performance. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods, ProDistill achieves state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill.

### Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees 
[[arxiv](https://arxiv.org/abs/2502.12678)] [[cool](https://papers.cool/arxiv/2502.12678)] [[pdf](https://arxiv.org/pdf/2502.12678)]
> **Authors**: Yongtao Wu,Luca Viano,Yihang Chen,Zhenyu Zhu,Kimon Antonakopoulos,Quanquan Gu,Volkan Cevher
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted as oral presentation in NeurIPS LanGame Workshop, revised from ICLR submission
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Multi-step Preference Optimization (MPO) is built upon the natural actor-critic framework~\citep{peters2008natural}. We further develop OMPO based on the optimistic online gradient descent algorithm~\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a rigorous analysis for both algorithms on convergence and show that OMPO requires $\mathcal{O}(ε^{-1})$ policy updates to converge to an $ε$-approximate Nash equilibrium. We also validate the effectiveness of our method on multi-turn conversations dataset and math reasoning dataset.

### Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.12631)] [[cool](https://papers.cool/arxiv/2502.12631)] [[pdf](https://arxiv.org/pdf/2502.12631)]
> **Authors**: Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning. The code will be released at https://github.com/Sunmmyy/OTPR.git.

### Implicit Repair with Reinforcement Learning in Emergent Communication 
[[arxiv](https://arxiv.org/abs/2502.12624)] [[cool](https://papers.cool/arxiv/2502.12624)] [[pdf](https://arxiv.org/pdf/2502.12624)]
> **Authors**: Fábio Vital,Alberto Sardinha,Francisco S. Melo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: AAMAS 2025 - full paper
- **标题**: None
- **领域**: 机器学习,多代理系统
- **Abstract**: Conversational repair is a mechanism used to detect and resolve miscommunication and misinformation problems when two or more agents interact. One particular and underexplored form of repair in emergent communication is the implicit repair mechanism, where the interlocutor purposely conveys the desired information in such a way as to prevent misinformation from any other interlocutor. This work explores how redundancy can modify the emergent communication protocol to continue conveying the necessary information to complete the underlying task, even with additional external environmental pressures such as noise. We focus on extending the signaling game, called the Lewis Game, by adding noise in the communication channel and inputs received by the agents. Our analysis shows that agents add redundancy to the transmitted messages as an outcome to prevent the negative impact of noise on the task success. Additionally, we observe that the emerging communication protocol's generalization capabilities remain equivalent to architectures employed in simpler games that are entirely deterministic. Additionally, our method is the only one suitable for producing robust communication protocols that can handle cases with and without noise while maintaining increased generalization performance levels.

### Uncertainty-Aware Graph Structure Learning 
[[arxiv](https://arxiv.org/abs/2502.12618)] [[cool](https://papers.cool/arxiv/2502.12618)] [[pdf](https://arxiv.org/pdf/2502.12618)]
> **Authors**: Shen Han,Zhiyao Zhou,Jiawei Chen,Zhezheng Hao,Sheng Zhou,Gang Wang,Yan Feng,Chun Chen,Can Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: This paper has been accepted by TheWebConf 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Neural Networks (GNNs) have become a prominent approach for learning from graph-structured data. However, their effectiveness can be significantly compromised when the graph structure is suboptimal. To address this issue, Graph Structure Learning (GSL) has emerged as a promising technique that refines node connections adaptively. Nevertheless, we identify two key limitations in existing GSL methods: 1) Most methods primarily focus on node similarity to construct relationships, while overlooking the quality of node information. Blindly connecting low-quality nodes and aggregating their ambiguous information can degrade the performance of other nodes. 2) The constructed graph structures are often constrained to be symmetric, which may limit the model's flexibility and effectiveness. To overcome these limitations, we propose an Uncertainty-aware Graph Structure Learning (UnGSL) strategy. UnGSL estimates the uncertainty of node information and utilizes it to adjust the strength of directional connections, where the influence of nodes with high uncertainty is adaptively reduced. Importantly, UnGSL serves as a plug-in module that can be seamlessly integrated into existing GSL methods with minimal additional computational cost. In our experiments, we implement UnGSL into six representative GSL methods, demonstrating consistent performance improvements.

### A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft Landing Problem 
[[arxiv](https://arxiv.org/abs/2502.12617)] [[cool](https://papers.cool/arxiv/2502.12617)] [[pdf](https://arxiv.org/pdf/2502.12617)]
> **Authors**: Vatsal Maru
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: This paper presents a noveldeepreinforcementlearningframework combining graphneuralnetworks with actor-critic architectures to address the aircraft landing problem. The framework achieves a 99.95% reduction in computational time compared to Mixed Integer Programming while maintaining safety compliance, and 38% higher runway throughput over First Come First Serve
- **标题**: None
- **领域**: 机器学习,人工智能,系统与控制
- **Abstract**: The Aircraft Landing Problem (ALP) is one of the challenging problems in aircraft transportation and management. The challenge is to schedule the arriving aircraft in a sequence so that the cost and delays are optimized. There are various solution approaches to solving this problem, most of which are based on operations research algorithms and meta-heuristics. Although traditional methods perform better on one or the other factors, there remains a problem of solving real-time rescheduling and computational scalability altogether. This paper presents a novel deep reinforcement learning (DRL) framework that combines graph neural networks with actor-critic architectures to address the ALP. This paper introduces three key contributions: A graph-based state representation that efficiently captures temporal and spatial relationships between aircraft, a specialized actor-critic architecture designed to handle multiple competing objectives in landing scheduling, and a runway balance strategy that ensures efficient resource utilization while maintaining safety constraints. The results show that the trained algorithm can be tested on different problem sets and the results are competitive to operation research algorithms. The experimental results on standard benchmark data sets demonstrate a 99.95 reduction in computational time compared to Mixed Integer Programming (MIP) and 38 higher runway throughput over First Come First Serve (FCFS) approaches. Therefore, the proposed solution is competitive to traditional approaches and achieves substantial advancements. Notably, it does not require retraining, making it particularly suitable for industrial deployment. The frameworks capability to generate solutions within 1 second enables real-time rescheduling, addressing critical requirements of air traffic management.

### Unveiling Mode Connectivity in Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.12608)] [[cool](https://papers.cool/arxiv/2502.12608)] [[pdf](https://arxiv.org/pdf/2502.12608)]
> **Authors**: Bingheng Li,Zhikai Chen,Haoyu Han,Shenglai Zeng,Jingzhe Liu,Jiliang Tang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A fundamental challenge in understanding graph neural networks (GNNs) lies in characterizing their optimization dynamics and loss landscape geometry, critical for improving interpretability and robustness. While mode connectivity, a lens for analyzing geometric properties of loss landscapes has proven insightful for other deep learning architectures, its implications for GNNs remain unexplored. This work presents the first investigation of mode connectivity in GNNs. We uncover that GNNs exhibit distinct non-linear mode connectivity, diverging from patterns observed in fully-connected networks or CNNs. Crucially, we demonstrate that graph structure, rather than model architecture, dominates this behavior, with graph properties like homophily correlating with mode connectivity patterns. We further establish a link between mode connectivity and generalization, proposing a generalization bound based on loss barriers and revealing its utility as a diagnostic tool. Our findings further bridge theoretical insights with practical implications: they rationalize domain alignment strategies in graph learning and provide a foundation for refining GNN training paradigms.

### Disentangling Long-Short Term State Under Unknown Interventions for Online Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.12603)] [[cool](https://papers.cool/arxiv/2502.12603)] [[pdf](https://arxiv.org/pdf/2502.12603)]
> **Authors**: Ruichu Cai,Haiqin Huang,Zhifang Jiang,Zijian Li,Changze Zhou,Yuequn Liu,Yuming Liu,Zhifeng Hao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ef:AAAI2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Current methods for time series forecasting struggle in the online scenario, since it is difficult to preserve long-term dependency while adapting short-term changes when data are arriving sequentially. Although some recent methods solve this problem by controlling the updates of latent states, they cannot disentangle the long/short-term states, leading to the inability to effectively adapt to nonstationary. To tackle this challenge, we propose a general framework to disentangle long/short-term states for online time series forecasting. Our idea is inspired by the observations where short-term changes can be led by unknown interventions like abrupt policies in the stock market. Based on this insight, we formalize a data generation process with unknown interventions on short-term states. Under mild assumptions, we further leverage the independence of short-term states led by unknown interventions to establish the identification theory to achieve the disentanglement of long/short-term states. Built on this theory, we develop a long short-term disentanglement model (LSTD) to extract the long/short-term states with long/short-term encoders, respectively. Furthermore, the LSTD model incorporates a smooth constraint to preserve the long-term dependencies and an interrupted dependency constraint to enforce the forgetting of short-term dependencies, together boosting the disentanglement of long/short-term states. Experimental results on several benchmark datasets show that our \textbf{LSTD} model outperforms existing methods for online time series forecasting, validating its efficacy in real-world applications.

### Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels 
[[arxiv](https://arxiv.org/abs/2502.12584)] [[cool](https://papers.cool/arxiv/2502.12584)] [[pdf](https://arxiv.org/pdf/2502.12584)]
> **Authors**: Jichan Chung,Irene Y. Chen
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Under review for ICML 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Semi-supervised learning (SSL) leverages limited labeled data alongside abundant unlabeled data to address labeling costs in machine learning. While recent foundation models enable zero-shot inference, attempts to integrate these capabilities into SSL through pseudo-labeling have shown mixed results due to unreliable zero-shot predictions. We present ZMT (Zero-Shot Multi-Task Learning), a framework that jointly optimizes zero-shot pseudo-labels and unsupervised representation learning objectives from contemporary SSL approaches. Our method introduces a multi-task learning-based mechanism that incorporates pseudo-labels while ensuring robustness to varying pseudo-label quality. Experiments across 8 datasets in vision, language, and audio domains demonstrate that ZMT reduces error by up to 56% compared to traditional SSL methods, with particularly compelling results when pseudo-labels are noisy and unreliable. ZMT represents a significant step toward making semi-supervised learning more effective and accessible in resource-constrained environments.

### HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading 
[[arxiv](https://arxiv.org/abs/2502.12574)] [[cool](https://papers.cool/arxiv/2502.12574)] [[pdf](https://arxiv.org/pdf/2502.12574)]
> **Authors**: Cheng Luo,Zefan Cai,Hanshi Sun,Jinqi Xiao,Bo Yuan,Wen Xiao,Junjie Hu,Jiawei Zhao,Beidi Chen,Anima Anandkumar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.

### Sample Efficient Omniprediction and Downstream Swap Regret for Non-Linear Losses 
[[arxiv](https://arxiv.org/abs/2502.12564)] [[cool](https://papers.cool/arxiv/2502.12564)] [[pdf](https://arxiv.org/pdf/2502.12564)]
> **Authors**: Jiuyao Lu,Aaron Roth,Mirah Shi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机科学与博弈论
- **Abstract**: We define "decision swap regret" which generalizes both prediction for downstream swap regret and omniprediction, and give algorithms for obtaining it for arbitrary multi-dimensional Lipschitz loss functions in online adversarial settings. We also give sample complexity bounds in the batch setting via an online-to-batch reduction. When applied to omniprediction, our algorithm gives the first polynomial sample-complexity bounds for Lipschitz loss functions -- prior bounds either applied only to linear loss (or binary outcomes) or scaled exponentially with the error parameter even under the assumption that the loss functions were convex. When applied to prediction for downstream regret, we give the first algorithm capable of guaranteeing swap regret bounds for all downstream agents with non-linear loss functions over a multi-dimensional outcome space: prior work applied only to linear loss functions, modeling risk neutral agents. Our general bounds scale exponentially with the dimension of the outcome space, but we give improved regret and sample complexity bounds for specific families of multidimensional functions of economic interest: constant elasticity of substitution (CES), Cobb-Douglas, and Leontief utility functions.

### Improving the Stability of GNN Force Field Models by Reducing Feature Correlation 
[[arxiv](https://arxiv.org/abs/2502.12548)] [[cool](https://papers.cool/arxiv/2502.12548)] [[pdf](https://arxiv.org/pdf/2502.12548)]
> **Authors**: Yujie Zeng,Wenlong He,Ihor Vasyltsov,Jiaxin Wei,Ying Zhang,Lin Chen,Yuehua Dai
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recently, Graph Neural Network based Force Field (GNNFF) models are widely used in Molecular Dynamics (MD) simulation, which is one of the most cost-effective means in semiconductor material research. However, even such models provide high accuracy in energy and force Mean Absolute Error (MAE) over trained (in-distribution) datasets, they often become unstable during long-time MD simulation when used for out-of-distribution datasets. In this paper, we propose a feature correlation based method for GNNFF models to enhance the stability of MD simulation. We reveal the negative relationship between feature correlation and the stability of GNNFF models, and design a loss function with a dynamic loss coefficient scheduler to reduce edge feature correlation that can be applied in general GNNFF training. We also propose an empirical metric to evaluate the stability in MD simulation. Experiments show our method can significantly improve stability for GNNFF models especially in out-of-distribution data with less than 3% computational overhead. For example, we can ensure the stable MD simulation time from 0.03ps to 10ps for Allegro model.

## 多代理系统(cs.MA:Multiagent Systems)

### Learning Symbolic Task Decompositions for Multi-Agent Teams 
[[arxiv](https://arxiv.org/abs/2502.13376)] [[cool](https://papers.cool/arxiv/2502.13376)] [[pdf](https://arxiv.org/pdf/2502.13376)]
> **Authors**: Ameesh Shah,Niklas Lauffer,Thomas Chen,Nikhil Pitta,Sanjit A. Seshia
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 8 pages, main track full paper at AAMAS 2025
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: One approach for improving sample efficiency in cooperative multi-agent learning is to decompose overall tasks into sub-tasks that can be assigned to individual agents. We study this problem in the context of reward machines: symbolic tasks that can be formally decomposed into sub-tasks. In order to handle settings without a priori knowledge of the environment, we introduce a framework that can learn the optimal decomposition from model-free interactions with the environment. Our method uses a task-conditioned architecture to simultaneously learn an optimal decomposition and the corresponding agents' policies for each sub-task. In doing so, we remove the need for a human to manually design the optimal decomposition while maintaining the sample-efficiency benefits of improved credit assignment. We provide experimental results in several deep reinforcement learning settings, demonstrating the efficacy of our approach. Our results indicate that our approach succeeds even in environments with codependent agent dynamics, enabling synchronous multi-agent learning not achievable in previous works.

### Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control 
[[arxiv](https://arxiv.org/abs/2502.13248)] [[cool](https://papers.cool/arxiv/2502.13248)] [[pdf](https://arxiv.org/pdf/2502.13248)]
> **Authors**: Hankang Gu,Shangbo Wang,Dongyao Jia,Yuli Zhang,Yanrong Luo,Guoqiang Mao,Jianping Wang,Eng Gee Lim
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules--GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks--RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks.

### Autonomous Vehicles Using Multi-Agent Reinforcement Learning for Routing Decisions Can Harm Urban Traffic 
[[arxiv](https://arxiv.org/abs/2502.13188)] [[cool](https://papers.cool/arxiv/2502.13188)] [[pdf](https://arxiv.org/pdf/2502.13188)]
> **Authors**: Anastasia Psarou,Ahmet Onur Akman,Łukasz Gorczyca,Michał Hoffmann,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习,机器人技术
- **Abstract**: Autonomous vehicles (AVs) using Multi-Agent Reinforcement Learning (MARL) for simultaneous route optimization may destabilize traffic environments, with human drivers possibly experiencing longer travel times. We study this interaction by simulating human drivers and AVs. Our experiments with standard MARL algorithms reveal that, even in trivial cases, policies often fail to converge to an optimal solution or require long training periods. The problem is amplified by the fact that we cannot rely entirely on simulated training, as there are no accurate models of human routing behavior. At the same time, real-world training in cities risks destabilizing urban traffic systems, increasing externalities, such as $CO_2$ emissions, and introducing non-stationarity as human drivers adapt unpredictably to AV behaviors. Centralization can improve convergence in some cases, however, it raises privacy concerns for the travelers' destination data. In this position paper, we argue that future research must prioritize realistic benchmarks, cautious deployment strategies, and tools for monitoring and regulating AV routing behaviors to ensure sustainable and equitable urban mobility systems.

### HedgeAgents: A Balanced-aware Multi-agent Financial Trading System 
[[arxiv](https://arxiv.org/abs/2502.13165)] [[cool](https://papers.cool/arxiv/2502.13165)] [[pdf](https://arxiv.org/pdf/2502.13165)]
> **Authors**: Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-19
> **comment**: This paper has been accepted by The Web Conference 2025 (WWW 2025) and selected for an oral presentation
- **标题**: None
- **领域**: 多代理系统,人工智能,交易和市场微观结构
- **Abstract**: As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ``hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (https://hedgeagents.github.io/).

### Multi-Agent Actor-Critic Generative AI for Query Resolution and Analysis 
[[arxiv](https://arxiv.org/abs/2502.13164)] [[cool](https://papers.cool/arxiv/2502.13164)] [[pdf](https://arxiv.org/pdf/2502.13164)]
> **Authors**: Mohammad Wali Ur Rahman,Ric Nevarez,Lamia Tasnim Mim,Salim Hariri
> **First submission**: 2025-02-16
> **First announcement**: 2025-02-19
> **comment**: Accepted for publication in IEEE Transactions on Artificial Intelligence
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: In this paper, we introduce MASQRAD (Multi-Agent Strategic Query Resolution and Diagnostic tool), a transformative framework for query resolution based on the actor-critic model, which utilizes multiple generative AI agents. MASQRAD is excellent at translating imprecise or ambiguous user inquiries into precise and actionable requests. This framework generates pertinent visualizations and responses to these focused queries, as well as thorough analyses and insightful interpretations for users. MASQRAD addresses the common shortcomings of existing solutions in domains that demand fast and precise data interpretation, such as their incapacity to successfully apply AI for generating actionable insights and their challenges with the inherent ambiguity of user queries. MASQRAD functions as a sophisticated multi-agent system but "masquerades" to users as a single AI entity, which lowers errors and enhances data interaction. This approach makes use of three primary AI agents: Actor Generative AI, Critic Generative AI, and Expert Analysis Generative AI. Each is crucial for creating, enhancing, and evaluating data interactions. The Actor AI generates Python scripts to generate data visualizations from large datasets within operational constraints, and the Critic AI rigorously refines these scripts through multi-agent debate. Finally, the Expert Analysis AI contextualizes the outcomes to aid in decision-making. With an accuracy rate of 87\% when handling tasks related to natural language visualization, MASQRAD establishes new benchmarks for automated data interpretation and showcases a noteworthy advancement that has the potential to revolutionize AI-driven applications.

### Understanding Dynamic Diffusion Process of LLM-based Agents under Information Asymmetry 
[[arxiv](https://arxiv.org/abs/2502.13160)] [[cool](https://papers.cool/arxiv/2502.13160)] [[pdf](https://arxiv.org/pdf/2502.13160)]
> **Authors**: Yiwen Zhang,Yifu Wu,Wenyue Hua,Xuming Hu
> **First submission**: 2025-02-15
> **First announcement**: 2025-02-19
> **comment**: 8 pages, 4 figures
- **标题**: None
- **领域**: 多代理系统,人工智能
- **Abstract**: Large language models have been used to simulate human society using multi-agent systems. Most current social simulation research emphasizes interactive behaviors in fixed environments, ignoring information opacity, relationship variability and diffusion diversity. In this paper, we study the dynamics of information diffusion in 12 asymmetric open environments defined by information content and distribution mechanisms. We first present a general framework to capture the features of information diffusion. Then, we designed a dynamic attention mechanism to help agents allocate attention to different information, addressing the limitations of LLM-based attention. Agents start by responding to external information stimuli within a five-agent group, increasing group size and forming information circles while developing relationships and sharing information. Additionally, we observe the emergence of information cocoons, the evolution of information gaps, and the accumulation of social capital, which are closely linked to psychological, sociological, and communication theories.

### Hypernetwork-based approach for optimal composition design in partially controlled multi-agent systems 
[[arxiv](https://arxiv.org/abs/2502.12605)] [[cool](https://papers.cool/arxiv/2502.12605)] [[pdf](https://arxiv.org/pdf/2502.12605)]
> **Authors**: Kyeonghyeon Park,David Molina Concha,Hyun-Rok Lee,Chi-Guhn Lee,Taesik Lee
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习
- **Abstract**: Partially Controlled Multi-Agent Systems (PCMAS) are comprised of controllable agents, managed by a system designer, and uncontrollable agents, operating autonomously. This study addresses an optimal composition design problem in PCMAS, which involves the system designer's problem, determining the optimal number and policies of controllable agents, and the uncontrollable agents' problem, identifying their best-response policies. Solving this bi-level optimization problem is computationally intensive, as it requires repeatedly solving multi-agent reinforcement learning problems under various compositions for both types of agents. To address these challenges, we propose a novel hypernetwork-based framework that jointly optimizes the system's composition and agent policies. Unlike traditional methods that train separate policy networks for each composition, the proposed framework generates policies for both controllable and uncontrollable agents through a unified hypernetwork. This approach enables efficient information sharing across similar configurations, thereby reducing computational overhead. Additional improvements are achieved by incorporating reward parameter optimization and mean action networks. Using real-world New York City taxi data, we demonstrate that our framework outperforms existing methods in approximating equilibrium policies. Our experimental results show significant improvements in key performance metrics, such as order response rate and served demand, highlighting the practical utility of controlling agents and their potential to enhance decision-making in PCMAS.

## 多媒体(cs.MM:Multimedia)

### GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis 
[[arxiv](https://arxiv.org/abs/2502.13196)] [[cool](https://papers.cool/arxiv/2502.13196)] [[pdf](https://arxiv.org/pdf/2502.13196)]
> **Authors**: Pedro Martin,António Rodrigues,João Ascenso,Maria Paula Queluz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,计算机视觉和模式识别
- **Abstract**: Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### Fast Data Aware Neural Architecture Search via Supernet Accelerated Evaluation 
[[arxiv](https://arxiv.org/abs/2502.12690)] [[cool](https://papers.cool/arxiv/2502.12690)] [[pdf](https://arxiv.org/pdf/2502.12690)]
> **Authors**: Emil Njor,Colby Banbury,Xenofon Fafoutis
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: :68T10; 68T20; 68T45
- **标题**: None
- **领域**: 神经和进化计算,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Tiny machine learning (TinyML) promises to revolutionize fields such as healthcare, environmental monitoring, and industrial maintenance by running machine learning models on low-power embedded systems. However, the complex optimizations required for successful TinyML deployment continue to impede its widespread adoption. A promising route to simplifying TinyML is through automatic machine learning (AutoML), which can distill elaborate optimization workflows into accessible key decisions. Notably, Hardware Aware Neural Architecture Searches - where a computer searches for an optimal TinyML model based on predictive performance and hardware metrics - have gained significant traction, producing some of today's most widely used TinyML models. Nevertheless, limiting optimization solely to neural network architectures can prove insufficient. Because TinyML systems must operate under extremely tight resource constraints, the choice of input data configuration, such as resolution or sampling rate, also profoundly impacts overall system efficiency. Achieving truly optimal TinyML systems thus requires jointly tuning both input data and model architecture. Despite its importance, this "Data Aware Neural Architecture Search" remains underexplored. To address this gap, we propose a new state-of-the-art Data Aware Neural Architecture Search technique and demonstrate its effectiveness on the novel TinyML ``Wake Vision'' dataset. Our experiments show that across varying time and hardware constraints, Data Aware Neural Architecture Search consistently discovers superior TinyML systems compared to purely architecture-focused methods, underscoring the critical role of data-aware optimization in advancing TinyML.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### NTP-INT: Network Traffic Prediction-Driven In-band Network Telemetry for High-load Switches 
[[arxiv](https://arxiv.org/abs/2502.12834)] [[cool](https://papers.cool/arxiv/2502.12834)] [[pdf](https://arxiv.org/pdf/2502.12834)]
> **Authors**: Penghui Zhang,Hua Zhang,Yuqi Dai,Cheng Zeng,Jingyu Wang,Jianxin Liao
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: In-band network telemetry (INT) is essential to network management due to its real-time visibility. However, because of the rapid increase in network devices and services, it has become crucial to have targeted access to detailed network information in a dynamic network environment. This paper proposes an intelligent network telemetry system called NTP-INT to obtain more fine-grained network information on high-load switches. Specifically, NTP-INT consists of three modules: network traffic prediction module, network pruning module, and probe path planning module. Firstly, the network traffic prediction module adopts a Multi-Temporal Graph Neural Network (MTGNN) to predict future network traffic and identify high-load switches. Then, we design the network pruning algorithm to generate a subnetwork covering all high-load switches to reduce the complexity of probe path planning. Finally, the probe path planning module uses an attention-mechanism-based deep reinforcement learning (DEL) model to plan efficient probe paths in the network slice. The experimental results demonstrate that NTP-INT can acquire more precise network information on high-load switches while decreasing the control overhead by 50\%.

### Reinforcement Learning for Dynamic Resource Allocation in Optical Networks: Hype or Hope? 
[[arxiv](https://arxiv.org/abs/2502.12804)] [[cool](https://papers.cool/arxiv/2502.12804)] [[pdf](https://arxiv.org/pdf/2502.12804)]
> **Authors**: Michael Doherty,Robin Matzner,Rasoul Sadeghi,Polina Bayvel,Alejandra Beghelli
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习,系统与控制
- **Abstract**: The application of reinforcement learning (RL) to dynamic resource allocation in optical networks has been the focus of intense research activity in recent years, with almost 100 peer-reviewed papers. We present a review of progress in the field, and identify significant gaps in benchmarking practices and reproducibility. To determine the strongest benchmark algorithms, we systematically evaluate several heuristics across diverse network topologies. We find that path count and sort criteria for path selection significantly affect the benchmark performance. We meticulously recreate the problems from five landmark papers and apply the improved benchmarks. Our comparisons demonstrate that simple heuristics consistently match or outperform the published RL solutions, often with an order of magnitude lower blocking probability. Furthermore, we present empirical lower bounds on network blocking using a novel defragmentation-based method, revealing that potential improvements over the benchmark heuristics are limited to 19--36\% increased traffic load for the same blocking performance in our examples. We make our simulation framework and results publicly available to promote reproducible research and standardized evaluation https://doi.org/10.5281/zenodo.12594495.

## 机器人技术(cs.RO:Robotics)

### Object-Pose Estimation With Neural Population Codes 
[[arxiv](https://arxiv.org/abs/2502.13403)] [[cool](https://papers.cool/arxiv/2502.13403)] [[pdf](https://arxiv.org/pdf/2502.13403)]
> **Authors**: Heiko Hoffmann,Richard Hoffmann
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Robotic assembly tasks require object-pose estimation, particularly for tasks that avoid costly mechanical constraints. Object symmetry complicates the direct mapping of sensory input to object rotation, as the rotation becomes ambiguous and lacks a unique training target. Some proposed solutions involve evaluating multiple pose hypotheses against the input or predicting a probability distribution, but these approaches suffer from significant computational overhead. Here, we show that representing object rotation with a neural population code overcomes these limitations, enabling a direct mapping to rotation and end-to-end learning. As a result, population codes facilitate fast and accurate pose estimation. On the T-LESS dataset, we achieve inference in 3.2 milliseconds on an Apple M1 CPU and a Maximum Symmetry-Aware Surface Distance accuracy of 84.7% using only gray-scale image input, compared to 69.7% accuracy when directly mapping to pose.

### SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation 
[[arxiv](https://arxiv.org/abs/2502.13143)] [[cool](https://papers.cool/arxiv/2502.13143)] [[pdf](https://arxiv.org/pdf/2502.13143)]
> **Authors**: Zekun Qi,Wenyao Zhang,Yufei Ding,Runpei Dong,Xinqiang Yu,Jingwen Li,Lingyun Xu,Baoyu Li,Xialin He,Guofan Fan,Jiazhao Zhang,Jiawei He,Jiayuan Gu,Xin Jin,Kaisheng Ma,Zhizheng Zhang,He Wang,Li Yi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Project page: https://qizekun.github.io/sofar/
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别
- **Abstract**: Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.

### Pre-training Auto-regressive Robotic Models with 4D Representations 
[[arxiv](https://arxiv.org/abs/2502.13142)] [[cool](https://papers.cool/arxiv/2502.13142)] [[pdf](https://arxiv.org/pdf/2502.13142)]
> **Authors**: Dantong Niu,Yuvan Sharma,Haoru Xue,Giscard Biamby,Junyi Zhang,Ziteng Ji,Trevor Darrell,Roei Herzig
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.

### RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations 
[[arxiv](https://arxiv.org/abs/2502.13134)] [[cool](https://papers.cool/arxiv/2502.13134)] [[pdf](https://arxiv.org/pdf/2502.13134)]
> **Authors**: Jingxiao Chen,Xinyao Li,Jiahang Cao,Zhengbang Zhu,Wentao Dong,Minghuan Liu,Ying Wen,Yong Yu,Liqing Zhang,Weinan Zhang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Project website: https://humanoid-interaction.github.io/
- **标题**: None
- **领域**: 机器人技术,人机交互,机器学习
- **Abstract**: Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans immediately. To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation. RHINO provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions. RHINO is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data. In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions. We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios.

### SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning 
[[arxiv](https://arxiv.org/abs/2502.12674)] [[cool](https://papers.cool/arxiv/2502.12674)] [[pdf](https://arxiv.org/pdf/2502.12674)]
> **Authors**: Peizhuo Li,Hongyi Li,Ge Sun,Jin Cheng,Xinrong Yang,Guillaume Bellegarda,Milad Shafiee,Yuhong Cao,Auke Ijspeert,Guillaume Sartoretti
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. In principle, this approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances, highlighting its potential for practical deployments in human-centric and safety-critical scenarios.

### Learning a High-quality Robotic Wiping Policy Using Systematic Reward Analysis and Visual-Language Model Based Curriculum 
[[arxiv](https://arxiv.org/abs/2502.12599)] [[cool](https://papers.cool/arxiv/2502.12599)] [[pdf](https://arxiv.org/pdf/2502.12599)]
> **Authors**: Yihong Liu,Dongyeop Kang,Sehoon Ha
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Autonomous robotic wiping is an important task in various industries, ranging from industrial manufacturing to sanitization in healthcare. Deep reinforcement learning (Deep RL) has emerged as a promising algorithm, however, it often suffers from a high demand for repetitive reward engineering. Instead of relying on manual tuning, we first analyze the convergence of quality-critical robotic wiping, which requires both high-quality wiping and fast task completion, to show the poor convergence of the problem and propose a new bounded reward formulation to make the problem feasible. Then, we further improve the learning process by proposing a novel visual-language model (VLM) based curriculum, which actively monitors the progress and suggests hyperparameter tuning. We demonstrate that the combined method can find a desirable wiping policy on surfaces with various curvatures, frictions, and waypoints, which cannot be learned with the baseline formulation. The demo of this project can be found at: https://sites.google.com/view/highqualitywiping.

### Design and Implementation of a Dual Uncrewed Surface Vessel Platform for Bathymetry Research under High-flow Conditions 
[[arxiv](https://arxiv.org/abs/2502.12539)] [[cool](https://papers.cool/arxiv/2502.12539)] [[pdf](https://arxiv.org/pdf/2502.12539)]
> **Authors**: Dinesh Kumar,Amin Ghorbanpour,Kin Yen,Iman Soltani
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Corresponding author: Iman Soltani (isoltani@ucdavis.edu)
- **标题**: None
- **领域**: 机器人技术,机器学习,系统与控制
- **Abstract**: Bathymetry, the study of underwater topography, relies on sonar mapping of submerged structures. These measurements, critical for infrastructure health monitoring, often require expensive instrumentation. The high financial risk associated with sensor damage or vessel loss creates a reluctance to deploy uncrewed surface vessels (USVs) for bathymetry. However, the crewed-boat bathymetry operations, are costly, pose hazards to personnel, and frequently fail to achieve the stable conditions necessary for bathymetry data collection, especially under high currents. Further research is essential to advance autonomous control, navigation, and data processing technologies, with a particular focus on bathymetry. There is a notable lack of accessible hardware platforms that allow for integrated research in both bathymetry-focused autonomous control and navigation, as well as data evaluation and processing. This paper addresses this gap through the design and implementation of two complementary USV systems tailored for uncrewed bathymetry research. This includes a low-cost USV for Navigation And Control research (NAC-USV) and a second, high-end USV equipped with a high-resolution multi-beam sonar and the associated hardware for Bathymetry data quality Evaluation and Post-processing research (BEP-USV). The NAC-USV facilitates the investigation of autonomous, fail-safe navigation and control, emphasizing the stability requirements for high-quality bathymetry data collection while minimizing the risk to equipment. The BEP-USV, which mirrors the NAC-USV hardware, is then used for additional control validation and in-depth exploration of bathymetry data evaluation and post-processing methodologies. We detail the design and implementation of both systems, and open source the design. Furthermore, we demonstrate the system's effectiveness in a range of operational scenarios.

## 声音(cs.SD:Sound)

### Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise 
[[arxiv](https://arxiv.org/abs/2502.13395)] [[cool](https://papers.cool/arxiv/2502.13395)] [[pdf](https://arxiv.org/pdf/2502.13395)]
> **Authors**: Tianye Huang,Aopeng Li,Xiang Li,Jing Zhang,Sijing Xian,Qi Zhang,Mingkong Lu,Guodong Chen,Liangming Xiong,Xiangyun Hu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 13 pages, 8 figures
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理,信号处理,光学
- **Abstract**: Distributed acoustic sensor (DAS) technology leverages optical fiber cables to detect acoustic signals, providing cost-effective and dense monitoring capabilities. It offers several advantages including resistance to extreme conditions, immunity to electromagnetic interference, and accurate detection. However, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared to geophones and is susceptible to various noise types, such as random noise, erratic noise, level noise, and long-period noise. This reduced S/N can negatively impact data analyses containing inversion and interpretation. While artificial intelligence has demonstrated excellent denoising capabilities, most existing methods rely on supervised learning with labeled data, which imposes stringent requirements on the quality of the labels. To address this issue, we develop a label-free unsupervised learning (UL) network model based on Context-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS data. The CP-UNet utilizes the Context Pyramid Module in the encoding and decoding process to extract features and reconstruct the DAS data. To enhance the connectivity between shallow and deep features, we add a Connected Module (CM) to both encoding and decoding section. Layer Normalization (LN) is utilized to replace the commonly employed Batch Normalization (BN), accelerating the convergence of the model and preventing gradient explosion during training. Huber-loss is adopted as our loss function whose parameters are experimentally determined. We apply the network to both the 2-D synthetic and filed data. Comparing to traditional denoising methods and the latest UL framework, our proposed method demonstrates superior noise reduction performance.

### SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation 
[[arxiv](https://arxiv.org/abs/2502.13128)] [[cool](https://papers.cool/arxiv/2502.13128)] [[pdf](https://arxiv.org/pdf/2502.13128)]
> **Authors**: Zihan Liu,Shuangrui Ding,Zhixiong Zhang,Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The generated samples are showcased on our project page at https://liuzh-19.github.io/SongGen/ , and the code will be available at https://github.com/LiuZH-19/SongGen .

### High-Fidelity Music Vocoder using Neural Audio Codecs 
[[arxiv](https://arxiv.org/abs/2502.12759)] [[cool](https://papers.cool/arxiv/2502.12759)] [[pdf](https://arxiv.org/pdf/2502.12759)]
> **Authors**: Luca A. Lanzendörfer,Florian Grötschla,Michael Ungersböck,Roger Wattenhofer
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Accepted at ICASSP 2025
- **标题**: None
- **领域**: 声音,机器学习
- **Abstract**: While neural vocoders have made significant progress in high-fidelity speech synthesis, their application on polyphonic music has remained underexplored. In this work, we propose DisCoder, a neural vocoder that leverages a generative adversarial encoder-decoder architecture informed by a neural audio codec to reconstruct high-fidelity 44.1 kHz audio from mel spectrograms. Our approach first transforms the mel spectrogram into a lower-dimensional representation aligned with the Descript Audio Codec (DAC) latent space before reconstructing it to an audio signal using a fine-tuned DAC decoder. DisCoder achieves state-of-the-art performance in music synthesis on several objective metrics and in a MUSHRA listening study. Our approach also shows competitive performance in speech synthesis, highlighting its potential as a universal vocoder.

### DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.12623)] [[cool](https://papers.cool/arxiv/2502.12623)] [[pdf](https://arxiv.org/pdf/2502.12623)]
> **Authors**: Zhuoyuan Mao,Mengjie Zhao,Qiyu Wu,Hiromi Wakaki,Yuki Mitsufuji
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,多媒体,音频和语音处理
- **Abstract**: Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the model's ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-alignment Transformer to enhance modality fusion prior to input into text LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We plan to open-source the models and the newly constructed datasets.

## 软件工程(cs.SE:Software Engineering)

### Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graph Construction 
[[arxiv](https://arxiv.org/abs/2502.13412)] [[cool](https://papers.cool/arxiv/2502.13412)] [[pdf](https://arxiv.org/pdf/2502.13412)]
> **Authors**: Yanbang Sun,Qing Huang,Xiaoxue Ren,Zhenchang Xing,Xiaohong Li,Junjie Wang
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The API Knowledge Graph (API KG) is a structured network that models API entities and their relations, providing essential semantic insights for tasks such as API recommendation, code generation, and API misuse detection. However, constructing a knowledge-rich and reliable API KG presents several challenges. Existing schema-based methods rely heavily on manual annotations to design KG schemas, leading to excessive manual overhead. On the other hand, schema-free methods, due to the lack of schema guidance, are prone to introducing noise, reducing the KG's reliability. To address these issues, we propose the Explore-Construct-Filter framework, an automated approach for API KG construction based on large language models (LLMs). This framework consists of three key modules: 1) KG exploration: LLMs simulate the workflow of annotators to automatically design a schema with comprehensive type triples, minimizing human intervention; 2) KG construction: Guided by the schema, LLMs extract instance triples to construct a rich yet unreliable API KG; 3) KG filtering: Removing invalid type triples and suspicious instance triples to construct a rich and reliable API KG. Experimental results demonstrate that our method surpasses the state-of-the-art method, achieving a 25.2% improvement in F1 score. Moreover, the Explore-Construct-Filter framework proves effective, with the KG exploration module increasing KG richness by 133.6% and the KG filtering module improving reliability by 26.6%. Finally, cross-model experiments confirm the generalizability of our framework.

### The Role of GitHub Copilot on Software Development: A Perspec-tive on Productivity, Security, Best Practices and Future Directions 
[[arxiv](https://arxiv.org/abs/2502.13199)] [[cool](https://papers.cool/arxiv/2502.13199)] [[pdf](https://arxiv.org/pdf/2502.13199)]
> **Authors**: Suresh Babu Nettur,Shanthi Karpurapu,Unnati Nettur,Likhit Sagar Gajja,Sravanthy Myneni,Akhil Dusi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Correspondence and co-first authors: nettursuresh@gmail.com, shanthi.karpurapu@gmail.com
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: GitHub Copilot is transforming software development by automating tasks and boosting productivity through AI-driven code generation. In this paper, we con-duct a literature survey to synthesize insights on Copilot's impact on productivity and security. We review academic journal databases, industry reports, and official docu-mentation to highlight key findings and challenges. While Copilot accelerates coding and prototyping, concerns over security vulnerabilities and intellectual property risks persist. Drawing from the literature, we provide a perspective on best practices and future directions for responsible AI adoption in software engineering, offering action-able insights for developers and organizations to integrate Copilot effectively while maintaining high standards of quality and security.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Evaluating link prediction: New perspectives and recommendations 
[[arxiv](https://arxiv.org/abs/2502.12777)] [[cool](https://papers.cool/arxiv/2502.12777)] [[pdf](https://arxiv.org/pdf/2502.12777)]
> **Authors**: Bhargavi Kalyani I,A Rama Prasad Mathi,Niladri Sett
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Link prediction (LP) is an important problem in network science and machine learning research. The state-of-the-art LP methods are usually evaluated in a uniform setup, ignoring several factors associated with the data and application specific needs. We identify a number of such factors, such as, network-type, problem-type, geodesic distance between the end nodes and its distribution over the classes, nature and applicability of LP methods, class imbalance and its impact on early retrieval, evaluation metric, etc., and present an experimental setup which allows us to evaluate LP methods in a rigorous and controlled manner. We perform extensive experiments with a variety of LP methods over real network datasets in this controlled setup, and gather valuable insights on the interactions of these factors with the performance of LP through an array of carefully designed hypotheses. Following the insights, we provide recommendations to be followed as best practice for evaluating LP methods.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Synthetic generation of 2D data records based on Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.13183)] [[cool](https://papers.cool/arxiv/2502.13183)] [[pdf](https://arxiv.org/pdf/2502.13183)]
> **Authors**: Darius Couchard,Oscar Olarte,Rob Haelterman
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 6 pages conference publication submitted to IEEE MeMeA 2025
- **标题**: None
- **领域**: 图像和视频处理,机器学习
- **Abstract**: Gas Chromatography coupled with Ion Mobility Spectrometry (GC-IMS) is a dual-separation analytical technique widely used for identifying components in gaseous samples by separating and analysing the arrival times of their constituent species. Data generated by GC-IMS is typically represented as two-dimensional spectra, providing rich information but posing challenges for data-driven analysis due to limited labelled datasets. This study introduces a novel method for generating synthetic 2D spectra using a deep learning framework based on Autoencoders. Although applied here to GC-IMS data, the approach is broadly applicable to any two-dimensional spectral measurements where labelled data are scarce. While performing component classification over a labelled dataset of GC-IMS records, the addition of synthesized records significantly has improved the classification performance, demonstrating the method's potential for overcoming dataset limitations in machine learning frameworks.

### Fundus2Globe: Generative AI-Driven 3D Digital Twins for Personalized Myopia Management 
[[arxiv](https://arxiv.org/abs/2502.13182)] [[cool](https://papers.cool/arxiv/2502.13182)] [[pdf](https://arxiv.org/pdf/2502.13182)]
> **Authors**: Danli Shi,Bowen Liu,Zhen Tian,Yue Wu,Jiancheng Yang,Ruoyu Chen,Bo Yang,Ou Xiao,Mingguang He
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 24 pages, 6 figures
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,信号处理
- **Abstract**: Myopia, projected to affect 50% population globally by 2050, is a leading cause of vision loss. Eyes with pathological myopia exhibit distinctive shape distributions, which are closely linked to the progression of vision-threatening complications. Recent understanding of eye-shape-based biomarkers requires magnetic resonance imaging (MRI), however, it is costly and unrealistic in routine ophthalmology clinics. We present Fundus2Globe, the first AI framework that synthesizes patient-specific 3D eye globes from ubiquitous 2D color fundus photographs (CFPs) and routine metadata (axial length, spherical equivalent), bypassing MRI dependency. By integrating a 3D morphable eye model (encoding biomechanical shape priors) with a latent diffusion model, our approach achieves submillimeter accuracy in reconstructing posterior ocular anatomy efficiently. Fundus2Globe uniquely quantifies how vision-threatening lesions (e.g., staphylomas) in CFPs correlate with MRI-validated 3D shape abnormalities, enabling clinicians to simulate posterior segment changes in response to refractive shifts. External validation demonstrates its robust generation performance, ensuring fairness across underrepresented groups. By transforming 2D fundus imaging into 3D digital replicas of ocular structures, Fundus2Globe is a gateway for precision ophthalmology, laying the foundation for AI-driven, personalized myopia management.

## 信号处理(eess.SP:Signal Processing)

### Deep-Unfolded Massive Grant-Free Transmission in Cell-Free Wireless Communication Systems 
[[arxiv](https://arxiv.org/abs/2502.13390)] [[cool](https://papers.cool/arxiv/2502.13390)] [[pdf](https://arxiv.org/pdf/2502.13390)]
> **Authors**: Gangle Sun,Mengyao Cao,Wenjin Wang,Wei Xu,Christoph Studer
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: To appear in the IEEE Transactions on Signal Processing
- **标题**: None
- **领域**: 信号处理,信息论,机器学习
- **Abstract**: Grant-free transmission and cell-free communication are vital in improving coverage and quality-of-service for massive machine-type communication. This paper proposes a novel framework of joint active user detection, channel estimation, and data detection (JACD) for massive grant-free transmission in cell-free wireless communication systems. We formulate JACD as an optimization problem and solve it approximately using forward-backward splitting. To deal with the discrete symbol constraint, we relax the discrete constellation to its convex hull and propose two approaches that promote solutions from the constellation set. To reduce complexity, we replace costly computations with approximate shrinkage operations and approximate posterior mean estimator computations. To improve active user detection (AUD) performance, we introduce a soft-output AUD module that considers both the data estimates and channel conditions. To jointly optimize all algorithm hyper-parameters and to improve JACD performance, we further deploy deep unfolding together with a momentum strategy, resulting in two algorithms called DU-ABC and DU-POEM. Finally, we demonstrate the efficacy of the proposed JACD algorithms via extensive system simulations.

### Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC Networks 
[[arxiv](https://arxiv.org/abs/2502.12736)] [[cool](https://papers.cool/arxiv/2502.12736)] [[pdf](https://arxiv.org/pdf/2502.12736)]
> **Authors**: Jingzhi Hu,Xin Li,Zhou Su,Jun Luo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: In wireless networks with integrated sensing and communications (ISAC), edge intelligence (EI) is expected to be developed at edge devices (ED) for sensing user activities based on channel state information (CSI). However, due to the CSI being highly specific to users' characteristics, the CSI-activity relationship is notoriously domain dependent, essentially demanding EI to learn sufficient datasets from various domains in order to gain cross-domain sensing capability. This poses a crucial challenge owing to the EDs' limited resources, for which storing datasets across all domains will be a significant burden. In this paper, we propose the EdgeCL framework, enabling the EI to continually learn-then-discard each incoming dataset, while remaining resilient to catastrophic forgetting. We design a transformer-based discriminator for handling sequences of noisy and nonequispaced CSI samples. Besides, we propose a distilled core-set based knowledge retention method with robustness-enhanced optimization to train the discriminator, preserving its performance for previous domains while preventing future forgetting. Experimental evaluations show that EdgeCL achieves 89% of performance compared to cumulative training while consuming only 3% of its memory, mitigating forgetting by 79%.

## 高能物理-实验(hep-ex:High Energy Physics - Experiment)

### Neuromorphic Readout for Hadron Calorimeters 
[[arxiv](https://arxiv.org/abs/2502.12693)] [[cool](https://papers.cool/arxiv/2502.12693)] [[pdf](https://arxiv.org/pdf/2502.12693)]
> **Authors**: Enrico Lupi,Abhishek,Max Aehle,Muhammad Awais,Alessandro Breccia,Riccardo Carroccio,Long Chen,Abhijit Das,Andrea De Vita,Tommaso Dorigo,Nicolas R. Gauger,Ralf Keidel,Jan Kieseler,Anders Mikkelsen,Federico Nardi,Xuan Tung Nguyen,Fredrik Sandin,Kylian Schmidt,Pietro Vischia,Joseph Willmore
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 12 figures, submitted to MDPI Particles
- **标题**: None
- **领域**: 高能物理-实验,新兴技术,机器学习,神经和进化计算
- **Abstract**: We simulate hadrons impinging on a homogeneous lead-tungstate (PbWO4) calorimeter to investigate how the resulting light yield and its temporal structure, as detected by an array of light-sensitive sensors, can be processed by a neuromorphic computing system. Our model encodes temporal photon distributions as spike trains and employs a fully connected spiking neural network to estimate the total deposited energy, as well as the position and spatial distribution of the light emissions within the sensitive material. The extracted primitives offer valuable topological information about the shower development in the material, achieved without requiring a segmentation of the active medium. A potential nanophotonic implementation using III-V semiconductor nanowires is discussed. It can be both fast and energy efficient.

## 数值分析(math.NA:Numerical Analysis)

### Pushing the Limits of the Reactive Affine Shaker Algorithm to Higher Dimensions 
[[arxiv](https://arxiv.org/abs/2502.12877)] [[cool](https://papers.cool/arxiv/2502.12877)] [[pdf](https://arxiv.org/pdf/2502.12877)]
> **Authors**: Roberto Battiti,Mauro Brunato
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Submitted to: the 19thLearningand Intelligent Optimization Conference (LION19), June 15-19 2025, Prague, Czech Republic (https://lion19.org/)
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: Bayesian Optimization (BO) for the minimization of expensive functions of continuous variables uses all the knowledge acquired from previous samples (${\boldsymbol x}_i$ and $f({\boldsymbol x}_i)$ values) to build a surrogate model based on Gaussian processes. The surrogate is then exploited to define the next point to sample, through a careful balance of exploration and exploitation. Initially intended for low-dimensional spaces, BO has recently been modified and used also for very large-dimensional spaces (up to about one thousand dimensions). In this paper we consider a much simpler algorithm, called "Reactive Affine Shaker" (RAS). The next sample is always generated with a uniform probability distribution inside a parallelepiped (the "box"). At each iteration, the form of the box is adapted during the search through an affine transformation, based only on the point $\boldsymbol x$ position and on the success or failure in improving the function. The function values are therefore not used directly to modify the search area and to generate the next sample. The entire dimensionality is kept (no active subspaces). Despite its extreme simplicity and its use of only stochastic local search, surprisingly the produced results are comparable to and not too far from the state-of-the-art results of high-dimensional versions of BO, although with some more function evaluations. An ablation study and an analysis of probability distribution of directions (improving steps and prevailing box orientation) in very large-dimensional spaces are conducted to understand more about the behavior of RAS and to assess the relative importance of the algorithmic building blocks for the final results.

### Frequency-domain alignment of heterogeneous, multidimensional separations data through complex orthogonal Procrustes analysis 
[[arxiv](https://arxiv.org/abs/2502.12810)] [[cool](https://papers.cool/arxiv/2502.12810)] [[pdf](https://arxiv.org/pdf/2502.12810)]
> **Authors**: Michael Sorochan Armstrong
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 12 pages, 1 figure
- **标题**: None
- **领域**: 数值分析,机器学习
- **Abstract**: Multidimensional separations data have the capacity to reveal detailed information about complex biological samples. However, data analysis has been an ongoing challenge in the area since the peaks that represent chemical factors may drift over the course of several analytical runs along the first and second dimension retention times. This makes higher-level analyses of the data difficult, since a 1-1 comparison of samples is seldom possible without sophisticated pre-processing routines. Further complicating the issue is the fact that closely co-eluting components will need to be resolved, typically using some variants of Parallel Factor Analysis (PARAFAC), Multivariate Curve Resolution (MCR), or the recently explored Shift-Invariant Multi-linearity. These algorithms work with a user-specified number of components, and regions of interest that are then summarized as a peak table that is invariant to shift. However, identifying regions of interest across truly heterogeneous data remains an ongoing issue, for automated deployment of these algorithms. This work offers a very simple solution to the alignment problem through a orthogonal Procrustes analysis of the frequency-domain representation of synthetic multidimensional separations data, for peaks that are logarithmically transformed to simulate shift while preserving the underlying topology of the data. Using this very simple method for analysis, two synthetic chromatograms can be compared under close to the worst possible scenarios for alignment.

## 大气和海洋物理(physics.ao-ph:Atmospheric and Oceanic Physics)

### Increasing NWP Thunderstorm Predictability Using Ensemble Data and Machine Learning 
[[arxiv](https://arxiv.org/abs/2502.13316)] [[cool](https://papers.cool/arxiv/2502.13316)] [[pdf](https://arxiv.org/pdf/2502.13316)]
> **Authors**: Kianusch Vahid Yousefnia,Tobias Bölle,Christoph Metzl
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 12 pages, 5 figures, 1 table. This work has been submitted to Weather Forecasting. Copyright in this work may be transferred without further notice
- **标题**: None
- **领域**: 大气和海洋物理,机器学习
- **Abstract**: While numerical weather prediction (NWP) models are essential for forecasting thunderstorms hours in advance, NWP uncertainty, which increases with lead time, limits the predictability of thunderstorm occurrence. This study investigates how ensemble NWP data and machine learning (ML) can enhance the skill of thunderstorm forecasts. Using our recently introduced neural network model, SALAMA 1D, which identifies thunderstorm occurrence in operational forecasts of the convection-permitting ICON-D2-EPS model for Central Europe, we demonstrate that ensemble-averaging significantly improves forecast skill. Notably, an 11-hour ensemble forecast matches the skill level of a 5-hour deterministic forecast. To explain this improvement, we derive an analytic expression linking skill differences to correlations between ensemble members, which aligns with observed performance gains. This expression generalizes to any binary classification model that processes ensemble members individually. Additionally, we show that ML models like SALAMA 1D can identify patterns of thunderstorm occurrence which remain predictable for longer lead times compared to raw NWP output. Our findings quantitatively explain the benefits of ensemble-averaging and encourage the development of ML methods for thunderstorm forecasting and beyond.

### CondensNet: Enabling stable long-term climate simulations via hybrid deep learning models with adaptive physical constraints 
[[arxiv](https://arxiv.org/abs/2502.13185)] [[cool](https://papers.cool/arxiv/2502.13185)] [[pdf](https://arxiv.org/pdf/2502.13185)]
> **Authors**: Xin Wang,Juntao Yang,Jeff Adie,Simon See,Kalli Furtado,Chen Chen,Troy Arcomano,Romit Maulik,Gianmarco Mengaldo
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 大气和海洋物理,人工智能,机器学习
- **Abstract**: Accurate and efficient climate simulations are crucial for understanding Earth's evolving climate. However, current general circulation models (GCMs) face challenges in capturing unresolved physical processes, such as cloud and convection. A common solution is to adopt cloud resolving models, that provide more accurate results than the standard subgrid parametrisation schemes typically used in GCMs. However, cloud resolving models, also referred to as super paramtetrizations, remain computationally prohibitive. Hybrid modeling, which integrates deep learning with equation-based GCMs, offers a promising alternative but often struggles with long-term stability and accuracy issues. In this work, we find that water vapor oversaturation during condensation is a key factor compromising the stability of hybrid models. To address this, we introduce CondensNet, a novel neural network architecture that embeds a self-adaptive physical constraint to correct unphysical condensation processes. CondensNet effectively mitigates water vapor oversaturation, enhancing simulation stability while maintaining accuracy and improving computational efficiency compared to super parameterization schemes. We integrate CondensNet into a GCM to form PCNN-GCM (Physics-Constrained Neural Network GCM), a hybrid deep learning framework designed for long-term stable climate simulations in real-world conditions, including ocean and land. PCNN-GCM represents a significant milestone in hybrid climate modeling, as it shows a novel way to incorporate physical constraints adaptively, paving the way for accurate, lightweight, and stable long-term climate simulations.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Dynamic directed functional connectivity as a neural biomarker for objective motor skill assessment 
[[arxiv](https://arxiv.org/abs/2502.13362)] [[cool](https://papers.cool/arxiv/2502.13362)] [[pdf](https://arxiv.org/pdf/2502.13362)]
> **Authors**: Anil Kamat,Rahul Rahul,Anirban Dutta,Lora Cavuoto,Uwe Kruger,Harry Burke,Matthew Hackett,Jack Norfleet,Steven Schwaitzberg,Suvranu De
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,机器学习
- **Abstract**: Objective motor skill assessment plays a critical role in fields such as surgery, where proficiency is vital for certification and patient safety. Existing assessment methods, however, rely heavily on subjective human judgment, which introduces bias and limits reproducibility. While recent efforts have leveraged kinematic data and neural imaging to provide more objective evaluations, these approaches often overlook the dynamic neural mechanisms that differentiate expert and novice performance. This study proposes a novel method for motor skill assessment based on dynamic directed functional connectivity (dFC) as a neural biomarker. By using electroencephalography (EEG) to capture brain dynamics and employing an attention-based Long Short-Term Memory (LSTM) model for non-linear Granger causality analysis, we compute dFC among key brain regions involved in psychomotor tasks. Coupled with hierarchical task analysis (HTA), our approach enables subtask-level evaluation of motor skills, offering detailed insights into neural coordination that underpins expert proficiency. A convolutional neural network (CNN) is then used to classify skill levels, achieving greater accuracy and specificity than established performance metrics in laparoscopic surgery. This methodology provides a reliable, objective framework for assessing motor skills, contributing to the development of tailored training protocols and enhancing the certification process.

## 定量方法(q-bio.QM:Quantitative Methods)

### Towards Quantum Tensor Decomposition in Biomedical Applications 
[[arxiv](https://arxiv.org/abs/2502.13140)] [[cool](https://papers.cool/arxiv/2502.13140)] [[pdf](https://arxiv.org/pdf/2502.13140)]
> **Authors**: Myson Burch,Jiasen Zhang,Gideon Idumah,Hakan Doga,Richard Lartey,Lamis Yehia,Mingrui Yang,Murat Yildirim,Mihriban Karaayvaz,Omar Shehab,Weihong Guo,Ying Ni,Laxmi Parida,Xiaojuan Li,Aritra Bose
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 31 pages, 7 figures
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Tensor decomposition has emerged as a powerful framework for feature extraction in multi-modal biomedical data. In this review, we present a comprehensive analysis of tensor decomposition methods such as Tucker, CANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse applications across biomedical domains such as imaging, multi-omics, and spatial transcriptomics. To systematically investigate the literature, we applied a topic modeling-based approach that identifies and groups distinct thematic sub-areas in biomedicine where tensor decomposition has been used, thereby revealing key trends and research directions. We evaluated challenges related to the scalability of latent spaces along with obtaining the optimal rank of the tensor, which often hinder the extraction of meaningful features from increasingly large and complex datasets. Additionally, we discuss recent advances in quantum algorithms for tensor decomposition, exploring how quantum computing can be leveraged to address these challenges. Our study includes a preliminary resource estimation analysis for quantum computing platforms and examines the feasibility of implementing quantum-enhanced tensor decomposition methods on near-term quantum devices. Collectively, this review not only synthesizes current applications and challenges of tensor decomposition in biomedical analyses but also outlines promising quantum computing strategies to enhance its impact on deriving actionable insights from complex biomedical data.

### NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation 
[[arxiv](https://arxiv.org/abs/2502.12638)] [[cool](https://papers.cool/arxiv/2502.12638)] [[pdf](https://arxiv.org/pdf/2502.12638)]
> **Authors**: Zhiyuan Liu,Yanchen Luo,Han Huang,Enzhi Zhang,Sihang Li,Junfeng Fang,Yaorui Shi,Xiang Wang,Kenji Kawaguchi,Tat-Seng Chua
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ICLR 2025, 10 pages
- **标题**: None
- **领域**: 定量方法,机器学习,生物分子
- **Abstract**: 3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.

## 量子物理学(quant-ph:Quantum Physics)

### Large Language Models Can Help Mitigate Barren Plateaus 
[[arxiv](https://arxiv.org/abs/2502.13166)] [[cool](https://papers.cool/arxiv/2502.13166)] [[pdf](https://arxiv.org/pdf/2502.13166)]
> **Authors**: Jun Zhuang,Chaowen Guan
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: TL;DR: We propose a newLLM-driven framework designed for mitigating barren plateaus
- **标题**: None
- **领域**: 量子物理学,人工智能,计算语言学,机器学习
- **Abstract**: In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum Neural Networks (QNNs) have emerged as a promising approach for various applications, yet their training is often hindered by barren plateaus (BPs), where gradient variance vanishes exponentially as the model size increases. To address this challenge, we propose a new Large Language Model (LLM)-driven search framework, AdaInit, that iteratively searches for optimal initial parameters of QNNs to maximize gradient variance and therefore mitigate BPs. Unlike conventional one-time initialization methods, AdaInit dynamically refines QNN's initialization using LLMs with adaptive prompting. Theoretical analysis of the Expected Improvement (EI) proves a supremum for the search, ensuring this process can eventually identify the optimal initial parameter of the QNN. Extensive experiments across four public datasets demonstrate that AdaInit significantly enhances QNN's trainability compared to classic initialization methods, validating its effectiveness in mitigating BPs.

### Benchmarking MedMNIST dataset on real quantum hardware 
[[arxiv](https://arxiv.org/abs/2502.13056)] [[cool](https://papers.cool/arxiv/2502.13056)] [[pdf](https://arxiv.org/pdf/2502.13056)]
> **Authors**: Gurinder Singh,Hongni Jin,Kenneth M. Merz Jr
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,机器学习
- **Abstract**: Quantum machine learning (QML) has emerged as a promising domain to leverage the computational capabilities of quantum systems to solve complex classification tasks. In this work, we present first comprehensive QML study by benchmarking the MedMNIST-a diverse collection of medical imaging datasets on a 127-qubit real IBM quantum hardware, to evaluate the feasibility and performance of quantum models (without any classical neural networks) in practical applications. This study explore recent advancements in quantum computing such as device-aware quantum circuits, error suppression and mitigation for medical image classification. Our methodology comprised of three stages: preprocessing, generation of noise-resilient and hardware-efficient quantum circuits, optimizing/training of quantum circuits on classical hardware, and inference on real IBM quantum hardware. Firstly, we process all input images in the preprocessing stage to reduce the spatial dimension due to the quantum hardware limitations. We generate hardware-efficient quantum circuits using backend properties expressible to learn complex patterns for medical image classification. After classical optimization of QML models, we perform the inference on real quantum hardware. We also incorporates advanced error suppression and mitigation techniques in our QML workflow including dynamical decoupling (DD), gate twirling, and matrix-free measurement mitigation (M3) to mitigate the effects of noise and improve classification performance. The experimental results showcase the potential of quantum computing for medical imaging and establishes a benchmark for future advancements in QML applied to healthcare.

## 应用领域(stat.AP:Applications)

### Performance Evaluation of Large Language Models in Statistical Programming 
[[arxiv](https://arxiv.org/abs/2502.13117)] [[cool](https://papers.cool/arxiv/2502.13117)] [[pdf](https://arxiv.org/pdf/2502.13117)]
> **Authors**: Xinyi Song,Kexin Xie,Lina Lee,Ruizhe Chen,Jared M. Clark,Hao He,Haoran He,Jie Min,Xinlei Zhang,Simin Zheng,Zhiyang Zhang,Xinwei Deng,Yili Hong
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 27 pages, 8 figures
- **标题**: None
- **领域**: 应用领域,人工智能
- **Abstract**: The programming capabilities of large language models (LLMs) have revolutionized automatic code generation and opened new avenues for automatic statistical analysis. However, the validity and quality of these generated codes need to be systematically evaluated before they can be widely adopted. Despite their growing prominence, a comprehensive evaluation of statistical code generated by LLMs remains scarce in the literature. In this paper, we assess the performance of LLMs, including two versions of ChatGPT and one version of Llama, in the domain of SAS programming for statistical analysis. Our study utilizes a set of statistical analysis tasks encompassing diverse statistical topics and datasets. Each task includes a problem description, dataset information, and human-verified SAS code. We conduct a comprehensive assessment of the quality of SAS code generated by LLMs through human expert evaluation based on correctness, effectiveness, readability, executability, and the accuracy of output results. The analysis of rating scores reveals that while LLMs demonstrate usefulness in generating syntactically correct code, they struggle with tasks requiring deep domain understanding and may produce redundant or incorrect results. This study offers valuable insights into the capabilities and limitations of LLMs in statistical programming, providing guidance for future advancements in AI-assisted coding systems for statistical analysis.

## 机器学习(stat.ML:Machine Learning)

### Task Shift: From Classification to Regression in Overparameterized Linear Models 
[[arxiv](https://arxiv.org/abs/2502.13285)] [[cool](https://papers.cool/arxiv/2502.13285)] [[pdf](https://arxiv.org/pdf/2502.13285)]
> **Authors**: Tyler LaBonte,Kuo-Wei Lai,Vidya Muthukumar
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Modern machine learning methods have recently demonstrated remarkable capability to generalize under task shift, where latent knowledge is transferred to a different, often more difficult, task under a similar data distribution. We investigate this phenomenon in an overparameterized linear regression setting where the task shifts from classification during training to regression during evaluation. In the zero-shot case, wherein no regression data is available, we prove that task shift is impossible in both sparse signal and random signal models for any Gaussian covariate distribution. In the few-shot case, wherein limited regression data is available, we propose a simple postprocessing algorithm which asymptotically recovers the ground-truth predictor. Our analysis leverages a fine-grained characterization of individual parameters arising from minimum-norm interpolation which may be of independent interest. Our results show that while minimum-norm interpolators for classification cannot transfer to regression a priori, they experience surprisingly structured attenuation which enables successful task shift with limited additional data.

### Model selection for behavioral learning data and applications to contextual bandits 
[[arxiv](https://arxiv.org/abs/2502.13186)] [[cool](https://papers.cool/arxiv/2502.13186)] [[pdf](https://arxiv.org/pdf/2502.13186)]
> **Authors**: Julien Aubert,Louis Köhler,Luc Lehéricy,Giulia Mezzadri,Patricia Reynaud-Bouret
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: ef:28th International Conference on Artificial Intelligence and Statistics (AISTATS), May 2025, Mai Khao, Thailand
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Learning for animals or humans is the process that leads to behaviors better adapted to the environment. This process highly depends on the individual that learns and is usually observed only through the individual's actions. This article presents ways to use this individual behavioral data to find the model that best explains how the individual learns. We propose two model selection methods: a general hold-out procedure and an AIC-type criterion, both adapted to non-stationary dependent data. We provide theoretical error bounds for these methods that are close to those of the standard i.i.d. case. To compare these approaches, we apply them to contextual bandit models and illustrate their use on both synthetic and experimental learning data in a human categorization task.

### A Neural Difference-of-Entropies Estimator for Mutual Information 
[[arxiv](https://arxiv.org/abs/2502.13085)] [[cool](https://papers.cool/arxiv/2502.13085)] [[pdf](https://arxiv.org/pdf/2502.13085)]
> **Authors**: Haoran Ni,Martin Lotz
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 23 pages, 17 figures
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: Estimating Mutual Information (MI), a key measure of dependence of random quantities without specific modelling assumptions, is a challenging problem in high dimensions. We propose a novel mutual information estimator based on parametrizing conditional densities using normalizing flows, a deep generative model that has gained popularity in recent years. This estimator leverages a block autoregressive structure to achieve improved bias-variance trade-offs on standard benchmark tasks.

### Likelihood-Ratio Regularized Quantile Regression: Adapting Conformal Prediction to High-Dimensional Covariate Shifts 
[[arxiv](https://arxiv.org/abs/2502.13030)] [[cool](https://papers.cool/arxiv/2502.13030)] [[pdf](https://arxiv.org/pdf/2502.13030)]
> **Authors**: Sunay Joshi,Shayan Kiyani,George Pappas,Edgar Dobriban,Hamed Hassani
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, and an image classification task from the WILDS repository.

### Asymptotic Optimism of Random-Design Linear and Kernel Regression Models 
[[arxiv](https://arxiv.org/abs/2502.12999)] [[cool](https://papers.cool/arxiv/2502.12999)] [[pdf](https://arxiv.org/pdf/2502.12999)]
> **Authors**: Hengrui Luo,Yunzhang Zhu
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 56 pages;
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We derived the closed-form asymptotic optimism of linear regression models under random designs, and generalizes it to kernel ridge regression. Using scaled asymptotic optimism as a generic predictive model complexity measure, we studied the fundamental different behaviors of linear regression model, tangent kernel (NTK) regression model and three-layer fully connected neural networks (NN). Our contribution is two-fold: we provided theoretical ground for using scaled optimism as a model predictive complexity measure; and we show empirically that NN with ReLUs behaves differently from kernel models under this measure. With resampling techniques, we can also compute the optimism for regression models with real data.

### Statistically Significant $k$NNAD by Selective Inference 
[[arxiv](https://arxiv.org/abs/2502.12978)] [[cool](https://papers.cool/arxiv/2502.12978)] [[pdf](https://arxiv.org/pdf/2502.12978)]
> **Authors**: Mizuki Niihori,Teruyuki Katsuoka,Tomohiro Shiraishi,Shuichi Nishino,Ichiro Takeuchi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 40 pages, 11 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In this paper, we investigate the problem of unsupervised anomaly detection using the k-Nearest Neighbor method. The k-Nearest Neighbor Anomaly Detection (kNNAD) is a simple yet effective approach for identifying anomalies across various domains and fields. A critical challenge in anomaly detection, including kNNAD, is appropriately quantifying the reliability of detected anomalies. To address this, we formulate kNNAD as a statistical hypothesis test and quantify the probability of false detection using $p$-values. The main technical challenge lies in performing both anomaly detection and statistical testing on the same data, which hinders correct $p$-value calculation within the conventional statistical testing framework. To resolve this issue, we introduce a statistical hypothesis testing framework called Selective Inference (SI) and propose a method named Statistically Significant NNAD (Stat-kNNAD). By leveraging SI, the Stat-kNNAD method ensures that detected anomalies are statistically significant with theoretical guarantees. The proposed Stat-kNNAD method is applicable to anomaly detection in both the original feature space and latent feature spaces derived from deep learning models. Through numerical experiments on synthetic data and applications to industrial product anomaly detection, we demonstrate the validity and effectiveness of the Stat-kNNAD method.

### Time-series attribution maps with regularized contrastive learning 
[[arxiv](https://arxiv.org/abs/2502.12977)] [[cool](https://papers.cool/arxiv/2502.12977)] [[pdf](https://arxiv.org/pdf/2502.12977)]
> **Authors**: Steffen Schneider,Rodrigo González Laiz,Anastasiia Filippova,Markus Frey,Mackenzie Weygandt Mathis
> **First submission**: 2025-02-17
> **First announcement**: 2025-02-19
> **comment**: Accepted at The 28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025). Code is available at https://github.com/AdaptiveMotorControlLab/CEBRA
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习,神经元和认知
- **Abstract**: Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized contrastive learning algorithm trained on time-series data plus a new attribution method called Inverted Neuron Gradient (collectively named xCEBRA). We show theoretically that xCEBRA has favorable properties for identifying the Jacobian matrix of the data generating process. Empirically, we demonstrate robust approximation of zero vs. non-zero entries in the ground-truth attribution map on synthetic datasets, and significant improvements across previous attribution methods based on feature ablation, Shapley values, and other gradient-based methods. Our work constitutes a first example of identifiable inference of time-series attribution maps and opens avenues to a better understanding of time-series data, such as for neural dynamics and decision-processes within neural networks.

### Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.12793)] [[cool](https://papers.cool/arxiv/2502.12793)] [[pdf](https://arxiv.org/pdf/2502.12793)]
> **Authors**: Eduardo Fernandes Montesuma,Adel El Habazi,Fred Ngole Mboula
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 15 pages, 9 figures, 1 table, under review
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Detecting anomalies in datasets is a longstanding problem in machine learning. In this context, anomalies are defined as a sample that significantly deviates from the remaining data. Meanwhile, optimal transport (OT) is a field of mathematics concerned with the transportation, between two probability measures, at least effort. In classical OT, the optimal transportation strategy of a measure to itself is the identity. In this paper, we tackle anomaly detection by forcing samples to displace its mass, while keeping the least effort objective. We call this new transportation problem Mass Repulsing Optimal Transport (MROT). Naturally, samples lying in low density regions of space will be forced to displace mass very far, incurring a higher transportation cost. We use these concepts to design a new anomaly score. Through a series of experiments in existing benchmarks, and fault detection problems, we show that our algorithm improves over existing methods.

### Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo 
[[arxiv](https://arxiv.org/abs/2502.12786)] [[cool](https://papers.cool/arxiv/2502.12786)] [[pdf](https://arxiv.org/pdf/2502.12786)]
> **Authors**: James Thornton,Louis Bethune,Ruixiang Zhang,Arwen Bradley,Preetum Nakkiran,Shuangfei Zhai
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: Initial submission to openreview on October 3, 2024 (https://openreview.net/forum?id=6GyX0YRw8P); accepted to AISTATS 2025
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Diffusion models may be formulated as a time-indexed sequence of energy-based models, where the score corresponds to the negative gradient of an energy function. As opposed to learning the score directly, an energy parameterization is attractive as the energy itself can be used to control generation via Monte Carlo samplers. Architectural constraints and training instability in energy parameterized models have so far yielded inferior performance compared to directly approximating the score or denoiser. We address these deficiencies by introducing a novel training regime for the energy function through distillation of pre-trained diffusion models, resembling a Helmholtz decomposition of the score vector field. We further showcase the synergies between energy and score by casting the diffusion sampling procedure as a Feynman Kac model where sampling is controlled using potentials from the learnt energy functions. The Feynman Kac model formalism enables composition and low temperature sampling through sequential Monte Carlo.

### Green LIME: Improving AI Explainability through Design of Experiments 
[[arxiv](https://arxiv.org/abs/2502.12753)] [[cool](https://papers.cool/arxiv/2502.12753)] [[pdf](https://arxiv.org/pdf/2502.12753)]
> **Authors**: Alexandra Stadler,Werner G. Müller,Radoslav Harman
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: In artificial intelligence (AI), the complexity of many models and processes often surpasses human interpretability, making it challenging to understand why a specific prediction is made. This lack of transparency is particularly problematic in critical fields like healthcare, where trust in a model's predictions is paramount. As a result, the explainability of machine learning (ML) and other complex models has become a key area of focus. Efforts to improve model interpretability often involve experimenting with AI systems and approximating their behavior through simpler mechanisms. However, these procedures can be resource-intensive. Optimal design of experiments, which seeks to maximize the information obtained from a limited number of observations, offers promising methods for improving the efficiency of these explainability techniques. To demonstrate this potential, we explore Local Interpretable Model-agnostic Explanations (LIME), a widely used method introduced by Ribeiro, Singh, and Guestrin, 2016. LIME provides explanations by generating new data points near the instance of interest and passing them through the model. While effective, this process can be computationally expensive, especially when predictions are costly or require many samples. LIME is highly versatile and can be applied to a wide range of models and datasets. In this work, we focus on models involving tabular data, regression tasks, and linear models as interpretable local approximations. By utilizing optimal design of experiments' techniques, we reduce the number of function evaluations of the complex model, thereby reducing the computational effort of LIME by a significant amount. We consider this modified version of LIME to be energy-efficient or "green".

### Federated Variational Inference for Bayesian Mixture Models 
[[arxiv](https://arxiv.org/abs/2502.12684)] [[cool](https://papers.cool/arxiv/2502.12684)] [[pdf](https://arxiv.org/pdf/2502.12684)]
> **Authors**: Jackie Rao,Francesca L. Crowe,Tom Marshall,Sylvia Richardson,Paul D. W. Kirk
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,方法论
- **Abstract**: We present a federated learning approach for Bayesian model-based clustering of large-scale binary and categorical datasets. We introduce a principled 'divide and conquer' inference procedure using variational inference with local merge and delete moves within batches of the data in parallel, followed by 'global' merge moves across batches to find global clustering structures. We show that these merge moves require only summaries of the data in each batch, enabling federated learning across local nodes without requiring the full dataset to be shared. Empirical results on simulated and benchmark datasets demonstrate that our method performs well in comparison to existing clustering algorithms. We validate the practical utility of the method by applying it to large scale electronic health record (EHR) data.

### Generalized Kernel Inducing Points by Duality Gap for Dataset Distillation 
[[arxiv](https://arxiv.org/abs/2502.12607)] [[cool](https://papers.cool/arxiv/2502.12607)] [[pdf](https://arxiv.org/pdf/2502.12607)]
> **Authors**: Tatsuya Aoyama,Hanting Yang,Hiroyuki Hanada,Satoshi Akahane,Tomonari Tanaka,Yoshito Okura,Yu Inatsu,Noriaki Hashimoto,Taro Murayama,Hanju Lee,Shinya Kojima,Ichiro Takeuchi
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose Duality Gap KIP (DGKIP), an extension of the Kernel Inducing Points (KIP) method for dataset distillation. While existing dataset distillation methods often rely on bi-level optimization, DGKIP eliminates the need for such optimization by leveraging duality theory in convex programming. The KIP method has been introduced as a way to avoid bi-level optimization; however, it is limited to the squared loss and does not support other loss functions (e.g., cross-entropy or hinge loss) that are more suitable for classification tasks. DGKIP addresses this limitation by exploiting an upper bound on parameter changes after dataset distillation using the duality gap, enabling its application to a wider range of loss functions. We also characterize theoretical properties of DGKIP by providing upper bounds on the test error and prediction consistency after dataset distillation. Experimental results on standard benchmarks such as MNIST and CIFAR-10 demonstrate that DGKIP retains the efficiency of KIP while offering broader applicability and robust performance.

### The Majority Vote Paradigm Shift: When Popular Meets Optimal 
[[arxiv](https://arxiv.org/abs/2502.12581)] [[cool](https://papers.cool/arxiv/2502.12581)] [[pdf](https://arxiv.org/pdf/2502.12581)]
> **Authors**: Antonio Purificato,Maria Sofia Bucarelli,Anil Kumar Nelakanti,Andrea Bacciu,Fabrizio Silvestri,Amin Mantrach
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 33 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Reliably labelling data typically requires annotations from multiple human workers. However, humans are far from being perfect. Hence, it is a common practice to aggregate labels gathered from multiple annotators to make a more confident estimate of the true label. Among many aggregation methods, the simple and well known Majority Vote (MV) selects the class label polling the highest number of votes. However, despite its importance, the optimality of MV's label aggregation has not been extensively studied. We address this gap in our work by characterising the conditions under which MV achieves the theoretically optimal lower bound on label estimation error. Our results capture the tolerable limits on annotation noise under which MV can optimally recover labels for a given class distribution. This certificate of optimality provides a more principled approach to model selection for label aggregation as an alternative to otherwise inefficient practices that sometimes include higher experts, gold labels, etc., that are all marred by the same human uncertainty despite huge time and monetary costs. Experiments on both synthetic and real world data corroborate our theoretical findings.

## 其他统计数据(stat.OT:Other Statistics)

### A Simplified and Numerically Stable Approach to the BG/NBD Churn Prediction model 
[[arxiv](https://arxiv.org/abs/2502.12912)] [[cool](https://papers.cool/arxiv/2502.12912)] [[pdf](https://arxiv.org/pdf/2502.12912)]
> **Authors**: Dylan Zammit,Christopher Zerafa
> **First submission**: 2025-02-18
> **First announcement**: 2025-02-19
> **comment**: 4 pages, numerically stable BG/NBD
- **标题**: None
- **领域**: 其他统计数据,机器学习,统计理论
- **Abstract**: This study extends the BG/NBD churn probability model, addressing its limitations in industries where customer behaviour is often influenced by seasonal events and possibly high purchase counts. We propose a modified definition of churn, considering a customer to have churned if they make no purchases within M days. Our contribution is twofold: First, we simplify the general equation for the specific case of zero purchases within M days. Second, we derive an alternative expression using numerical techniques to mitigate numerical overflow or underflow issues. This approach provides a more practical and robust method for predicting customer churn in industries with irregular purchase patterns.

## 其他论文

- [Fast Kd-trees for the Kullback--Leibler Divergence and other Decomposable Bregman Divergences](https://arxiv.org/abs/2502.13425)
  - **标题**: None
  - **Filtered Reason**: none of cs.CG in whitelist
- [Beeping Deterministic CONGEST Algorithms in Graphs](https://arxiv.org/abs/2502.13424)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [AutoTEE: Automated Migration and Protection of Programs in Trusted Execution Environments](https://arxiv.org/abs/2502.13379)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.SE in whitelist
- [Graph-Based Algorithms for Diverse Similarity Search](https://arxiv.org/abs/2502.13336)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [How to Sell a Service with Uncertain Outcomes](https://arxiv.org/abs/2502.13334)
  - **标题**: None
  - **Filtered Reason**: none of cs.GT in whitelist
- [Making the Write Connections: Linking Writing Support Tools with Writer's Needs](https://arxiv.org/abs/2502.13320)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Debiasing Functions of Private Statistics in Postprocessing](https://arxiv.org/abs/2502.13314)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,stat.ME in whitelist
- [The "Who'', "What'', and "How'' of Responsible AI Governance: A Systematic Review and Meta-Analysis of (Actor, Stage)-Specific Tools](https://arxiv.org/abs/2502.13294)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [Beyond Training: Social Dynamics of AI Adoption in Industry](https://arxiv.org/abs/2502.13281)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [PCB Renewal: Iterative Reuse of PCB Substrates for Sustainable Electronic Making](https://arxiv.org/abs/2502.13255)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.RO,cs.CY in whitelist
- [Expanding the Classical V-Model for the Development of Complex Systems Incorporating AI](https://arxiv.org/abs/2502.13184)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [HARP: A Taxonomy for Heterogeneous and Hierarchical Processors for Mixed-reuse Workloads](https://arxiv.org/abs/2502.13113)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.AR in whitelist
- [AI and the Transformation of Accountability and Discretion in Urban Governance](https://arxiv.org/abs/2502.13101)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Investigating Issues that Lead to Code Technical Debt in Machine Learning Systems](https://arxiv.org/abs/2502.13011)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Learning More Effective Representations for Dense Retrieval through Deliberate Thinking Before Search](https://arxiv.org/abs/2502.12974)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Generative AI and Information Asymmetry: Impacts on Adverse Selection and Moral Hazard](https://arxiv.org/abs/2502.12969)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [AI-Enabled Rent-Seeking: How Generative AI Alters Market Transparency and Efficiency](https://arxiv.org/abs/2502.12956)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Query Rewriting via LLMs](https://arxiv.org/abs/2502.12918)
  - **标题**: None
  - **Filtered Reason**: none of cs.DB in whitelist
- [Network visualisations related to special functions based on the Scopus data since 1940](https://arxiv.org/abs/2502.12891)
  - **标题**: None
  - **Filtered Reason**: none of cs.DL,math.HO in whitelist
- [A Survey on DRL based UAV Communications and Networking: DRL Fundamentals, Applications and Implementations](https://arxiv.org/abs/2502.12875)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [RobotIQ: Empowering Mobile Robots with Human-Level Planning for Real-World Execution](https://arxiv.org/abs/2502.12862)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [InstructRobot: A Model-Free Framework for Mapping Natural Language Instructions into Robot Motion](https://arxiv.org/abs/2502.12861)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Finding Maximum Weight 2-Packing Sets on Arbitrary Graphs](https://arxiv.org/abs/2502.12856)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [Generalized De Bruijn Words, Invertible Necklaces, and the Burrows-Wheeler Transform](https://arxiv.org/abs/2502.12844)
  - **标题**: None
  - **Filtered Reason**: none of cs.DM,math.CO,cs.FL,cs.DS in whitelist
- [An Attention-Assisted AI Model for Real-Time Underwater Sound Speed Estimation Leveraging Remote Sensing Sea Surface Temperature Data](https://arxiv.org/abs/2502.12817)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.SP in whitelist
- [SparkAttention: High-Performance Multi-Head Attention for Large Models on Volta GPU Architecture](https://arxiv.org/abs/2502.12784)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Revisiting Token Sliding on Chordal Graphs](https://arxiv.org/abs/2502.12749)
  - **标题**: None
  - **Filtered Reason**: none of cs.DS in whitelist
- [Surrogate Modeling for Scalable Evaluation of Distributed Computing Systems for HEP Applications](https://arxiv.org/abs/2502.12741)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.PF,hep-ex in whitelist
- [LiMo-Calib: On-Site Fast LiDAR-Motor Calibration for Quadruped Robot-Based Panoramic 3D Sensing System](https://arxiv.org/abs/2502.12655)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [TechSinger: Technique Controllable Multilingual Singing Voice Synthesis via Flow Matching](https://arxiv.org/abs/2502.12572)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD in whitelist
- [Distributed On-Device LLM Inference With Over-the-Air Computation](https://arxiv.org/abs/2502.12559)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
