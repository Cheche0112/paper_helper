> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-13

共有267篇相关领域论文, 另有26篇其他

## 太阳和恒星天体物理学(astro-ph.SR:Solar and Stellar Astrophysics)

### A Machine Learning-Ready Data Processing Tool for Near Real-Time Forecasting 
[[arxiv](https://arxiv.org/abs/2502.08555)] [[cool](https://papers.cool/arxiv/2502.08555)] [[pdf](https://arxiv.org/pdf/2502.08555)]
> **Authors**: Maher A Dayeh,Michael J Starkey,Subhamoy Chatterjee,Heather Elliott,Samuel Hart,Kimberly Moreland
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: ef:IAC-24,D5,IP,12,x89662, 2024
- **标题**: None
- **领域**: 太阳和恒星天体物理学,天体物理学仪器和方法,机器学习
- **Abstract**: Space weather forecasting is critical for mitigating radiation risks in space exploration and protecting Earth-based technologies from geomagnetic disturbances. This paper presents the development of a Machine Learning (ML)- ready data processing tool for Near Real-Time (NRT) space weather forecasting. By merging data from diverse NRT sources such as solar imagery, magnetic field measurements, and energetic particle fluxes, the tool addresses key gaps in current space weather prediction capabilities. The tool processes and structures the data for machine learning models, focusing on time-series forecasting and event detection for extreme solar events. It provides users with a framework to download, process, and label data for ML applications, streamlining the workflow for improved NRT space weather forecasting and scientific research.

## 人工智能(cs.AI:Artificial Intelligence)

### On the Promise for Assurance of Differentiable Neurosymbolic Reasoning Paradigms 
[[arxiv](https://arxiv.org/abs/2502.08932)] [[cool](https://papers.cool/arxiv/2502.08932)] [[pdf](https://arxiv.org/pdf/2502.08932)]
> **Authors**: Luke E. Richards,Jessie Yaros,Jasen Babcock,Coung Ly,Robin Cosbey,Timothy Doster,Cynthia Matuszek
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别
- **Abstract**: To create usable and deployable Artificial Intelligence (AI) systems, there requires a level of assurance in performance under many different conditions. Many times, deployed machine learning systems will require more classic logic and reasoning performed through neurosymbolic programs jointly with artificial neural network sensing. While many prior works have examined the assurance of a single component of the system solely with either the neural network alone or entire enterprise systems, very few works have examined the assurance of integrated neurosymbolic systems. Within this work, we assess the assurance of end-to-end fully differentiable neurosymbolic systems that are an emerging method to create data-efficient and more interpretable models. We perform this investigation using Scallop, an end-to-end neurosymbolic library, across classification and reasoning tasks in both the image and audio domains. We assess assurance across adversarial robustness, calibration, user performance parity, and interpretability of solutions for catching misaligned solutions. We find end-to-end neurosymbolic methods present unique opportunities for assurance beyond their data efficiency through our empirical results but not across the board. We find that this class of neurosymbolic models has higher assurance in cases where arithmetic operations are defined and where there is high dimensionality to the input space, where fully neural counterparts struggle to learn robust reasoning operations. We identify the relationship between neurosymbolic models' interpretability to catch shortcuts that later result in increased adversarial vulnerability despite performance parity. Finally, we find that the promise of data efficiency is typically only in the case of class imbalanced reasoning problems.

### Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models 
[[arxiv](https://arxiv.org/abs/2502.08922)] [[cool](https://papers.cool/arxiv/2502.08922)] [[pdf](https://arxiv.org/pdf/2502.08922)]
> **Authors**: Xin Zhou,Yiwen Guo,Ruotian Ma,Tao Gui,Qi Zhang,Xuanjing Huang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Aligning Large Language Models (LLMs) with human preferences is crucial for their deployment in real-world applications. Recent advancements in Self-Rewarding Language Models suggest that an LLM can use its internal reward models (such as LLM-as-a-Judge) \cite{yuanself} to generate preference data, improving alignment performance without costly human annotation. However, we find that different internal reward models within the same LLM often generate inconsistent preferences. This inconsistency raises concerns about the reliability of self-generated preference data, hinders overall alignment performance, and highlights the need for further research to ensure reliable and coherent alignment with human preferences. To address this limitation, we propose Self-Consistent Internal Rewards (SCIR), a novel framework designed to enhance consistency among internal reward models during training. In each training step, we collect preference predictions from multiple pre-defined internal reward models and enforce consistency and confidence through an inconsistency penalty mechanism, thereby improving the reliability of these internal reward models. We selectively use data with consistent predictions for preference optimization, ensuring the quality of the preference data. By employing self-consistent internal rewards, our method significantly improves the alignment performance and reward modeling capability of LLMs, outperforming baseline methods by a notable margin.

### Reinforced Large Language Model is a formal theorem prover 
[[arxiv](https://arxiv.org/abs/2502.08908)] [[cool](https://papers.cool/arxiv/2502.08908)] [[pdf](https://arxiv.org/pdf/2502.08908)]
> **Authors**: Zhiling Luo
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: To take advantage of Large Language Model in theorem formalization and proof, we propose a reinforcement learning framework to iteratively optimize the pretrained LLM by rolling out next tactics and comparing them with the expected ones. The experiment results show that it helps to achieve a higher accuracy compared with directly fine-tuned LLM.

### MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via Event-Driven Text-Code Cyclic Training 
[[arxiv](https://arxiv.org/abs/2502.08904)] [[cool](https://papers.cool/arxiv/2502.08904)] [[pdf](https://arxiv.org/pdf/2502.08904)]
> **Authors**: Xinxin You,Xien Liu,Qixin Sun,Huan Zhang,Kaiyin Zhou,Shaohui Liu,GuoPing Hu,ShiJin Wang,Si Liu,Ji Wu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Recent methodologies utilizing synthetic datasets have aimed to address inconsistent hallucinations in large language models (LLMs); however,these approaches are primarily tailored to specific tasks, limiting their generalizability. Inspired by the strong performance of code-trained models in logic-intensive domains, we propose a novel framework that leverages event-based text to generate corresponding code and employs cyclic training to transfer the logical consistency of code to natural language effectively. Our method significantly reduces inconsistent hallucinations across three leading LLMs and two categories of natural language tasks while maintaining overall performance. This framework effectively alleviates hallucinations without necessitating adaptation to downstream tasks, demonstrating generality and providing new perspectives to tackle the challenge of inconsistent hallucinations.

### Data Sensor Fusion In Digital Twin Technology For Enhanced Capabilities In A Home Environment 
[[arxiv](https://arxiv.org/abs/2502.08874)] [[cool](https://papers.cool/arxiv/2502.08874)] [[pdf](https://arxiv.org/pdf/2502.08874)]
> **Authors**: Benjamin Momoh,Salisu Yahaya
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,信号处理
- **Abstract**: This paper investigates the integration of data sensor fusion in digital twin technology to bolster home environment capabilities, particularly in the context of challenges brought on by the coronavirus pandemic and its economic effects. The study underscores the crucial role of digital transformation in not just adapting to, but also mitigating disruptions during the fourth industrial revolution. Using the Wit Motion sensor, data was collected for activities such as walking, working, sitting, and lying, with sensors measuring accelerometers, gyroscopes, and magnetometers. The research integrates Cyber-physical systems, IoT, AI, and robotics to fortify digital twin capabilities. The paper compares sensor fusion methods, including feature-level fusion, decision-level fusion, and Kalman filter fusion, alongside machine learning models like SVM, GBoost, and Random Forest to assess model effectiveness. Results show that sensor fusion significantly improves the accuracy and reliability of these models, as it compensates for individual sensor weaknesses, particularly with magnetometers. Despite higher accuracy in ideal conditions, integrating data from multiple sensors ensures more consistent and reliable results in real-world settings, thereby establishing a robust system that can be confidently applied in practical scenarios.

### Off-Switching Not Guaranteed 
[[arxiv](https://arxiv.org/abs/2502.08864)] [[cool](https://papers.cool/arxiv/2502.08864)] [[pdf](https://arxiv.org/pdf/2502.08864)]
> **Authors**: Sven Neth
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Forthcoming in Philosophical Studies
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of Human-AI cooperation in which AI agents always defer to humans because they are uncertain about our preferences. I explain two reasons why AI agents might not defer. First, AI agents might not value learning. Second, even if AI agents value learning, they might not be certain to learn our actual preferences.

### EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges 
[[arxiv](https://arxiv.org/abs/2502.08859)] [[cool](https://papers.cool/arxiv/2502.08859)] [[pdf](https://arxiv.org/pdf/2502.08859)]
> **Authors**: Clinton J. Wang,Dean Lee,Cristina Menghini,Johannes Mols,Jack Doughty,Adam Khoja,Jayson Lynch,Sean Hendryx,Summer Yue,Dan Hendrycks
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.

### Estimating Probabilities of Causation with Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.08858)] [[cool](https://papers.cool/arxiv/2502.08858)] [[pdf](https://arxiv.org/pdf/2502.08858)]
> **Authors**: Shuai Wang,Ang Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages + 2 pages reference + 3 pages supplementary material, 5 figures, submitted to UAI 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Probabilities of causation play a crucial role in modern decision-making. This paper addresses the challenge of predicting probabilities of causation for subpopulations with insufficient data using machine learning models. Tian and Pearl first defined and derived tight bounds for three fundamental probabilities of causation: the probability of necessity and sufficiency (PNS), the probability of sufficiency (PS), and the probability of necessity (PN). However, estimating these probabilities requires both experimental and observational distributions specific to each subpopulation, which are often unavailable or impractical to obtain with limited population-level data. We assume that the probabilities of causation for each subpopulation are determined by its characteristics. To estimate these probabilities for subpopulations with insufficient data, we propose using machine learning models that draw insights from subpopulations with sufficient data. Our evaluation of multiple machine learning models indicates that, given sufficient population-level data and an appropriate choice of machine learning model and activation function, PNS can be effectively predicted. Through simulation studies, we show that our multilayer perceptron (MLP) model with the Mish activation function achieves a mean absolute error (MAE) of approximately 0.02 in predicting PNS for 32,768 subpopulations using data from around 2,000 subpopulations.

### Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model 
[[arxiv](https://arxiv.org/abs/2502.08820)] [[cool](https://papers.cool/arxiv/2502.08820)] [[pdf](https://arxiv.org/pdf/2502.08820)]
> **Authors**: Emre Can Acikgoz,Jeremiah Greer,Akul Datta,Ze Yang,William Zeng,Oussama Elachqar,Emmanouil Koukoumidis,Dilek Hakkani-Tür,Gokhan Tur
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学
- **Abstract**: Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.

### Contextual bandits with entropy-based human feedback 
[[arxiv](https://arxiv.org/abs/2502.08759)] [[cool](https://papers.cool/arxiv/2502.08759)] [[pdf](https://arxiv.org/pdf/2502.08759)]
> **Authors**: Raihan Seraj,Lili Meng,Tristan Sylvain
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In recent years, preference-based human feedback mechanisms have become essential for enhancing model performance across diverse applications, including conversational AI systems such as ChatGPT. However, existing approaches often neglect critical aspects, such as model uncertainty and the variability in feedback quality. To address these challenges, we introduce an entropy-based human feedback framework for contextual bandits, which dynamically balances exploration and exploitation by soliciting expert feedback only when model entropy exceeds a predefined threshold. Our method is model-agnostic and can be seamlessly integrated with any contextual bandit agent employing stochastic policies. Through comprehensive experiments, we show that our approach achieves significant performance improvements while requiring minimal human feedback, even under conditions of suboptimal feedback quality. This work not only presents a novel strategy for feedback solicitation but also highlights the robustness and efficacy of incorporating human guidance into machine learning systems. Our code is publicly available: https://github.com/BorealisAI/CBHF

### From PowerPoint UI Sketches to Web-Based Applications: Pattern-Driven Code Generation for GIS Dashboard Development Using Knowledge-Augmented LLMs, Context-Aware Visual Prompting, and the React Framework 
[[arxiv](https://arxiv.org/abs/2502.08756)] [[cool](https://papers.cool/arxiv/2502.08756)] [[pdf](https://arxiv.org/pdf/2502.08756)]
> **Authors**: Haowen Xu,Xiao-Ying Yu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,软件工程
- **Abstract**: Developing web-based GIS applications, commonly known as CyberGIS dashboards, for querying and visualizing GIS data in environmental research often demands repetitive and resource-intensive efforts. While Generative AI offers automation potential for code generation, it struggles with complex scientific applications due to challenges in integrating domain knowledge, software engineering principles, and UI design best practices. This paper introduces a knowledge-augmented code generation framework that retrieves software engineering best practices, domain expertise, and advanced technology stacks from a specialized knowledge base to enhance Generative Pre-trained Transformers (GPT) for front-end development. The framework automates the creation of GIS-based web applications (e.g., dashboards, interfaces) from user-defined UI wireframes sketched in tools like PowerPoint or Adobe Illustrator. A novel Context-Aware Visual Prompting method, implemented in Python, extracts layouts and interface features from these wireframes to guide code generation. Our approach leverages Large Language Models (LLMs) to generate front-end code by integrating structured reasoning, software engineering principles, and domain knowledge, drawing inspiration from Chain-of-Thought (CoT) prompting and Retrieval-Augmented Generation (RAG). A case study demonstrates the framework's capability to generate a modular, maintainable web platform hosting multiple dashboards for visualizing environmental and energy data (e.g., time-series, shapefiles, rasters) from user-sketched wireframes. By employing a knowledge-driven approach, the framework produces scalable, industry-standard front-end code using design patterns such as Model-View-ViewModel (MVVM) and frameworks like React. This significantly reduces manual effort in design and coding, pioneering an automated and efficient method for developing smart city software.

### High-Throughput SAT Sampling 
[[arxiv](https://arxiv.org/abs/2502.08673)] [[cool](https://papers.cool/arxiv/2502.08673)] [[pdf](https://arxiv.org/pdf/2502.08673)]
> **Authors**: Arash Ardakani,Minwoo Kang,Kevin He,Qijing Huang,John Wawrzynek
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: 7 pages
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: In this work, we present a novel technique for GPU-accelerated Boolean satisfiability (SAT) sampling. Unlike conventional sampling algorithms that directly operate on conjunctive normal form (CNF), our method transforms the logical constraints of SAT problems by factoring their CNF representations into simplified multi-level, multi-output Boolean functions. It then leverages gradient-based optimization to guide the search for a diverse set of valid solutions. Our method operates directly on the circuit structure of refactored SAT instances, reinterpreting the SAT problem as a supervised multi-output regression task. This differentiable technique enables independent bit-wise operations on each tensor element, allowing parallel execution of learning processes. As a result, we achieve GPU-accelerated sampling with significant runtime improvements ranging from $33.6\times$ to $523.6\times$ over state-of-the-art heuristic samplers. We demonstrate the superior performance of our sampling method through an extensive evaluation on $60$ instances from a public domain benchmark suite utilized in previous studies.

### Personalizing Education through an Adaptive LMS with Integrated LLMs 
[[arxiv](https://arxiv.org/abs/2502.08655)] [[cool](https://papers.cool/arxiv/2502.08655)] [[pdf](https://arxiv.org/pdf/2502.08655)]
> **Authors**: Kyle Spriggs,Meng Cheng Lau,Kalpdrum Passi
> **First submission**: 2025-01-24
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The widespread adoption of large language models (LLMs) marks a transformative era in technology, especially within the educational sector. This paper explores the integration of LLMs within learning management systems (LMSs) to develop an adaptive learning management system (ALMS) personalized for individual learners across various educational stages. Traditional LMSs, while facilitating the distribution of educational materials, fall short in addressing the nuanced needs of diverse student populations, particularly in settings with limited instructor availability. Our proposed system leverages the flexibility of AI to provide a customizable learning environment that adjusts to each user's evolving needs. By integrating a suite of general-purpose and domain-specific LLMs, this system aims to minimize common issues such as factual inaccuracies and outdated information, characteristic of general LLMs like OpenAI's ChatGPT. This paper details the development of an ALMS that not only addresses privacy concerns and the limitations of existing educational tools but also enhances the learning experience by maintaining engagement through personalized educational content.

### Ensemble based approach to quantifying uncertainty of LLM based classifications 
[[arxiv](https://arxiv.org/abs/2502.08631)] [[cool](https://papers.cool/arxiv/2502.08631)] [[pdf](https://arxiv.org/pdf/2502.08631)]
> **Authors**: Srijith Rajamohan,Ahmed Salhin,Josh Frazier,Rohit Kumar,Yu-Cheng Tsai,Todd Cook
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.

### Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data 
[[arxiv](https://arxiv.org/abs/2502.08547)] [[cool](https://papers.cool/arxiv/2502.08547)] [[pdf](https://arxiv.org/pdf/2502.08547)]
> **Authors**: Doudou Zhou,Han Tong,Linshanshan Wang,Suqi Liu,Xin Xiong,Ziming Gan,Romain Griffier,Boris Hejblum,Yun-Chung Liu,Chuan Hong,Clara-Lea Bonzel,Tianrun Cai,Kevin Pan,Yuk-Lam Ho,Lauren Costa,Vidul A. Panickan,J. Michael Gaziano,Kenneth Mandl,Vianney Jouhet,Rodolphe Thiebaut,Zongqi Xia,Kelly Cho,Katherine Liao,Tianxi Cai
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions. The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups. To address these challenges, we developed the GAME algorithm. Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network. Jointly trained embeddings are created using transfer and federated learning to preserve data privacy. In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis. We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions.

### Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities? 
[[arxiv](https://arxiv.org/abs/2502.08503)] [[cool](https://papers.cool/arxiv/2502.08503)] [[pdf](https://arxiv.org/pdf/2502.08503)]
> **Authors**: Jiahe Jin,Yanheng He,Mingyan Yang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs.

### Improving Existing Optimization Algorithms with LLMs 
[[arxiv](https://arxiv.org/abs/2502.08298)] [[cool](https://papers.cool/arxiv/2502.08298)] [[pdf](https://arxiv.org/pdf/2502.08298)]
> **Authors**: Camilo Chacón Sartori,Christian Blum
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :I.2.7; I.2.8
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习,软件工程
- **Abstract**: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/

### The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks 
[[arxiv](https://arxiv.org/abs/2502.08235)] [[cool](https://papers.cool/arxiv/2502.08235)] [[pdf](https://arxiv.org/pdf/2502.08235)]
> **Authors**: Alejandro Cuadron,Dacheng Li,Wenjie Ma,Xingyao Wang,Yichuan Wang,Siyuan Zhuang,Shu Liu,Luis Gaspar Schroeder,Tian Xia,Huanzhi Mao,Nicholas Thumiger,Aditya Desai,Ion Stoica,Ana Klimovic,Graham Neubig,Joseph E. Gonzalez
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/AlexCuadron/Overthinking.

### SycEval: Evaluating LLM Sycophancy 
[[arxiv](https://arxiv.org/abs/2502.08177)] [[cool](https://papers.cool/arxiv/2502.08177)] [[pdf](https://arxiv.org/pdf/2502.08177)]
> **Authors**: Aaron Fanous,Jacob Goldberg,Ank A. Agarwal,Joanna Lin,Anson Zhou,Roxana Daneshjou,Sanmi Koyejo
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 10 pages
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$, $p<0.001$), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$). Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$, $p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer AI applications.

### ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning 
[[arxiv](https://arxiv.org/abs/2502.08148)] [[cool](https://papers.cool/arxiv/2502.08148)] [[pdf](https://arxiv.org/pdf/2502.08148)]
> **Authors**: Vy Vo,Lizhen Qu,Tao Feng,Yuncheng Hua,Xiaoxi Kang,Songhai Fan,Tim Dwyer,Lay-Ki Soon,Gholamreza Haffari
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: ef:Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where \texttt{CRAB} \citep{romanou2023crab} is one such recent benchmark along this line. In this paper, we introduce \texttt{ACCESS}, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, \texttt{ACCESS} focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from \texttt{GLUCOSE} \citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract $1,4$K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in \texttt{ACCESS} can be leveraged for enhancing QA reasoning performance in LLMs.

### Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences 
[[arxiv](https://arxiv.org/abs/2502.08142)] [[cool](https://papers.cool/arxiv/2502.08142)] [[pdf](https://arxiv.org/pdf/2502.08142)]
> **Authors**: Shanshan Han,Salman Avestimehr,Chaoyang He
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: arXiv admin note: text overlap with arXiv:2406.10847
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%.

## 硬件架构(cs.AR:Hardware Architecture)

### InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs 
[[arxiv](https://arxiv.org/abs/2502.08807)] [[cool](https://papers.cool/arxiv/2502.08807)] [[pdf](https://arxiv.org/pdf/2502.08807)]
> **Authors**: Zifan He,Anderson Truong,Yingqi Cao,Jason Cong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,机器学习
- **Abstract**: The rise of deep neural networks (DNNs) has driven a boom in AI services, which results in an increased demand for computing power and memory. In modern DNNs, the data sizes produced and consumed are highly varied across operations (high data volume variation, HDV). Because existing design paradigms use fixed execution patterns that lead to either low computational efficiency due to pipeline stalls or frequent off-chip memory accesses to manage large intermediate data, HDV applications are challenging to accelerate on FPGAs. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR), a novel accelerator design for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and model parameters. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfiguration time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task kernels in various HDV DNNs using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits $1.8\times$ and $7.1 \times$ speedups correspondingly. We also implement InTAR for GPT-2 medium as a more complex example, which achieves a speedup of $\mathbf{3.65 \sim 39.14\times}$ and a $\mathbf{1.72 \sim 10.44\times}$ boost in DSP efficiency compared to the corresponding SoTA accelerators on FPGAs.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies 
[[arxiv](https://arxiv.org/abs/2502.08534)] [[cool](https://papers.cool/arxiv/2502.08534)] [[pdf](https://arxiv.org/pdf/2502.08534)]
> **Authors**: Gian-Luca Geuken,Patrick Kurzeja,David Wiedemann,Jörn Mosler
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :74B20; 68T07ACM Class:J.2; I.2.1
- **标题**: None
- **领域**: 计算工程、金融和科学,人工智能
- **Abstract**: This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elementary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls.

## 计算语言学(cs.CL:Computation and Language)

### Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs for Clinical Reasoning 
[[arxiv](https://arxiv.org/abs/2502.08954)] [[cool](https://papers.cool/arxiv/2502.08954)] [[pdf](https://arxiv.org/pdf/2502.08954)]
> **Authors**: Leon Nissen,Philipp Zagar,Vishnu Ravi,Aydin Zahedivash,Lara Marie Reimer,Stephan Jonas,Oliver Aalami,Paul Schmiedmayer
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The deployment of Large Language Models (LLM) on mobile devices offers significant potential for medical applications, enhancing privacy, security, and cost-efficiency by eliminating reliance on cloud-based services and keeping sensitive health data local. However, the performance and accuracy of on-device LLMs in real-world medical contexts remain underexplored. In this study, we benchmark publicly available on-device LLMs using the AMEGA dataset, evaluating accuracy, computational efficiency, and thermal limitation across various mobile devices. Our results indicate that compact general-purpose models like Phi-3 Mini achieve a strong balance between speed and accuracy, while medically fine-tuned models such as Med42 and Aloe attain the highest accuracy. Notably, deploying LLMs on older devices remains feasible, with memory constraints posing a greater challenge than raw processing power. Our study underscores the potential of on-device LLMs for healthcare while emphasizing the need for more efficient inference and models tailored to real-world clinical reasoning.

### Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding 
[[arxiv](https://arxiv.org/abs/2502.08947)] [[cool](https://papers.cool/arxiv/2502.08947)] [[pdf](https://arxiv.org/pdf/2502.08947)]
> **Authors**: Fenella Harcourt,Naderdel Piero,Gilbert Sutherland,Daphne Holloway,Harriet Bracknell,Julian Ormsby
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Token representations in high-dimensional latent spaces often exhibit redundancy, limiting computational efficiency and reducing structural coherence across model layers. Hierarchical latent space folding introduces a structured transformation mechanism that enforces a multi-scale organization within learned embeddings, refining representational compactness while preserving essential contextual distinctions. The proposed approach incorporates dynamic folding operations that iteratively adjust token embeddings through structured transformations, influencing both short-range and long-range dependencies in sequential processing tasks. Empirical evaluation demonstrates a reduction in representational variance across layers, contributing to more stable perplexity distributions and enhancing predictive confidence in text generation. The structured redistribution of attention head utilization leads to more efficient allocation of computational resources, particularly in deeper layers, where hierarchical refinements improve contextual abstraction. Comparative analysis of activation sparsity patterns suggests that hierarchical adjustments selectively reinforce critical pathways while reducing computational overhead in non-essential regions of the model. Statistical assessments of token reordering frequencies reveal that hierarchical modifications introduce subtle shifts in sequential dependencies, improving contextual alignment while maintaining syntactic correctness. Computational trade-offs associated with hierarchical folding introduce marginal increases in training time per epoch, yet empirical findings indicate that inference efficiency benefits from the structured representation adjustments. The results highlight the impact of hierarchical latent space folding on optimizing model performance through improved representation structuring and computational efficiency.

### The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding 
[[arxiv](https://arxiv.org/abs/2502.08946)] [[cool](https://papers.cool/arxiv/2502.08946)] [[pdf](https://arxiv.org/pdf/2502.08946)]
> **Authors**: Mo Yu,Lemao Liu,Junjie Wu,Tsz Ting Chung,Shunchi Zhang,Jiangnan Li,Dit-Yan Yeung,Jie Zhou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: NAACL 2025 Main Conference. First 5 authors contributed equally. Project page: https://physico-benchmark.github.io/
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.

### Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis 
[[arxiv](https://arxiv.org/abs/2502.08943)] [[cool](https://papers.cool/arxiv/2502.08943)] [[pdf](https://arxiv.org/pdf/2502.08943)]
> **Authors**: Wenbo Zhang,Hengrui Cai,Wenyu Chen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 10 pages, 1 table, 4 Figures
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. We also introduce $\mathbb P\left(\text{correct}\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantic prompts, enabling error detection and quality control in benchmark construction.

### CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality 
[[arxiv](https://arxiv.org/abs/2502.08923)] [[cool](https://papers.cool/arxiv/2502.08923)] [[pdf](https://arxiv.org/pdf/2502.08923)]
> **Authors**: Razvan-Gabriel Dumitru,Minglai Yang,Vikas Yadav,Mihai Surdeanu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 33 pages, 18 figures, 19 tables
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We introduce CopySpec, an innovative technique designed to tackle the inefficiencies LLMs face when generating responses that closely resemble previous outputs. CopySpec identifies repeated sequences in the model's chat history and speculates that the same tokens will follow, enabling seamless copying without compromising output quality or requiring additional GPU memory. To evaluate the effectiveness of our approach, we conducted experiments using five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper, transforms the second turn of MT-Bench into a request for variations of the first turn's answer, simulating real-world scenarios where users request modifications to prior responses. Our results demonstrate significant speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select MT-Redundant categories, and 2.66x on the third turn of GSM-8K's self-correction tasks. Moreover, we show that CopySpec integrates seamlessly with speculative decoding, yielding an average 49% additional speed-up over speculative decoding for the second turn of MT-Redundant across all eight categories. While LLMs, even with speculative decoding, suffer from slower inference as context sizes grow, CopySpec leverages the expanded context to accelerate inference, making it faster as the context size increases. Our code and dataset are publicly available at https://github.com/RazvanDu/CopySpec.

### InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU 
[[arxiv](https://arxiv.org/abs/2502.08910)] [[cool](https://papers.cool/arxiv/2502.08910)] [[pdf](https://arxiv.org/pdf/2502.08910)]
> **Authors**: Heejun Lee,Geon Park,Jaduk Suh,Sung Ju Hwang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 21 pages
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.

### Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs 
[[arxiv](https://arxiv.org/abs/2502.08909)] [[cool](https://papers.cool/arxiv/2502.08909)] [[pdf](https://arxiv.org/pdf/2502.08909)]
> **Authors**: Premtim Sahitaj,Iffat Maab,Junichi Yamagishi,Jawan Kolanowski,Sebastian Möller,Vera Schmitt
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Fact-checking is necessary to address the increasing volume of misinformation. Traditional fact-checking relies on manual analysis to verify claims, but it is slow and resource-intensive. This study establishes baseline comparisons for Automated Fact-Checking (AFC) using Large Language Models (LLMs) across multiple labeling schemes (binary, three-class, five-class) and extends traditional claim verification by incorporating analysis, verdict classification, and explanation in a structured setup to provide comprehensive justifications for real-world claims. We evaluate Llama-3 models of varying sizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024) using evidence retrieved via restricted web searches. We utilize TIGERScore as a reference-free evaluation metric to score the justifications. Our results show that larger LLMs consistently outperform smaller LLMs in classification accuracy and justification quality without fine-tuning. We find that smaller LLMs in a one-shot scenario provide comparable task performance to fine-tuned Small Language Models (SLMs) with large context sizes, while larger LLMs consistently surpass them. Evidence integration improves performance across all models, with larger LLMs benefiting most. Distinguishing between nuanced labels remains challenging, emphasizing the need for further exploration of labeling schemes and alignment with evidences. Our findings demonstrate the potential of retrieval-augmented AFC with LLMs.

### Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages? 
[[arxiv](https://arxiv.org/abs/2502.08900)] [[cool](https://papers.cool/arxiv/2502.08900)] [[pdf](https://arxiv.org/pdf/2502.08900)]
> **Authors**: Shira Wein
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-poised to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream technical utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Arápaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.

### Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication 
[[arxiv](https://arxiv.org/abs/2502.08896)] [[cool](https://papers.cool/arxiv/2502.08896)] [[pdf](https://arxiv.org/pdf/2502.08896)]
> **Authors**: Weicheng Ma,Hefan Zhang,Ivory Yang,Shiyu Ji,Joice Chen,Farnoosh Hashemi,Shubham Mohole,Ethan Gearey,Michael Macy,Saeed Hassanpour,Soroush Vosoughi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted to NAACL 2025 Main Conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework's potential to significantly advance research in both computational and social science domains concerning persuasive communication.

### LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information 
[[arxiv](https://arxiv.org/abs/2502.08888)] [[cool](https://papers.cool/arxiv/2502.08888)] [[pdf](https://arxiv.org/pdf/2502.08888)]
> **Authors**: Ruichao Yang,Jing Ma,Wei Gao,Hongzhan Lin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted by ACM TIST
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The proliferation of misinformation, such as rumors on social media, has drawn significant attention, prompting various expressions of stance among users. Although rumor detection and stance detection are distinct tasks, they can complement each other. Rumors can be identified by cross-referencing stances in related posts, and stances are influenced by the nature of the rumor. However, existing stance detection methods often require post-level stance annotations, which are costly to obtain. We propose a novel LLM-enhanced MIL approach to jointly predict post stance and claim class labels, supervised solely by claim labels, using an undirected microblog propagation model. Our weakly supervised approach relies only on bag-level labels of claim veracity, aligning with multi-instance learning (MIL) principles. To achieve this, we transform the multi-class problem into multiple MIL-based binary classification problems. We then employ a discriminative attention layer to aggregate the outputs from these classifiers into finer-grained classes. Experiments conducted on three rumor datasets and two stance datasets demonstrate the effectiveness of our approach, highlighting strong connections between rumor veracity and expressed stances in responding posts. Our method shows promising performance in joint rumor and stance detection compared to the state-of-the-art methods.

### BrainWavLM: Fine-tuning Speech Representations with Brain Responses to Language 
[[arxiv](https://arxiv.org/abs/2502.08866)] [[cool](https://papers.cool/arxiv/2502.08866)] [[pdf](https://arxiv.org/pdf/2502.08866)]
> **Authors**: Nishitha Vattikonda,Aditya R. Vaidya,Richard J. Antonello,Alexander G. Huth
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 15 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Speech encoding models use auditory representations to predict how the human brain responds to spoken language stimuli. Most performant encoding models linearly map the hidden states of artificial neural networks to brain data, but this linear restriction may limit their effectiveness. In this work, we use low-rank adaptation (LoRA) to fine-tune a WavLM-based encoding model end-to-end on a brain encoding objective, producing a model we name BrainWavLM. We show that fine-tuning across all of cortex improves average encoding performance with greater stability than without LoRA. This improvement comes at the expense of low-level regions like auditory cortex (AC), but selectively fine-tuning on these areas improves performance in AC, while largely retaining gains made in the rest of cortex. Fine-tuned models generalized across subjects, indicating that they learned robust brain-like representations of the speech stimuli. Finally, by training linear probes, we showed that the brain data strengthened semantic representations in the speech model without any explicit annotations. Our results demonstrate that brain fine-tuning produces best-in-class speech encoding models, and that non-linear methods have the potential to bridge the gap between artificial and biological representations of semantics.

### Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.08826)] [[cool](https://papers.cool/arxiv/2502.08826)] [[pdf](https://arxiv.org/pdf/2502.08826)]
> **Authors**: Mohammad Mahdi Abootorabi,Amirhosein Zobeiri,Mahdi Dehghani,Mohammadali Mohammadkhani,Bardia Mohammadi,Omid Ghahroodi,Mahdieh Soleymani Baghshah,Ehsaneddin Asgari
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: GitHub repository: https://github.com/llm-lab-org/Multimodal-RAG-Survey
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.

### Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts 
[[arxiv](https://arxiv.org/abs/2502.08825)] [[cool](https://papers.cool/arxiv/2502.08825)] [[pdf](https://arxiv.org/pdf/2502.08825)]
> **Authors**: Weisi Liu,Guangzeng Han,Xiaolei Huang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: accept to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Time is implicitly embedded in classification process: classifiers are usually built on existing data while to be applied on future data whose distributions (e.g., label and token) may change. However, existing state-of-the-art classification models merely consider the temporal variations and primarily focus on English corpora, which leaves temporal studies less explored, let alone under multilingual settings. In this study, we fill the gap by treating time as domains (e.g., 2024 vs. 2025), examining temporal effects, and developing a domain adaptation framework to generalize classifiers over time on multiple languages. Our framework proposes Mixture of Temporal Experts (MoTE) to leverage both semantic and data distributional shifts to learn and adapt temporal trends into classification models. Our analysis shows classification performance varies over time across different languages, and we experimentally demonstrate that MoTE can enhance classifier generalizability over temporal data shifts. Our study provides analytic insights and addresses the need for time-aware models that perform robustly in multilingual scenarios.

### Lexical Manifold Reconfiguration in Large Language Models: A Novel Architectural Approach for Contextual Modulation 
[[arxiv](https://arxiv.org/abs/2502.08818)] [[cool](https://papers.cool/arxiv/2502.08818)] [[pdf](https://arxiv.org/pdf/2502.08818)]
> **Authors**: Koinis Vassilis,Godfrey Milbourne,Harriet Featherstone,Xanthe Peverell,Yorick Bletchley,Zachary Montford
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Contextual adaptation in token embeddings plays a central role in determining how well language models maintain coherence and retain semantic relationships over extended text sequences. Static embeddings often impose constraints on lexical flexibility, leading to suboptimal performance when faced with complex sentence structures or domain-specific terminology shifts. To address this limitation, a structured approach was developed for dynamically reconfiguring token embeddings through continuous geometric transformations, ensuring that representations evolved in response to evolving discourse structures. A manifold-based transformation mechanism was integrated to regulate lexical positioning, allowing embeddings to undergo controlled shifts while preserving linguistic relationships across varying textual contexts. Empirical evaluations demonstrated that embedding reconfiguration contributed to reductions in perplexity, improved lexical coherence, and enhanced sentence-level continuity, particularly in structured and domain-adaptive text generation tasks. Comparative analyses of embedding drift indicated that dynamically restructured representations maintained stronger contextual consistency, reducing misalignment in token dependencies while preserving fluency in language modeling outputs. Computational overhead assessments confirmed that while training complexity increased due to the iterative refinement of embeddings, inference remained efficient, ensuring practical feasibility for real-time generation. Evaluations across multiple datasets further demonstrated that dynamically modulated embeddings exhibited broader lexical diversity, reducing repetitive token patterns and enabling a more adaptable representation learning process.

### A Systematic Review on the Evaluation of Large Language Models in Theory of Mind Tasks 
[[arxiv](https://arxiv.org/abs/2502.08796)] [[cool](https://papers.cool/arxiv/2502.08796)] [[pdf](https://arxiv.org/pdf/2502.08796)]
> **Authors**: Karahan Sarıtaş,Kıvanç Tezören,Yavuz Durmazkeser
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机与社会,人机交互
- **Abstract**: In recent years, evaluating the Theory of Mind (ToM) capabilities of large language models (LLMs) has received significant attention within the research community. As the field rapidly evolves, navigating the diverse approaches and methodologies has become increasingly complex. This systematic review synthesizes current efforts to assess LLMs' ability to perform ToM tasks, an essential aspect of human cognition involving the attribution of mental states to oneself and others. Despite notable advancements, the proficiency of LLMs in ToM remains a contentious issue. By categorizing benchmarks and tasks through a taxonomy rooted in cognitive science, this review critically examines evaluation techniques, prompting strategies, and the inherent limitations of LLMs in replicating human-like mental state reasoning. A recurring theme in the literature reveals that while LLMs demonstrate emerging competence in ToM tasks, significant gaps persist in their emulation of human cognitive abilities.

### If Multi-Agent Debate is the Answer, What is the Question? 
[[arxiv](https://arxiv.org/abs/2502.08788)] [[cool](https://papers.cool/arxiv/2502.08788)] [[pdf](https://arxiv.org/pdf/2502.08788)]
> **Authors**: Hangfan Zhang,Zhiyao Cui,Xinrun Wang,Qiaosheng Zhang,Zhen Wang,Dinghao Wu,Shuyue Hu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: This position paper takes a critical view of the status quo of MAD research, and outline multiple potential directions to improve MAD
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.

### Zero-Shot Belief: A Hard Problem for LLMs 
[[arxiv](https://arxiv.org/abs/2502.08777)] [[cool](https://papers.cool/arxiv/2502.08777)] [[pdf](https://arxiv.org/pdf/2502.08777)]
> **Authors**: John Murzaku,Owen Rambow
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Submitted to ACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We present two LLM-based approaches to zero-shot source-and-target belief prediction on FactBank: a unified system that identifies events, sources, and belief labels in a single pass, and a hybrid approach that uses a fine-tuned DeBERTa tagger for event detection. We show that multiple open-sourced, closed-source, and reasoning-based LLMs struggle with the task. Using the hybrid approach, we achieve new state-of-the-art results on FactBank and offer a detailed error analysis. Our approach is then tested on the Italian belief corpus ModaFact.

### Universal Model Routing for Efficient LLM Inference 
[[arxiv](https://arxiv.org/abs/2502.08773)] [[cool](https://papers.cool/arxiv/2502.08773)] [[pdf](https://arxiv.org/pdf/2502.08773)]
> **Authors**: Wittawat Jitkrittum,Harikrishna Narasimhan,Ankit Singh Rawat,Jeevesh Juneja,Zifeng Wang,Chen-Yu Lee,Pradeep Shenoy,Rina Panigrahy,Aditya Krishna Menon,Sanjiv Kumar
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models' significant advances in capabilities are accompanied by significant increases in inference costs. Model routing is a simple technique for reducing inference cost, wherein one maintains a pool of candidate LLMs, and learns to route each prompt to the smallest feasible LLM. Existing works focus on learning a router for a fixed pool of LLMs. In this paper, we consider the problem of dynamic routing, where new, previously unobserved LLMs are available at test time. We propose a new approach to this problem that relies on representing each LLM as a feature vector, derived based on predictions on a set of representative prompts. Based on this, we detail two effective strategies, relying on cluster-based routing and a learned cluster map respectively. We prove that these strategies are estimates of a theoretically optimal routing rule, and provide an excess risk bound to quantify their errors. Experiments on a range of public benchmarks show the effectiveness of the proposed strategies in routing amongst more than 30 unseen LLMs.

### SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence 
[[arxiv](https://arxiv.org/abs/2502.08767)] [[cool](https://papers.cool/arxiv/2502.08767)] [[pdf](https://arxiv.org/pdf/2502.08767)]
> **Authors**: Zhining Liu,Rana Ali Amjad,Ravinarayana Adkathimar,Tianxin Wei,Hanghang Tong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 16 pages, 5 figures, 8 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information - an issue common in real-world scenarios. To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and factually grounded responses without additional training or iterative prompting. We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency. Our code and documentation are available at https://github.com/ZhiningLiu1998/SelfElicit.

### IHEval: Evaluating Language Models on Following the Instruction Hierarchy 
[[arxiv](https://arxiv.org/abs/2502.08745)] [[cool](https://papers.cool/arxiv/2502.08745)] [[pdf](https://arxiv.org/pdf/2502.08745)]
> **Authors**: Zhihan Zhang,Shiyang Li,Zixuan Zhang,Xin Liu,Haoming Jiang,Xianfeng Tang,Yifan Gao,Zheng Li,Haodong Wang,Zhaoxuan Tan,Yichuan Li,Qingyu Yin,Bing Yin,Meng Jiang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.

### Data Augmentation to Improve Large Language Models in Food Hazard and Product Detection 
[[arxiv](https://arxiv.org/abs/2502.08687)] [[cool](https://papers.cool/arxiv/2502.08687)] [[pdf](https://arxiv.org/pdf/2502.08687)]
> **Authors**: Areeg Fahad Rasheed,M. Zarkoosh,Shimam Amer Chasib,Safa F. Abbas
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The primary objective of this study is to demonstrate the impact of data augmentation using ChatGPT-4o-mini on food hazard and product analysis. The augmented data is generated using ChatGPT-4o-mini and subsequently used to train two large language models: RoBERTa-base and Flan-T5-base. The models are evaluated on test sets. The results indicate that using augmented data helped improve model performance across key metrics, including recall, F1 score, precision, and accuracy, compared to using only the provided dataset. The full code, including model training and the augmented dataset, can be found in this repository: https://github.com/AREEG94FAHAD/food-hazard-prdouct-cls

### Assessing the Impact of the Quality of Textual Data on Feature Representation and Machine Learning Models 
[[arxiv](https://arxiv.org/abs/2502.08669)] [[cool](https://papers.cool/arxiv/2502.08669)] [[pdf](https://arxiv.org/pdf/2502.08669)]
> **Authors**: Tabinda Sarwar,Antonio Jose Jimeno Yepes,Lawrence Cavedon
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Background: Data collected in controlled settings typically results in high-quality datasets. However, in real-world applications, the quality of data collection is often compromised. It is well established that the quality of a dataset significantly impacts the performance of machine learning models. Methods: A rudimentary error rate metric was developed to evaluate textual dataset quality at the token level. Mixtral Large Language Model (LLM) was used to quantify and correct errors in low quality datasets. The study analyzed two healthcare datasets: the high-quality MIMIC-III public hospital dataset and a lower-quality private dataset from Australian aged care homes. Errors were systematically introduced into MIMIC at varying rates, while the ACH dataset quality was improved using the LLM. Results: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH datasets respectively, we used Mixtral to introduce errors in MIMIC and correct errors in ACH. Mixtral correctly detected errors in 63% of progress notes, with 17% containing a single token misclassified due to medical terminology. LLMs demonstrated potential for improving progress note quality by addressing various errors. Under varying error rates, feature representation performance was tolerant to lower error rates (<10%) but declined significantly at higher rates. Conclusions: The study revealed that models performed relatively well on datasets with lower error rates (<10%), but their performance declined significantly as error rates increased (>=10%). Therefore, it is crucial to evaluate the quality of a dataset before utilizing it for machine learning tasks. For datasets with higher error rates, implementing corrective measures is essential to ensure the reliability and effectiveness of machine learning models.

### Style Extraction on Text Embeddings Using VAE and Parallel Dataset 
[[arxiv](https://arxiv.org/abs/2502.08668)] [[cool](https://papers.cool/arxiv/2502.08668)] [[pdf](https://arxiv.org/pdf/2502.08668)]
> **Authors**: InJin Kong,Shinyee Kang,Yuna Park,Sooyong Kim,Sanghyun Park
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: 9 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This study investigates the stylistic differences among various Bible translations using a Variational Autoencoder (VAE) model. By embedding textual data into high-dimensional vectors, the study aims to detect and analyze stylistic variations between translations, with a specific focus on distinguishing the American Standard Version (ASV) from other translations. The results demonstrate that each translation exhibits a unique stylistic distribution, which can be effectively identified using the VAE model. These findings suggest that the VAE model is proficient in capturing and differentiating textual styles, although it is primarily optimized for distinguishing a single style. The study highlights the model's potential for broader applications in AI-based text generation and stylistic analysis, while also acknowledging the need for further model refinement to address the complexity of multi-dimensional stylistic relationships. Future research could extend this methodology to other text domains, offering deeper insights into the stylistic features embedded within various types of textual data.

### Hallucination, Monofacts, and Miscalibration: An Empirical Investigation 
[[arxiv](https://arxiv.org/abs/2502.08666)] [[cool](https://papers.cool/arxiv/2502.08666)] [[pdf](https://arxiv.org/pdf/2502.08666)]
> **Authors**: Muqing Miao,Michael Kearns
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: Code available at https://github.com/mmiao2/Hallucination.git
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent theoretical work by [Kalai and Vempala 2024] proves that a particular notion of hallucination rate in LLMs must be lower bounded by the training data monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration. Through systematic experiments with n-gram models and in-context learning with LLMs, we empirically investigate and validate this theory by examining how different underlying data distributions affect the monofact rate and a model's tendency to hallucinate. We then vary model miscalibration through controlled upweighting of training samples while holding monofact rates constant, allowing us to isolate miscalibration's reduction effect on hallucination. These findings suggest that both the distribution of fact frequencies in training data and the calibration-hallucination trade-off are inherent to probabilistic language generation. Our results also suggest that current practices of aggressive deduplication in training data may need to be reconsidered, as selective duplication could serve as a principled mechanism for reducing hallucination.

### Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis 
[[arxiv](https://arxiv.org/abs/2502.08663)] [[cool](https://papers.cool/arxiv/2502.08663)] [[pdf](https://arxiv.org/pdf/2502.08663)]
> **Authors**: Emanuele Ricco,Lorenzo Cima,Roberto Di Pietro
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Hallucinations are one of the major issues affecting LLMs, hindering their wide adoption in production systems. While current research solutions for detecting hallucinations are mainly based on heuristics, in this paper we introduce a mathematically sound methodology to reason about hallucination, and leverage it to build a tool to detect hallucinations. To the best of our knowledge, we are the first to show that hallucinated content has structural differences with respect to correct content. To prove this result, we resort to the Minkowski distances in the embedding space. Our findings demonstrate statistically significant differences in the embedding distance distributions, that are also scale free -- they qualitatively hold regardless of the distance norm used and the number of keywords, questions, or responses. We leverage these structural differences to develop a tool to detect hallucinated responses, achieving an accuracy of 66\% for a specific configuration of system parameters -- comparable with the best results in the field. In conclusion, the suggested methodology is promising and novel, possibly paving the way for further research in the domain, also along the directions highlighted in our future work.

### RoToR: Towards More Reliable Responses for Order-Invariant Inputs 
[[arxiv](https://arxiv.org/abs/2502.08662)] [[cool](https://papers.cool/arxiv/2502.08662)] [[pdf](https://arxiv.org/pdf/2502.08662)]
> **Authors**: Soyoung Yoon,Dongha Ahn,Youngwon Lee,Minkyu Jung,HyungJoo Jang,Seung-won Hwang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to a mixture of order-invariant and sensitive inputs in practical listwise problems. To overcome, we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph Question Answering (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner.

### Few-shot LLM Synthetic Data with Distribution Matching 
[[arxiv](https://arxiv.org/abs/2502.08661)] [[cool](https://papers.cool/arxiv/2502.08661)] [[pdf](https://arxiv.org/pdf/2502.08661)]
> **Authors**: Jiyuan Ren,Zhaocheng Du,Zhihao Wen,Qinglin Jia,Sunhao Dai,Chuhan Wu,Zhenhua Dong
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-13
> **comment**: 10 pages, 5 figures, accepted at www 2025
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: As large language models (LLMs) advance, their ability to perform in-context learning and few-shot language generation has improved significantly. This has spurred using LLMs to produce high-quality synthetic data to enhance the performance of smaller models like online retrievers or weak LLMs. However, LLM-generated synthetic data often differs from the real data in key language attributes (e.g., styles, tones, content proportions, etc.). As a result, mixing these synthetic data directly with real data may distort the original data distribution, potentially hindering performance improvements. To solve this, we introduce SynAlign: a synthetic data generation and filtering framework based on key attribute distribution matching. Before generation, SynAlign employs an uncertainty tracker surrogated by the Gaussian Process model to iteratively select data clusters distinct from selected ones as demonstrations for new data synthesis, facilitating the efficient exploration diversity of the real data. Then, a latent attribute reasoning method is employed: the LLM summarizes linguistic attributes of demonstrations and then synthesizes new data based on them. This approach facilitates synthesizing diverse data with linguistic attributes that appear in real data.After generation, the Maximum Mean Discrepancy is used as the objective function to learn the sampling weight of each synthetic data, ensuring distribution matching with the real data. Our experiments on multiple text prediction tasks show significant performance improvements. We also conducted an online A/B test on an online retriever to demonstrate SynAlign's effectiveness.

### Semantic Role Labeling: A Systematical Survey 
[[arxiv](https://arxiv.org/abs/2502.08660)] [[cool](https://papers.cool/arxiv/2502.08660)] [[pdf](https://arxiv.org/pdf/2502.08660)]
> **Authors**: Huiyao Chen,Meishan Zhang,Jing Li,Min Zhang,Lilja Øvrelid,Jan Hajič,Hao Fei
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Semantic role labeling (SRL) is a central natural language processing (NLP) task aiming to understand the semantic roles within texts, facilitating a wide range of downstream applications. While SRL has garnered extensive and enduring research, there is currently a lack of a comprehensive survey that thoroughly organizes and synthesizes the field. This paper aims to review the entire research trajectory of the SRL community over the past two decades. We begin by providing a complete definition of SRL. To offer a comprehensive taxonomy, we categorize SRL methodologies into four key perspectives: model architectures, syntax feature modeling, application scenarios, and multi-modal extensions. Further, we discuss SRL benchmarks, evaluation metrics, and paradigm modeling approaches, while also exploring practical applications across various domains. Finally, we analyze future research directions in SRL, addressing the evolving role of SRL in the age of large language models (LLMs) and its potential impact on the broader NLP landscape. We maintain a public repository and consistently update related resources at: https://github.com/DreamH1gh/Awesome-SRL

### Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions 
[[arxiv](https://arxiv.org/abs/2502.08657)] [[cool](https://papers.cool/arxiv/2502.08657)] [[pdf](https://arxiv.org/pdf/2502.08657)]
> **Authors**: Jingxin Xu,Guoshun Nan,Sheng Guan,Sicong Leng,Yilian Liu,Zixiao Wang,Yuyang Ma,Zhili Zhou,Yanzhao Hou,Xiaofeng Tao
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-13
> **comment**: This work has been submitted to the IEEE for possible publication
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than 50 human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on 9 popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.

### Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples 
[[arxiv](https://arxiv.org/abs/2502.08638)] [[cool](https://papers.cool/arxiv/2502.08638)] [[pdf](https://arxiv.org/pdf/2502.08638)]
> **Authors**: Andrianos Michail,Simon Clematide,Rico Sennrich
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that does not require a large evaluation corpus, only parallel sentences of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than challenging distractors generated by a large language model. We create a case study of our introduced CLSD task for the language pair German-French in the news domain. Within this case study, we find that models that are also fine-tuned for retrieval tasks benefit from pivoting through English, while bitext mining models perform best directly cross-lingually. A fine-grained similarity analysis enabled by our distractor generation strategy indicate that different embedding models are sensitive to different types of perturbations.

### SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent 
[[arxiv](https://arxiv.org/abs/2502.08599)] [[cool](https://papers.cool/arxiv/2502.08599)] [[pdf](https://arxiv.org/pdf/2502.08599)]
> **Authors**: Keyeun Lee,Seo Hyeong Kim,Seolhee Lee,Jinsu Eun,Yena Ko,Hayeon Jeon,Esther Hehsun Kim,Seonghye Cho,Soeun Yang,Eun-mee Kim,Hajin Lim
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual's multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum's effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters' identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.

### Quality-Aware Decoding: Unifying Quality Estimation and Decoding 
[[arxiv](https://arxiv.org/abs/2502.08561)] [[cool](https://papers.cool/arxiv/2502.08561)] [[pdf](https://arxiv.org/pdf/2502.08561)]
> **Authors**: Sai Koneru,Matthias Huck,Miriam Exel,Jan Niehues
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Under Review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Quality Estimation (QE) models for Neural Machine Translation (NMT) predict the quality of the hypothesis without having access to the reference. An emerging research direction in NMT involves the use of QE models, which have demonstrated high correlations with human judgment and can enhance translations through Quality-Aware Decoding. Although several approaches have been proposed based on sampling multiple candidate translations and picking the best candidate, none have integrated these models directly into the decoding process. In this paper, we address this by proposing a novel token-level QE model capable of reliably scoring partial translations. We build a uni-directional QE model for this, as decoder models are inherently trained and efficient on partial sequences. We then present a decoding strategy that integrates the QE model for Quality-Aware decoding and demonstrate that the translation quality improves when compared to the N-best list re-ranking with state-of-the-art QE models (up to $1.39$ XCOMET-XXL $\uparrow$). Finally, we show that our approach provides significant benefits in document translation tasks, where the quality of N-best lists is typically suboptimal. Code can be found at https://ai4lt.iar.kit.edu/english/projects\_kontextmt.php

### LLMs can implicitly learn from mistakes in-context 
[[arxiv](https://arxiv.org/abs/2502.08550)] [[cool](https://papers.cool/arxiv/2502.08550)] [[pdf](https://arxiv.org/pdf/2502.08550)]
> **Authors**: Lisa Alazraki,Maximilian Mozes,Jon Ander Campos,Yi Chern Tan,Marek Rei,Max Bartolo
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.

### Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation 
[[arxiv](https://arxiv.org/abs/2502.08514)] [[cool](https://papers.cool/arxiv/2502.08514)] [[pdf](https://arxiv.org/pdf/2502.08514)]
> **Authors**: Mahnaz Koupaee,Jake W. Vincent,Saab Mansour,Igor Shalyminov,Han He,Hwanjun Song,Raphael Shu,Jianfeng He,Yi Nian,Amy Wing-mei Wong,Kyu J. Han,Hang Su
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.

### Measuring Diversity in Synthetic Datasets 
[[arxiv](https://arxiv.org/abs/2502.08512)] [[cool](https://papers.cool/arxiv/2502.08512)] [[pdf](https://arxiv.org/pdf/2502.08512)]
> **Authors**: Yuchang Zhu,Huizhe Zhang,Bingzhe Wu,Jintang Li,Zibin Zheng,Peilin Zhao,Liang Chen,Yatao Bian
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: https://github.com/BlueWhaleLab/DCScore.

### Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction 
[[arxiv](https://arxiv.org/abs/2502.08507)] [[cool](https://papers.cool/arxiv/2502.08507)] [[pdf](https://arxiv.org/pdf/2502.08507)]
> **Authors**: Wei Li,Wen Luo,Guangyue Peng,Houfeng Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted by NAACL 2025 main conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples.

### Salamandra Technical Report 
[[arxiv](https://arxiv.org/abs/2502.08489)] [[cool](https://papers.cool/arxiv/2502.08489)] [[pdf](https://arxiv.org/pdf/2502.08489)]
> **Authors**: Aitor Gonzalez-Agirre,Marc Pàmies,Joan Llop,Irene Baucells,Severino Da Dalt,Daniel Tamayo,José Javier Saiz,Ferran Espuña,Jaume Prats,Javier Aula-Blasco,Mario Mina,Iñigo Pikabea,Adrián Rubio,Alexander Shvets,Anna Sallés,Iñaki Lacunza,Jorge Palomar,Júlia Falcão,Lucía Tormo,Luis Vasquez-Reina,Montserrat Marimon,Oriol Pareras,Valle Ruiz-Fernández,Marta Villegas
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and safety.With this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models.

### Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning 
[[arxiv](https://arxiv.org/abs/2502.08482)] [[cool](https://papers.cool/arxiv/2502.08482)] [[pdf](https://arxiv.org/pdf/2502.08482)]
> **Authors**: Qifan Yu,Zhenyu He,Sijie Li,Xun Zhou,Jun Zhang,Jingjing Xu,Di He
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: work in progress
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at https://github.com/qifanyu/RELAY.

### Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring 
[[arxiv](https://arxiv.org/abs/2502.08450)] [[cool](https://papers.cool/arxiv/2502.08450)] [[pdf](https://arxiv.org/pdf/2502.08450)]
> **Authors**: Heejin Do,Taehee Park,Sangwon Ryu,Gary Geunbae Lee
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: NAACL 2025 (Findings)
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method's generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts.

### Better Embeddings with Coupled Adam 
[[arxiv](https://arxiv.org/abs/2502.08441)] [[cool](https://papers.cool/arxiv/2502.08441)] [[pdf](https://arxiv.org/pdf/2502.08441)]
> **Authors**: Felix Stollenwerk,Tobias Stollenwerk
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 8 figures; figures corrected
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.

### From Haystack to Needle: Label Space Reduction for Zero-shot Classification 
[[arxiv](https://arxiv.org/abs/2502.08436)] [[cool](https://papers.cool/arxiv/2502.08436)] [[pdf](https://arxiv.org/pdf/2502.08436)]
> **Authors**: Nathan Vandemoortele,Bram Steenwinckel,Femke Ongenae,Sofie Van Hoecke
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Under review at ICML 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.

### A Semantic Parsing Algorithm to Solve Linear Ordering Problems 
[[arxiv](https://arxiv.org/abs/2502.08415)] [[cool](https://papers.cool/arxiv/2502.08415)] [[pdf](https://arxiv.org/pdf/2502.08415)]
> **Authors**: Maha Alkhairy,Vincent Homer,Brendan O'Connor
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 3 figures, 9 pages main paper and 6 pages references and appendix
- **标题**: None
- **领域**: 计算语言学,计算机科学中的逻辑
- **Abstract**: We develop an algorithm to semantically parse linear ordering problems, which require a model to arrange entities using deductive reasoning. Our method takes as input a number of premises and candidate statements, parsing them to a first-order logic of an ordering domain, and then utilizes constraint logic programming to infer the truth of proposed statements about the ordering. Our semantic parser transforms Heim and Kratzer's syntax-based compositional formal semantic rules to a computational algorithm. This transformation involves introducing abstract types and templates based on their rules, and introduces a dynamic component to interpret entities within a contextual framework. Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to answer multiple choice questions in BIG-bench's logical_deduction multiple choice problems, achieving perfect accuracy, compared to 67.06% for the best-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM. These promising results demonstrate the benefit of developing a semantic parsing algorithm driven by first-order logic constructs.

### IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance 
[[arxiv](https://arxiv.org/abs/2502.08395)] [[cool](https://papers.cool/arxiv/2502.08395)] [[pdf](https://arxiv.org/pdf/2502.08395)]
> **Authors**: Paul Röttger,Musashi Hinck,Valentin Hofmann,Kobi Hackenburg,Valentina Pyatkin,Faeze Brahman,Dirk Hovy
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: under review
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.

### Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding 
[[arxiv](https://arxiv.org/abs/2502.08363)] [[cool](https://papers.cool/arxiv/2502.08363)] [[pdf](https://arxiv.org/pdf/2502.08363)]
> **Authors**: Konstantin Berestizshevsky,Renzo Andri,Lukas Cavigelli
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages, 11 figures, work under submission
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$θ$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$θ$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.

### Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG 
[[arxiv](https://arxiv.org/abs/2502.08356)] [[cool](https://papers.cool/arxiv/2502.08356)] [[pdf](https://arxiv.org/pdf/2502.08356)]
> **Authors**: Kushagra Bhushan,Yatin Nandwani,Dinesh Khandelwal,Sonam Gupta,Gaurav Pandey,Dinesh Raghu,Sachindra Joshi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 22 pages, 14 tables, to be published in NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\% relative gain in token-level recall while preserving the LLM's generalization capabilities.

### Contextual Compression Encoding for Large Language Models: A Novel Framework for Multi-Layered Parameter Space Pruning 
[[arxiv](https://arxiv.org/abs/2502.08323)] [[cool](https://papers.cool/arxiv/2502.08323)] [[pdf](https://arxiv.org/pdf/2502.08323)]
> **Authors**: Barnaby Schmitt,Alistair Grosvenor,Matthias Cunningham,Clementine Walsh,Julius Pembrokeshire,Jonathan Teel
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Context-aware compression techniques have gained increasing attention as model sizes continue to grow, introducing computational bottlenecks that hinder efficient deployment. A structured encoding approach was proposed to selectively eliminate redundant parameter groups while ensuring that representational fidelity was preserved across multiple layers. Contextual Compression Encoding (CCE) introduced a multi-stage encoding mechanism that dynamically restructured parameter distributions, allowing for significant reductions in memory footprint and computational complexity. Experimental evaluations demonstrated that models compressed through CCE retained linguistic expressivity and coherence, maintaining accuracy across a range of text generation and classification tasks. Layer-wise analysis revealed that middle-network layers exhibited higher compression ratios, aligning with the observation that self-attention and feed-forward transformations contained redundancies that could be reorganized without impairing functional capacity. Comparisons against conventional quantization and pruning methods confirmed that CCE provided a more balanced trade-off between efficiency and model retention, achieving reductions in energy consumption and inference latency without requiring extensive retraining. Computational efficiency improvements were particularly evident in deployment scenarios involving resource-constrained environments, where reductions in memory usage enabled more scalable implementations. Further analyses of internal network behavior showed that compressed models exhibited stable activation distributions and adapted dynamically to input variations, reinforcing the viability of structured compression strategies for optimizing large-scale architectures.

### MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection 
[[arxiv](https://arxiv.org/abs/2502.08319)] [[cool](https://papers.cool/arxiv/2502.08319)] [[pdf](https://arxiv.org/pdf/2502.08319)]
> **Authors**: Lubna Al-Henaki,Hend Al-Khalifa,Abdulmalik Al-Salman,Hajar Alqubayshi,Hind Al-Twailay,Gheeda Alghamdi,Hawra Aljasim
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 12 pages, 3 figuers, 4 tabels
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Propaganda is a form of persuasion that has been used throughout history with the intention goal of influencing people's opinions through rhetorical and psychological persuasion techniques for determined ends. Although Arabic ranked as the fourth most-used language on the internet, resources for propaganda detection in languages other than English, especially Arabic, remain extremely limited. To address this gap, the first Arabic dataset for Multi-label Propaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE is an open-source extension of the existing Arabic propaganda dataset, ArPro, with the addition of sentiment and emotion annotations for each text. This dataset comprises 8,000 annotated news articles, which is the largest propaganda dataset to date. For each task, several baselines have been developed using large language models (LLMs), such as GPT-4o-mini, and pre-trained language models (PLMs), including three BERT-based models. The dataset, annotation guidelines, and source code are all publicly released to facilitate future research and development in Arabic language models and contribute to a deeper understanding of how various opinion dimensions interact in news media1.

### Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting 
[[arxiv](https://arxiv.org/abs/2502.08317)] [[cool](https://papers.cool/arxiv/2502.08317)] [[pdf](https://arxiv.org/pdf/2502.08317)]
> **Authors**: Jiarui Wu,Zhuo Liu,Hangfeng He
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 19 pages, accepted to NAACL Findings
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations.

### Compromising Honesty and Harmlessness in Language Models via Deception Attacks 
[[arxiv](https://arxiv.org/abs/2502.08301)] [[cool](https://papers.cool/arxiv/2502.08301)] [[pdf](https://arxiv.org/pdf/2502.08301)]
> **Authors**: Laurène Vaugrante,Francesca Carlon,Maluna Menke,Thilo Hagendorff
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.

### Redefining Simplicity: Benchmarking Large Language Models from Lexical to Document Simplification 
[[arxiv](https://arxiv.org/abs/2502.08281)] [[cool](https://papers.cool/arxiv/2502.08281)] [[pdf](https://arxiv.org/pdf/2502.08281)]
> **Authors**: Jipeng Qiang,Minjiang Huang,Yi Zhu,Yunhao Yuan,Chaowei Zhang,Kui Yu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Text simplification (TS) refers to the process of reducing the complexity of a text while retaining its original meaning and key information. Existing work only shows that large language models (LLMs) have outperformed supervised non-LLM-based methods on sentence simplification. This study offers the first comprehensive analysis of LLM performance across four TS tasks: lexical, syntactic, sentence, and document simplification. We compare lightweight, closed-source and open-source LLMs against traditional non-LLM methods using automatic metrics and human evaluations. Our experiments reveal that LLMs not only outperform non-LLM approaches in all four tasks but also often generate outputs that exceed the quality of existing human-annotated references. Finally, we present some future directions of TS in the era of LLMs.

### What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations 
[[arxiv](https://arxiv.org/abs/2502.08279)] [[cool](https://papers.cool/arxiv/2502.08279)] [[pdf](https://arxiv.org/pdf/2502.08279)]
> **Authors**: Dongqi Liu,Chenxi Whitehouse,Xi Yu,Louis Mahon,Rohit Saxena,Zheng Zhao,Yifu Qiu,Mirella Lapata,Vera Demberg
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机视觉和模式识别
- **Abstract**: Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization.

### Dealing with Annotator Disagreement in Hate Speech Classification 
[[arxiv](https://arxiv.org/abs/2502.08266)] [[cool](https://papers.cool/arxiv/2502.08266)] [[pdf](https://arxiv.org/pdf/2502.08266)]
> **Authors**: Somaiyeh Dehghan,Mehmet Umut Sen,Berrin Yanikoglu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :I.2; I.2.7
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse.

### Exploring the Potential of Large Language Models to Simulate Personality 
[[arxiv](https://arxiv.org/abs/2502.08265)] [[cool](https://papers.cool/arxiv/2502.08265)] [[pdf](https://arxiv.org/pdf/2502.08265)]
> **Authors**: Maria Molchanova,Anna Mikhailova,Anna Korzanova,Lidiia Ostyakova,Alexandra Dolidze
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on EMNLP2024
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills.

### Inference-time sparse attention with asymmetric indexing 
[[arxiv](https://arxiv.org/abs/2502.08246)] [[cool](https://papers.cool/arxiv/2502.08246)] [[pdf](https://arxiv.org/pdf/2502.08246)]
> **Authors**: Pierre-Emmanuel Mazaré,Gergely Szilvasy,Maria Lomeli,Francisco Massa,Naila Murray,Hervé Jégou,Matthijs Douze
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compliant vector search algorithms, yet the standard partitioning methods yield poor results in this context, because (1) keys and queries follow different distributions and (2) the effect of RoPE positional encoding. In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern. It works on pretrained language models without finetuning, as it only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, our method typically reduces by a factor 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\% when compared to FlashAttention-v2.

### LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention 
[[arxiv](https://arxiv.org/abs/2502.08213)] [[cool](https://papers.cool/arxiv/2502.08213)] [[pdf](https://arxiv.org/pdf/2502.08213)]
> **Authors**: Konstantin Kolomeitsev
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Code and pre-trained weights available at https://huggingface.co/kkolomeitsev/llm-modules
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method.

### Enhancing LLM Character-Level Manipulation via Divide and Conquer 
[[arxiv](https://arxiv.org/abs/2502.08180)] [[cool](https://papers.cool/arxiv/2502.08180)] [[pdf](https://arxiv.org/pdf/2502.08180)]
> **Authors**: Zhen Xiong,Yujun Cai,Bryan Hooi,Nanyun Peng,Kai-Wei Chang,Zhecheng Li,Yiwei Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and $\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks.

### ParetoRAG: Leveraging Sentence-Context Attention for Robust and Efficient Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.08178)] [[cool](https://papers.cool/arxiv/2502.08178)] [[pdf](https://arxiv.org/pdf/2502.08178)]
> **Authors**: Ruobing Yao,Yifei Zhang,Shuang Song,Yuhua Liu,Neng Gao,Chenyang Tu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external knowledge, they still face persistent challenges in retrieval inefficiency and the inability of LLMs to filter out irrelevant information. We present ParetoRAG, an unsupervised framework that optimizes RAG systems through sentence-level refinement guided by the Pareto principle. By decomposing paragraphs into sentences and dynamically re-weighting core content while preserving contextual coherence, ParetoRAG achieves dual improvements in both retrieval precision and generation quality without requiring additional training or API resources. This framework has been empirically validated across various datasets, LLMs, and retrievers.

### SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation 
[[arxiv](https://arxiv.org/abs/2502.08168)] [[cool](https://papers.cool/arxiv/2502.08168)] [[pdf](https://arxiv.org/pdf/2502.08168)]
> **Authors**: Zhiming Ma,Xiayang Xiao,Sihao Dong,Peidong Wang,HaiPeng Wang,Qingyun Pan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As a powerful all-weather Earth observation tool, synthetic aperture radar (SAR) remote sensing enables critical military reconnaissance, maritime surveillance, and infrastructure monitoring. Although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified. The project will be released at https://github.com/JimmyMa99/SARChat.

### Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.08130)] [[cool](https://papers.cool/arxiv/2502.08130)] [[pdf](https://arxiv.org/pdf/2502.08130)]
> **Authors**: Sonam Gupta,Yatin Nandwani,Asaf Yehudai,Dinesh Khandelwal,Dinesh Raghu,Sachindra Joshi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 10 pages, Accepted to NAACL Findings 2025. arXiv admin note: text overlap with arXiv:2409.04787
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.

### Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance 
[[arxiv](https://arxiv.org/abs/2502.08127)] [[cool](https://papers.cool/arxiv/2502.08127)] [[pdf](https://arxiv.org/pdf/2502.08127)]
> **Authors**: Lingfei Qian,Weipeng Zhou,Yan Wang,Xueqing Peng,Jimin Huang,Qianqian Xie
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Ongoing work, 13 pages, 2 figures, 3 Tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.

## 密码学和安全(cs.CR:Cryptography and Security)

### Generative AI for Internet of Things Security: Challenges and Opportunities 
[[arxiv](https://arxiv.org/abs/2502.08886)] [[cool](https://papers.cool/arxiv/2502.08886)] [[pdf](https://arxiv.org/pdf/2502.08886)]
> **Authors**: Yan Lin Aung,Ivan Christian,Ye Dong,Xiaodong Ye,Sudipta Chattopadhyay,Jianying Zhou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: As Generative AI (GenAI) continues to gain prominence and utility across various sectors, their integration into the realm of Internet of Things (IoT) security evolves rapidly. This work delves into an examination of the state-of-the-art literature and practical applications on how GenAI could improve and be applied in the security landscape of IoT. Our investigation aims to map the current state of GenAI implementation within IoT security, exploring their potential to fortify security measures further. Through the compilation, synthesis, and analysis of the latest advancements in GenAI technologies applied to IoT, this paper not only introduces fresh insights into the field, but also lays the groundwork for future research directions. It explains the prevailing challenges within IoT security, discusses the effectiveness of GenAI in addressing these issues, and identifies significant research gaps through MITRE Mitigations. Accompanied with three case studies, we provide a comprehensive overview of the progress and future prospects of GenAI applications in IoT security. This study serves as a foundational resource to improve IoT security through the innovative application of GenAI, thus contributing to the broader discourse on IoT security and technology integration.

### Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards 
[[arxiv](https://arxiv.org/abs/2502.08610)] [[cool](https://papers.cool/arxiv/2502.08610)] [[pdf](https://arxiv.org/pdf/2502.08610)]
> **Authors**: Keerthana Madhavan,Abbas Yazdinejad,Fattane Zarrinkalam,Ali Dehghantanha
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks.

### Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark 
[[arxiv](https://arxiv.org/abs/2502.08332)] [[cool](https://papers.cool/arxiv/2502.08332)] [[pdf](https://arxiv.org/pdf/2502.08332)]
> **Authors**: Yuhang Cai,Yaofei Wang,Donghui Hu,Gu Chen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.

### Local Differential Privacy is Not Enough: A Sample Reconstruction Attack against Federated Learning with Local Differential Privacy 
[[arxiv](https://arxiv.org/abs/2502.08151)] [[cool](https://papers.cool/arxiv/2502.08151)] [[pdf](https://arxiv.org/pdf/2502.08151)]
> **Authors**: Zhichao You,Xuewen Dong,Shujun Li,Ximeng Liu,Siqi Ma,Yulong Shen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: ef:IEEE Transactions on Information Forensics and Security, 2025
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Reconstruction attacks against federated learning (FL) aim to reconstruct users' samples through users' uploaded gradients. Local differential privacy (LDP) is regarded as an effective defense against various attacks, including sample reconstruction in FL, where gradients are clipped and perturbed. Existing attacks are ineffective in FL with LDP since clipped and perturbed gradients obliterate most sample information for reconstruction. Besides, existing attacks embed additional sample information into gradients to improve the attack effect and cause gradient expansion, leading to a more severe gradient clipping in FL with LDP. In this paper, we propose a sample reconstruction attack against LDP-based FL with any target models to reconstruct victims' sensitive samples to illustrate that FL with LDP is not flawless. Considering gradient expansion in reconstruction attacks and noise in LDP, the core of the proposed attack is gradient compression and reconstructed sample denoising. For gradient compression, an inference structure based on sample characteristics is presented to reduce redundant gradients against LDP. For reconstructed sample denoising, we artificially introduce zero gradients to observe noise distribution and scale confidence interval to filter the noise. Theoretical proof guarantees the effectiveness of the proposed attack. Evaluations show that the proposed attack is the only attack that reconstructs victims' training samples in LDP-based FL and has little impact on the target model's accuracy. We conclude that LDP-based FL needs further improvements to defend against sample reconstruction attacks effectively.

### Provably Robust Federated Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.08123)] [[cool](https://papers.cool/arxiv/2502.08123)] [[pdf](https://arxiv.org/pdf/2502.08123)]
> **Authors**: Minghong Fang,Xilong Wang,Neil Zhenqiang Gong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: To appear in The Web Conference 2025
- **标题**: None
- **领域**: 密码学和安全,分布式、并行和集群计算,机器学习
- **Abstract**: Federated reinforcement learning (FRL) allows agents to jointly learn a global decision-making policy under the guidance of a central server. While FRL has advantages, its decentralized design makes it prone to poisoning attacks. To mitigate this, Byzantine-robust aggregation techniques tailored for FRL have been introduced. Yet, in our work, we reveal that these current Byzantine-robust techniques are not immune to our newly introduced Normalized attack. Distinct from previous attacks that targeted enlarging the distance of policy updates before and after an attack, our Normalized attack emphasizes on maximizing the angle of deviation between these updates. To counter these threats, we develop an ensemble FRL approach that is provably secure against both known and our newly proposed attacks. Our ensemble method involves training multiple global policies, where each is learnt by a group of agents using any foundational aggregation rule. These well-trained global policies then individually predict the action for a specific test state. The ultimate action is chosen based on a majority vote for discrete action systems or the geometric median for continuous ones. Our experimental results across different settings show that the Normalized attack can greatly disrupt non-ensemble Byzantine-robust methods, and our ensemble approach offers substantial resistance against poisoning attacks.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Towards Understanding Why Data Augmentation Improves Generalization 
[[arxiv](https://arxiv.org/abs/2502.08940)] [[cool](https://papers.cool/arxiv/2502.08940)] [[pdf](https://arxiv.org/pdf/2502.08940)]
> **Authors**: Jingyang Li,Jiachun Pan,Kim-Chuan Toh,Pan Zhou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习,机器学习
- **Abstract**: Data augmentation is a cornerstone technique in deep learning, widely used to improve model generalization. Traditional methods like random cropping and color jittering, as well as advanced techniques such as CutOut, Mixup, and CutMix, have achieved notable success across various domains. However, the mechanisms by which data augmentation improves generalization remain poorly understood, and existing theoretical analyses typically focus on individual techniques without a unified explanation. In this work, we present a unified theoretical framework that elucidates how data augmentation enhances generalization through two key effects: partial semantic feature removal and feature mixing. Partial semantic feature removal reduces the model's reliance on individual feature, promoting diverse feature learning and better generalization. Feature mixing, by scaling down original semantic features and introducing noise, increases training complexity, driving the model to develop more robust features. Advanced methods like CutMix integrate both effects, achieving complementary benefits. Our theoretical insights are further supported by experimental results, validating the effectiveness of this unified perspective.

### Dynamic watermarks in images generated by diffusion models 
[[arxiv](https://arxiv.org/abs/2502.08927)] [[cool](https://papers.cool/arxiv/2502.08927)] [[pdf](https://arxiv.org/pdf/2502.08927)]
> **Authors**: Yunzhuo Chen,Naveed Akhtar,Nur Al Hasan Haldar,Ajmal Mian
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.

### PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology 
[[arxiv](https://arxiv.org/abs/2502.08916)] [[cool](https://papers.cool/arxiv/2502.08916)] [[pdf](https://arxiv.org/pdf/2502.08916)]
> **Authors**: Fatemeh Ghezloo,Mehmet Saygin Seyfioglu,Rustin Soraki,Wisdom O. Ikezogwo,Beibin Li,Tejoram Vivekanandan,Joann G. Elmore,Ranjay Krishna,Linda Shapiro
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,多代理系统
- **Abstract**: Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at https://pathfinder-dx.github.io/

### Diffusion Models Through a Global Lens: Are They Culturally Inclusive? 
[[arxiv](https://arxiv.org/abs/2502.08914)] [[cool](https://papers.cool/arxiv/2502.08914)] [[pdf](https://arxiv.org/pdf/2502.08914)]
> **Authors**: Zahra Bayramli,Ayhan Suleymanzade,Na Min An,Huzama Ahmad,Eunsu Kim,Junyeong Park,James Thorne,Alice Oh
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 17 figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion models whether they can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CultDiff-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures.

### DiffoRA: Enabling Parameter-Efficient LLM Fine-Tuning via Differential Low-Rank Matrix Adaptation 
[[arxiv](https://arxiv.org/abs/2502.08905)] [[cool](https://papers.cool/arxiv/2502.08905)] [[pdf](https://arxiv.org/pdf/2502.08905)]
> **Authors**: Tangyu Jiang,Haodi Wang,Chun Yuan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively researched for large language models in the downstream tasks. Among all the existing approaches, the Low-Rank Adaptation (LoRA) has gained popularity for its streamlined design by incorporating low-rank matrices into existing pre-trained models. Though effective, LoRA allocates every module an identical low-rank matrix, which ignores the varying properties and contributions across different components. Moreover, the existing adaptive LoRA solutions rely highly on intuitive importance scoring indicators to adjust the interior rank of the decomposition matrices. In this paper, we propose a new PEFT scheme called DiffoRA, which is theoretically grounded and enables module-wise adoption of LoRA. At the core of our DiffoRA lies a Differential Adaptation Matrix (DAM) to determine which module is the most suitable and essential for fine-tuning. We explain how the designed matrix impacts the convergence rate and generalization capability of a pre-trained model. Furthermore, we construct the DAM via continuous relaxation and discretization with weight-sharing optimizations. We fully implement our DiffoRA and design comprehensive experiments to evaluate its performance. The experimental results demonstrate that our approach achieves the best model accuracy over all the state-of-the-art baselines across various benchmarks.

### ShapeLib: designing a library of procedural 3D shape abstractions with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.08884)] [[cool](https://papers.cool/arxiv/2502.08884)] [[pdf](https://arxiv.org/pdf/2502.08884)]
> **Authors**: R. Kenny Jones,Paul Guerrero,Niloy J. Mitra,Daniel Ritchie
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,图形
- **Abstract**: Procedural representations are desirable, versatile, and popular shape encodings. Authoring them, either manually or using data-driven procedures, remains challenging, as a well-designed procedural representation should be compact, intuitive, and easy to manipulate. A long-standing problem in shape analysis studies how to discover a reusable library of procedural functions, with semantically aligned exposed parameters, that can explain an entire shape family. We present ShapeLib as the first method that leverages the priors of frontier LLMs to design a library of 3D shape abstraction functions. Our system accepts two forms of design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. We discover procedural abstractions that match this design intent by proposing, and then validating, function applications and implementations. The discovered shape functions in the library are not only expressive but also generalize beyond the seed set to a full family of shapes. We train a recognition network that learns to infer shape programs based on our library from different visual modalities (primitives, voxels, point clouds). Our shape functions have parameters that are semantically interpretable and can be modified to produce plausible shape variations. We show that this allows inferred programs to be successfully manipulated by an LLM given a text prompt. We evaluate ShapeLib on different datasets and show clear advantages over existing methods and alternative formulations.

### Survey on Single-Image Reflection Removal using Deep Learning Techniques 
[[arxiv](https://arxiv.org/abs/2502.08836)] [[cool](https://papers.cool/arxiv/2502.08836)] [[pdf](https://arxiv.org/pdf/2502.08836)]
> **Authors**: Kangning Yang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Jinlong Li,Chiu Man Ho,Zibo Meng
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The phenomenon of reflection is quite common in digital images, posing significant challenges for various applications such as computer vision, photography, and image processing. Traditional methods for reflection removal often struggle to achieve clean results while maintaining high fidelity and robustness, particularly in real-world scenarios. Over the past few decades, numerous deep learning-based approaches for reflection removal have emerged, yielding impressive results. In this survey, we conduct a comprehensive review of the current literature by focusing on key venues such as ICCV, ECCV, CVPR, NeurIPS, etc., as these conferences and journals have been central to advances in the field. Our review follows a structured paper selection process, and we critically assess both single-stage and two-stage deep learning methods for reflection removal. The contribution of this survey is three-fold: first, we provide a comprehensive summary of the most recent work on single-image reflection removal; second, we outline task hypotheses, current deep learning techniques, publicly available datasets, and relevant evaluation metrics; and third, we identify key challenges and opportunities in deep learning-based reflection removal, highlighting the potential of this rapidly evolving research area.

### DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps 
[[arxiv](https://arxiv.org/abs/2502.08821)] [[cool](https://papers.cool/arxiv/2502.08821)] [[pdf](https://arxiv.org/pdf/2502.08821)]
> **Authors**: Jocelyn Dzuong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 5 pages, 3 figures, submitted to IJCAI 2025 demo track
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.

### SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models 
[[arxiv](https://arxiv.org/abs/2502.08779)] [[cool](https://papers.cool/arxiv/2502.08779)] [[pdf](https://arxiv.org/pdf/2502.08779)]
> **Authors**: Vishal Narnaware,Ashmal Vayani,Rohit Gupta,Sirnam Swetha,Mubarak Shah
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful societal prejudices, undermining the fairness and equity of AI applications. As LMMs grow increasingly influential, addressing and mitigating inherent biases related to stereotypes, harmful generations, and ambiguous assumptions in real-world scenarios has become essential. However, existing datasets evaluating stereotype biases in LMMs often lack diversity and rely on synthetic images, leaving a gap in bias evaluation for real-world visual contexts. To address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most comprehensive framework to date for assessing stereotype biases across nine diverse categories with non-synthetic images. SB-bench rigorously evaluates LMMs through carefully curated, visually grounded scenarios, challenging them to reason accurately about visual stereotypes. It offers a robust evaluation framework featuring real-world visual samples, image variations, and multiple-choice question formats. By introducing visually grounded queries that isolate visual biases from textual ones, SB-bench enables a precise and nuanced assessment of a model's reasoning capabilities across varying levels of difficulty. Through rigorous testing of state-of-the-art open-source and closed-source LMMs, SB-bench provides a systematic approach to assessing stereotype biases in LMMs across key social dimensions. This benchmark represents a significant step toward fostering fairness in AI systems and reducing harmful biases, laying the groundwork for more equitable and socially responsible LMMs. Our code and dataset are publicly available.

### Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal Brain in 3D Ultrasound 
[[arxiv](https://arxiv.org/abs/2502.08774)] [[cool](https://papers.cool/arxiv/2502.08774)] [[pdf](https://arxiv.org/pdf/2502.08774)]
> **Authors**: Joshua Omolegan,Pak Hei Yeung,Madeleine K. Wyburd,Linde Hesse,Monique Haak,Intergrowth-21st Consortium,Ana I. L. Namburete,Nicola K. Dinsdale
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 5 pages, 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Monitoring the growth of subcortical regions of the fetal brain in ultrasound (US) images can help identify the presence of abnormal development. Manually segmenting these regions is a challenging task, but recent work has shown that it can be automated using deep learning. However, applying pretrained models to unseen freehand US volumes often leads to a degradation of performance due to the vast differences in acquisition and alignment. In this work, we first demonstrate that test time adaptation (TTA) can be used to improve model performance in the presence of both real and simulated domain shifts. We further propose a novel TTA method by incorporating a normative atlas as a prior for anatomy. In the presence of various types of domain shifts, we benchmark the performance of different TTA methods and demonstrate the improvements brought by our proposed approach, which may further facilitate automated monitoring of fetal brain development. Our code is available at https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.

### Multispectral Remote Sensing for Weed Detection in West Australian Agricultural Lands 
[[arxiv](https://arxiv.org/abs/2502.08678)] [[cool](https://papers.cool/arxiv/2502.08678)] [[pdf](https://arxiv.org/pdf/2502.08678)]
> **Authors**: Haitian Wang,Muhammad Ibrahim,Yumeng Miao,D ustin Severtson,Atif Mansoor,Ajmal S. Mian
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages, 9 figures, 1 table, Accepted for oral presentation at IEEE 25th International Conference on Digital Image Computing: Techniques and Applications (DICTA 2024). Conference Proceeding: 979-8-3503-7903-7/24/\$31.00 (C) 2024 IEEE
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: The Kondinin region in Western Australia faces significant agricultural challenges due to pervasive weed infestations, causing economic losses and ecological impacts. This study constructs a tailored multispectral remote sensing dataset and an end-to-end framework for weed detection to advance precision agriculture practices. Unmanned aerial vehicles were used to collect raw multispectral data from two experimental areas (E2 and E8) over four years, covering 0.6046 km^{2} and ground truth annotations were created with GPS-enabled vehicles to manually label weeds and crops. The dataset is specifically designed for agricultural applications in Western Australia. We propose an end-to-end framework for weed detection that includes extensive preprocessing steps, such as denoising, radiometric calibration, image alignment, orthorectification, and stitching. The proposed method combines vegetation indices (NDVI, GNDVI, EVI, SAVI, MSAVI) with multispectral channels to form classification features, and employs several deep learning models to identify weeds based on the input features. Among these models, ResNet achieves the highest performance, with a weed detection accuracy of 0.9213, an F1-Score of 0.8735, an mIOU of 0.7888, and an mDC of 0.8865, validating the efficacy of the dataset and the proposed weed detection method.

### Poly-Autoregressive Prediction for Modeling Interactions 
[[arxiv](https://arxiv.org/abs/2502.08646)] [[cool](https://papers.cool/arxiv/2502.08646)] [[pdf](https://arxiv.org/pdf/2502.08646)]
> **Authors**: Neerja Thakkar,Tara Sadjadpour,Jathushan Rajasegaran,Shiry Ginosar,Jitendra Malik
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We introduce a simple framework for predicting the behavior of an agent in multi-agent settings. In contrast to autoregressive (AR) tasks, such as language processing, our focus is on scenarios with multiple agents whose interactions are shaped by physical constraints and internal motivations. To this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego agent's future behavior by reasoning about the ego agent's state history and the past and current states of other interacting agents. At its core, PAR represents the behavior of all agents as a sequence of tokens, each representing an agent's state at a specific timestep. With minimal data pre-processing changes, we show that PAR can be applied to three different problems: human action forecasting in social situations, trajectory prediction for autonomous vehicles, and object pose forecasting during hand-object interaction. Using a small proof-of-concept transformer backbone, PAR outperforms AR across these three scenarios. The project website can be found at https://neerja.me/PAR/.

### SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation 
[[arxiv](https://arxiv.org/abs/2502.08642)] [[cool](https://papers.cool/arxiv/2502.08642)] [[pdf](https://arxiv.org/pdf/2502.08642)]
> **Authors**: Ellie Arar,Yarden Frenkel,Daniel Cohen-Or,Ariel Shamir,Yael Vinker
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: https://swiftsketch.github.io/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce SwiftSketch, a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a synthetic dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style.

### Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion 
[[arxiv](https://arxiv.org/abs/2502.08560)] [[cool](https://papers.cool/arxiv/2502.08560)] [[pdf](https://arxiv.org/pdf/2502.08560)]
> **Authors**: Lemuel Puglisi,Daniel C. Alexander,Daniele Ravì
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: arXiv admin note: text overlap with arXiv:2405.03328
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: https://github.com/LemuelPuglisi/BrLP.

### Human-Centric Foundation Models: Perception, Generation and Agentic Modeling 
[[arxiv](https://arxiv.org/abs/2502.08556)] [[cool](https://papers.cool/arxiv/2502.08556)] [[pdf](https://arxiv.org/pdf/2502.08556)]
> **Authors**: Shixiang Tang,Yizhou Wang,Lu Chen,Yuan Wang,Sida Peng,Dan Xu,Wanli Ouyang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **Abstract**: Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling.

### Copula-based mixture model identification for subgroup clustering with imaging applications 
[[arxiv](https://arxiv.org/abs/2502.08549)] [[cool](https://papers.cool/arxiv/2502.08549)] [[pdf](https://arxiv.org/pdf/2502.08549)]
> **Authors**: Fei Zheng,Nicolas Duchateau
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Model-based clustering techniques have been widely applied to various application areas, while most studies focus on canonical mixtures with unique component distribution form. However, this strict assumption is often hard to satisfy. In this paper, we consider the more flexible Copula-Based Mixture Models (CBMMs) for clustering, which allow heterogeneous component distributions composed by flexible choices of marginal and copula forms. More specifically, we propose an adaptation of the Generalized Iterative Conditional Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner, where the marginal and copula forms and their parameters are estimated iteratively. GICE is adapted from its original version developed for switching Markov model identification with the choice of realization time. Our CBMM-GICE clustering method is then tested on synthetic two-cluster data (N=2000 samples) with discussion of the factors impacting its convergence. Finally, it is compared to the Expectation Maximization identified mixture models with unique component form on the entire MNIST database (N=70000), and on real cardiac magnetic resonance data (N=276) to illustrate its value for imaging applications.

### Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval 
[[arxiv](https://arxiv.org/abs/2502.08544)] [[cool](https://papers.cool/arxiv/2502.08544)] [[pdf](https://arxiv.org/pdf/2502.08544)]
> **Authors**: Kevin Flanagan,Dima Damen,Michael Wray
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 16 pages, 9 figures. Accepted at WACV 2025. Paper webpage: https://keflanagan.github.io/Moment-of-Untruth
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Video Moment Retrieval is a common task to evaluate the performance of visual-language models - it involves localising start and end times of moments in videos from query sentences. The current task formulation assumes that the queried moment is present in the video, resulting in false positive moment predictions when irrelevant query sentences are provided. In this paper we propose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which considers both moment retrieval accuracy and negative query rejection accuracy. We make the distinction between In-Domain and Out-of-Domain negative queries and provide new evaluation benchmarks for two popular video moment retrieval datasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA video moment retrieval approaches to adapt to Negative-Aware Video Moment Retrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle NA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\%$) scores while retaining moment retrieval scores to within $3.87\%$ Recall@1. Dataset splits and code are available at https://github.com/keflanagan/MomentofUntruth

### A Survey on Image Quality Assessment: Insights, Analysis, and Future Outlook 
[[arxiv](https://arxiv.org/abs/2502.08540)] [[cool](https://papers.cool/arxiv/2502.08540)] [[pdf](https://arxiv.org/pdf/2502.08540)]
> **Authors**: Chengqian Ma,Zhengyi Shi,Zhiqiang Lu,Shenghao Xie,Fei Chao,Yao Sui
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Image quality assessment (IQA) represents a pivotal challenge in image-focused technologies, significantly influencing the advancement trajectory of image processing and computer vision. Recently, IQA has witnessed a notable surge in innovative research efforts, driven by the emergence of novel architectural paradigms and sophisticated computational techniques. This survey delivers an extensive analysis of contemporary IQA methodologies, organized according to their application scenarios, serving as a beneficial reference for both beginners and experienced researchers. We analyze the advantages and limitations of current approaches and suggest potential future research pathways. The survey encompasses both general and specific IQA methodologies, including conventional statistical measures, machine learning techniques, and cutting-edge deep learning models such as convolutional neural networks (CNNs) and Transformer models. The analysis within this survey highlights the necessity for distortion-specific IQA methods tailored to various application scenarios, emphasizing the significance of practicality, interpretability, and ease of implementation in future developments.

### mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data 
[[arxiv](https://arxiv.org/abs/2502.08468)] [[cool](https://papers.cool/arxiv/2502.08468)] [[pdf](https://arxiv.org/pdf/2502.08468)]
> **Authors**: Haonan Chen,Liang Wang,Nan Yang,Yutao Zhu,Ziliang Zhao,Furu Wei,Zhicheng Dou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in https://github.com/haon-chen/mmE5.

### Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions 
[[arxiv](https://arxiv.org/abs/2502.08438)] [[cool](https://papers.cool/arxiv/2502.08438)] [[pdf](https://arxiv.org/pdf/2502.08438)]
> **Authors**: Prajwal Gatti,Kshitij Parikh,Dhriti Prasanna Paul,Manish Gupta,Anand Mishra
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted at AAAI 2024, 9 pages. Project Website: https://vl2g.github.io/projects/cstbir
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,信息检索,多媒体
- **Abstract**: Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for numbats. Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., numbat digging in the ground. In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of difficult-to-name but easy-to-draw objects and text describing difficult-to-sketch but easy-to-verbalize object attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at our project website.

### Handwritten Text Recognition: A Survey 
[[arxiv](https://arxiv.org/abs/2502.08417)] [[cool](https://papers.cool/arxiv/2502.08417)] [[pdf](https://arxiv.org/pdf/2502.08417)]
> **Authors**: Carlos Garrido-Munoz,Antonio Rios-Vila,Jorge Calvo-Zaragoza
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) \emph{up to line-level}, encompassing word and line recognition, and (2) \emph{beyond line-level}, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.

### ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification 
[[arxiv](https://arxiv.org/abs/2502.08391)] [[cool](https://papers.cool/arxiv/2502.08391)] [[pdf](https://arxiv.org/pdf/2502.08391)]
> **Authors**: Jiangbo Shi,Chen Li,Tieliang Gong,Yefeng Zheng,Huazhu Fu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: CVPR 2024 (Updated version with corrections for typos and errors.)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multiple instance learning (MIL)-based framework has become the mainstream for processing the whole slide image (WSI) with giga-pixel size and hierarchical image context in digital pathology. However, these methods heavily depend on a substantial number of bag-level labels and solely learn from the original slides, which are easily affected by variations in data distribution. Recently, vision language model (VLM)-based methods introduced the language prior by pre-training on large-scale pathological image-text pairs. However, the previous text prompt lacks the consideration of pathological prior knowledge, therefore does not substantially boost the model's performance. Moreover, the collection of such pairs and the pre-training process are very time-consuming and source-intensive.To solve the above problems, we propose a dual-scale vision-language multiple instance learning (ViLa-MIL) framework for whole slide image classification. Specifically, we propose a dual-scale visual descriptive text prompt based on the frozen large language model (LLM) to boost the performance of VLM effectively. To transfer the VLM to process WSI efficiently, for the image branch, we propose a prototype-guided patch decoder to aggregate the patch features progressively by grouping similar patches into the same prototype; for the text branch, we introduce a context-guided text decoder to enhance the text features by incorporating the multi-granular image contexts. Extensive studies on three multi-cancer and multi-center subtyping datasets demonstrate the superiority of ViLa-MIL.

### AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception 
[[arxiv](https://arxiv.org/abs/2502.08374)] [[cool](https://papers.cool/arxiv/2502.08374)] [[pdf](https://arxiv.org/pdf/2502.08374)]
> **Authors**: Yuanhao Huang,Qinfan Zhang,Jiandong Xing,Mengyue Cheng,Haiyang Yu,Yilong Ren,Xiao Xiong
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 27th IEEE International Conference on Intelligent Transportation Systems (ITSC)
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Perception module of Autonomous vehicles (AVs) are increasingly susceptible to be attacked, which exploit vulnerabilities in neural networks through adversarial inputs, thereby compromising the AI safety. Some researches focus on creating covert adversarial samples, but existing global noise techniques are detectable and difficult to deceive the human visual system. This paper introduces a novel adversarial attack method, AdvSwap, which creatively utilizes wavelet-based high-frequency information swapping to generate covert adversarial samples and fool the camera. AdvSwap employs invertible neural network for selective high-frequency information swapping, preserving both forward propagation and data integrity. The scheme effectively removes the original label data and incorporates the guidance image data, producing concealed and robust adversarial samples. Experimental evaluations and comparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can make concealed attacks on common traffic targets. The generates adversarial samples are also difficult to perceive by humans and algorithms. Meanwhile, the method has strong attacking robustness and attacking transferability.

### Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger vision learners for medical image segmentation 
[[arxiv](https://arxiv.org/abs/2502.08347)] [[cool](https://papers.cool/arxiv/2502.08347)] [[pdf](https://arxiv.org/pdf/2502.08347)]
> **Authors**: Fenghe Tang,Qingsong Yao,Wenxin Ma,Chenxu Wu,Zihang Jiang,S. Kevin Zhou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 19 pages, Code: https://github.com/FengheTan9/Hi-End-MAE
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Medical image segmentation remains a formidable challenge due to the label scarcity. Pre-training Vision Transformer (ViT) through masked image modeling (MIM) on large-scale unlabeled medical datasets presents a promising solution, providing both computational efficiency and model generalization for various downstream tasks. However, current ViT-based MIM pre-training frameworks predominantly emphasize local aggregation representations in output layers and fail to exploit the rich representations across different ViT layers that better capture fine-grained semantic information needed for more precise medical downstream tasks. To fill the above gap, we hereby present Hierarchical Encoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training solution, which centers on two key innovations: (1) Encoder-driven reconstruction, which encourages the encoder to learn more informative features to guide the reconstruction of masked patches; and (2) Hierarchical dense decoding, which implements a hierarchical decoding structure to capture rich representations across different layers. We pre-train Hi-End-MAE on a large-scale dataset of 10K CT scans and evaluated its performance across seven public medical image segmentation benchmarks. Extensive experiments demonstrate that Hi-End-MAE achieves superior transfer learning capabilities across various downstream tasks, revealing the potential of ViT in medical imaging applications. The code is available at: https://github.com/FengheTan9/Hi-End-MAE

### Foundation Models in Computational Pathology: A Review of Challenges, Opportunities, and Impact 
[[arxiv](https://arxiv.org/abs/2502.08333)] [[cool](https://papers.cool/arxiv/2502.08333)] [[pdf](https://arxiv.org/pdf/2502.08333)]
> **Authors**: Mohsin Bilal,Aadam,Manahil Raza,Youssef Altherwy,Anas Alsuhaibani,Abdulrahman Abduljabbar,Fahdah Almarshad,Paul Golding,Nasir Rajpoot
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 63 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: From self-supervised, vision-only models to contrastive visual-language frameworks, computational pathology has rapidly evolved in recent years. Generative AI "co-pilots" now demonstrate the ability to mine subtle, sub-visual tissue cues across the cellular-to-pathology spectrum, generate comprehensive reports, and respond to complex user queries. The scale of data has surged dramatically, growing from tens to millions of multi-gigapixel tissue images, while the number of trainable parameters in these models has risen to several billion. The critical question remains: how will this new wave of generative and multi-purpose AI transform clinical diagnostics? In this article, we explore the true potential of these innovations and their integration into clinical practice. We review the rapid progress of foundation models in pathology, clarify their applications and significance. More precisely, we examine the very definition of foundational models, identifying what makes them foundational, general, or multipurpose, and assess their impact on computational pathology. Additionally, we address the unique challenges associated with their development and evaluation. These models have demonstrated exceptional predictive and generative capabilities, but establishing global benchmarks is crucial to enhancing evaluation standards and fostering their widespread clinical adoption. In computational pathology, the broader impact of frontier AI ultimately depends on widespread adoption and societal acceptance. While direct public exposure is not strictly necessary, it remains a powerful tool for dispelling misconceptions, building trust, and securing regulatory support.

### When do they StOP?: A First Step Towards Automatically Identifying Team Communication in the Operating Room 
[[arxiv](https://arxiv.org/abs/2502.08299)] [[cool](https://papers.cool/arxiv/2502.08299)] [[pdf](https://arxiv.org/pdf/2502.08299)]
> **Authors**: Keqi Chen,Lilien Schewski,Vinkle Srivastav,Joël Lavanchy,Didier Mutter,Guido Beldi,Sandra Keller,Nicolas Padoy
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Purpose: Surgical performance depends not only on surgeons' technical skills but also on team communication within and across the different professional groups present during the operation. Therefore, automatically identifying team communication in the OR is crucial for patient safety and advances in the development of computer-assisted surgical workflow analysis and intra-operative support systems. To take the first step, we propose a new task of detecting communication briefings involving all OR team members, i.e. the team Time-out and the StOP?-protocol, by localizing their start and end times in video recordings of surgical operations. Methods: We generate an OR dataset of real surgeries, called Team-OR, with more than one hundred hours of surgical videos captured by the multi-view camera system in the OR. The dataset contains temporal annotations of 33 Time-out and 22 StOP?-protocol activities in total. We then propose a novel group activity detection approach, where we encode both scene context and action features, and use an efficient neural network model to output the results. Results: The experimental results on the Team-OR dataset show that our approach outperforms existing state-of-the-art temporal action detection approaches. It also demonstrates the lack of research on group activities in the OR, proving the significance of our dataset. Conclusion: We investigate the Team Time-Out and the StOP?-protocol in the OR, by presenting the first OR dataset with temporal annotations of group activities protocols, and introducing a novel group activity detection approach that outperforms existing approaches. Code is available at https://github.com/CAMMA-public/Team-OR.

### Fully-Geometric Cross-Attention for Point Cloud Registration 
[[arxiv](https://arxiv.org/abs/2502.08285)] [[cool](https://papers.cool/arxiv/2502.08285)] [[pdf](https://arxiv.org/pdf/2502.08285)]
> **Authors**: Weijie Wang,Guofeng Mei,Jian Zhang,Nicu Sebe,Bruno Lepri,Fabio Poiesi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Point cloud registration approaches often fail when the overlap between point clouds is low due to noisy point correspondences. This work introduces a novel cross-attention mechanism tailored for Transformer-based architectures that tackles this problem, by fusing information from coordinates and features at the super-point level between point clouds. This formulation has remained unexplored primarily because it must guarantee rotation and translation invariance since point clouds reside in different and independent reference frames. We integrate the Gromov-Wasserstein distance into the cross-attention formulation to jointly compute distances between points across different point clouds and account for their geometric structure. By doing so, points from two distinct point clouds can attend to each other under arbitrary rigid transformations. At the point level, we also devise a self-attention mechanism that aggregates the local geometric structure information into point features for fine matching. Our formulation boosts the number of inlier correspondences, thereby yielding more precise registration results compared to state-of-the-art approaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch, KITTI, and 3DCSR datasets.

### UniCoRN: Unified Commented Retrieval Network with LMMs 
[[arxiv](https://arxiv.org/abs/2502.08254)] [[cool](https://papers.cool/arxiv/2502.08254)] [[pdf](https://arxiv.org/pdf/2502.08254)]
> **Authors**: Maximilian Jaritz,Matthieu Guillaumin,Sabine Sternig,Loris Bazzani
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multimodal retrieval methods have limitations in handling complex, compositional queries that require reasoning about the visual content of both the query and the retrieved entities. On the other hand, Large Multimodal Models (LMMs) can answer with language to more complex visual questions, but without the inherent ability to retrieve relevant entities to support their answers. We aim to address these limitations with UniCoRN, a Unified Commented Retrieval Network that combines the strengths of composed multimodal retrieval methods and generative language approaches, going beyond Retrieval-Augmented Generation (RAG). We introduce an entity adapter module to inject the retrieved multimodal entities back into the LMM, so it can attend to them while generating answers and comments. By keeping the base LMM frozen, UniCoRN preserves its original capabilities while being able to perform both retrieval and text generation tasks under a single integrated framework. To assess these new abilities, we introduce the Commented Retrieval task (CoR) and a corresponding dataset, with the goal of retrieving an image that accurately answers a given question and generate an additional textual response that provides further clarification and details about the visual information. We demonstrate the effectiveness of UniCoRN on several datasets showing improvements of +4.5% recall over the state of the art for composed multimodal retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.

### Learning Human Skill Generators at Key-Step Levels 
[[arxiv](https://arxiv.org/abs/2502.08234)] [[cool](https://papers.cool/arxiv/2502.08234)] [[pdf](https://arxiv.org/pdf/2502.08234)]
> **Authors**: Yilu Wu,Chenhui Zhu,Shuai Wang,Hanlin Wang,Jing Wang,Zhaoxiang Zhang,Limin Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We are committed to learning human skill generators at key-step levels. The generation of skills is a challenging endeavor, but its successful implementation could greatly facilitate human skill learning and provide more experience for embodied intelligence. Although current video generation models can synthesis simple and atomic human operations, they struggle with human skills due to their complex procedure process. Human skills involve multi-step, long-duration actions and complex scene transitions, so the existing naive auto-regressive methods for synthesizing long videos cannot generate human skills. To address this, we propose a novel task, the Key-step Skill Generation (KS-Gen), aimed at reducing the complexity of generating human skill videos. Given the initial state and a skill description, the task is to generate video clips of key steps to complete the skill, rather than a full-length video. To support this task, we introduce a carefully curated dataset and define multiple evaluation metrics to assess performance. Considering the complexity of KS-Gen, we propose a new framework for this task. First, a multimodal large language model (MLLM) generates descriptions for key steps using retrieval argument. Subsequently, we use a Key-step Image Generator (KIG) to address the discontinuity between key steps in skill videos. Finally, a video generation model uses these descriptions and key-step images to generate video clips of the key steps with high temporal consistency. We offer a detailed analysis of the results, hoping to provide more insights on human skill generation. All models and data are available at https://github.com/MCG-NJU/KS-Gen.

### TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents 
[[arxiv](https://arxiv.org/abs/2502.08226)] [[cool](https://papers.cool/arxiv/2502.08226)] [[pdf](https://arxiv.org/pdf/2502.08226)]
> **Authors**: Kunal Singh,Shreyas Singh,Mukund Khanna
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages 5 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.

### Riemannian Complex Hermit Positive Definite Convolution Network for Polarimetric SAR Image Classification 
[[arxiv](https://arxiv.org/abs/2502.08137)] [[cool](https://papers.cool/arxiv/2502.08137)] [[pdf](https://arxiv.org/pdf/2502.08137)]
> **Authors**: Junfei Shi,Mengmeng Nie,Yuke Li,Haiyan Jin,Weisi Lin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 9 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Deep learning can learn high-level semantic features in Euclidean space effectively for PolSAR images, while they need to covert the complex covariance matrix into a feature vector or complex-valued vector as the network input. However, the complex covariance matrices are essentially a complex Hermit positive definite (HPD) matrix endowed in Riemannian manifold rather than Euclidean space. The matrix's real and imagery parts are with the same significance, as the imagery part represents the phase information. The matrix vectorization will destroy the geometric structure and manifold characteristics of complex covariance matrices. To learn complex HPD matrices directly, we propose a Riemannian complex HPD convolution network(HPD\_CNN) for PolSAR images. This method consists of a complex HPD unfolding network(HPDnet) and a CV-3DCNN enhanced network. The proposed complex HPDnet defines the HPD mapping, rectifying and the logEig layers to learn geometric features of complex matrices. In addition, a fast eigenvalue decomposition method is designed to reduce computation burden. Finally, a Riemannian-to-Euclidean enhanced network is defined to enhance contextual information for classification. Experimental results on two real PolSSAR datasets demonstrate the proposed method can achieve superior performance than the state-of-the-art methods especially in heterogeneous regions.

### A Survey on Data Curation for Visual Contrastive Learning: Why Crafting Effective Positive and Negative Pairs Matters 
[[arxiv](https://arxiv.org/abs/2502.08134)] [[cool](https://papers.cool/arxiv/2502.08134)] [[pdf](https://arxiv.org/pdf/2502.08134)]
> **Authors**: Shasvat Desai,Debasmita Ghose,Deep Chakraborty
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 9 pages, 3 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Visual contrastive learning aims to learn representations by contrasting similar (positive) and dissimilar (negative) pairs of data samples. The design of these pairs significantly impacts representation quality, training efficiency, and computational cost. A well-curated set of pairs leads to stronger representations and faster convergence. As contrastive pre-training sees wider adoption for solving downstream tasks, data curation becomes essential for optimizing its effectiveness. In this survey, we attempt to create a taxonomy of existing techniques for positive and negative pair curation in contrastive learning, and describe them in detail.

## 计算机与社会(cs.CY:Computers and Society)

### Unlocking Mental Health: Exploring College Students' Well-being through Smartphone Behaviors 
[[arxiv](https://arxiv.org/abs/2502.08766)] [[cool](https://papers.cool/arxiv/2502.08766)] [[pdf](https://arxiv.org/pdf/2502.08766)]
> **Authors**: Wei Xuan,Meghna Roy Chowdhury,Yi Ding,Yixue Zhao
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Published at International Conference on Mobile Software Engineering and Systems (MOBILESoft 2025)
- **标题**: None
- **领域**: 计算机与社会,人机交互,机器学习,软件工程
- **Abstract**: The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.

### LegalScore: Development of a Benchmark for Evaluating AI Models in Legal Career Exams in Brazil 
[[arxiv](https://arxiv.org/abs/2502.08652)] [[cool](https://papers.cool/arxiv/2502.08652)] [[pdf](https://arxiv.org/pdf/2502.08652)]
> **Authors**: Roberto Caparroz,Marcelo Roitman,Beatriz G. Chow,Caroline Giusti,Larissa Torhacs,Pedro A. Sola,João H. M. Diogo,Luiza Balby,Carolina D. L. Vasconcelos,Leonardo R. Caparroz,Albano P. Franco
> **First submission**: 2025-01-17
> **First announcement**: 2025-02-13
> **comment**: Main article 25 pages, Appendices from page 26
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This research introduces LegalScore, a specialized index for assessing how generative artificial intelligence models perform in a selected range of career exams that require a legal background in Brazil. The index evaluates fourteen different types of artificial intelligence models' performance, from proprietary to open-source models, in answering objective questions applied to these exams. The research uncovers the response of the models when applying English-trained large language models to Brazilian legal contexts, leading us to reflect on the importance and the need for Brazil-specific training data in generative artificial intelligence models. Performance analysis shows that while proprietary and most known models achieved better results overall, local and smaller models indicated promising performances due to their Brazilian context alignment in training. By establishing an evaluation framework with metrics including accuracy, confidence intervals, and normalized scoring, LegalScore enables systematic assessment of artificial intelligence performance in legal examinations in Brazil. While the study demonstrates artificial intelligence's potential value for exam preparation and question development, it concludes that significant improvements are needed before AI can match human performance in advanced legal assessments. The benchmark creates a foundation for continued research, highlighting the importance of local adaptation in artificial intelligence development.

### Democratizing AI Governance: Balancing Expertise and Public Participation 
[[arxiv](https://arxiv.org/abs/2502.08651)] [[cool](https://papers.cool/arxiv/2502.08651)] [[pdf](https://arxiv.org/pdf/2502.08651)]
> **Authors**: Lucile Ter-Minassian
> **First submission**: 2025-01-16
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: The development and deployment of artificial intelligence (AI) systems, with their profound societal impacts, raise critical challenges for governance. Historically, technological innovations have been governed by concentrated expertise with limited public input. However, AI's pervasive influence across domains such as healthcare, employment, and justice necessitates inclusive governance approaches. This article explores the tension between expert-led oversight and democratic participation, analyzing models of participatory and deliberative democracy. Using case studies from France and Brazil, we highlight how inclusive frameworks can bridge the gap between technical complexity and public accountability. Recommendations are provided for integrating these approaches into a balanced governance model tailored to the European Union, emphasizing transparency, diversity, and adaptive regulation to ensure that AI governance reflects societal values while maintaining technical rigor. This analysis underscores the importance of hybrid frameworks that unite expertise and public voice in shaping the future of AI policy.

### From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms 
[[arxiv](https://arxiv.org/abs/2502.08166)] [[cool](https://papers.cool/arxiv/2502.08166)] [[pdf](https://arxiv.org/pdf/2502.08166)]
> **Authors**: Jessica Dai,Paula Gradu,Inioluwa Deborah Raji,Benjamin Recht
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,机器学习
- **Abstract**: When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the incident database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Incremental Approximate Single-Source Shortest Paths with Predictions 
[[arxiv](https://arxiv.org/abs/2502.08125)] [[cool](https://papers.cool/arxiv/2502.08125)] [[pdf](https://arxiv.org/pdf/2502.08125)]
> **Authors**: Samuel McCauley,Benjamin Moseley,Aidin Niaparast,Helia Niaparast,Shikha Singh
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: The algorithms-with-predictions framework has been used extensively to develop online algorithms with improved beyond-worst-case competitive ratios. Recently, there is growing interest in leveraging predictions for designing data structures with improved beyond-worst-case running times. In this paper, we study the fundamental data structure problem of maintaining approximate shortest paths in incremental graphs in the algorithms-with-predictions model. Given a sequence $σ$ of edges that are inserted one at a time, the goal is to maintain approximate shortest paths from the source to each vertex in the graph at each time step. Before any edges arrive, the data structure is given a prediction of the online edge sequence $\hatσ$ which is used to ``warm start'' its state. As our main result, we design a learned algorithm that maintains $(1+ε)$-approximate single-source shortest paths, which runs in $\tilde{O}(m η\log W/ε)$ time, where $W$ is the weight of the heaviest edge and $η$ is the prediction error. We show these techniques immediately extend to the all-pairs shortest-path setting as well. Our algorithms are consistent (performing nearly as fast as the offline algorithm) when predictions are nearly perfect, have a smooth degradation in performance with respect to the prediction error and, in the worst case, match the best offline algorithm up to logarithmic factors. As a building block, we study the offline incremental approximate single-source shortest-paths problem. In this problem, the edge sequence $σ$ is known a priori and the goal is to efficiently return the length of the shortest paths in the intermediate graph $G_t$ consisting of the first $t$ edges, for all $t$. Note that the offline incremental problem is defined in the worst-case setting (without predictions) and is of independent interest.

## 新兴技术(cs.ET:Emerging Technologies)

### Scalable Thermodynamic Second-order Optimization 
[[arxiv](https://arxiv.org/abs/2502.08603)] [[cool](https://papers.cool/arxiv/2502.08603)] [[pdf](https://arxiv.org/pdf/2502.08603)]
> **Authors**: Kaelan Donatella,Samuel Duffield,Denis Melanson,Maxwell Aifer,Phoebe Klett,Rajath Salegame,Zach Belateche,Gavin Crooks,Antonio J. Martinez,Patrick J. Coles
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 5 figures
- **标题**: None
- **领域**: 新兴技术,机器学习
- **Abstract**: Many hardware proposals have aimed to accelerate inference in AI workloads. Less attention has been paid to hardware acceleration of training, despite the enormous societal impact of rapid training of AI models. Physics-based computers, such as thermodynamic computers, offer an efficient means to solve key primitives in AI training algorithms. Optimizers that normally would be computationally out-of-reach (e.g., due to expensive matrix inversions) on digital hardware could be unlocked with physics-based hardware. In this work, we propose a scalable algorithm for employing thermodynamic computers to accelerate a popular second-order optimizer called Kronecker-factored approximate curvature (K-FAC). Our asymptotic complexity analysis predicts increasing advantage with our algorithm as $n$, the number of neurons per layer, increases. Numerical experiments show that even under significant quantization noise, the benefits of second-order optimization can be preserved. Finally, we predict substantial speedups for large-scale vision and graph problems based on realistic hardware characteristics.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Auction Design using Value Prediction with Hallucinations 
[[arxiv](https://arxiv.org/abs/2502.08792)] [[cool](https://papers.cool/arxiv/2502.08792)] [[pdf](https://arxiv.org/pdf/2502.08792)]
> **Authors**: Ilan Lobel,Humberto Moreira,Omar Mouchtaki
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,人工智能
- **Abstract**: We investigate a Bayesian mechanism design problem where a seller seeks to maximize revenue by selling an indivisible good to one of n buyers, incorporating potentially unreliable predictions (signals) of buyers' private values derived from a machine learning model. We propose a framework where these signals are sometimes reflective of buyers' true valuations but other times are hallucinations, which are uncorrelated with the buyers' true valuations. Our main contribution is a characterization of the optimal auction under this framework. Our characterization establishes a near-decomposition of how to treat types above and below the signal. For the one buyer case, the seller's optimal strategy is to post one of three fairly intuitive prices depending on the signal, which we call the "ignore", "follow" and "cap" actions.

### Data Pricing for Graph Neural Networks without Pre-purchased Inspection 
[[arxiv](https://arxiv.org/abs/2502.08284)] [[cool](https://papers.cool/arxiv/2502.08284)] [[pdf](https://arxiv.org/pdf/2502.08284)]
> **Authors**: Yiping Liu,Mengxiao Zhang,Jiamou Liu,Song Yang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted by AAMAS-2025
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: Machine learning (ML) models have become essential tools in various scenarios. Their effectiveness, however, hinges on a substantial volume of data for satisfactory performance. Model marketplaces have thus emerged as crucial platforms bridging model consumers seeking ML solutions and data owners possessing valuable data. These marketplaces leverage model trading mechanisms to properly incentive data owners to contribute their data, and return a well performing ML model to the model consumers. However, existing model trading mechanisms often assume the data owners are willing to share their data before being paid, which is not reasonable in real world. Given that, we propose a novel mechanism, named Structural Importance based Model Trading (SIMT) mechanism, that assesses the data importance and compensates data owners accordingly without disclosing the data. Specifically, SIMT procures feature and label data from data owners according to their structural importance, and then trains a graph neural network for model consumers. Theoretically, SIMT ensures incentive compatible, individual rational and budget feasible. The experiments on five popular datasets validate that SIMT consistently outperforms vanilla baselines by up to $40\%$ in both MacroF1 and MicroF1.

## 人机交互(cs.HC:Human-Computer Interaction)

### Exploring Emotion-Sensitive LLM-Based Conversational AI 
[[arxiv](https://arxiv.org/abs/2502.08920)] [[cool](https://papers.cool/arxiv/2502.08920)] [[pdf](https://arxiv.org/pdf/2502.08920)]
> **Authors**: Antonin Brun,Ruying Liu,Aryan Shukla,Frances Watson,Jonathan Gratch
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 7 pages, 2 figures, 1 table
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Conversational AI chatbots have become increasingly common within the customer service industry. Despite improvements in their emotional development, they often lack the authenticity of real customer service interactions or the competence of service providers. By comparing emotion-sensitive and emotion-insensitive LLM-based chatbots across 30 participants, we aim to explore how emotional sensitivity in chatbots influences perceived competence and overall customer satisfaction in service interactions. Additionally, we employ sentiment analysis techniques to analyze and interpret the emotional content of user inputs. We highlight that perceptions of chatbot trustworthiness and competence were higher in the case of the emotion-sensitive chatbot, even if issue resolution rates were not affected. We discuss implications of improved user satisfaction from emotion-sensitive chatbots and potential applications in support services.

### Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies 
[[arxiv](https://arxiv.org/abs/2502.08554)] [[cool](https://papers.cool/arxiv/2502.08554)] [[pdf](https://arxiv.org/pdf/2502.08554)]
> **Authors**: Sunnie S. Y. Kim,Jennifer Wortman Vaughan,Q. Vera Liao,Tania Lombrozo,Olga Russakovsky
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: CHI 2025. This version includes the appendix
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.

### Word Synchronization Challenge: A Benchmark for Word Association Responses for LLMs 
[[arxiv](https://arxiv.org/abs/2502.08312)] [[cool](https://papers.cool/arxiv/2502.08312)] [[pdf](https://arxiv.org/pdf/2502.08312)]
> **Authors**: Tanguy Cazalets,Joni Dambre
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: This paper introduces the Word Synchronization Challenge, a novel benchmark to evaluate large language models (LLMs) in Human-Computer Interaction (HCI). This benchmark uses a dynamic game-like framework to test LLMs ability to mimic human cognitive processes through word associations. By simulating complex human interactions, it assesses how LLMs interpret and align with human thought patterns during conversational exchanges, which are essential for effective social partnerships in HCI. Initial findings highlight the influence of model sophistication on performance, offering insights into the models capabilities to engage in meaningful social interactions and adapt behaviors in human-like ways. This research advances the understanding of LLMs potential to replicate or diverge from human cognitive functions, paving the way for more nuanced and empathetic human-machine collaborations.

## 信息检索(cs.IR:Information Retrieval)

### QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval 
[[arxiv](https://arxiv.org/abs/2502.08557)] [[cool](https://papers.cool/arxiv/2502.08557)] [[pdf](https://arxiv.org/pdf/2502.08557)]
> **Authors**: Wonduk Seo,Seunghyun Lee
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages
- **标题**: None
- **领域**: 信息检索,计算语言学,机器学习,多代理系统
- **Abstract**: Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce QA-Expand, a novel and effective framework for query expansion. It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further rewrites and filters these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up to 13% over state-of-the-art methods, offering a robust solution for modern retrieval challenges.

### Fine-Tuning Topics through Weighting Aspect Keywords 
[[arxiv](https://arxiv.org/abs/2502.08496)] [[cool](https://papers.cool/arxiv/2502.08496)] [[pdf](https://arxiv.org/pdf/2502.08496)]
> **Authors**: Ali Nazari,Michael Weiss
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 8 figures, 3 tables
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Topic modeling often requires examining topics from multiple perspectives to uncover hidden patterns, especially in less explored areas. This paper presents an approach to address this need, utilizing weighted keywords from various aspects derived from a domain knowledge. The research method starts with standard topic modeling. Then, it adds a process consisting of four key steps. First, it defines keywords for each aspect. Second, it gives weights to these keywords based on their relevance. Third, it calculates relevance scores for aspect-weighted keywords and topic keywords to create aspect-topic models. Fourth, it uses these scores to tune relevant new documents. Finally, the generated topic models are interpreted and validated. The findings show that top-scoring documents are more likely to be about the same aspect of a topic. This highlights the model's effectiveness in finding the related documents to the aspects.

### Graph Foundation Models for Recommendation: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.08346)] [[cool](https://papers.cool/arxiv/2502.08346)] [[pdf](https://arxiv.org/pdf/2502.08346)]
> **Authors**: Bin Wu,Yihang Wang,Yuanhao Zeng,Jiawei Liu,Jiashu Zhao,Cheng Yang,Yawen Li,Long Xia,Dawei Yin,Chuan Shi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.

### MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural Network for Recommendation 
[[arxiv](https://arxiv.org/abs/2502.08161)] [[cool](https://papers.cool/arxiv/2502.08161)] [[pdf](https://arxiv.org/pdf/2502.08161)]
> **Authors**: Xiangjin Xie,Yuxin Chen,Ruipeng Wang,Kai Ouyang,Zihan Zhang,Hai-Tao Zheng,Buyue Qian,Hansen Zheng,Bo Hu,Chengxiang Zhuo,Zang Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 10 pages, 6 figures
- **标题**: None
- **领域**: 信息检索,人工智能
- **Abstract**: Graph neural networks have been widely used in recent recommender systems, where negative sampling plays an important role. Existing negative sampling methods restrict the relationship between nodes as either hard positive pairs or hard negative pairs. This leads to the loss of structural information, and lacks the mechanism to generate positive pairs for nodes with few neighbors. To overcome limitations, we propose a novel soft link-based sampling method, namely MixDec Sampling, which consists of Mixup Sampling module and Decay Sampling module. The Mixup Sampling augments node features by synthesizing new nodes and soft links, which provides sufficient number of samples for nodes with few neighbors. The Decay Sampling strengthens the digestion of graph structure information by generating soft links for node embedding learning. To the best of our knowledge, we are the first to model sampling relationships between nodes by soft links in GNN-based recommender systems. Extensive experiments demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation performance of several representative GNN-based models on various recommendation benchmarks.

### SS4Rec: Continuous-Time Sequential Recommendation with State Space Models 
[[arxiv](https://arxiv.org/abs/2502.08132)] [[cool](https://papers.cool/arxiv/2502.08132)] [[pdf](https://arxiv.org/pdf/2502.08132)]
> **Authors**: Wei Xiao,Huiying Wang,Qifeng Zhou,Qing Wang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: Sequential recommendation is a key area in the field of recommendation systems aiming to model user interest based on historical interaction sequences with irregular intervals. While previous recurrent neural network-based and attention-based approaches have achieved significant results, they have limitations in capturing system continuity due to the discrete characteristics. In the context of continuous-time modeling, state space model (SSM) offers a potential solution, as it can effectively capture the dynamic evolution of user interest over time. However, existing SSM-based approaches ignore the impact of irregular time intervals within historical user interactions, making it difficult to model complexed user-item transitions in sequences. To address this issue, we propose a hybrid SSM-based model called SS4Rec for continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM to handle irregular time intervals and a relation-aware SSM to model contextual dependencies, enabling it to infer user interest from both temporal and sequential perspectives. In the training process, the time-aware SSM and the relation-aware SSM are discretized by variable stepsizes according to user interaction time intervals and input data, respectively. This helps capture the continuous dependency from irregular time intervals and provides time-specific personalized recommendations. Experimental studies on five benchmark datasets demonstrate the superiority and effectiveness of SS4Rec.

## 机器学习(cs.LG:Machine Learning)

### Modeling Time-evolving Causality over Data Streams 
[[arxiv](https://arxiv.org/abs/2502.08963)] [[cool](https://papers.cool/arxiv/2502.08963)] [[pdf](https://arxiv.org/pdf/2502.08963)]
> **Authors**: Naoki Chihara,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted by KDD'25
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Given an extensive, semi-infinite collection of multivariate coevolving data sequences (e.g., sensor/web activity streams) whose observations influence each other, how can we discover the time-changing cause-and-effect relationships in co-evolving data streams? How efficiently can we reveal dynamical patterns that allow us to forecast future values? In this paper, we present a novel streaming method, ModePlait, which is designed for modeling such causal relationships (i.e., time-evolving causality) in multivariate co-evolving data streams and forecasting their future values. The solution relies on characteristics of the causal relationships that evolve over time in accordance with the dynamic changes of exogenous variables. ModePlait has the following properties: (a) Effective: it discovers the time-evolving causality in multivariate co-evolving data streams by detecting the transitions of distinct dynamical patterns adaptively. (b) Accurate: it enables both the discovery of time-evolving causality and the forecasting of future values in a streaming fashion. (c) Scalable: our algorithm does not depend on data stream length and thus is applicable to very large sequences. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of discovering the time-evolving causality as well as forecasting.

### A Comprehensive Survey on Imbalanced Data Learning 
[[arxiv](https://arxiv.org/abs/2502.08960)] [[cool](https://papers.cool/arxiv/2502.08960)] [[pdf](https://arxiv.org/pdf/2502.08960)]
> **Authors**: Xinyi Gao,Dongting Xie,Yihang Zhang,Zhengren Wang,Conghui He,Hongzhi Yin,Wentao Zhang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: With the expansion of data availability, machine learning (ML) has achieved remarkable breakthroughs in both academia and industry. However, imbalanced data distributions are prevalent in various types of raw data and severely hinder the performance of ML by biasing the decision-making processes. To deepen the understanding of imbalanced data and facilitate the related research and applications, this survey systematically analyzing various real-world data formats and concludes existing researches for different data formats into four distinct categories: data re-balancing, feature representation, training strategy, and ensemble learning. This structured analysis help researchers comprehensively understand the pervasive nature of imbalance across diverse data format, thereby paving a clearer path toward achieving specific research goals. we provide an overview of relevant open-source libraries, spotlight current challenges, and offer novel insights aimed at fostering future advancements in this critical area of study.

### Biologically Plausible Brain Graph Transformer 
[[arxiv](https://arxiv.org/abs/2502.08958)] [[cool](https://papers.cool/arxiv/2502.08958)] [[pdf](https://arxiv.org/pdf/2502.08958)]
> **Authors**: Ciyuan Peng,Yuelong Huang,Qichao Dong,Shuo Yu,Feng Xia,Chengqi Zhang,Yaochu Jin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 27pages, 16figures, published as a conference paper at ICLR 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: State-of-the-art brain graph analysis methods fail to fully encode the small-world architecture of brain graphs (accompanied by the presence of hubs and functional modules), and therefore lack biological plausibility to some extent. This limitation hinders their ability to accurately represent the brain's structural and functional properties, thereby restricting the effectiveness of machine learning models in tasks such as brain disorder detection. In this work, we propose a novel Biologically Plausible Brain Graph Transformer (BioBGT) that encodes the small-world architecture inherent in brain graphs. Specifically, we present a network entanglement-based node importance encoding technique that captures the structural importance of nodes in global information propagation during brain graph communication, highlighting the biological properties of the brain structure. Furthermore, we introduce a functional module-aware self-attention to preserve the functional segregation and integration characteristics of brain graphs in the learned representations. Experimental results on three benchmark datasets demonstrate that BioBGT outperforms state-of-the-art models, enhancing biologically plausible brain graph representations for various brain graph analytical tasks

### Self-Supervised Graph Contrastive Pretraining for Device-level Integrated Circuits 
[[arxiv](https://arxiv.org/abs/2502.08949)] [[cool](https://papers.cool/arxiv/2502.08949)] [[pdf](https://arxiv.org/pdf/2502.08949)]
> **Authors**: Sungyoung Lee,Ziyi Wang,Seunggeun Kim,Taekyun Lee,David Z. Pan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Self-supervised graph representation learning has driven significant advancements in domains such as social network analysis, molecular design, and electronics design automation (EDA). However, prior works in EDA have mainly focused on the representation of gate-level digital circuits, failing to capture analog and mixed-signal circuits. To address this gap, we introduce DICE: Device-level Integrated Circuits Encoder, the first self-supervised pretrained graph neural network (GNN) model for any circuit expressed at the device level. DICE is a message-passing neural network (MPNN) trained through graph contrastive learning, and its pretraining process is simulation-free, incorporating two novel data augmentation techniques. Experimental results demonstrate that DICE achieves substantial performance gains across three downstream tasks, underscoring its effectiveness for both analog and digital circuits.

### Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative 
[[arxiv](https://arxiv.org/abs/2502.08942)] [[cool](https://papers.cool/arxiv/2502.08942)] [[pdf](https://arxiv.org/pdf/2502.08942)]
> **Authors**: Zihao Li,Xiao Lin,Zhining Liu,Jiaru Zou,Ziwei Wu,Lecheng Zheng,Dongqi Fu,Yada Zhu,Hendrik Hamann,Hanghang Tong,Jingrui He
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Preprint, 37 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. Consequently, effectively integrating the text modality remains challenging. In this work, we highlight an intuitive yet significant observation that has been overlooked by existing works: time-series-paired texts exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance and achieve outperformance without modifying model architectures.

### Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation 
[[arxiv](https://arxiv.org/abs/2502.08941)] [[cool](https://papers.cool/arxiv/2502.08941)] [[pdf](https://arxiv.org/pdf/2502.08941)]
> **Authors**: Han-Dong Lim,Donghwan Lee
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Removed colored text. arXiv admin note: substantial text overlap with arXiv:2402.15781
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad'' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.

### Reevaluating Policy Gradient Methods for Imperfect-Information Games 
[[arxiv](https://arxiv.org/abs/2502.08938)] [[cool](https://papers.cool/arxiv/2502.08938)] [[pdf](https://arxiv.org/pdf/2502.08938)]
> **Authors**: Max Rudolph,Nathan Lichtle,Sobhan Mohammadpour,Alexandre Bayen,J. Zico Kolter,Amy Zhang,Gabriele Farina,Eugene Vinitsky,Samuel Sokota
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In the past decade, motivated by the putative failure of naive self-play deep reinforcement learning (DRL) in adversarial imperfect-information games, researchers have developed numerous DRL algorithms based on fictitious play (FP), double oracle (DO), and counterfactual regret minimization (CFR). In light of recent results of the magnetic mirror descent algorithm, we hypothesize that simpler generic policy gradient methods like PPO are competitive with or superior to these FP, DO, and CFR-based DRL approaches. To facilitate the resolution of this hypothesis, we implement and release the first broadly accessible exact exploitability computations for four large games. Using these games, we conduct the largest-ever exploitability comparison of DRL algorithms for imperfect-information games. Over 5600 training runs, FP, DO, and CFR-based approaches fail to outperform generic policy gradient methods. Code is available at https://github.com/nathanlct/IIG-RL-Benchmark and https://github.com/gabrfarina/exp-a-spiel .

### AutoLike: Auditing Social Media Recommendations through User Interactions 
[[arxiv](https://arxiv.org/abs/2502.08933)] [[cool](https://papers.cool/arxiv/2502.08933)] [[pdf](https://arxiv.org/pdf/2502.08933)]
> **Authors**: Hieu Le,Salma Elmalaki,Zubair Shafiq,Athina Markopoulou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 6 figures, 3 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern social media platforms, such as TikTok, Facebook, and YouTube, rely on recommendation systems to personalize content for users based on user interactions with endless streams of content, such as "For You" pages. However, these complex algorithms can inadvertently deliver problematic content related to self-harm, mental health, and eating disorders. We introduce AutoLike, a framework to audit recommendation systems in social media platforms for topics of interest and their sentiments. To automate the process, we formulate the problem as a reinforcement learning problem. AutoLike drives the recommendation system to serve a particular type of content through interactions (e.g., liking). We apply the AutoLike framework to the TikTok platform as a case study. We evaluate how well AutoLike identifies TikTok content automatically across nine topics of interest; and conduct eight experiments to demonstrate how well it drives TikTok's recommendation system towards particular topics and sentiments. AutoLike has the potential to assist regulators in auditing recommendation systems for problematic content. (Warning: This paper contains qualitative examples that may be viewed as offensive or harmful.)

### Escaping Collapse: The Strength of Weak Data for Large Language Model Training 
[[arxiv](https://arxiv.org/abs/2502.08924)] [[cool](https://papers.cool/arxiv/2502.08924)] [[pdf](https://arxiv.org/pdf/2502.08924)]
> **Authors**: Kareem Amin,Sara Babakniya,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even "collapse", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. We find that the requirements are nearly minimal. We describe a training procedure that converges to an optimal LLM even if almost all of the non-synthetic training data is of poor quality. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. Our training procedure subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples -- in much the same way that boosting focuses the efforts of the weak learner -- leads to improved performance.

### CLEAR: Cluster-based Prompt Learning on Heterogeneous Graphs 
[[arxiv](https://arxiv.org/abs/2502.08918)] [[cool](https://papers.cool/arxiv/2502.08918)] [[pdf](https://arxiv.org/pdf/2502.08918)]
> **Authors**: Feiyang Wang,Zhongbao Zhang,Junda Ye,Li Sun,Jianzhong Qi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: accepted by PAKDD 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Prompt learning has attracted increasing attention in the graph domain as a means to bridge the gap between pretext and downstream tasks. Existing studies on heterogeneous graph prompting typically use feature prompts to modify node features for specific downstream tasks, which do not concern the structure of heterogeneous graphs. Such a design also overlooks information from the meta-paths, which are core to learning the high-order semantics of the heterogeneous graphs. To address these issues, we propose CLEAR, a Cluster-based prompt LEARNING model on heterogeneous graphs. We present cluster prompts that reformulate downstream tasks as heterogeneous graph reconstruction. In this way, we align the pretext and downstream tasks to share the same training objective. Additionally, our cluster prompts are also injected into the meta-paths such that the prompt learning process incorporates high-order semantic information entailed by the meta-paths. Extensive experiments on downstream tasks confirm the superiority of CLEAR. It consistently outperforms state-of-the-art models, achieving up to 5% improvement on the F1 metric for node classification.

### Linear-Time User-Level DP-SCO via Robust Statistics 
[[arxiv](https://arxiv.org/abs/2502.08889)] [[cool](https://papers.cool/arxiv/2502.08889)] [[pdf](https://arxiv.org/pdf/2502.08889)]
> **Authors**: Badih Ghazi,Ravi Kumar,Daogao Liu,Pasin Manurangsi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,数据结构和算法,机器学习
- **Abstract**: User-level differentially private stochastic convex optimization (DP-SCO) has garnered significant attention due to the paramount importance of safeguarding user privacy in modern large-scale machine learning applications. Current methods, such as those based on differentially private stochastic gradient descent (DP-SGD), often struggle with high noise accumulation and suboptimal utility due to the need to privatize every intermediate iterate. In this work, we introduce a novel linear-time algorithm that leverages robust statistics, specifically the median and trimmed mean, to overcome these challenges. Our approach uniquely bounds the sensitivity of all intermediate iterates of SGD with gradient estimation based on robust statistics, thereby significantly reducing the gradient estimation noise for privacy purposes and enhancing the privacy-utility trade-off. By sidestepping the repeated privatization required by previous methods, our algorithm not only achieves an improved theoretical privacy-utility trade-off but also maintains computational efficiency. We complement our algorithm with an information-theoretic lower bound, showing that our upper bound is optimal up to logarithmic factors and the dependence on $ε$. This work sets the stage for more robust and efficient privacy-preserving techniques in machine learning, with implications for future research and application in the field.

### 2D Integrated Bayesian Tomography of Plasma Electron Density Profile for HL-3 Based on Gaussian Process 
[[arxiv](https://arxiv.org/abs/2502.08882)] [[cool](https://papers.cool/arxiv/2502.08882)] [[pdf](https://arxiv.org/pdf/2502.08882)]
> **Authors**: Cong Wang,Renjie Yang,Dong Li,Zongyu Yang,Zhijun Wang,Yixiong Wei,Jing Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper introduces an integrated Bayesian model that combines line integral measurements and point values using Gaussian Process (GP). The proposed method leverages Gaussian Process Regression (GPR) to incorporate point values into 2D profiles and employs coordinate mapping to integrate magnetic flux information for 2D inversion. The average relative error of the reconstructed profile, using the integrated Bayesian tomography model with normalized magnetic flux, is as low as 3.60*10^(-4). Additionally, sensitivity tests were conducted on the number of grids, the standard deviation of synthetic diagnostic data, and noise levels, laying a solid foundation for the application of the model to experimental data. This work not only achieves accurate 2D inversion using the integrated Bayesian model but also provides a robust framework for decoupling pressure information from equilibrium reconstruction, thus making it possible to optimize equilibrium reconstruction using inversion results.

### WENDy for Nonlinear-in-Parameters ODEs 
[[arxiv](https://arxiv.org/abs/2502.08881)] [[cool](https://papers.cool/arxiv/2502.08881)] [[pdf](https://arxiv.org/pdf/2502.08881)]
> **Authors**: Nic Rummel,Daniel A. Messenger,Stephen Becker,Vanja Dukic,David M. Bortz
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended to accommodate systems of ordinary differential equations that are nonlinear-in-parameters. The extension rests on derived analytic expressions for a likelihood function, its gradient and its Hessian matrix. WENDy makes use of these to approximate a maximum likelihood estimator based on optimization routines suited for non-convex optimization problems. The resulting parameter estimation algorithm has better accuracy, a substantially larger domain of convergence, and is often orders of magnitude faster than the conventional output error least squares method (based on forward solvers). The algorithm is efficiently implemented in Julia. We demonstrate the algorithm's ability to accommodate the weak form optimization for both additive normal and multiplicative log-normal noise, and present results on a suite of benchmark systems of ordinary differential equations. In order to demonstrate the practical benefits of our approach, we present extensive comparisons between our method and output error methods in terms of accuracy, precision, bias, and coverage.

### Robust Graph-Based Semi-Supervised Learning via $p$-Conductances 
[[arxiv](https://arxiv.org/abs/2502.08873)] [[cool](https://papers.cool/arxiv/2502.08873)] [[pdf](https://arxiv.org/pdf/2502.08873)]
> **Authors**: Sawyer Jack Robertson,Chester Holtz,Zhengchao Wan,Gal Mishne,Alexander Cloninger
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 29 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,离散数学,优化与控制
- **Abstract**: We study the problem of semi-supervised learning on graphs in the regime where data labels are scarce or possibly corrupted. We propose an approach called $p$-conductance learning that generalizes the $p$-Laplace and Poisson learning methods by introducing an objective reminiscent of $p$-Laplacian regularization and an affine relaxation of the label constraints. This leads to a family of probability measure mincut programs that balance sparse edge removal with accurate distribution separation. Our theoretical analysis connects these programs to well-known variational and probabilistic problems on graphs (including randomized cuts, effective resistance, and Wasserstein distance) and provides motivation for robustness when labels are diffused via the heat kernel. Computationally, we develop a semismooth Newton-conjugate gradient algorithm and extend it to incorporate class-size estimates when converting the continuous solutions into label assignments. Empirical results on computer vision and citation datasets demonstrate that our approach achieves state-of-the-art accuracy in low label-rate, corrupted-label, and partial-label regimes.

### When and why randomised exploration works (in linear bandits) 
[[arxiv](https://arxiv.org/abs/2502.08870)] [[cool](https://papers.cool/arxiv/2502.08870)] [[pdf](https://arxiv.org/pdf/2502.08870)]
> **Authors**: Marc Abeille,David Janz,Ciara Pike-Burke
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We provide an approach for the analysis of randomised exploration algorithms like Thompson sampling that does not rely on forced optimism or posterior inflation. With this, we demonstrate that in the $d$-dimensional linear bandit setting, when the action space is smooth and strongly convex, randomised exploration algorithms enjoy an $n$-step regret bound of the order $O(d\sqrt{n} \log(n))$. Notably, this shows for the first time that there exist non-trivial linear bandit settings where Thompson sampling can achieve optimal dimension dependence in the regret.

### Harnessing Vision Models for Time Series Analysis: A Survey 
[[arxiv](https://arxiv.org/abs/2502.08869)] [[cool](https://papers.cool/arxiv/2502.08869)] [[pdf](https://arxiv.org/pdf/2502.08869)]
> **Authors**: Jingchao Ni,Ziming Zhao,ChengAo Shen,Hanghang Tong,Dongjin Song,Wei Cheng,Dongsheng Luo,Haifeng Chen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.

### A Systematic Evaluation of Generative Models on Tabular Transportation Data 
[[arxiv](https://arxiv.org/abs/2502.08856)] [[cool](https://papers.cool/arxiv/2502.08856)] [[pdf](https://arxiv.org/pdf/2502.08856)]
> **Authors**: Chengen Wang,Alvaro Cardenas,Gurcan Comert,Murat Kantarcioglu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The sharing of large-scale transportation data is beneficial for transportation planning and policymaking. However, it also raises significant security and privacy concerns, as the data may include identifiable personal information, such as individuals' home locations. To address these concerns, synthetic data generation based on real transportation data offers a promising solution that allows privacy protection while potentially preserving data utility. Although there are various synthetic data generation techniques, they are often not tailored to the unique characteristics of transportation data, such as the inherent structure of transportation networks formed by all trips in the datasets. In this paper, we use New York City taxi data as a case study to conduct a systematic evaluation of the performance of widely used tabular data generative models. In addition to traditional metrics such as distribution similarity, coverage, and privacy preservation, we propose a novel graph-based metric tailored specifically for transportation data. This metric evaluates the similarity between real and synthetic transportation networks, providing potentially deeper insights into their structural and functional alignment. We also introduced an improved privacy metric to address the limitations of the commonly-used one. Our experimental results reveal that existing tabular data generative models often fail to perform as consistently as claimed in the literature, particularly when applied to transportation data use cases. Furthermore, our novel graph metric reveals a significant gap between synthetic and real data. This work underscores the potential need to develop generative models specifically tailored to take advantage of the unique characteristics of emerging domains, such as transportation.

### A Reversible Solver for Diffusion SDEs 
[[arxiv](https://arxiv.org/abs/2502.08834)] [[cool](https://papers.cool/arxiv/2502.08834)] [[pdf](https://arxiv.org/pdf/2502.08834)]
> **Authors**: Zander W. Blasingame,Chen Liu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Diffusion models have quickly become the state-of-the-art for generation tasks across many different data modalities. An important ability of diffusion models is the ability to encode samples from the data distribution back into the sampling prior distribution. This is useful for performing alterations to real data samples along with guided generation via the continuous adjoint equations. We propose an algebraically reversible solver for diffusion SDEs that can exactly invert real data samples into the prior distribution.

### PLayer-FL: A Principled Approach to Personalized Layer-wise Cross-Silo Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.08829)] [[cool](https://papers.cool/arxiv/2502.08829)] [[pdf](https://arxiv.org/pdf/2502.08829)]
> **Authors**: Ahmed Elhussein,Gamze Gürsoy
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Non-identically distributed data is a major challenge in Federated Learning (FL). Personalized FL tackles this by balancing local model adaptation with global model consistency. One variant, partial FL, leverages the observation that early layers learn more transferable features by federating only early layers. However, current partial FL approaches use predetermined, architecture-specific rules to select layers, limiting their applicability. We introduce Principled Layer-wise-FL (PLayer-FL), which uses a novel federation sensitivity metric to identify layers that benefit from federation. This metric, inspired by model pruning, quantifies each layer's contribution to cross-client generalization after the first training epoch, identifying a transition point in the network where the benefits of federation diminish. We first demonstrate that our federation sensitivity metric shows strong correlation with established generalization measures across diverse architectures. Next, we show that PLayer-FL outperforms existing FL algorithms on a range of tasks, also achieving more uniform performance improvements across clients.

### A Survey on Data-Centric AI: Tabular Learning from Reinforcement Learning and Generative AI Perspective 
[[arxiv](https://arxiv.org/abs/2502.08828)] [[cool](https://papers.cool/arxiv/2502.08828)] [[pdf](https://arxiv.org/pdf/2502.08828)]
> **Authors**: Wangyang Ying,Cong Wei,Nanxu Gong,Xinyuan Wang,Haoyue Bai,Arun Vignesh Malarkkan,Sixun Dong,Dongjie Wang,Denghui Zhang,Yanjie Fu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Tabular data is one of the most widely used data formats across various domains such as bioinformatics, healthcare, and marketing. As artificial intelligence moves towards a data-centric perspective, improving data quality is essential for enhancing model performance in tabular data-driven applications. This survey focuses on data-driven tabular data optimization, specifically exploring reinforcement learning (RL) and generative approaches for feature selection and feature generation as fundamental techniques for refining data spaces. Feature selection aims to identify and retain the most informative attributes, while feature generation constructs new features to better capture complex data patterns. We systematically review existing generative methods for tabular data engineering, analyzing their latest advancements, real-world applications, and respective strengths and limitations. This survey emphasizes how RL-based and generative techniques contribute to the automation and intelligence of feature engineering. Finally, we summarize the existing challenges and discuss future research directions, aiming to provide insights that drive continued innovation in this field.

### A First-order Generative Bilevel Optimization Framework for Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.08808)] [[cool](https://papers.cool/arxiv/2502.08808)] [[pdf](https://arxiv.org/pdf/2502.08808)]
> **Authors**: Quan Xiao,Hui Yuan,A F M Saif,Gaowen Liu,Ramana Kompella,Mengdi Wang,Tianyi Chen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Diffusion models, which iteratively denoise data samples to synthesize high-quality outputs, have achieved empirical success across domains. However, optimizing these models for downstream tasks often involves nested bilevel structures, such as tuning hyperparameters for fine-tuning tasks or noise schedules in training dynamics, where traditional bilevel methods fail due to the infinite-dimensional probability space and prohibitive sampling costs. We formalize this challenge as a generative bilevel optimization problem and address two key scenarios: (1) fine-tuning pre-trained models via an inference-only lower-level solver paired with a sample-efficient gradient estimator for the upper level, and (2) training diffusion models from scratch with noise schedule optimization by reparameterizing the lower-level problem and designing a computationally tractable gradient estimator. Our first-order bilevel framework overcomes the incompatibility of conventional bilevel methods with diffusion processes, offering theoretical grounding and computational practicality. Experiments demonstrate that our method outperforms existing fine-tuning and hyperparameter search baselines.

### Deep EEG Super-Resolution: Upsampling EEG Spatial Resolution with Generative Adversarial Networks 
[[arxiv](https://arxiv.org/abs/2502.08803)] [[cool](https://papers.cool/arxiv/2502.08803)] [[pdf](https://arxiv.org/pdf/2502.08803)]
> **Authors**: Isaac Corley,Yufei Huang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Electroencephalography (EEG) activity contains a wealth of information about what is happening within the human brain. Recording more of this data has the potential to unlock endless future applications. However, the cost of EEG hardware is increasingly expensive based upon the number of EEG channels being recorded simultaneously. We combat this problem in this paper by proposing a novel deep EEG super-resolution (SR) approach based on Generative Adversarial Networks (GANs). This approach can produce high spatial resolution EEG data from low resolution samples, by generating channel-wise upsampled data to effectively interpolate numerous missing channels, thus reducing the need for expensive EEG equipment. We tested the performance using an EEG dataset from a mental imagery task. Our proposed GAN model provided 10^4 fold and 10^2 fold reduction in mean-squared error (MSE) and mean-absolute error (MAE), respectively, over the baseline bicubic interpolation method. We further validate our method by training a classifier on the original classification task, which displayed minimal loss in accuracy while using the super-resolved data. The proposed SR EEG by GAN is a promising approach to improve the spatial resolution of low density EEG headsets.

### Low-Resolution Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.08795)] [[cool](https://papers.cool/arxiv/2502.08795)] [[pdf](https://arxiv.org/pdf/2502.08795)]
> **Authors**: Eduardo Lobo Lustosa Cabral,Larissa Driemeier
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 22 pages, 13 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The expanding scale of large neural network models introduces significant challenges, driving efforts to reduce memory usage and enhance computational efficiency. Such measures are crucial to ensure the practical implementation and effective application of these sophisticated models across a wide array of use cases. This study examines the impact of parameter bit precision on model performance compared to standard 32-bit models, with a focus on multiclass object classification in images. The models analyzed include those with fully connected layers, convolutional layers, and transformer blocks, with model weight resolution ranging from 1 bit to 4.08 bits. The findings indicate that models with lower parameter bit precision achieve results comparable to 32-bit models, showing promise for use in memory-constrained devices. While low-resolution models with a small number of parameters require more training epochs to achieve accuracy comparable to 32-bit models, those with a large number of parameters achieve similar performance within the same number of epochs. Additionally, data augmentation can destabilize training in low-resolution models, but including zero as a potential value in the weight parameters helps maintain stability and prevents performance degradation. Overall, 2.32-bit weights offer the optimal balance of memory reduction, performance, and efficiency. However, further research should explore other dataset types and more complex and larger models. These findings suggest a potential new era for optimized neural network models with reduced memory requirements and improved computational efficiency, though advancements in dedicated hardware are necessary to fully realize this potential.

### Spectral Journey: How Transformers Predict the Shortest Path 
[[arxiv](https://arxiv.org/abs/2502.08794)] [[cool](https://papers.cool/arxiv/2502.08794)] [[pdf](https://arxiv.org/pdf/2502.08794)]
> **Authors**: Andrew Cohen,Andrey Gromov,Kaiyu Yang,Yuandong Tian
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 12 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Decoder-only transformers lead to a step-change in capability of large language models. However, opinions are mixed as to whether they are really planning or reasoning. A path to making progress in this direction is to study the model's behavior in a setting with carefully controlled data. Then interpret the learned representations and reverse-engineer the computation performed internally. We study decoder-only transformer language models trained from scratch to predict shortest paths on simple, connected and undirected graphs. In this setting, the representations and the dynamics learned by the model are interpretable. We present three major results: (1) Two-layer decoder-only language models can learn to predict shortest paths on simple, connected graphs containing up to 10 nodes. (2) Models learn a graph embedding that is correlated with the spectral decomposition of the line graph. (3) Following the insights, we discover a novel approximate path-finding algorithm Spectral Line Navigator (SLN) that finds shortest path by greedily selecting nodes in the space of spectral embedding of the line graph.

### Decision Tree Based Wrappers for Hearing Loss 
[[arxiv](https://arxiv.org/abs/2502.08785)] [[cool](https://papers.cool/arxiv/2502.08785)] [[pdf](https://arxiv.org/pdf/2502.08785)]
> **Authors**: Miguel Rabuge,Nuno Lourenço
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,声音
- **Abstract**: Audiology entities are using Machine Learning (ML) models to guide their screening towards people at risk. Feature Engineering (FE) focuses on optimizing data for ML models, with evolutionary methods being effective in feature selection and construction tasks. This work aims to benchmark an evolutionary FE wrapper, using models based on decision trees as proxies. The FEDORA framework is applied to a Hearing Loss (HL) dataset, being able to reduce data dimensionality and statistically maintain baseline performance. Compared to traditional methods, FEDORA demonstrates superior performance, with a maximum balanced accuracy of 76.2%, using 57 features. The framework also generated an individual that achieved 72.8% balanced accuracy using a single feature.

### Learning Discontinuous Galerkin Solutions to Elliptic Problems via Small Linear Convolutional Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.08783)] [[cool](https://papers.cool/arxiv/2502.08783)] [[pdf](https://arxiv.org/pdf/2502.08783)]
> **Authors**: Adrian Celaya,Yimo Wang,David Fuentes,Beatrice Riviere
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: In recent years, there has been an increasing interest in using deep learning and neural networks to tackle scientific problems, particularly in solving partial differential equations (PDEs). However, many neural network-based methods, such as physics-informed neural networks, depend on automatic differentiation and the sampling of collocation points, which can result in a lack of interpretability and lower accuracy compared to traditional numerical methods. To address this issue, we propose two approaches for learning discontinuous Galerkin solutions to PDEs using small linear convolutional neural networks. Our first approach is supervised and depends on labeled data, while our second approach is unsupervised and does not rely on any training data. In both cases, our methods use substantially fewer parameters than similar numerics-based neural networks while also demonstrating comparable accuracy to the true and DG solutions for elliptic problems.

### Recurrent Memory for Online Interdomain Gaussian Processes 
[[arxiv](https://arxiv.org/abs/2502.08736)] [[cool](https://papers.cool/arxiv/2502.08736)] [[pdf](https://arxiv.org/pdf/2502.08736)]
> **Authors**: Wenlong Chen,Naoki Kiyohara,Harrison Bo Hua Zhu,Yingzhen Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 13 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We propose a novel online Gaussian process (GP) model that is capable of capturing long-term memory in sequential data in an online regression setting. Our model, Online HiPPO Sparse Variational Gaussian Process Regression (OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators) framework, which is popularized in the RNN domain due to its long-range memory modeling capabilities. We interpret the HiPPO time-varying orthogonal projections as inducing variables with time-dependent orthogonal polynomial basis functions, which allows the SGPR inducing points to memorize the process history. We show that the HiPPO framework fits naturally into the interdomain GP framework and demonstrate that the kernel matrices can also be updated online in a recurrence form based on the ODE evolution of HiPPO. We evaluate our method on time series regression tasks, showing that it outperforms the existing online GP method in terms of predictive performance and computational efficiency

### New Bounds for Sparse Variational Gaussian Processes 
[[arxiv](https://arxiv.org/abs/2502.08730)] [[cool](https://papers.cool/arxiv/2502.08730)] [[pdf](https://arxiv.org/pdf/2502.08730)]
> **Authors**: Michalis K. Titsias
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 17 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP models. At the core of these methods is the assumption that the true posterior distribution over training function values ${\bf f}$ and inducing variables ${\bf u}$ is approximated by a variational distribution that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its factorization. While this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution $q({\bf f} | {\bf u})$ that depends on $N$ extra parameters, where $N$ is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce bias when learning the hyperpaparameters and can lead to better predictive performance.

### A Comparative Study of Machine Learning Algorithms for Stock Price Prediction Using Insider Trading Data 
[[arxiv](https://arxiv.org/abs/2502.08728)] [[cool](https://papers.cool/arxiv/2502.08728)] [[pdf](https://arxiv.org/pdf/2502.08728)]
> **Authors**: Amitabh Chakravorty,Nelly Elsayed
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 5 pages, accepted to publish
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The research paper empirically investigates several machine learning algorithms to forecast stock prices depending on insider trading information. Insider trading offers special insights into market sentiment, pointing to upcoming changes in stock prices. This study examines the effectiveness of algorithms like decision trees, random forests, support vector machines (SVM) with different kernels, and K-Means Clustering using a dataset of Tesla stock transactions. Examining past data from April 2020 to March 2023, this study focuses on how well these algorithms identify trends and forecast stock price fluctuations. The paper uses Recursive Feature Elimination (RFE) and feature importance analysis to optimize the feature set and, hence, increase prediction accuracy. While it requires substantially greater processing time than other models, SVM with the Radial Basis Function (RBF) kernel displays the best accuracy. This paper highlights the trade-offs between accuracy and efficiency in machine learning models and proposes the possibility of pooling multiple data sources to raise prediction performance. The results of this paper aim to help financial analysts and investors in choosing strong algorithms to optimize investment strategies.

### Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics 
[[arxiv](https://arxiv.org/abs/2502.08696)] [[cool](https://papers.cool/arxiv/2502.08696)] [[pdf](https://arxiv.org/pdf/2502.08696)]
> **Authors**: Sebastian Sanokowski,Wilhelm Berghammer,Martin Ennemoser,Haoyu Peter Wang,Sepp Hochreiter,Sebastian Lehner
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,统计力学,人工智能,计算物理,机器学习
- **Abstract**: Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain. However, existing methods face limitations in memory scaling and thus the number of attainable diffusion steps since they require backpropagation through the entire generative process. To overcome these limitations we introduce two novel training methods for discrete diffusion samplers, one grounded in the policy gradient theorem and the other one leveraging Self-Normalized Neural Importance Sampling (SN-NIS). These methods yield memory-efficient training and achieve state-of-the-art results in unsupervised combinatorial optimization. Numerous scientific applications additionally require the ability of unbiased sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte Carlo that enable for the first time the application of discrete diffusion models to this problem. We validate our methods on Ising model benchmarks and find that they outperform popular autoregressive approaches. Our work opens new avenues for applying diffusion models to a wide range of scientific applications in discrete domains that were hitherto restricted to exact likelihood models.

### Efficient Split Learning LSTM Models for FPGA-based Edge IoT Devices 
[[arxiv](https://arxiv.org/abs/2502.08692)] [[cool](https://papers.cool/arxiv/2502.08692)] [[pdf](https://arxiv.org/pdf/2502.08692)]
> **Authors**: Romina Soledad Molina,Vukan Ninkovic,Dejan Vukobratovic,Maria Liz Crespo,Marco Zennaro
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted for publication at IEEE ICMLCN 2025
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Split Learning (SL) recently emerged as an efficient paradigm for distributed Machine Learning (ML) suitable for the Internet Of Things (IoT)-Cloud systems. However, deploying SL on resource-constrained edge IoT platforms poses a significant challenge in terms of balancing the model performance against the processing, memory, and energy resources. In this work, we present a practical study of deploying SL framework on a real-world Field-Programmable Gate Array (FPGA)-based edge IoT platform. We address the SL framework applied to a time-series processing model based on Recurrent Neural Networks (RNNs). Set in the context of river water quality monitoring and using real-world data, we train, optimize, and deploy a Long Short-Term Memory (LSTM) model on a given edge IoT FPGA platform in different SL configurations. Our results demonstrate the importance of aligning design choices with specific application requirements, whether it is maximizing speed, minimizing power, or optimizing for resource constraints.

### Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2502.08690)] [[cool](https://papers.cool/arxiv/2502.08690)] [[pdf](https://arxiv.org/pdf/2502.08690)]
> **Authors**: Hoigi Seo,Wongi Jeong,Jae-sun Seo,Se Young Chun
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.

### Advancing machine fault diagnosis: A detailed examination of convolutional neural networks 
[[arxiv](https://arxiv.org/abs/2502.08689)] [[cool](https://papers.cool/arxiv/2502.08689)] [[pdf](https://arxiv.org/pdf/2502.08689)]
> **Authors**: Govind Vashishtha,Sumika Chauhan,Mert Sehri,Justyna Hebda-Sobkowicz,Radoslaw Zimroz,Patrick Dumond,Rajesh Kumar
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The growing complexity of machinery and the increasing demand for operational efficiency and safety have driven the development of advanced fault diagnosis techniques. Among these, convolutional neural networks (CNNs) have emerged as a powerful tool, offering robust and accurate fault detection and classification capabilities. This comprehensive review delves into the application of CNNs in machine fault diagnosis, covering its theoretical foundation, architectural variations, and practical implementations. The strengths and limitations of CNNs are analyzed in this domain, discussing their effectiveness in handling various fault types, data complexities, and operational environments. Furthermore, we explore the evolving landscape of CNN-based fault diagnosis, examining recent advancements in data augmentation, transfer learning, and hybrid architectures. Finally, we highlight future research directions and potential challenges to further enhance the application of CNNs for reliable and proactive machine fault diagnosis.

### EEG Artifact Detection and Correction with Deep Autoencoders 
[[arxiv](https://arxiv.org/abs/2502.08686)] [[cool](https://papers.cool/arxiv/2502.08686)] [[pdf](https://arxiv.org/pdf/2502.08686)]
> **Authors**: David Aquilué-Llorens,Aureli Soria-Frisch
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: EEG signals convey important information about brain activity both in healthy and pathological conditions. However, they are inherently noisy, which poses significant challenges for accurate analysis and interpretation. Traditional EEG artifact removal methods, while effective, often require extensive expert intervention. This study presents LSTEEG, a novel LSTM-based autoencoder designed for the detection and correction of artifacts in EEG signals. Leveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear dependencies in sequential EEG data. LSTEEG demonstrates superior performance in both artifact detection and correction tasks compared to other state-of-the-art convolutional autoencoders. Our methodology enhances the interpretability and utility of the autoencoder's latent space, enabling data-driven automated artefact removal in EEG its application in downstream tasks. This research advances the field of efficient and accurate multi-channel EEG preprocessing, and promotes the implementation and usage of automated EEG analysis pipelines for brain health applications.

### Beyond Models! Explainable Data Valuation and Metric Adaption for Recommendation 
[[arxiv](https://arxiv.org/abs/2502.08685)] [[cool](https://papers.cool/arxiv/2502.08685)] [[pdf](https://arxiv.org/pdf/2502.08685)]
> **Authors**: Renqi Jia,Xiaokun Zhang,Bowei He,Qiannan Zhu,Weitao Xu,Jiehao Chen,Chen Ma
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: User behavior records serve as the foundation for recommender systems. While the behavior data exhibits ease of acquisition, it often suffers from varying quality. Current methods employ data valuation to discern high-quality data from low-quality data. However, they tend to employ black-box design, lacking transparency and interpretability. Besides, they are typically tailored to specific evaluation metrics, leading to limited generality across various tasks. To overcome these issues, we propose an explainable and versatile framework DVR which can enhance the efficiency of data utilization tailored to any requirements of the model architectures and evaluation metrics. For explainable data valuation, a data valuator is presented to evaluate the data quality via calculating its Shapley value from the game-theoretic perspective, ensuring robust mathematical properties and reliability. In order to accommodate various evaluation metrics, including differentiable and non-differentiable ones, a metric adapter is devised based on reinforcement learning, where a metric is treated as the reinforcement reward that guides model optimization. Extensive experiments conducted on various benchmarks verify that our framework can improve the performance of current recommendation algorithms on various metrics including ranking accuracy, diversity, and fairness. Specifically, our framework achieves up to 34.7\% improvements over existing methods in terms of representative NDCG metric. The code is available at https://github.com/renqii/DVR.

### Self-Evaluation for Job-Shop Scheduling 
[[arxiv](https://arxiv.org/abs/2502.08684)] [[cool](https://papers.cool/arxiv/2502.08684)] [[pdf](https://arxiv.org/pdf/2502.08684)]
> **Authors**: Imanol Echeverria,Maialen Murua,Roberto Santana
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Combinatorial optimization problems, such as scheduling and route planning, are crucial in various industries but are computationally intractable due to their NP-hard nature. Neural Combinatorial Optimization methods leverage machine learning to address these challenges but often depend on sequential decision-making, which is prone to error accumulation as small mistakes propagate throughout the process. Inspired by self-evaluation techniques in Large Language Models, we propose a novel framework that generates and evaluates subsets of assignments, moving beyond traditional stepwise approaches. Applied to the Job-Shop Scheduling Problem, our method integrates a heterogeneous graph neural network with a Transformer to build a policy model and a self-evaluation function. Experimental validation on challenging, well-known benchmarks demonstrates the effectiveness of our approach, surpassing state-of-the-art methods.

### A Deep Learning approach for parametrized and time dependent Partial Differential Equations using Dimensionality Reduction and Neural ODEs 
[[arxiv](https://arxiv.org/abs/2502.08683)] [[cool](https://papers.cool/arxiv/2502.08683)] [[pdf](https://arxiv.org/pdf/2502.08683)]
> **Authors**: Alessandro Longhi,Danny Lathouwers,Zoltán Perkó
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Partial Differential Equations (PDEs) are central to science and engineering. Since solving them is computationally expensive, a lot of effort has been put into approximating their solution operator via both traditional and recently increasingly Deep Learning (DL) techniques. A conclusive methodology capable of accounting both for (continuous) time and parameter dependency in such DL models however is still lacking. In this paper, we propose an autoregressive and data-driven method using the analogy with classical numerical solvers for time-dependent, parametric and (typically) nonlinear PDEs. We present how Dimensionality Reduction (DR) can be coupled with Neural Ordinary Differential Equations (NODEs) in order to learn the solution operator of arbitrary PDEs. The idea of our work is that it is possible to map the high-fidelity (i.e., high-dimensional) PDE solution space into a reduced (low-dimensional) space, which subsequently exhibits dynamics governed by a (latent) Ordinary Differential Equation (ODE). Solving this (easier) ODE in the reduced space allows avoiding solving the PDE in the high-dimensional solution space, thus decreasing the computational burden for repeated calculations for e.g., uncertainty quantification or design optimization purposes. The main outcome of this work is the importance of exploiting DR as opposed to the recent trend of building large and complex architectures: we show that by leveraging DR we can deliver not only more accurate predictions, but also a considerably lighter and faster DL model compared to existing methodologies.

### On the Role of Pre-trained Embeddings in Binary Code Analysis 
[[arxiv](https://arxiv.org/abs/2502.08682)] [[cool](https://papers.cool/arxiv/2502.08682)] [[pdf](https://arxiv.org/pdf/2502.08682)]
> **Authors**: Alwin Maier,Felix Weissberg,Konrad Rieck
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :I.2.6
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep learning has enabled remarkable progress in binary code analysis. In particular, pre-trained embeddings of assembly code have become a gold standard for solving analysis tasks, such as measuring code similarity or recognizing functions. These embeddings are capable of learning a vector representation from unlabeled code. In contrast to natural language processing, however, label information is not scarce for many tasks in binary code analysis. For example, labeled training data for function boundaries, optimization levels, and argument types can be easily derived from debug information provided by a compiler. Consequently, the main motivation of embeddings does not transfer directly to binary code analysis. In this paper, we explore the role of pre-trained embeddings from a critical perspective. To this end, we systematically evaluate recent embeddings for assembly code on five downstream tasks using a corpus of 1.2 million functions from the Debian distribution. We observe that several embeddings perform similarly when sufficient labeled data is available, and that differences reported in prior work are hardly noticeable. Surprisingly, we find that end-to-end learning without pre-training performs best on average, which calls into question the need for specialized embeddings. By varying the amount of labeled data, we eventually derive guidelines for when embeddings offer advantages and when end-to-end learning is preferable for binary code analysis.

### Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges 
[[arxiv](https://arxiv.org/abs/2502.08680)] [[cool](https://papers.cool/arxiv/2502.08680)] [[pdf](https://arxiv.org/pdf/2502.08680)]
> **Authors**: Safal Shrestha,Minwu Kim,Keith Ross
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models.

### Deep Learning-Driven Malware Classification with API Call Sequence Analysis and Concept Drift Handling 
[[arxiv](https://arxiv.org/abs/2502.08679)] [[cool](https://papers.cool/arxiv/2502.08679)] [[pdf](https://arxiv.org/pdf/2502.08679)]
> **Authors**: Bishwajit Prasad Gond,Durga Prasad Mohapatra
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全
- **Abstract**: Malware classification in dynamic environments presents a significant challenge due to concept drift, where the statistical properties of malware data evolve over time, complicating detection efforts. To address this issue, we propose a deep learning framework enhanced with a genetic algorithm to improve malware classification accuracy and adaptability. Our approach incorporates mutation operations and fitness score evaluations within genetic algorithms to continuously refine the deep learning model, ensuring robustness against evolving malware threats. Experimental results demonstrate that this hybrid method significantly enhances classification performance and adaptability, outperforming traditional static models. Our proposed approach offers a promising solution for real-time malware classification in ever-changing cybersecurity landscapes.

### Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks 
[[arxiv](https://arxiv.org/abs/2502.08644)] [[cool](https://papers.cool/arxiv/2502.08644)] [[pdf](https://arxiv.org/pdf/2502.08644)]
> **Authors**: Hoony Kang,Wolfgang Losert
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 13 pages, 3 figures. v.2 comments: Updated email, updated typo on p.11: h -> h^2 for RMSE. v.3 comments: Updated reference style, added reference to Github repository
- **标题**: None
- **领域**: 机器学习,人工智能,动力系统,适应和自组织系统,生物物理学
- **Abstract**: The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models.

### Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs 
[[arxiv](https://arxiv.org/abs/2502.08640)] [[cool](https://papers.cool/arxiv/2502.08640)] [[pdf](https://arxiv.org/pdf/2502.08640)]
> **Authors**: Mantas Mazeika,Xuwang Yin,Rishub Tamirisa,Jaehyuk Lim,Bruce W. Lee,Richard Ren,Long Phan,Norman Mu,Adam Khoja,Oliver Zhang,Dan Hendrycks
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Website: https://www.emergent-values.ai
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别,计算机与社会
- **Abstract**: As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.

### Necessary and Sufficient Oracles: Toward a Computational Taxonomy For Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.08632)] [[cool](https://papers.cool/arxiv/2502.08632)] [[pdf](https://arxiv.org/pdf/2502.08632)]
> **Authors**: Dhruv Rohatgi,Dylan J. Foster
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 84 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,计算复杂度
- **Abstract**: Algorithms for reinforcement learning (RL) in large state spaces crucially rely on supervised learning subroutines to estimate objects such as value functions or transition probabilities. Since only the simplest supervised learning problems can be solved provably and efficiently, practical performance of an RL algorithm depends on which of these supervised learning "oracles" it assumes access to (and how they are implemented). But which oracles are better or worse? Is there a minimal oracle? In this work, we clarify the impact of the choice of supervised learning oracle on the computational complexity of RL, as quantified by the oracle strength. First, for the task of reward-free exploration in Block MDPs in the standard episodic access model -- a ubiquitous setting for RL with function approximation -- we identify two-context regression as a minimal oracle, i.e. an oracle that is both necessary and sufficient (under a mild regularity assumption). Second, we identify one-context regression as a near-minimal oracle in the stronger reset access model, establishing a provable computational benefit of resets in the process. Third, we broaden our focus to Low-Rank MDPs, where we give cryptographic evidence that the analogous oracle from the Block MDP setting is insufficient.

### Randomness of Low-Layer Parameters Determines Confusing Samples in Terms of Interaction Representations of a DNN 
[[arxiv](https://arxiv.org/abs/2502.08625)] [[cool](https://papers.cool/arxiv/2502.08625)] [[pdf](https://arxiv.org/pdf/2502.08625)]
> **Authors**: Junpeng Zhang,Lei Cheng,Qing Li,Liang Lin,Quanshi Zhang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: In this paper, we find that the complexity of interactions encoded by a deep neural network (DNN) can explain its generalization power. We also discover that the confusing samples of a DNN, which are represented by non-generalizable interactions, are determined by its low-layer parameters. In comparison, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Two DNNs with different low-layer parameters usually have fully different sets of confusing samples, even though they have similar performance. This finding extends the understanding of the lottery ticket hypothesis, and well explains distinctive representation power of different DNNs.

### Forecasting Drought Using Machine Learning in California 
[[arxiv](https://arxiv.org/abs/2502.08622)] [[cool](https://papers.cool/arxiv/2502.08622)] [[pdf](https://arxiv.org/pdf/2502.08622)]
> **Authors**: Nan K. Li,Angela Chang,David Sherman
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Drought is a frequent and costly natural disaster in California, with major negative impacts on agricultural production and water resource availability, particularly groundwater. This study investigated the performance of applying different machine learning approaches to predicting the U.S. Drought Monitor classification in California. Four approaches were used: a convolutional neural network (CNN), random forest, XGBoost, and long short term memory (LSTM) recurrent neural network, and compared to a baseline persistence model. We evaluated the models' performance in predicting severe drought (USDM drought category D2 or higher) using a macro F1 binary classification metric. The LSTM model emerged as the top performer, followed by XGBoost, CNN, and random forest. Further evaluation of our results at the county level suggested that the LSTM model would perform best in counties with more consistent drought patterns and where severe drought was more common, and the LSTM model would perform worse where drought scores increased rapidly. Utilizing 30 weeks of historical data, the LSTM model successfully forecasted drought scores for a 12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less than half a drought category on a scale of 0 to 5. Additionally, the LSTM achieved a macro F1 score of 0.9, indicating high accuracy in binary classification for severe drought conditions. Evaluation of different window and future horizon sizes in weeks suggested that at least 24 weeks of data would result in the best performance, with best performance for shorter horizon sizes, particularly less than eight weeks.

### Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.08612)] [[cool](https://papers.cool/arxiv/2502.08612)] [[pdf](https://arxiv.org/pdf/2502.08612)]
> **Authors**: Saurabh Kataria,Ran Xiao,Timothy Ruchti,Matthew Clark,Jiaying Lu,Randall J. Lee,Jocelyn Grunwell,Xiao Hu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Non-invasive patient monitoring for tracking and predicting adverse acute health events is an emerging area of research. We pursue in-hospital cardiac arrest (IHCA) prediction using only single-channel finger photoplethysmography (PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator Network (FEAN) leverages powerful representations from pre-trained PPG foundation models (PPG-GPT of size up to 1 Billion) stacked with sequential classification models. We propose two FEAN variants ("1H", "FH") which use the latest one-hour and (max) 24-hour history to make decisions respectively. Our study is the first to present IHCA prediction results in ICU patients using only unimodal (continuous PPG signal) waveform deep representations. With our best model, we obtain an average of 0.79 AUROC over 24~h prediction window before CA event onset with our model peaking performance at 0.82 one hour before CA. We also provide a comprehensive analysis of our model through architectural tuning and PaCMAP visualization of patient health trajectory in latent space.

### Robustly Learning Monotone Generalized Linear Models via Data Augmentation 
[[arxiv](https://arxiv.org/abs/2502.08611)] [[cool](https://papers.cool/arxiv/2502.08611)] [[pdf](https://arxiv.org/pdf/2502.08611)]
> **Authors**: Nikos Zarifis,Puqian Wang,Ilias Diakonikolas,Jelena Diakonikolas
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制,统计理论
- **Abstract**: We study the task of learning Generalized Linear models (GLMs) in the agnostic model under the Gaussian distribution. We give the first polynomial-time algorithm that achieves a constant-factor approximation for \textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners succeed for a substantially smaller class of activations. Our work resolves a well-known open problem, by developing a robust counterpart to the classical GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more generally, encompassing all monotone activations with bounded $(2+ζ)$-moments, for any fixed $ζ>0$ -- a condition that is essentially necessary. To obtain our results, we leverage a novel data augmentation technique with decreasing Gaussian noise injection and prove a number of structural results that may be useful in other settings.

### Distillation Scaling Laws 
[[arxiv](https://arxiv.org/abs/2502.08606)] [[cool](https://papers.cool/arxiv/2502.08606)] [[pdf](https://arxiv.org/pdf/2502.08606)]
> **Authors**: Dan Busbridge,Amitis Shidani,Floris Weers,Jason Ramapuram,Etai Littwin,Russ Webb
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 67 pages, 54 figures, 13 tables
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.

### CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection 
[[arxiv](https://arxiv.org/abs/2502.08605)] [[cool](https://papers.cool/arxiv/2502.08605)] [[pdf](https://arxiv.org/pdf/2502.08605)]
> **Authors**: Karish Grover,Geoffrey J. Gordon,Christos Faloutsos
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Does the intrinsic curvature of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such geometric outliers, focusing only on structural and attribute-level anomalies. To this end, we propose CurvGAD - a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. CurvGAD introduces two parallel pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant geometry reconstruction, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) Curvature-invariant structure and attribute reconstruction, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, CurvGAD refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to 6.5% over state-of-the-art GAD methods.

### Two-stage hybrid models for enhancing forecasting accuracy on heterogeneous time series 
[[arxiv](https://arxiv.org/abs/2502.08600)] [[cool](https://papers.cool/arxiv/2502.08600)] [[pdf](https://arxiv.org/pdf/2502.08600)]
> **Authors**: Junru Ren,Shaomin Wu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 14 pages, 2 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Compared to local models built in a series-by-series manner, global models leverage relevant information across time series, resulting in improved forecasting performance and generalization capacity. Constructing global models on a set of time series is becoming mainstream in the field of time series forecasting. However, the advantages of global models may not always be realized when dealing with heterogeneous data. While they can adapt to heterogeneous datasets by increasing the model complexity, the model cannot be infinitely complex due to the finite sample size, which poses challenges for the application of global models. Additionally, determining whether the time series data is homogeneous or heterogeneous can be ambiguous in practice. To address these research gaps, this paper argues that the heterogeneity of the data should be defined by the global model used, and for each series, the portion not modelled by the global model represents heterogeneity. It further proposes two-stage hybrid models, which include a second stage to identify and model heterogeneous patterns. In this second stage, we can estimate either all local models or sub-global models across different domains divided based on heterogeneity. Experiments on four open datasets reveal that the proposed methods significantly outperform five existing models, indicating they contribute to fully unleash the potential of global models on heterogeneous datasets.

### Enhancing Diffusion Models Efficiency by Disentangling Total-Variance and Signal-to-Noise Ratio 
[[arxiv](https://arxiv.org/abs/2502.08598)] [[cool](https://papers.cool/arxiv/2502.08598)] [[pdf](https://arxiv.org/pdf/2502.08598)]
> **Authors**: Khaled Kahouli,Winfried Ripken,Stefan Gugler,Oliver T. Unke,Klaus-Robert Müller,Shinichi Nakajima
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The long sampling time of diffusion models remains a significant bottleneck, which can be mitigated by reducing the number of diffusion time steps. However, the quality of samples with fewer steps is highly dependent on the noise schedule, i.e., the specific manner in which noise is introduced and the signal is reduced at each step. Although prior work has improved upon the original variance-preserving and variance-exploding schedules, these approaches $\textit{passively}$ adjust the total variance, without direct control over it. In this work, we propose a novel total-variance/signal-to-noise-ratio disentangled (TV/SNR) framework, where TV and SNR can be controlled independently. Our approach reveals that different existing schedules, where the TV explodes exponentially, can be $\textit{improved}$ by setting a constant TV schedule while preserving the same SNR schedule. Furthermore, generalizing the SNR schedule of the optimal transport flow matching significantly improves the performance in molecular structure generation, achieving few step generation of stable molecules. A similar tendency is observed in image generation, where our approach with a uniform diffusion time grid performs comparably to the highly tailored EDM sampler.

### Toward Universal Laws of Outlier Propagation 
[[arxiv](https://arxiv.org/abs/2502.08593)] [[cool](https://papers.cool/arxiv/2502.08593)] [[pdf](https://arxiv.org/pdf/2502.08593)]
> **Authors**: Aram Ebtekar,Yuhao Wang,Dominik Janzing
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We argue that Algorithmic Information Theory (AIT) admits a principled way to quantify outliers in terms of so-called randomness deficiency. For the probability distribution generated by a causal Bayesian network, we show that the randomness deficiency of the joint state decomposes into randomness deficiencies of each causal mechanism, subject to the Independence of Mechanisms Principle. Accordingly, anomalous joint observations can be quantitatively attributed to their root causes, i.e., the mechanisms that behaved anomalously. As an extension of Levin's law of randomness conservation, we show that weak outliers cannot cause strong ones when Independence of Mechanisms holds. We show how these information theoretic laws provide a better understanding of the behaviour of outliers defined with respect to existing scores.

### Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks 
[[arxiv](https://arxiv.org/abs/2502.08586)] [[cool](https://papers.cool/arxiv/2502.08586)] [[pdf](https://arxiv.org/pdf/2502.08586)]
> **Authors**: Ang Li,Yin Zhou,Vethavikashini Chithrra Raghuram,Tom Goldstein,Micah Goldblum
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.

### Scalable Bilevel Loss Balancing for Multi-Task Learning 
[[arxiv](https://arxiv.org/abs/2502.08585)] [[cool](https://papers.cool/arxiv/2502.08585)] [[pdf](https://arxiv.org/pdf/2502.08585)]
> **Authors**: Peiyao Xiao,Chaosheng Dong,Shaofeng Zou,Kaiyi Ji
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Multi-task learning (MTL) has been widely adopted for its ability to simultaneously learn multiple tasks. While existing gradient manipulation methods often yield more balanced solutions than simple scalarization-based approaches, they typically incur a significant computational overhead of $\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In this paper, we propose BiLB4MTL, a simple and scalable loss balancing approach for MTL, formulated from a novel bilevel optimization perspective. Our method incorporates three key components: (i) an initial loss normalization, (ii) a bilevel loss-balancing formulation, and (iii) a scalable first-order algorithm that requires only $\mathcal{O}(1)$ time and memory. Theoretically, we prove that BiLB4MTL guarantees convergence not only to a stationary point of the bilevel loss balancing problem but also to an $ε$-accurate Pareto stationary point for all $K$ loss functions under mild conditions. Extensive experiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves state-of-the-art performance in both accuracy and efficiency. Code is available at https://github.com/OptMN-Lab/-BiLB4MTL.

### A method for classification of data with uncertainty using hypothesis testing 
[[arxiv](https://arxiv.org/abs/2502.08582)] [[cool](https://papers.cool/arxiv/2502.08582)] [[pdf](https://arxiv.org/pdf/2502.08582)]
> **Authors**: Shoma Yokura,Akihisa Ichiki
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Binary classification is a task that involves the classification of data into one of two distinct classes. It is widely utilized in various fields. However, conventional classifiers tend to make overconfident predictions for data that belong to overlapping regions of the two class distributions or for data outside the distributions (out-of-distribution data). Therefore, conventional classifiers should not be applied in high-risk fields where classification results can have significant consequences. In order to address this issue, it is necessary to quantify uncertainty and adopt decision-making approaches that take it into account. Many methods have been proposed for this purpose; however, implementing these methods often requires performing resampling, improving the structure or performance of models, and optimizing the thresholds of classifiers. We propose a new decision-making approach using two types of hypothesis testing. This method is capable of detecting ambiguous data that belong to the overlapping regions of two class distributions, as well as out-of-distribution data that are not included in the training data distribution. In addition, we quantify uncertainty using the empirical distribution of feature values derived from the training data obtained through the trained model. The classification threshold is determined by the $α$-quantile and ($1-α$)-quantile, where the significance level $α$ is set according to each specific situation.

### FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.08577)] [[cool](https://papers.cool/arxiv/2502.08577)] [[pdf](https://arxiv.org/pdf/2502.08577)]
> **Authors**: Davide Domini,Gianluca Aguzzi,Lukas Esterle,Mirko Viroli
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.

### COAST: Intelligent Time-Adaptive Neural Operators 
[[arxiv](https://arxiv.org/abs/2502.08574)] [[cool](https://papers.cool/arxiv/2502.08574)] [[pdf](https://arxiv.org/pdf/2502.08574)]
> **Authors**: Zhikai Wu,Shiyang Zhang,Sizhuang He,Sifan Wang,Min Zhu,Anran Jiao,Lu Lu,David van Dijk
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems.

### Beyond Predictions: A Participatory Framework for Multi-Stakeholder Decision-Making 
[[arxiv](https://arxiv.org/abs/2502.08542)] [[cool](https://papers.cool/arxiv/2502.08542)] [[pdf](https://arxiv.org/pdf/2502.08542)]
> **Authors**: Vittoria Vineis,Giuseppe Perelli,Gabriele Tolomei
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,多代理系统
- **Abstract**: Conventional decision-support systems, primarily based on supervised learning, focus on outcome prediction models to recommend actions. However, they often fail to account for the complexities of multi-actor environments, where diverse and potentially conflicting stakeholder preferences must be balanced. In this paper, we propose a novel participatory framework that redefines decision-making as a multi-stakeholder optimization problem, capturing each actor's preferences through context-dependent reward functions. Our framework leverages $k$-fold cross-validation to fine-tune user-provided outcome prediction models and evaluate decision strategies, including compromise functions mediating stakeholder trade-offs. We introduce a synthetic scoring mechanism that exploits user-defined preferences across multiple metrics to rank decision-making strategies and identify the optimal decision-maker. The selected decision-maker can then be used to generate actionable recommendations for new data. We validate our framework using two real-world use cases, demonstrating its ability to deliver recommendations that effectively balance multiple metrics, achieving results that are often beyond the scope of purely prediction-based methods. Ablation studies demonstrate that our framework, with its modular, model-agnostic, and inherently transparent design, integrates seamlessly with various predictive models, reward structures, evaluation metrics, and sample sizes, making it particularly suited for complex, high-stakes decision-making contexts.

### Matrix Completion with Graph Information: A Provable Nonconvex Optimization Approach 
[[arxiv](https://arxiv.org/abs/2502.08536)] [[cool](https://papers.cool/arxiv/2502.08536)] [[pdf](https://arxiv.org/pdf/2502.08536)]
> **Authors**: Yao Wang,Yiyang Yang,Kaidong Wang,Shanxing Gao,Xiuwu Liao
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 41 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: We consider the problem of matrix completion with graphs as side information depicting the interrelations between variables. The key challenge lies in leveraging the similarity structure of the graph to enhance matrix recovery. Existing approaches, primarily based on graph Laplacian regularization, suffer from several limitations: (1) they focus only on the similarity between neighboring variables, while overlooking long-range correlations; (2) they are highly sensitive to false edges in the graphs and (3) they lack theoretical guarantees regarding statistical and computational complexities. To address these issues, we propose in this paper a novel graph regularized matrix completion algorithm called GSGD, based on preconditioned projected gradient descent approach. We demonstrate that GSGD effectively captures the higher-order correlation information behind the graphs, and achieves superior robustness and stability against the false edges. Theoretically, we prove that GSGD achieves linear convergence to the global optimum with near-optimal sample complexity, providing the first theoretical guarantees for both recovery accuracy and efficacy in the perspective of nonconvex optimization. Our numerical experiments on both synthetic and real-world data further validate that GSGD achieves superior recovery accuracy and scalability compared with several popular alternatives.

### On Different Notions of Redundancy in Conditional-Independence-Based Discovery of Graphical Models 
[[arxiv](https://arxiv.org/abs/2502.08531)] [[cool](https://papers.cool/arxiv/2502.08531)] [[pdf](https://arxiv.org/pdf/2502.08531)]
> **Authors**: Philipp M. Faller,Dominik Janzing
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The goal of conditional-independence-based discovery of graphical models is to find a graph that represents the independence structure of variables in a given dataset. To learn such a representation, conditional-independence-based approaches conduct a set of statistical tests that suffices to identify the graphical representation under some assumptions on the underlying distribution of the data. In this work, we highlight that due to the conciseness of the graphical representation, there are often many tests that are not used in the construction of the graph. These redundant tests have the potential to detect or sometimes correct errors in the learned model. We show that not all tests contain this additional information and that such redundant tests have to be applied with care. Precisely, we argue that particularly those conditional (in)dependence statements are interesting that follow only from graphical assumptions but do not hold for every probability distribution.

### LLM Pretraining with Continuous Concepts 
[[arxiv](https://arxiv.org/abs/2502.08524)] [[cool](https://papers.cool/arxiv/2502.08524)] [[pdf](https://arxiv.org/pdf/2502.08524)]
> **Authors**: Jihoon Tack,Jack Lanchantin,Jane Yu,Andrew Cohen,Ilia Kulikov,Janice Lan,Shibo Hao,Yuandong Tian,Jason Weston,Xian Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.

### FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.08518)] [[cool](https://papers.cool/arxiv/2502.08518)] [[pdf](https://arxiv.org/pdf/2502.08518)]
> **Authors**: Dezhong Yao,Yuexin Shi,Tongtong Liu,Zhiqiang Xu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups.

### The Paradox of Stochasticity: Limited Creativity and Computational Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data 
[[arxiv](https://arxiv.org/abs/2502.08515)] [[cool](https://papers.cool/arxiv/2502.08515)] [[pdf](https://arxiv.org/pdf/2502.08515)]
> **Authors**: Evgenii Evstafev
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This study examines how temperature settings and model architectures affect the generation of structured fictional data (names, birthdates) across three large language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest. By systematically testing temperature values from 0.0 to 1.0 in increments of 0.1, we conducted 330 trials yielding 889 structured entities, validated for syntactic consistency. Key findings reveal that model architecture significantly influences computational efficiency, with mistral:latest and llama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to expectations, temperature showed no correlation with processing time, challenging assumptions about stochastic sampling costs. Output diversity remained limited, as models consistently defaulted to common name archetypes (e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names clustered at intermediate values (0.3-0.7). These results demonstrate that architectural optimizations, rather than temperature adjustments, dominate performance in structured generation tasks. The findings emphasize prioritizing model selection over hyperparameter tuning for efficiency and suggest explicit diversity constraints are necessary to mitigate default output biases in synthetic data pipelines.

### Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based Framework for Effective Label Propagation 
[[arxiv](https://arxiv.org/abs/2502.08505)] [[cool](https://papers.cool/arxiv/2502.08505)] [[pdf](https://arxiv.org/pdf/2502.08505)]
> **Authors**: Tao Wen,Elynn Chen,Yuzhou Chen,Qi Lei
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph Neural Networks (GNNs) have recently become the predominant tools for studying graph data. Despite state-of-the-art performance on graph classification tasks, GNNs are overwhelmingly trained in a single domain under supervision, thus necessitating a prohibitively high demand for labels and resulting in poorly transferable representations. To address this challenge, we propose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework to bridge the gap between graph data and traditional domain adaptation methods. It extracts graph topological information holistically with a tensor architecture and then reduces domain discrepancy through label propagation. It is readily compatible with general GNNs and domain adaptation techniques with minimal adjustment through pseudo-labeling. Experiments on various real-world benchmarks show that our LP-TGNN outperforms baselines by a notable margin. We also validate and analyze each component of the proposed framework in the ablation study.

### One-Shot Federated Learning with Classifier-Free Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.08488)] [[cool](https://papers.cool/arxiv/2502.08488)] [[pdf](https://arxiv.org/pdf/2502.08488)]
> **Authors**: Obaidullah Zaland,Shutong Jin,Florian T. Pokorny,Monowar Bhuyan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Federated learning (FL) enables collaborative learning without data centralization but introduces significant communication costs due to multiple communication rounds between clients and the server. One-shot federated learning (OSFL) addresses this by forming a global model with a single communication round, often relying on the server's model distillation or auxiliary dataset generation - often through pre-trained diffusion models (DMs). Existing DM-assisted OSFL methods, however, typically employ classifier-guided DMs, which require training auxiliary classifier models at each client, introducing additional computation overhead. This work introduces OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses foundation models to devise category-specific data representations at each client, seamlessly integrated into a classifier-free diffusion model pipeline for server-side data generation. OSCAR is a simple yet cost-effective OSFL approach that outperforms the state-of-the-art on four benchmarking datasets while reducing the communication load by at least 99%.

### Training-Free Restoration of Pruned Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.08474)] [[cool](https://papers.cool/arxiv/2502.08474)] [[pdf](https://arxiv.org/pdf/2502.08474)]
> **Authors**: Keonho Lee,Minsoo Kim,Dong-Wan Choi
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-13
> **comment**: Under Review in TNNLS since May 2022
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Although network pruning has been highly popularized to compress deep neural networks, its resulting accuracy heavily depends on a fine-tuning process that is often computationally expensive and requires the original data. However, this may not be the case in real-world scenarios, and hence a few recent works attempt to restore pruned networks without any expensive retraining process. Their strong assumption is that every neuron being pruned can be replaced with another one quite similar to it, but unfortunately this does not hold in many neural networks, where the similarity between neurons is extremely low in some layers. In this article, we propose a more rigorous and robust method of restoring pruned networks in a fine-tuning free and data-free manner, called LBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned assumption in a way that each pruned neuron leaves its pieces of information to as many preserved neurons as possible and thereby multiple neurons together obtain a more robust approximation to the original output of the neuron who just left. Our method is based on a theoretical analysis on how to formulate the reconstruction error between the original network and its approximation, which nicely leads to a closed form solution for our derived loss function. Through the extensive experiments, LBYL is confirmed to be indeed more effective to approximate the original network and consequently able to achieve higher accuracy for restored networks, compared to the recent approaches exploiting the similarity between two neurons. The very first version of this work, which contains major technical and theoretical components, was submitted to NeurIPS 2021 and ICML 2022.

### Learning Theory for Kernel Bilevel Optimization 
[[arxiv](https://arxiv.org/abs/2502.08457)] [[cool](https://papers.cool/arxiv/2502.08457)] [[pdf](https://arxiv.org/pdf/2502.08457)]
> **Authors**: Fares El Khoury,Edouard Pauwels,Samuel Vaiter,Michael Arbel
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Bilevel optimization has emerged as a technique for addressing a wide range of machine learning problems that involve an outer objective implicitly determined by the minimizer of an inner problem. In this paper, we investigate the generalization properties for kernel bilevel optimization problems where the inner objective is optimized over a Reproducing Kernel Hilbert Space. This setting enables rich function approximation while providing a foundation for rigorous theoretical analysis. In this context, we establish novel generalization error bounds for the bilevel problem under finite-sample approximation. Our approach adopts a functional perspective, inspired by (Petrulionyte et al., 2024), and leverages tools from empirical process theory and maximal inequalities for degenerate $U$-processes to derive uniform error bounds. These generalization error estimates allow to characterize the statistical accuracy of gradient-based methods applied to the empirical discretization of the bilevel problem.

### Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware Minimization Based on Loss Geometry 
[[arxiv](https://arxiv.org/abs/2502.08448)] [[cool](https://papers.cool/arxiv/2502.08448)] [[pdf](https://arxiv.org/pdf/2502.08448)]
> **Authors**: Albert Kjøller Jacobsen,Georgios Arvanitidis
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent studies on deep neural networks show that flat minima of the loss landscape correlate with improved generalization. Sharpness-aware minimization (SAM) efficiently finds flat regions by updating the parameters according to the gradient at an adversarial perturbation. The perturbation depends on the Euclidean metric, making SAM non-invariant under reparametrizations, which blurs sharpness and generalization. We propose Monge SAM (M-SAM), a reparametrization invariant version of SAM by considering a Riemannian metric in the parameter space induced naturally by the loss surface. Compared to previous approaches, M-SAM works under any modeling choice, relies only on mild assumptions while being as computationally efficient as SAM. We theoretically argue that M-SAM varies between SAM and gradient descent (GD), which increases robustness to hyperparameter selection and reduces attraction to suboptimal equilibria like saddle points. We demonstrate this behavior both theoretically and empirically on a multi-modal representation alignment task.

### LucidAtlas$: Learning Uncertainty-Aware, Covariate-Disentangled, Individualized Atlas Representations 
[[arxiv](https://arxiv.org/abs/2502.08445)] [[cool](https://papers.cool/arxiv/2502.08445)] [[pdf](https://arxiv.org/pdf/2502.08445)]
> **Authors**: Yining Jiao,Sreekalyani Bhamidi,Huaizhi Qu,Carlton Zdanski,Julia Kimbell,Andrew Prince,Cameron Worden,Samuel Kirse,Christopher Rutter,Benjamin Shields,William Dunn,Jisan Mahmud,Tianlong Chen,Marc Niethammer
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 28 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation. We develop $\texttt{LucidAtlas}$, an approach that can represent spatially varying information, and can capture the influence of covariates as well as population uncertainty. As a versatile atlas representation, $\texttt{LucidAtlas}$ offers robust capabilities for covariate interpretation, individualized prediction, population trend analysis, and uncertainty estimation, with the flexibility to incorporate prior knowledge. Additionally, we discuss the trustworthiness and potential risks of neural additive models for analyzing dependent covariates and then introduce a marginalization approach to explain the dependence of an individual predictor on the models' response (the atlas). To validate our method, we demonstrate its generalizability on two medical datasets. Our findings underscore the critical role of by-construction interpretable models in advancing scientific discovery. Our code will be publicly available upon acceptance.

### Closer through commonality: Enhancing hypergraph contrastive learning with shared groups 
[[arxiv](https://arxiv.org/abs/2502.08432)] [[cool](https://papers.cool/arxiv/2502.08432)] [[pdf](https://arxiv.org/pdf/2502.08432)]
> **Authors**: Daeyoung Roh,Donghee Han,Daehee Kim,Keejun Han,Mun Yi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 11page, 5 figures, 6 tables, 2024 IEEE International Conference on Big Data
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Hypergraphs provide a superior modeling framework for representing complex multidimensional relationships in the context of real-world interactions that often occur in groups, overcoming the limitations of traditional homogeneous graphs. However, there have been few studies on hypergraphbased contrastive learning, and existing graph-based contrastive learning methods have not been able to fully exploit the highorder correlation information in hypergraphs. Here, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method designed to exploit the complex high-dimensional information inherent in hypergraphs. While avoiding traditional graph augmentation methods that corrupt the hypergraph topology, the proposed method provides a simple and efficient learning augmentation function by adding noise to node features. Furthermore, we expands beyond the traditional dichotomous relationship between positive and negative samples in contrastive learning by introducing a new relationship of weak positives. It demonstrates the importance of fine-graining positive samples in contrastive learning. Therefore, HyFi is able to produce highquality embeddings, and outperforms both supervised and unsupervised baselines in average rank on node classification across 10 datasets. Our approach effectively exploits high-dimensional hypergraph information, shows significant improvement over existing graph-based contrastive learning methods, and is efficient in terms of training speed and GPU memory cost. The source code is available at https://github.com/Noverse0/HyFi.git.

### Enhanced Load Forecasting with GAT-LSTM: Leveraging Grid and Temporal Features 
[[arxiv](https://arxiv.org/abs/2502.08376)] [[cool](https://papers.cool/arxiv/2502.08376)] [[pdf](https://arxiv.org/pdf/2502.08376)]
> **Authors**: Ugochukwu Orji,Çiçek Güven,Dan Stowell
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信号处理
- **Abstract**: Accurate power load forecasting is essential for the efficient operation and planning of electrical grids, particularly given the increased variability and complexity introduced by renewable energy sources. This paper introduces GAT-LSTM, a hybrid model that combines Graph Attention Networks (GAT) and Long Short-Term Memory (LSTM) networks. A key innovation of the model is the incorporation of edge attributes, such as line capacities and efficiencies, into the attention mechanism, enabling it to dynamically capture spatial relationships grounded in grid-specific physical and operational constraints. Additionally, by employing an early fusion of spatial graph embeddings and temporal sequence features, the model effectively learns and predicts complex interactions between spatial dependencies and temporal patterns, providing a realistic representation of the dynamics of power grids. Experimental evaluations on the Brazilian Electricity System dataset demonstrate that the GAT-LSTM model significantly outperforms state-of-the-art models, achieving reductions of 21. 8% in MAE, 15. 9% in RMSE and 20. 2% in MAPE. These results underscore the robustness and adaptability of the GAT-LSTM model, establishing it as a powerful tool for applications in grid management and energy planning.

### Towards Principled Multi-Agent Task Agnostic Exploration 
[[arxiv](https://arxiv.org/abs/2502.08365)] [[cool](https://papers.cool/arxiv/2502.08365)] [[pdf](https://arxiv.org/pdf/2502.08365)]
> **Authors**: Riccardo Zamboni,Mirco Mutti,Marcello Restelli
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.

### A Survey on Pre-Trained Diffusion Model Distillations 
[[arxiv](https://arxiv.org/abs/2502.08364)] [[cool](https://papers.cool/arxiv/2502.08364)] [[pdf](https://arxiv.org/pdf/2502.08364)]
> **Authors**: Xuhui Fan,Zhangkai Wu,Hongyu Wu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion Models~(DMs) have emerged as the dominant approach in Generative Artificial Intelligence (GenAI), owing to their remarkable performance in tasks such as text-to-image synthesis. However, practical DMs, such as stable diffusion, are typically trained on massive datasets and thus usually require large storage. At the same time, many steps may be required, i.e., recursively evaluating the trained neural network, to generate a high-quality image, which results in significant computational costs during sample generation. As a result, distillation methods on pre-trained DM have become widely adopted practices to develop smaller, more efficient models capable of rapid, few-step generation in low-resource environment. When these distillation methods are developed from different perspectives, there is an urgent need for a systematic survey, particularly from a methodological perspective. In this survey, we review distillation methods through three aspects: output loss distillation, trajectory distillation and adversarial distillation. We also discuss current challenges and outline future research directions in the conclusion.

### Loss Landscape Analysis for Reliable Quantized ML Models for Scientific Sensing 
[[arxiv](https://arxiv.org/abs/2502.08355)] [[cool](https://papers.cool/arxiv/2502.08355)] [[pdf](https://arxiv.org/pdf/2502.08355)]
> **Authors**: Tommaso Baldi,Javier Campos,Olivia Weng,Caleb Geniesse,Nhan Tran,Ryan Kastner,Alessandro Biondi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this paper, we propose a method to perform empirical analysis of the loss landscape of machine learning (ML) models. The method is applied to two ML models for scientific sensing, which necessitates quantization to be deployed and are subject to noise and perturbations due to experimental conditions. Our method allows assessing the robustness of ML models to such effects as a function of quantization precision and under different regularization techniques -- two crucial concerns that remained underexplored so far. By investigating the interplay between performance, efficiency, and robustness by means of loss landscape analysis, we both established a strong correlation between gently-shaped landscapes and robustness to input and weight perturbations and observed other intriguing and non-obvious phenomena. Our method allows a systematic exploration of such trade-offs a priori, i.e., without training and testing multiple models, leading to more efficient development workflows. This work also highlights the importance of incorporating robustness into the Pareto optimization of ML models, enabling more reliable and adaptive scientific sensing systems.

### Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy 
[[arxiv](https://arxiv.org/abs/2502.08353)] [[cool](https://papers.cool/arxiv/2502.08353)] [[pdf](https://arxiv.org/pdf/2502.08353)]
> **Authors**: Ruizhan Xue,Huimin Deng,Fang He,Maojun Wang,Zeyu Zhang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Submitted to IJCAI 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness.

### Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems 
[[arxiv](https://arxiv.org/abs/2502.08340)] [[cool](https://papers.cool/arxiv/2502.08340)] [[pdf](https://arxiv.org/pdf/2502.08340)]
> **Authors**: Yuxin Pan,Ruohong Liu,Yize Chen,Zhiguang Cao,Fangzhen Lin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Accepted as a Full Paper at AAMAS 2025 (24th International Conference on Autonomous Agents and Multiagent Systems)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.)

### Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters 
[[arxiv](https://arxiv.org/abs/2502.08337)] [[cool](https://papers.cool/arxiv/2502.08337)] [[pdf](https://arxiv.org/pdf/2502.08337)]
> **Authors**: Soumyendu Sarkar,Avisek Naug,Antonio Guillen,Vineet Gundecha,Ricardo Luna Gutierrez,Sahand Ghorbanpour,Sajad Mousavi,Ashwin Ramesh Babu,Desik Rengarajan,Cullen Bash
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,系统与控制
- **Abstract**: Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability.

### Model-Free Counterfactual Subset Selection at Scale 
[[arxiv](https://arxiv.org/abs/2502.08326)] [[cool](https://papers.cool/arxiv/2502.08326)] [[pdf](https://arxiv.org/pdf/2502.08326)]
> **Authors**: Minh Hieu Nguyen,Viet Hung Doan,Anh Tuan Nguyen,Jun Jo,Quoc Viet Hung Nguyen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,数据库,数据结构和算法,信息检索
- **Abstract**: Ensuring transparency in AI decision-making requires interpretable explanations, particularly at the instance level. Counterfactual explanations are a powerful tool for this purpose, but existing techniques frequently depend on synthetic examples, introducing biases from unrealistic assumptions, flawed models, or skewed data. Many methods also assume full dataset availability, an impractical constraint in real-time environments where data flows continuously. In contrast, streaming explanations offer adaptive, real-time insights without requiring persistent storage of the entire dataset. This work introduces a scalable, model-free approach to selecting diverse and relevant counterfactual examples directly from observed data. Our algorithm operates efficiently in streaming settings, maintaining $O(\log k)$ update complexity per item while ensuring high-quality counterfactual selection. Empirical evaluations on both real-world and synthetic datasets demonstrate superior performance over baseline methods, with robust behavior even under adversarial conditions.

### HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.08302)] [[cool](https://papers.cool/arxiv/2502.08302)] [[pdf](https://arxiv.org/pdf/2502.08302)]
> **Authors**: Shibo Feng,Peilin Zhao,Liu Liu,Pengcheng Wu,Zhiqi Shen
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.

### Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes 
[[arxiv](https://arxiv.org/abs/2502.08282)] [[cool](https://papers.cool/arxiv/2502.08282)] [[pdf](https://arxiv.org/pdf/2502.08282)]
> **Authors**: Vinod Kumar Chauhan,Lei Clifton,Gaurav Nigam,David A. Clifton
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 6 pages (double column), 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \textit{composite treatments}, on a set of outcome variables of interest, referred to as \textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.

### GenIAS: Generator for Instantiating Anomalies in time Series 
[[arxiv](https://arxiv.org/abs/2502.08262)] [[cool](https://papers.cool/arxiv/2502.08262)] [[pdf](https://arxiv.org/pdf/2502.08262)]
> **Authors**: Zahra Zamanzadeh Darban,Qizhou Wang,Geoffrey I. Webb,Shirui Pan,Charu C. Aggarwal,Mahsa Salehi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A recent and promising approach for building time series anomaly detection (TSAD) models is to inject synthetic samples of anomalies within real data sets. The existing injection mechanisms have significant limitations - most of them rely on ad hoc, hand-crafted strategies which fail to capture the natural diversity of anomalous patterns, or are restricted to univariate time series settings. To address these challenges, we design a generative model for TSAD using a variational autoencoder, which is referred to as a Generator for Instantiating Anomalies in Time Series (GenIAS). GenIAS is designed to produce diverse and realistic synthetic anomalies for TSAD tasks. By employing a novel learned perturbation mechanism in the latent space and injecting the perturbed patterns in different segments of time series, GenIAS can generate anomalies with greater diversity and varying scales. Further, guided by a new triplet loss function, which uses a min-max margin and a new variance-scaling approach to further enforce the learning of compact normal patterns, GenIAS ensures that anomalies are distinct from normal samples while remaining realistic. The approach is effective for both univariate and multivariate time series. We demonstrate the diversity and realism of the generated anomalies. Our extensive experiments demonstrate that GenIAS - when integrated into a TSAD task - consistently outperforms seventeen traditional and deep anomaly detection models, thereby highlighting the potential of generative models for time series anomaly generation.

### Balancing optimism and pessimism in offline-to-online learning 
[[arxiv](https://arxiv.org/abs/2502.08259)] [[cool](https://papers.cool/arxiv/2502.08259)] [[pdf](https://arxiv.org/pdf/2502.08259)]
> **Authors**: Sentenac Flore,Lee Albin,Szepesvari Csaba
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently "covered" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting.

### Keep your distance: learning dispersed embeddings on $\mathbb{S}_d$ 
[[arxiv](https://arxiv.org/abs/2502.08231)] [[cool](https://papers.cool/arxiv/2502.08231)] [[pdf](https://arxiv.org/pdf/2502.08231)]
> **Authors**: Evgeniia Tokarchuk,Hua Chang Bakker,Vlad Niculae
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Learning well-separated features in high-dimensional spaces, such as text or image embeddings, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the dispersion of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a hypersphere, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, and moreover, dispersion is usually traded off with some other task-oriented training objective, making existing theoretical and numerical solutions inapplicable. Therefore, it is common to rely on gradient-based methods to encourage dispersion, usually by minimizing some function of the pairwise distances. In this work, we first give an overview of existing methods from disconnected literature, making new connections and highlighting similarities. Next, we introduce some new angles. We propose to reinterpret pairwise dispersion using a maximum mean discrepancy (MMD) motivation. We then propose an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an effective alternative regularizer for dispersion on generic domains. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.

### Enhancing Sample Selection by Cutting Mislabeled Easy Examples 
[[arxiv](https://arxiv.org/abs/2502.08227)] [[cool](https://papers.cool/arxiv/2502.08227)] [[pdf](https://arxiv.org/pdf/2502.08227)]
> **Authors**: Suqin Yuan,Lei Feng,Bo Han,Tongliang Liu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sample selection is a prevalent approach in learning with noisy labels, aiming to identify confident samples for training. Although existing sample selection methods have achieved decent results by reducing the noise rate of the selected subset, they often overlook that not all mislabeled examples harm the model's performance equally. In this paper, we demonstrate that mislabeled examples correctly predicted by the model early in the training process are particularly harmful to model performance. We refer to these examples as Mislabeled Easy Examples (MEEs). To address this, we propose Early Cutting, which introduces a recalibration step that employs the model's later training state to re-select the confident subset identified early in training, thereby avoiding misleading confidence from early learning and effectively filtering out MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets demonstrate that our method effectively improves sample selection and model performance by reducing MEEs.

### Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation 
[[arxiv](https://arxiv.org/abs/2502.08211)] [[cool](https://papers.cool/arxiv/2502.08211)] [[pdf](https://arxiv.org/pdf/2502.08211)]
> **Authors**: Jinda Xu,Yuhao Song,Daming Wang,Weiwei Zhao,Minghua Chen,Kangliang Chen,Qinya Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.

### Equivariant Masked Position Prediction for Efficient Molecular Representation 
[[arxiv](https://arxiv.org/abs/2502.08209)] [[cool](https://papers.cool/arxiv/2502.08209)] [[pdf](https://arxiv.org/pdf/2502.08209)]
> **Authors**: Junyi An,Chao Qu,Yun-Fei Shi,XinHao Liu,Qianwei Tang,Fenglei Cao,Yuan Qi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 24 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in https://github.com/ajy112/EMPP.

### Exploring Exploration in Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.08208)] [[cool](https://papers.cool/arxiv/2502.08208)] [[pdf](https://arxiv.org/pdf/2502.08208)]
> **Authors**: Leonard Papenmeier,Nuojin Cheng,Stephen Becker,Luigi Nardi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 28 pages, 34 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: A well-balanced exploration-exploitation trade-off is crucial for successful acquisition functions in Bayesian optimization. However, there is a lack of quantitative measures for exploration, making it difficult to analyze and compare different acquisition functions. This work introduces two novel approaches - observation traveling salesman distance and observation entropy - to quantify the exploration characteristics of acquisition functions based on their selected observations. Using these measures, we examine the explorative nature of several well-known acquisition functions across a diverse set of black-box problems, uncover links between exploration and empirical performance, and reveal new relationships among existing acquisition functions. Beyond enabling a deeper understanding of acquisition functions, these measures also provide a foundation for guiding their design in a more principled and systematic manner.

### Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency 
[[arxiv](https://arxiv.org/abs/2502.08206)] [[cool](https://papers.cool/arxiv/2502.08206)] [[pdf](https://arxiv.org/pdf/2502.08206)]
> **Authors**: Abdelkrim Alahyane,Céline Comte,Matthieu Jonckheere,Éric Moulines
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,表现,优化与控制,可能性
- **Abstract**: Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date. Second, we observe that naively optimizing this metric leads us to slow down the system drastically by overemphazing staleness at the detriment of throughput. This motivates us to introduce an alternative metric that also takes system speed into account, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show that these optimizations enhance accuracy by 10% to 30%.

### Wisdom of the Crowds in Forecasting: Forecast Summarization for Supporting Future Event Prediction 
[[arxiv](https://arxiv.org/abs/2502.08205)] [[cool](https://papers.cool/arxiv/2502.08205)] [[pdf](https://arxiv.org/pdf/2502.08205)]
> **Authors**: Anisha Saha,Adam Jatowt
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,信息检索
- **Abstract**: Future Event Prediction (FEP) is an essential activity whose demand and application range across multiple domains. While traditional methods like simulations, predictive and time-series forecasting have demonstrated promising outcomes, their application in forecasting complex events is not entirely reliable due to the inability of numerical data to accurately capture the semantic information related to events. One forecasting way is to gather and aggregate collective opinions on the future to make predictions as cumulative perspectives carry the potential to help estimating the likelihood of upcoming events. In this work, we organize the existing research and frameworks that aim to support future event prediction based on crowd wisdom through aggregating individual forecasts. We discuss the challenges involved, available datasets, as well as the scope of improvement and future research directions for this task. We also introduce a novel data model to represent individual forecast statements.

### Privacy amplification by random allocation 
[[arxiv](https://arxiv.org/abs/2502.08202)] [[cool](https://papers.cool/arxiv/2502.08202)] [[pdf](https://arxiv.org/pdf/2502.08202)]
> **Authors**: Vitaly Feldman,Moshe Shenfeld
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We consider the privacy guarantees of an algorithm in which a user's data is used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$ differentially private steps. We demonstrate that the privacy guarantees of this sampling scheme can be upper bound by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+ o(1))k/t $. Further, we provide two additional analysis techniques that lead to numerical improvements in some parameter regimes. The case of $k=1$ has been previously studied in the context of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024). Privacy analysis of Balle et al. (2020) relies on privacy amplification by shuffling which leads to overly conservative bounds. Privacy analysis of Chua et al. (2024a) relies on Monte Carlo simulations that are computationally prohibitive in many practical scenarios and have additional inherent limitations.

### Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning 
[[arxiv](https://arxiv.org/abs/2502.08181)] [[cool](https://papers.cool/arxiv/2502.08181)] [[pdf](https://arxiv.org/pdf/2502.08181)]
> **Authors**: M. Anwar Ma'sum,Mahardhika Pratama,Igor Skrjanc
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.

### DNNs May Determine Major Properties of Their Outputs Early, with Timing Possibly Driven by Bias 
[[arxiv](https://arxiv.org/abs/2502.08167)] [[cool](https://papers.cool/arxiv/2502.08167)] [[pdf](https://arxiv.org/pdf/2502.08167)]
> **Authors**: Song Park,Sanghyuk Chun,Byeongho Heo,Dongyoon Han
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: First two authors contributed equally
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: This paper argues that deep neural networks (DNNs) mostly determine their outputs during the early stages of inference, where biases inherent in the model play a crucial role in shaping this process. We draw a parallel between this phenomenon and human decision-making, which often relies on fast, intuitive heuristics. Using diffusion models (DMs) as a case study, we demonstrate that DNNs often make early-stage decision-making influenced by the type and extent of bias in their design and training. Our findings offer a new perspective on bias mitigation, efficient inference, and the interpretation of machine learning systems. By identifying the temporal dynamics of decision-making in DNNs, this paper aims to inspire further discussion and research within the machine learning community.

### Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly 
[[arxiv](https://arxiv.org/abs/2502.08160)] [[cool](https://papers.cool/arxiv/2502.08160)] [[pdf](https://arxiv.org/pdf/2502.08160)]
> **Authors**: Zhaomin Wu,Zhen Qin,Junyi Hou,Haodong Zhao,Qinbin Li,Bingsheng He,Lixin Fan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications.

### DGSense: A Domain Generalization Framework for Wireless Sensing 
[[arxiv](https://arxiv.org/abs/2502.08155)] [[cool](https://papers.cool/arxiv/2502.08155)] [[pdf](https://arxiv.org/pdf/2502.08155)]
> **Authors**: Rui Zhou,Yu Cheng,Songlin Li,Hongwang Zhang,Chenxu Liu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 15 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining.

### Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling 
[[arxiv](https://arxiv.org/abs/2502.08150)] [[cool](https://papers.cool/arxiv/2502.08150)] [[pdf](https://arxiv.org/pdf/2502.08150)]
> **Authors**: Yang Cao,Bo Chen,Xiaoyu Li,Yingyu Liang,Zhizhou Sha,Zhenmei Shi,Zhao Song,Mingda Wan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the \textit{half-moons} dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of \textbf{0.714}, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning.

### Knowledge-Guided Wasserstein Distributionally Robust Optimization 
[[arxiv](https://arxiv.org/abs/2502.08146)] [[cool](https://papers.cool/arxiv/2502.08146)] [[pdf](https://arxiv.org/pdf/2502.08146)]
> **Authors**: Zitao Wang,Ziyuan Wang,Molei Liu,Nian Si
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Transfer learning is a popular strategy to leverage external knowledge and improve statistical efficiency, particularly with a limited target sample. We propose a novel knowledge-guided Wasserstein Distributionally Robust Optimization (KG-WDRO) framework that adaptively incorporates multiple sources of external knowledge to overcome the conservativeness of vanilla WDRO, which often results in overly pessimistic shrinkage toward zero. Our method constructs smaller Wasserstein ambiguity sets by controlling the transportation along directions informed by the source knowledge. This strategy can alleviate perturbations on the predictive projection of the covariates and protect against information loss. Theoretically, we establish the equivalence between our WDRO formulation and the knowledge-guided shrinkage estimation based on collinear similarity, ensuring tractability and geometrizing the feasible set. This also reveals a novel and general interpretation for recent shrinkage-based transfer learning approaches from the perspective of distributional robustness. In addition, our framework can adjust for scaling differences in the regression models between the source and target and accommodates general types of regularization such as lasso and ridge. Extensive simulations demonstrate the superior performance and adaptivity of KG-WDRO in enhancing small-sample transfer learning.

### Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers 
[[arxiv](https://arxiv.org/abs/2502.08145)] [[cool](https://papers.cool/arxiv/2502.08145)] [[pdf](https://arxiv.org/pdf/2502.08145)]
> **Authors**: Siddharth Singh,Prajwal Singhania,Aditya Ranjan,John Kirchenbauer,Jonas Geiping,Yuxin Wen,Neel Jain,Abhimanyu Hans,Manli Shu,Aditya Tomar,Tom Goldstein,Abhinav Bhatele
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s). While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore "catastrophic memorization", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.

### Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in Multi-Armed Bandits using Stability-Penalty Matching 
[[arxiv](https://arxiv.org/abs/2502.08143)] [[cool](https://papers.cool/arxiv/2502.08143)] [[pdf](https://arxiv.org/pdf/2502.08143)]
> **Authors**: Quan Nguyen,Shinji Ito,Junpei Komiyama,Nishant A. Mehta
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Existing data-dependent and best-of-both-worlds regret bounds for multi-armed bandits problems have limited adaptivity as they are either data-dependent but not best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal $O(\sqrt{T\ln{T}})$ worst-case guarantee in the adversarial regime. To overcome these limitations, we propose real-time stability-penalty matching (SPM), a new method for obtaining regret bounds that are simultaneously data-dependent, best-of-both-worlds and $T$-optimal for multi-armed bandits problems. In particular, we show that real-time SPM obtains bounds with worst-case guarantees of order $O(\sqrt{T})$ in the adversarial regime and $O(\ln{T})$ in the stochastic regime while simultaneously being adaptive to data-dependent quantities such as sparsity, variations, and small losses. Our results are obtained by extending the SPM technique for tuning the learning rates in the follow-the-regularized-leader (FTRL) framework, which further indicates that the combination of SPM and FTRL is a promising approach for proving new adaptive bounds in online learning problems.

### LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits 
[[arxiv](https://arxiv.org/abs/2502.08141)] [[cool](https://papers.cool/arxiv/2502.08141)] [[pdf](https://arxiv.org/pdf/2502.08141)]
> **Authors**: Zikai Zhou,Qizheng Zhang,Hermann Kumbong,Kunle Olukotun
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,硬件架构,计算语言学,表现
- **Abstract**: Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization - mapping, threshold selection, and precision assignment - while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance-precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50%. Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments.

### In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation 
[[arxiv](https://arxiv.org/abs/2502.08136)] [[cool](https://papers.cool/arxiv/2502.08136)] [[pdf](https://arxiv.org/pdf/2502.08136)]
> **Authors**: Frank Cole,Yulong Lu,Tianhao Zhang,Yuxuan Zhao
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems. Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks. This result demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator. In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests a depth-separation phenomenon for transformers in the in-context learning of dynamical systems. Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data.

## 计算机科学中的逻辑(cs.LO:Logic in Computer Science)

### Proceedings 40th International Conference on Logic Programming 
[[arxiv](https://arxiv.org/abs/2502.08453)] [[cool](https://papers.cool/arxiv/2502.08453)] [[pdf](https://arxiv.org/pdf/2502.08453)]
> **Authors**: Pedro Cabalar,Francesco Fabiano,Martin Gebser,Gopal Gupta,Theresa Swift
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: ef:EPTCS 416, 2025
- **标题**: None
- **领域**: 计算机科学中的逻辑,人工智能
- **Abstract**: Since the first conference In Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include technical communications about, and abstracts for presentations given at the 40th ICLP held October 14-17, in Dallas Texas, USA. The papers and abstracts in this volume include the following areas and topics. Formal and operational semantics: including non-monotonic reasoning, probabilistic reasoning, argumentation, and semantic issues of combining logic with neural models. Language design and programming methodologies such as answer set programming. inductive logic programming, and probabilistic programming. Program analysis and logic-based validation of generated programs. Implementation methodologies including constraint implementation, tabling, Logic-based prompt engineering, and the interaction of logic programming with LLMs.

## 多代理系统(cs.MA:Multiagent Systems)

### Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control 
[[arxiv](https://arxiv.org/abs/2502.08681)] [[cool](https://papers.cool/arxiv/2502.08681)] [[pdf](https://arxiv.org/pdf/2502.08681)]
> **Authors**: Barbera de Mol,Davide Barbieri,Jan Viebahn,Davide Grossi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :I.2.11; I.2.8; I.2.1; I.2.6
- **标题**: None
- **领域**: 多代理系统,人工智能,机器学习
- **Abstract**: Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Mapping the Landscape of Generative AI in Network Monitoring and Management 
[[arxiv](https://arxiv.org/abs/2502.08576)] [[cool](https://papers.cool/arxiv/2502.08576)] [[pdf](https://arxiv.org/pdf/2502.08576)]
> **Authors**: Giampaolo Bovenzi,Francesco Cerasuolo,Domenico Ciuonzo,Davide Di Monda,Idio Guarino,Antonio Montieri,Valerio Persico,Antonio Pescapè
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 32 pages, 9 figure, 10 tables
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,机器学习
- **Abstract**: Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.

## 机器人技术(cs.RO:Robotics)

### 3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated Prompt Synthesis and Supervised Reasoning 
[[arxiv](https://arxiv.org/abs/2502.08903)] [[cool](https://papers.cool/arxiv/2502.08903)] [[pdf](https://arxiv.org/pdf/2502.08903)]
> **Authors**: Guoqin Tang,Qingxuan Jia,Zeyuan Huang,Gang Chen,Ning Ji,Zhipeng Yao
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Vision-language models (VLMs) have achieved remarkable success in scene understanding and perception tasks, enabling robots to plan and execute actions adaptively in dynamic environments. However, most multimodal large language models lack robust 3D scene localization capabilities, limiting their effectiveness in fine-grained robotic operations. Additionally, challenges such as low recognition accuracy, inefficiency, poor transferability, and reliability hinder their use in precision tasks. To address these limitations, we propose a novel framework that integrates a 2D prompt synthesis module by mapping 2D images to point clouds, and incorporates a small language model (SLM) for supervising VLM outputs. The 2D prompt synthesis module enables VLMs, trained on 2D images and text, to autonomously extract precise 3D spatial information without manual intervention, significantly enhancing 3D scene understanding. Meanwhile, the SLM supervises VLM outputs, mitigating hallucinations and ensuring reliable, executable robotic control code generation. Our framework eliminates the need for retraining in new environments, thereby improving cost efficiency and operational robustness. Experimental results that the proposed framework achieved a 96.0\% Task Success Rate (TSR), outperforming other methods. Ablation studies demonstrated the critical role of both the 2D prompt synthesis module and the output supervision module (which, when removed, caused a 67\% TSR drop). These findings validate the framework's effectiveness in improving 3D recognition, task planning, and robotic task execution.

### Acoustic Wave Manipulation Through Sparse Robotic Actuation 
[[arxiv](https://arxiv.org/abs/2502.08784)] [[cool](https://papers.cool/arxiv/2502.08784)] [[pdf](https://arxiv.org/pdf/2502.08784)]
> **Authors**: Tristan Shah,Noam Smilovich,Feruza Amirkulova,Samer Gerges,Stas Tiomkin
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: ICRA 2025
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Recent advancements in robotics, control, and machine learning have facilitated progress in the challenging area of object manipulation. These advancements include, among others, the use of deep neural networks to represent dynamics that are partially observed by robot sensors, as well as effective control using sparse control signals. In this work, we explore a more general problem: the manipulation of acoustic waves, which are partially observed by a robot capable of influencing the waves through spatially sparse actuators. This problem holds great potential for the design of new artificial materials, ultrasonic cutting tools, energy harvesting, and other applications. We develop an efficient data-driven method for robot learning that is applicable to either focusing scattered acoustic energy in a designated region or suppressing it, depending on the desired task. The proposed method is better in terms of a solution quality and computational complexity as compared to a state-of-the-art learning based method for manipulation of dynamical systems governed by partial differential equations. Furthermore our proposed method is competitive with a classical semi-analytical method in acoustics research on the demonstrated tasks. We have made the project code publicly available, along with a web page featuring video demonstrations: https://gladisor.github.io/waves/.

### LIR-LIVO: A Lightweight,Robust LiDAR/Vision/Inertial Odometry with Illumination-Resilient Deep Features 
[[arxiv](https://arxiv.org/abs/2502.08676)] [[cool](https://papers.cool/arxiv/2502.08676)] [[pdf](https://arxiv.org/pdf/2502.08676)]
> **Authors**: Shujie Zhou,Zihao Wang,Xinye Dai,Weiwei Song,Shengfeng Gu
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,信号处理,系统与控制
- **Abstract**: In this paper, we propose LIR-LIVO, a lightweight and robust LiDAR-inertial-visual odometry system designed for challenging illumination and degraded environments. The proposed method leverages deep learning-based illumination-resilient features and LiDAR-Inertial-Visual Odometry (LIVO). By incorporating advanced techniques such as uniform depth distribution of features enabled by depth association with LiDAR point clouds and adaptive feature matching utilizing Superpoint and LightGlue, LIR-LIVO achieves state-of-the-art (SOTA) accuracy and robustness with low computational cost. Experiments are conducted on benchmark datasets, including NTU-VIRAL, Hilti'22, and R3LIVE-Dataset. The corresponding results demonstrate that our proposed method outperforms other SOTA methods on both standard and challenging datasets. Particularly, the proposed method demonstrates robust pose estimation under poor ambient lighting conditions in the Hilti'22 dataset. The code of this work is publicly accessible on GitHub to facilitate advancements in the robotics community.

### Analyzable Parameters Dominated Vehicle Platoon Dynamics Modeling and Analysis: A Physics-Encoded Deep Learning Approach 
[[arxiv](https://arxiv.org/abs/2502.08658)] [[cool](https://papers.cool/arxiv/2502.08658)] [[pdf](https://arxiv.org/pdf/2502.08658)]
> **Authors**: Hao Lyu,Yanyong Guo,Pan Liu,Shuo Feng,Weilin Ren,Quansheng Yue
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: Recently, artificial intelligence (AI)-enabled nonlinear vehicle platoon dynamics modeling plays a crucial role in predicting and optimizing the interactions between vehicles. Existing efforts lack the extraction and capture of vehicle behavior interaction features at the platoon scale. More importantly, maintaining high modeling accuracy without losing physical analyzability remains to be solved. To this end, this paper proposes a novel physics-encoded deep learning network, named PeMTFLN, to model the nonlinear vehicle platoon dynamics. Specifically, an analyzable parameters encoded computational graph (APeCG) is designed to guide the platoon to respond to the driving behavior of the lead vehicle while ensuring local stability. Besides, a multi-scale trajectory feature learning network (MTFLN) is constructed to capture platoon following patterns and infer the physical parameters required for APeCG from trajectory data. The human-driven vehicle trajectory datasets (HIGHSIM) were used to train the proposed PeMTFLN. The trajectories prediction experiments show that PeMTFLN exhibits superior compared to the baseline models in terms of predictive accuracy in speed and gap. The stability analysis result shows that the physical parameters in APeCG is able to reproduce the platoon stability in real-world condition. In simulation experiments, PeMTFLN performs low inference error in platoon trajectories generation. Moreover, PeMTFLN also accurately reproduces ground-truth safety statistics. The code of proposed PeMTFLN is open source.

### Learning Humanoid Standing-up Control across Diverse Postures 
[[arxiv](https://arxiv.org/abs/2502.08378)] [[cool](https://papers.cool/arxiv/2502.08378)] [[pdf](https://arxiv.org/pdf/2502.08378)]
> **Authors**: Tao Huang,Junli Ren,Huayi Wang,Zirui Wang,Qingwei Ben,Muning Wen,Xiao Chen,Jianan Li,Jiangmiao Pang
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Humanoid Standing-up Control, 12 pages
- **标题**: None
- **领域**: 机器人技术,人工智能,机器学习
- **Abstract**: Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST effectively learns posture-adaptive motions by leveraging a multi-critic architecture and curriculum-based training on diverse simulated terrains. To ensure successful real-world deployment, we constrain the motion with smoothness regularization and implicit motion speed bound to alleviate oscillatory and violent motions on physical hardware, respectively. After simulation-based training, the learned control policies are directly deployed on the Unitree G1 humanoid robot. Our experimental results demonstrate that the controllers achieve smooth, stable, and robust standing-up motions across a wide range of laboratory and outdoor environments. Videos are available at https://taohuang13.github.io/humanoid-standingup.github.io/.

## 声音(cs.SD:Sound)

### TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument 
[[arxiv](https://arxiv.org/abs/2502.08939)] [[cool](https://papers.cool/arxiv/2502.08939)] [[pdf](https://arxiv.org/pdf/2502.08939)]
> **Authors**: Kyungsu Kim,Junghyun Koo,Sungho Lee,Haesun Joung,Kyogu Lee
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 5 pages, 1 figure, to be published in ICASSP 2025
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth

### Hookpad Aria: A Copilot for Songwriters 
[[arxiv](https://arxiv.org/abs/2502.08122)] [[cool](https://papers.cool/arxiv/2502.08122)] [[pdf](https://arxiv.org/pdf/2502.08122)]
> **Authors**: Chris Donahue,Shih-Lun Wu,Yewon Kim,Dave Carlton,Ryan Miyakawa,John Thickstun
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Extended abstract presented in the Late-Breaking Demo Session at ISMIR 2024 (ISMIR LBD 2024)
- **标题**: None
- **领域**: 声音,人工智能,机器学习
- **Abstract**: We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs. More information about Hookpad Aria is available at https://www.hooktheory.com/hookpad/aria

## 软件工程(cs.SE:Software Engineering)

### CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context, and Verification 
[[arxiv](https://arxiv.org/abs/2502.08806)] [[cool](https://papers.cool/arxiv/2502.08806)] [[pdf](https://arxiv.org/pdf/2502.08806)]
> **Authors**: Jiacheng Xu,Bo Pang,Jin Qu,Hiroaki Hayashi,Caiming Xiong,Yingbo Zhou
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 16 pages
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习
- **Abstract**: Software testing is a critical aspect of software development, yet generating test cases remains a routine task for engineers. This paper presents a benchmark, CLOVER, to evaluate models' capabilities in generating and completing test cases under specific conditions. Spanning from simple assertion completions to writing test cases that cover specific code blocks across multiple files, these tasks are based on 12 python repositories, analyzing 845 problems with context lengths ranging from 4k to 128k tokens. Utilizing code testing frameworks, we propose a method to construct retrieval contexts using coverage information. While models exhibit comparable performance with short contexts, notable differences emerge with 16k contexts. Notably, models like GPT-4o and Claude 3.5 can effectively leverage relevant snippets; however, all models score below 35\% on the complex Task III, even with the oracle context provided, underscoring the benchmark's significance and the potential for model improvement. The benchmark is containerized for code execution across tasks, and we will release the code, data, and construction methodologies.

## 社交和信息网络(cs.SI:Social and Information Networks)

### AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society 
[[arxiv](https://arxiv.org/abs/2502.08691)] [[cool](https://papers.cool/arxiv/2502.08691)] [[pdf](https://arxiv.org/pdf/2502.08691)]
> **Authors**: Jinghua Piao,Yuwei Yan,Jun Zhang,Nian Li,Junbo Yan,Xiaochong Lan,Zhihong Lu,Zhiheng Zheng,Jing Yi Wang,Di Zhou,Chen Gao,Fengli Xu,Fang Zhang,Ke Rong,Jun Su,Yong Li
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能
- **Abstract**: Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Color Universal Design Neural Network for the Color Vision Deficiencies 
[[arxiv](https://arxiv.org/abs/2502.08671)] [[cool](https://papers.cool/arxiv/2502.08671)] [[pdf](https://arxiv.org/pdf/2502.08671)]
> **Authors**: Sunyong Seo,Jinho Park
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: 12 pages, 10 figures
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Information regarding images should be visually understood by anyone, including those with color deficiency. However, such information is not recognizable if the color that seems to be distorted to the color deficiencies meets an adjacent object. The aim of this paper is to propose a color universal design network, called CUD-Net, that generates images that are visually understandable by individuals with color deficiency. CUD-Net is a convolutional deep neural network that can preserve color and distinguish colors for input images by regressing the node point of a piecewise linear function and using a specific filter for each image. To generate CUD images for color deficiencies, we follow a four-step process. First, we refine the CUD dataset based on specific criteria by color experts. Second, we expand the input image information through pre-processing that is specialized for color deficiency vision. Third, we employ a multi-modality fusion architecture to combine features and process the expanded images. Finally, we propose a conjugate loss function based on the composition of the predicted image through the model to address one-to-many problems that arise from the dataset. Our approach is able to produce high-quality CUD images that maintain color and contrast stability. The code for CUD-Net is available on the GitHub repository

### Unpaired Image-to-Image Translation with Content Preserving Perspective: A Review 
[[arxiv](https://arxiv.org/abs/2502.08667)] [[cool](https://papers.cool/arxiv/2502.08667)] [[pdf](https://arxiv.org/pdf/2502.08667)]
> **Authors**: Mehran Safayani,Behnaz Mirzapour,Hanieh aghaebrahimiyan,Nasrin Salehi,Hamid Ravaee
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Image-to-image translation (I2I) transforms an image from a source domain to a target domain while preserving source content. Most computer vision applications are in the field of image-to-image translation, such as style transfer, image segmentation, and photo enhancement. The degree of preservation of the content of the source images in the translation process can be different according to the problem and the intended application. From this point of view, in this paper, we divide the different tasks in the field of image-to-image translation into three categories: Fully Content preserving, Partially Content preserving, and Non-Content preserving. We present different tasks, datasets, methods, results of methods for these three categories in this paper. We make a categorization for I2I methods based on the architecture of different models and study each category separately. In addition, we introduce well-known evaluation criteria in the I2I translation field. Specifically, nearly 70 different I2I models were analyzed, and more than 10 quantitative evaluation metrics and 30 distinct tasks and datasets relevant to the I2I translation problem were both introduced and assessed. Translating from simulation to real images could be well viewed as an application of fully content preserving or partially content preserving unsupervised image-to-image translation methods. So, we provide a benchmark for Sim-to-Real translation, which can be used to evaluate different methods. In general, we conclude that because of the different extent of the obligation to preserving content in various applications, it is better to consider this issue in choosing a suitable I2I model for a specific application.

### Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale Implicit Neural Representation 
[[arxiv](https://arxiv.org/abs/2502.08634)] [[cool](https://papers.cool/arxiv/2502.08634)] [[pdf](https://arxiv.org/pdf/2502.08634)]
> **Authors**: Jun Lyu,Lipeng Ning,William Consagra,Qiang Liu,Richard J. Rushmore,Berkin Bilgic,Yogesh Rathi
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,机器学习
- **Abstract**: Purpose: To develop and validate a novel image reconstruction technique using implicit neural representations (INR) for multi-view thick-slice acquisitions while reducing the scan time but maintaining high signal-to-noise ratio (SNR). Methods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised neural network-based algorithm designed to reconstruct MRI data from multi-view thick slices, effectively reducing scan time by 2-fold while maintaining fine anatomical details. We compare our method to both bicubic interpolation and the current state-of-the-art regularized least-squares super-resolution reconstruction (LS-SRR) technique. Validation is performed using ground-truth ex-vivo monkey brain data, and we demonstrate superior reconstruction quality across several in-vivo human datasets. Notably, we achieve the reconstruction of a whole human brain in-vivo T2-weighted image with an unprecedented 180μm isotropic spatial resolution, accomplished in just 17 minutes of scan time on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in terms of reconstruction quality with 22.4% lower relative error (RE) and 7.5% lower full-width half maximum (FWHM) indicating better preservation of fine structural details in nearly half the scan time. Conclusion: ROVER-MRI offers an efficient and robust approach for mesoscale MR imaging, enabling rapid, high-resolution whole-brain scans. Its versatility holds great promise for research applications requiring anatomical details and time-efficient imaging.

## 信号处理(eess.SP:Signal Processing)

### Compression of Site-Specific Deep Neural Networks for Massive MIMO Precoding 
[[arxiv](https://arxiv.org/abs/2502.08758)] [[cool](https://papers.cool/arxiv/2502.08758)] [[pdf](https://arxiv.org/pdf/2502.08758)]
> **Authors**: Ghazal Kasalaee,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: This preprint comprises 6 pages and features 3 figures. It has been accepted to the IEEE International Conference onMachineLearningand Computer Networking (ICMLCN) 2025
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: The deployment of deep learning (DL) models for precoding in massive multiple-input multiple-output (mMIMO) systems is often constrained by high computational demands and energy consumption. In this paper, we investigate the compute energy efficiency of mMIMO precoders using DL-based approaches, comparing them to conventional methods such as zero forcing and weighted minimum mean square error (WMMSE). Our energy consumption model accounts for both memory access and calculation energy within DL accelerators. We propose a framework that incorporates mixed-precision quantization-aware training and neural architecture search to reduce energy usage without compromising accuracy. Using a ray-tracing dataset covering various base station sites, we analyze how site-specific conditions affect the energy efficiency of compressed models. Our results show that deep neural network compression generates precoders with up to 35 times higher energy efficiency than WMMSE at equal performance, depending on the scenario and the desired rate. These results establish a foundation and a benchmark for the development of energy-efficient DL-based mMIMO precoders.

### A Low-Complexity Plug-and-Play Deep Learning Model for Massive MIMO Precoding Across Sites 
[[arxiv](https://arxiv.org/abs/2502.08757)] [[cool](https://papers.cool/arxiv/2502.08757)] [[pdf](https://arxiv.org/pdf/2502.08757)]
> **Authors**: Ali Hasanzadeh Karkan,Ahmed Ibrahim,Jean-François Frigon,François Leduc-Primeau
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: This preprint comprises 6 pages and features 2 figures. It has been accepted to the IEEE International Conference onMachineLearningand Computer Networking (ICMLCN) 2025
- **标题**: None
- **领域**: 信号处理,机器学习
- **Abstract**: Massive multiple-input multiple-output (mMIMO) technology has transformed wireless communication by enhancing spectral efficiency and network capacity. This paper proposes a novel deep learning-based mMIMO precoder to tackle the complexity challenges of existing approaches, such as weighted minimum mean square error (WMMSE), while leveraging meta-learning domain generalization and a teacher-student architecture to improve generalization across diverse communication environments. When deployed to a previously unseen site, the proposed model achieves excellent sum-rate performance while maintaining low computational complexity by avoiding matrix inversions and by using a simpler neural network structure. The model is trained and tested on a custom ray-tracing dataset composed of several base station locations. The experimental results indicate that our method effectively balances computational efficiency with high sum-rate performance while showcasing strong generalization performance in unseen environments. Furthermore, with fine-tuning, the proposed model outperforms WMMSE across all tested sites and SNR conditions while reducing complexity by at least 73$\times$.

### Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or Learning-Based? 
[[arxiv](https://arxiv.org/abs/2502.08637)] [[cool](https://papers.cool/arxiv/2502.08637)] [[pdf](https://arxiv.org/pdf/2502.08637)]
> **Authors**: Xiaoxia Xu,Xidong Mu,Yuanwei Liu,Arumugam Nallanathan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: Submitted to IEEE
- **标题**: None
- **领域**: 信号处理,信息论,机器学习
- **Abstract**: A novel pinching antenna system (PASS)-enabled downlink multi-user multiple-input single-output (MISO) framework is proposed. PASS consists of multiple waveguides spanning over thousands of wavelength, which equip numerous low-cost dielectric particles, named pinching antennas (PAs), to radiate signals into free space. The positions of PAs can be reconfigured to change both the large-scale path losses and phases of signals, thus facilitating the novel pinching beamforming design. A sum rate maximization problem is formulated, which jointly optimizes the transmit and pinching beamforming to adaptively achieve constructive signal enhancement and destructive interference mitigation. To solve this highly coupled and nonconvex problem, both optimization-based and learning-based methods are proposed. 1) For the optimization-based method, a majorization-minimization and penalty dual decomposition (MM-PDD) algorithm is developed, which handles the nonconvex complex exponential component using a Lipschitz surrogate function and then invokes PDD for problem decoupling. 2) For the learning-based method, a novel Karush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which enables KKT solutions to be reconstructed in a data-driven manner by learning dual variables. Following this idea, a KDL-Tranformer algorithm is developed, which captures both inter-PA/inter-user dependencies and channel-state-information (CSI)-beamforming dependencies by attention mechanisms. Simulation results demonstrate that: i) The proposed PASS framework significantly outperforms conventional massive multiple input multiple output (MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve over 30% system performance than MM-PDD algorithm, while achieving a millisecond-level response on modern GPUs.

### Semantic Learning for Molecular Communication in Internet of Bio-Nano Things 
[[arxiv](https://arxiv.org/abs/2502.08426)] [[cool](https://papers.cool/arxiv/2502.08426)] [[pdf](https://arxiv.org/pdf/2502.08426)]
> **Authors**: Hanlin Cai,Ozgur B. Akan
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 4 pages, 3 figures, 1 table
- **标题**: None
- **领域**: 信号处理,新兴技术,机器学习,图像和视频处理
- **Abstract**: Molecular communication (MC) provides a foundational framework for information transmission in the Internet of Bio-Nano Things (IoBNT), where efficiency and reliability are crucial. However, the inherent limitations of molecular channels, such as low transmission rates, noise, and inter-symbol interference (ISI), limit their ability to support complex data transmission. This paper proposes an end-to-end semantic learning framework designed to optimize task-oriented molecular communication, with a focus on biomedical diagnostic tasks under resource-constrained conditions. The proposed framework employs a deep encoder-decoder architecture to efficiently extract, quantize, and decode semantic features, prioritizing task-relevant semantic information to enhance diagnostic classification performance. Additionally, a probabilistic channel network is introduced to approximate molecular propagation dynamics, enabling gradient-based optimization for end-to-end learning. Experimental results demonstrate that the proposed semantic framework improves diagnostic accuracy by at least 25% compared to conventional JPEG compression with LDPC coding methods under resource-constrained communication scenarios.

## 系统与控制(eess.SY:Systems and Control)

### Integrated Optimization and Game Theory Framework for Fair Cost Allocation in Community Microgrids 
[[arxiv](https://arxiv.org/abs/2502.08953)] [[cool](https://papers.cool/arxiv/2502.08953)] [[pdf](https://arxiv.org/pdf/2502.08953)]
> **Authors**: K. Victor Sam Moses Babu,Pratyush Chakraborty,Mayukha Pal
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: Fair cost allocation in community microgrids remains a significant challenge due to the complex interactions between multiple participants with varying load profiles, distributed energy resources, and storage systems. Traditional cost allocation methods often fail to adequately address the dynamic nature of participant contributions and benefits, leading to inequitable distribution of costs and reduced participant satisfaction. This paper presents a novel framework integrating multi-objective optimization with cooperative game theory for fair and efficient microgrid operation and cost allocation. The proposed approach combines mixed-integer linear programming for optimal resource dispatch with Shapley value analysis for equitable benefit distribution, ensuring both system efficiency and participant satisfaction. The framework was validated using real-world data across six distinct operational scenarios, demonstrating significant improvements in both technical and economic performance. Results show peak demand reductions ranging from 7.8% to 62.6%, solar utilization rates reaching 114.8% through effective storage integration, and cooperative gains of up to $1,801.01 per day. The Shapley value-based allocation achieved balanced benefit-cost distributions, with net positions ranging from -16.0% to +14.2% across different load categories, ensuring sustainable participant cooperation.

### Demand Response Optimization MILP Framework for Microgrids with DERs 
[[arxiv](https://arxiv.org/abs/2502.08764)] [[cool](https://papers.cool/arxiv/2502.08764)] [[pdf](https://arxiv.org/pdf/2502.08764)]
> **Authors**: K. Victor Sam Moses Babu,Pratyush Chakraborty,Mayukha Pal
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: The integration of renewable energy sources in microgrids introduces significant operational challenges due to their intermittent nature and the mismatch between generation and demand patterns. Effective demand response (DR) strategies are crucial for maintaining system stability and economic efficiency, particularly in microgrids with high renewable penetration. This paper presents a comprehensive mixed-integer linear programming (MILP) framework for optimizing DR operations in a microgrid with solar generation and battery storage systems. The framework incorporates load classification, dynamic price thresholding, and multi-period coordination for optimal DR event scheduling. Analysis across seven distinct operational scenarios demonstrates consistent peak load reduction of 10\% while achieving energy cost savings ranging from 13.1\% to 38.0\%. The highest performance was observed in scenarios with high solar generation, where the framework achieved 38.0\% energy cost reduction through optimal coordination of renewable resources and DR actions. The results validate the framework's effectiveness in managing diverse operational challenges while maintaining system stability and economic efficiency.

## 历史与概述(math.HO:History and Overview)

### Mathematical Data Science 
[[arxiv](https://arxiv.org/abs/2502.08620)] [[cool](https://papers.cool/arxiv/2502.08620)] [[pdf](https://arxiv.org/pdf/2502.08620)]
> **Authors**: Michael R. Douglas,Kyu-Hwan Lee
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 历史与概述,机器学习,组合学,数论,表征论
- **Abstract**: Can machine learning help discover new mathematical structures? In this article we discuss an approach to doing this which one can call "mathematical data science". In this paradigm, one studies mathematical objects collectively rather than individually, by creating datasets and doing machine learning experiments and interpretations. After an overview, we present two case studies: murmurations in number theory and loadings of partitions related to Kronecker coefficients in representation theory and combinatorics.

## 数值分析(math.NA:Numerical Analysis)

### Numerical Schemes for Signature Kernels 
[[arxiv](https://arxiv.org/abs/2502.08470)] [[cool](https://papers.cool/arxiv/2502.08470)] [[pdf](https://arxiv.org/pdf/2502.08470)]
> **Authors**: Thomas Cass,Francesco Piatti,Jeffrey Pei
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: :65N22
- **标题**: None
- **领域**: 数值分析,机器学习,偏微分方程分析
- **Abstract**: Signature kernels have emerged as a powerful tool within kernel methods for sequential data. In the paper "The Signature Kernel is the solution of a Goursat PDE", the authors identify a kernel trick that demonstrates that, for continuously differentiable paths, the signature kernel satisfies a Goursat problem for a hyperbolic partial differential equation (PDE) in two independent time variables. While finite difference methods have been explored for this PDE, they face limitations in accuracy and stability when handling highly oscillatory inputs. In this work, we introduce two advanced numerical schemes that leverage polynomial representations of boundary conditions through either approximation or interpolation techniques, and rigorously establish the theoretical convergence of the polynomial approximation scheme. Experimental evaluations reveal that our approaches yield improvements of several orders of magnitude in mean absolute percentage error (MAPE) compared to traditional finite difference schemes, without increasing computational complexity. Furthermore, like finite difference methods, our algorithms can be GPU-parallelized to reduce computational complexity from quadratic to linear in the length of the input sequences, thereby improving scalability for high-frequency data. We have implemented these algorithms in a dedicated Python library, which is publicly available at: https://github.com/FrancescoPiatti/polysigkernel.

## 优化与控制(math.OC:Optimization and Control)

### Strong bounds for large-scale Minimum Sum-of-Squares Clustering 
[[arxiv](https://arxiv.org/abs/2502.08397)] [[cool](https://papers.cool/arxiv/2502.08397)] [[pdf](https://arxiv.org/pdf/2502.08397)]
> **Authors**: Anna Livia Croella,Veronica Piccialli,Antonio M. Sudoso
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: Clustering is a fundamental technique in data analysis and machine learning, used to group similar data points together. Among various clustering methods, the Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used. MSSC aims to minimize the total squared Euclidean distance between data points and their corresponding cluster centroids. Due to the unsupervised nature of clustering, achieving global optimality is crucial, yet computationally challenging. The complexity of finding the global solution increases exponentially with the number of data points, making exact methods impractical for large-scale datasets. Even obtaining strong lower bounds on the optimal MSSC objective value is computationally prohibitive, making it difficult to assess the quality of heuristic solutions. We address this challenge by introducing a novel method to validate heuristic MSSC solutions through optimality gaps. Our approach employs a divide-and-conquer strategy, decomposing the problem into smaller instances that can be handled by an exact solver. The decomposition is guided by an auxiliary optimization problem, the "anticlustering problem", for which we design an efficient heuristic. Computational experiments demonstrate the effectiveness of the method for large-scale instances, achieving optimality gaps below 3% in most cases while maintaining reasonable computational times. These results highlight the practicality of our approach in assessing feasible clustering solutions for large datasets, bridging a critical gap in MSSC evaluation.

## 方法论(stat.ME:Methodology)

### Treatment response as a latent variable 
[[arxiv](https://arxiv.org/abs/2502.08776)] [[cool](https://papers.cool/arxiv/2502.08776)] [[pdf](https://arxiv.org/pdf/2502.08776)]
> **Authors**: Christopher Tosh,Boyuan Zhang,Wesley Tansey
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,机器学习,机器学习
- **Abstract**: Scientists often need to analyze the samples in a study that responded to treatment in order to refine their hypotheses and find potential causal drivers of response. Natural variation in outcomes makes teasing apart responders from non-responders a statistical inference problem. To handle latent responses, we introduce the causal two-groups (C2G) model, a causal extension of the classical two-groups model. The C2G model posits that treated samples may or may not experience an effect, according to some prior probability. We propose two empirical Bayes procedures for the causal two-groups model, one under semi-parametric conditions and another under fully nonparametric conditions. The semi-parametric model assumes additive treatment effects and is identifiable from observed data. The nonparametric model is unidentifiable, but we show it can still be used to test for response in each treated sample. We show empirically and theoretically that both methods for selecting responders control the false discovery rate at the target level with near-optimal power. We also propose two novel estimands of interest and provide a strategy for deriving estimand intervals in the unidentifiable nonparametric model. On a cancer immunotherapy dataset, the nonparametric C2G model recovers clinically-validated predictive biomarkers of both positive and negative outcomes. Code is available at https://github.com/tansey-lab/causal2groups.

## 机器学习(stat.ML:Machine Learning)

### A Bayesian Nonparametric Perspective on Mahalanobis Distance for Out of Distribution Detection 
[[arxiv](https://arxiv.org/abs/2502.08695)] [[cool](https://papers.cool/arxiv/2502.08695)] [[pdf](https://arxiv.org/pdf/2502.08695)]
> **Authors**: Randolph W. Linderman,Yiran Chen,Scott W. Linderman
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 32 pages, 5 figures, code is available at https://github.com/rwl93/bnp4ood
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Bayesian nonparametric methods are naturally suited to the problem of out-of-distribution (OOD) detection. However, these techniques have largely been eschewed in favor of simpler methods based on distances between pre-trained or learned embeddings of data points. Here we show a formal relationship between Bayesian nonparametric models and the relative Mahalanobis distance score (RMDS), a commonly used method for OOD detection. Building on this connection, we propose Bayesian nonparametric mixture models with hierarchical priors that generalize the RMDS. We evaluate these models on the OpenOOD detection benchmark and show that Bayesian nonparametric methods can improve upon existing OOD methods, especially in regimes where training classes differ in their covariance structure and where there are relatively few data points per class.

### Concentration Inequalities for the Stochastic Optimization of Unbounded Objectives with Application to Denoising Score Matching 
[[arxiv](https://arxiv.org/abs/2502.08628)] [[cool](https://papers.cool/arxiv/2502.08628)] [[pdf](https://arxiv.org/pdf/2502.08628)]
> **Authors**: Jeremiah Birrell
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 30 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We derive novel concentration inequalities that bound the statistical error for a large class of stochastic optimization problems, focusing on the case of unbounded objective functions. Our derivations utilize the following tools: 1) A new form of McDiarmid's inequality that is based on sample dependent one component difference bounds and which leads to a novel uniform law of large numbers result for unbounded functions. 2) A Rademacher complexity bound for families of functions that satisfy an appropriate local Lipschitz property. As an application of these results, we derive statistical error bounds for denoising score matching (DSM), an application that inherently requires one to consider unbounded objective functions, even when the data distribution has bounded support. In addition, our results establish the benefit of sample reuse in algorithms that employ easily sampled auxiliary random variables in addition to the training data, e.g., as in DSM, which uses auxiliary Gaussian random variables.

### Multifidelity Simulation-based Inference for Computationally Expensive Simulators 
[[arxiv](https://arxiv.org/abs/2502.08416)] [[cool](https://papers.cool/arxiv/2502.08416)] [[pdf](https://arxiv.org/pdf/2502.08416)]
> **Authors**: Anastasia N. Krouglova,Hayden R. Johnson,Basile Confavreux,Michael Deistler,Pedro J. Gonçalves
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Across many domains of science, stochastic models are an essential tool to understand the mechanisms underlying empirically observed data. Models can be of different levels of detail and accuracy, with models of high-fidelity (i.e., high accuracy) to the phenomena under study being often preferable. However, inferring parameters of high-fidelity models via simulation-based inference is challenging, especially when the simulator is computationally expensive. We introduce MF-NPE, a multifidelity approach to neural posterior estimation that leverages inexpensive low-fidelity simulations to infer parameters of high-fidelity simulators within a limited simulation budget. MF-NPE performs neural posterior estimation with limited high-fidelity resources by virtue of transfer learning, with the ability to prioritize individual observations using active learning. On one statistical task with analytical ground-truth and two real-world tasks, MF-NPE shows comparable performance to current approaches while requiring up to two orders of magnitude fewer high-fidelity simulations. Overall, MF-NPE opens new opportunities to perform efficient Bayesian inference on computationally expensive simulators.

### Sparse Estimation of Inverse Covariance and Partial Correlation Matrices via Joint Partial Regression 
[[arxiv](https://arxiv.org/abs/2502.08414)] [[cool](https://papers.cool/arxiv/2502.08414)] [[pdf](https://arxiv.org/pdf/2502.08414)]
> **Authors**: Samuel Erickson,Tobias Rydén
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present a new method for estimating high-dimensional sparse partial correlation and inverse covariance matrices, which exploits the connection between the inverse covariance matrix and linear regression. The method is a two-stage estimation method wherein each individual feature is regressed on all other features while positive semi-definiteness is enforced simultaneously. We provide statistical rates of convergence for the proposed method which match, and improve upon, the state-of-the-art for inverse covariance and partial correlation matrix estimation, respectively. We also propose an efficient proximal splitting algorithm for numerically computing the estimate. The effectiveness of the proposed method is demonstrated on both synthetic and real-world data.

### Multi-View Oriented GPLVM: Expressiveness and Efficiency 
[[arxiv](https://arxiv.org/abs/2502.08253)] [[cool](https://papers.cool/arxiv/2502.08253)] [[pdf](https://arxiv.org/pdf/2502.08253)]
> **Authors**: Zi Yang,Ying Li,Zhidi Lin,Michael Minyi Zhang,Pablo M. Olmos
> **First submission**: 2025-02-12
> **First announcement**: 2025-02-13
> **comment**: 8 pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The multi-view Gaussian process latent variable model (MV-GPLVM) aims to learn a unified representation from multi-view data but is hindered by challenges such as limited kernel expressiveness and low computational efficiency. To overcome these issues, we first introduce a new duality between the spectral density and the kernel function. By modeling the spectral density with a bivariate Gaussian mixture, we then derive a generic and expressive kernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the inherent computational inefficiency of the NG-SM kernel, we propose a random Fourier feature approximation. Combined with a tailored reparameterization trick, this approximation enables scalable variational inference for both the model and the unified latent representations. Numerical evaluations across a diverse range of multi-view datasets demonstrate that our proposed method consistently outperforms state-of-the-art models in learning meaningful latent representations.

## 其他论文

- [Quantum Software Engineering and Potential of Quantum Computing in Software Engineering Research: A Review](https://arxiv.org/abs/2502.08925)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Brain in the Dark: Design Principles for Neuromimetic Inference under the Free Energy Principle](https://arxiv.org/abs/2502.08860)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [Generative AI & Changing Work: Systematic Review of Practitioner-led Work Transformations through the Lens of Job Crafting](https://arxiv.org/abs/2502.08854)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Optimal Dataset Size for Recommender Systems: Evaluating Algorithms' Performance via Downsampling](https://arxiv.org/abs/2502.08845)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [LayeredSense: Hierarchical Recognition of Complex Daily Activities Using Wearable Sensors](https://arxiv.org/abs/2502.08833)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [ClipRover: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots](https://arxiv.org/abs/2502.08791)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Europe's AI Imperative -- A Pragmatic Blueprint for Global Tech Leadership](https://arxiv.org/abs/2502.08781)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Beyond the Lens: Quantifying the Impact of Scientific Documentaries through Amazon Reviews](https://arxiv.org/abs/2502.08705)
  - **标题**: None
  - **Filtered Reason**: none of cs.DL,cs.CY,physics.ed-ph in whitelist
- [Enhanced LSTM by Attention Mechanism for Early Detection of Parkinson's Disease through Voice Signals](https://arxiv.org/abs/2502.08672)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD in whitelist
- [Deployment-friendly Lane-changing Intention Prediction Powered by Brain-inspired Spiking Neural Networks](https://arxiv.org/abs/2502.08659)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Who is Responsible? The Data, Models, Users or Regulations? Responsible Generative AI for a Sustainable Future](https://arxiv.org/abs/2502.08650)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data](https://arxiv.org/abs/2502.08649)
  - **标题**: None
  - **Filtered Reason**: none of stat.ME,cs.CY,cs.DB in whitelist
- [Scientific Map of Artificial Intelligence in Communication (2004-2024)](https://arxiv.org/abs/2502.08648)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [SportsBuddy: Designing and Evaluating an AI-Powered Sports Video Storytelling Tool Through Real-World Deployment](https://arxiv.org/abs/2502.08621)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Foundations of Digital Circuits: Denotation, Operational, and Algebraic Semantics](https://arxiv.org/abs/2502.08497)
  - **标题**: None
  - **Filtered Reason**: none of math.CT,cs.PL,cs.LO,math.LO in whitelist
- [The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities](https://arxiv.org/abs/2502.08381)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [SoK: Where to Fuzz? Assessing Target Selection Methods in Directed Fuzzing](https://arxiv.org/abs/2502.08341)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE,cs.CR in whitelist
- [Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model](https://arxiv.org/abs/2502.08309)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [MoLoRec: A Generalizable and Efficient Framework for LLM-Based Recommendation](https://arxiv.org/abs/2502.08271)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [FixDrive: Automatically Repairing Autonomous Vehicle Driving Behaviour for $0.08 per Violation](https://arxiv.org/abs/2502.08260)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [GenAI as Digital Plastic: Understanding Synthetic Media Through Critical AI Literacy](https://arxiv.org/abs/2502.08249)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [ChemZIP: Accelerated Modeling of Complex Aerothermochemical Interactions in Novel Turbomachines for Sustainable High-Temperature Chemical Processes](https://arxiv.org/abs/2502.08232)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE in whitelist
- [Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause Analysis](https://arxiv.org/abs/2502.08224)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Typographic Attacks in a Multi-Image Setting](https://arxiv.org/abs/2502.08193)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Memory Offloading for Large Language Model Inference with Latency SLO Guarantees](https://arxiv.org/abs/2502.08182)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Intention is All You Need: Refining Your Code from Your Intention](https://arxiv.org/abs/2502.08172)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
