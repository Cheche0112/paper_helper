> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-05

共有276篇相关领域论文, 另有36篇其他

## 天体物理学仪器和方法(astro-ph.IM:Instrumentation and Methods for Astrophysics)

### Astromer 2 
[[arxiv](https://arxiv.org/abs/2502.02717)] [[cool](https://papers.cool/arxiv/2502.02717)] [[pdf](https://arxiv.org/pdf/2502.02717)]
> **Authors**: Cristobal Donoso-Oliva,Ignacio Becker,Pavlos Protopapas,Guillermo Cabrera-Vives,Martina Cádiz-Leyton,Daniel Moreno-Cartagena
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 10 pages, 17 figures
- **标题**: None
- **领域**: 天体物理学仪器和方法,人工智能,机器学习
- **Abstract**: Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.

## 人工智能(cs.AI:Artificial Intelligence)

### SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions 
[[arxiv](https://arxiv.org/abs/2502.02883)] [[cool](https://papers.cool/arxiv/2502.02883)] [[pdf](https://arxiv.org/pdf/2502.02883)]
> **Authors**: Xiaofan Yu,Lanxiang Hu,Benjamin Reichman,Dylan Chu,Rushil Chandrupatla,Xiyuan Zhang,Larry Heck,Tajana Rosing
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Under review
- **标题**: None
- **领域**: 人工智能,人机交互
- **Abstract**: Natural language interaction with sensing systems is crucial for enabling all users to comprehend sensor data and its impact on their everyday lives. However, existing systems, which typically operate in a Question Answering (QA) manner, are significantly limited in terms of the duration and complexity of sensor data they can handle. In this work, we introduce SensorChat, the first end-to-end QA system designed for long-term sensor monitoring with multimodal and high-dimensional data including time series. SensorChat effectively answers both qualitative (requiring high-level reasoning) and quantitative (requiring accurate responses derived from sensor data) questions in real-world scenarios. To achieve this, SensorChat uses an innovative three-stage pipeline that includes question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) for intuitive human interactions and to guide the sensor data query process. Unlike existing multimodal LLMs, SensorChat incorporates an explicit query stage to precisely extract factual information from long-duration sensor data. We implement SensorChat and demonstrate its capability for real-time interactions on a cloud server while also being able to run entirely on edge platforms after quantization. Comprehensive QA evaluations show that SensorChat achieves up to 26% higher answer accuracy than state-of-the-art systems on quantitative questions. Additionally, a user study with eight volunteers highlights SensorChat's effectiveness in handling qualitative and open-ended questions.

### Fully Autonomous AI Agents Should Not be Developed 
[[arxiv](https://arxiv.org/abs/2502.02649)] [[cool](https://papers.cool/arxiv/2502.02649)] [[pdf](https://arxiv.org/pdf/2502.02649)]
> **Authors**: Margaret Mitchell,Avijit Ghosh,Alexandra Sasha Luccioni,Giada Pistilli
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.

### Secure & Personalized Music-to-Video Generation via CHARCHA 
[[arxiv](https://arxiv.org/abs/2502.02610)] [[cool](https://papers.cool/arxiv/2502.02610)] [[pdf](https://arxiv.org/pdf/2502.02610)]
> **Authors**: Mehul Agarwal,Gauri Agarwal,Santiago Benoit,Andrew Lippman,Jean Oh
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-05
> **comment**: NeurIPS 2024 CreativeAITrack
- **标题**: None
- **领域**: 人工智能,计算机视觉和模式识别,人机交互,多媒体
- **Abstract**: Music is a deeply personal experience and our aim is to enhance this with a fully-automated pipeline for personalized music video generation. Our work allows listeners to not just be consumers but co-creators in the music video generation process by creating personalized, consistent and context-driven visuals based on lyrics, rhythm and emotion in the music. The pipeline combines multimodal translation and generation techniques and utilizes low-rank adaptation on listeners' images to create immersive music videos that reflect both the music and the individual. To ensure the ethical use of users' identity, we also introduce CHARCHA (patent pending), a facial identity verification protocol that protects people against unauthorized use of their face while at the same time collecting authorized images from users for personalizing their videos. This paper thus provides a secure and innovative framework for creating deeply personalized music videos.

### Anytime Incremental $ρ$POMDP Planning in Continuous Spaces 
[[arxiv](https://arxiv.org/abs/2502.02549)] [[cool](https://papers.cool/arxiv/2502.02549)] [[pdf](https://arxiv.org/pdf/2502.02549)]
> **Authors**: Ron Benchetrit,Idan Lev-Yehudi,Andrey Zhitnikov,Vadim Indelman
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Submitted to IJCAI 2025
- **标题**: None
- **领域**: 人工智能,机器学习,机器人技术
- **Abstract**: Partially Observable Markov Decision Processes (POMDPs) provide a robust framework for decision-making under uncertainty in applications such as autonomous driving and robotic exploration. Their extension, $ρ$POMDPs, introduces belief-dependent rewards, enabling explicit reasoning about uncertainty. Existing online $ρ$POMDP solvers for continuous spaces rely on fixed belief representations, limiting adaptability and refinement - critical for tasks such as information-gathering. We present $ρ$POMCPOW, an anytime solver that dynamically refines belief representations, with formal guarantees of improvement over time. To mitigate the high computational cost of updating belief-dependent rewards, we propose a novel incremental computation approach. We demonstrate its effectiveness for common entropy estimators, reducing computational cost by orders of magnitude. Experimental results show that $ρ$POMCPOW outperforms state-of-the-art solvers in both efficiency and solution quality.

### Towards graph neural networks for provably solving convex optimization problems 
[[arxiv](https://arxiv.org/abs/2502.02446)] [[cool](https://papers.cool/arxiv/2502.02446)] [[pdf](https://arxiv.org/pdf/2502.02446)]
> **Authors**: Chendi Qian,Christopher Morris
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习,神经和进化计算
- **Abstract**: Recently, message-passing graph neural networks (MPNNs) have shown potential for solving combinatorial and continuous optimization problems due to their ability to capture variable-constraint interactions. While existing approaches leverage MPNNs to approximate solutions or warm-start traditional solvers, they often lack guarantees for feasibility, particularly in convex optimization settings. Here, we propose an iterative MPNN framework to solve convex optimization problems with provable feasibility guarantees. First, we demonstrate that MPNNs can provably simulate standard interior-point methods for solving quadratic problems with linear constraints, covering relevant problems such as SVMs. Secondly, to ensure feasibility, we introduce a variant that starts from a feasible point and iteratively restricts the search within the feasible region. Experimental results show that our approach outperforms existing neural baselines in solution quality and feasibility, generalizes well to unseen problem sizes, and, in some cases, achieves faster solution times than state-of-the-art solvers such as Gurobi.

### The Elicitation Game: Evaluating Capability Elicitation Techniques 
[[arxiv](https://arxiv.org/abs/2502.02180)] [[cool](https://papers.cool/arxiv/2502.02180)] [[pdf](https://arxiv.org/pdf/2502.02180)]
> **Authors**: Felix Hofstätter,Teun van der Weij,Jayden Teoh,Henning Bartsch,Francis Rhys Ward
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Capability evaluations are required to understand and regulate AI systems that may be deployed or further developed. Therefore, it is important that evaluations provide an accurate estimation of an AI system's capabilities. However, in numerous cases, previously latent capabilities have been elicited from models, sometimes long after initial release. Accordingly, substantial efforts have been made to develop methods for eliciting latent capabilities from models. In this paper, we evaluate the effectiveness of capability elicitation techniques by intentionally training model organisms -- language models with hidden capabilities that are revealed by a password. We introduce a novel method for training model organisms, based on circuit breaking, which is more robust to elicitation techniques than standard password-locked models. We focus on elicitation techniques based on prompting and activation steering, and compare these to fine-tuning methods. Prompting techniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting, while steering fails to do so. For a code-generation task, only fine-tuning can elicit the hidden capabilities of our novel model organism. Additionally, our results suggest that combining techniques improves elicitation. Still, if possible, fine-tuning should be the method of choice to improve the trustworthiness of capability evaluations.

### Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing 
[[arxiv](https://arxiv.org/abs/2502.02153)] [[cool](https://papers.cool/arxiv/2502.02153)] [[pdf](https://arxiv.org/pdf/2502.02153)]
> **Authors**: Thien Q. Tran,Akifumi Wachi,Rei Sato,Takumi Tanabe,Youhei Akimoto
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 37 pages
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing models from the existing safety-alignment methods, we found that, while they generally improved overall safety performance, they failed to ensure safety in specific categories. Our study first identified the difficulty of eliminating such vulnerabilities without sacrificing the model's helpfulness. We observed that, while smaller KL penalty parameters, increased training iterations, and dataset cleansing can enhance safety, they do not necessarily improve the trade-off between safety and helpfulness. We discovered that safety alignment could even induce undesired effects and result in a model that prefers generating negative tokens leading to rejective responses, regardless of the input context. To address this, we introduced a learning-free method, Token-level Safety-Debiased Inference (TSDI), to estimate and correct this bias during the generation process using randomly constructed prompts. Our experiments demonstrated that our method could enhance the model's helpfulness while maintaining safety, thus improving the trade-off Pareto-front.

### Risk-Aware Driving Scenario Analysis with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02145)] [[cool](https://papers.cool/arxiv/2502.02145)] [[pdf](https://arxiv.org/pdf/2502.02145)]
> **Authors**: Yuan Gao,Mattia Piccinini,Johannes Betz
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: IEEE Intelligent Vehicles Symposium 2025
- **标题**: None
- **领域**: 人工智能,计算语言学,机器人技术
- **Abstract**: Large Language Models (LLMs) can capture nuanced contextual relationships, reasoning, and complex problem-solving. By leveraging their ability to process and interpret large-scale information, LLMs have shown potential to address domain-specific challenges, including those in autonomous driving systems. This paper proposes a novel framework that leverages LLMs for risk-aware analysis of generated driving scenarios. We hypothesize that LLMs can effectively evaluate whether driving scenarios generated by autonomous driving testing simulators are safety-critical. To validate this hypothesis, we conducted an empirical evaluation to assess the effectiveness of LLMs in performing this task. This framework will also provide feedback to generate the new safety-critical scenario by using adversarial method to modify existing non-critical scenarios and test their effectiveness in validating motion planning algorithms. Code and scenarios are available at: https://github.com/yuangao-tum/Riskaware-Scenario-analyse

### Standard Neural Computation Alone Is Insufficient for Logical Intelligence 
[[arxiv](https://arxiv.org/abs/2502.02135)] [[cool](https://papers.cool/arxiv/2502.02135)] [[pdf](https://arxiv.org/pdf/2502.02135)]
> **Authors**: Youngsung Kim
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Neural networks, as currently designed, fall short of achieving true logical intelligence. Modern AI models rely on standard neural computation-inner-product-based transformations and nonlinear activations-to approximate patterns from data. While effective for inductive learning, this architecture lacks the structural guarantees necessary for deductive inference and logical consistency. As a result, deep networks struggle with rule-based reasoning, structured generalization, and interpretability without extensive post-hoc modifications. This position paper argues that standard neural layers must be fundamentally rethought to integrate logical reasoning. We advocate for Logical Neural Units (LNUs)-modular components that embed differentiable approximations of logical operations (e.g., AND, OR, NOT) directly within neural architectures. We critique existing neurosymbolic approaches, highlight the limitations of standard neural computation for logical inference, and present LNUs as a necessary paradigm shift in AI. Finally, we outline a roadmap for implementation, discussing theoretical foundations, architectural integration, and key challenges for future research.

### CH-MARL: Constrained Hierarchical Multiagent Reinforcement Learning for Sustainable Maritime Logistics 
[[arxiv](https://arxiv.org/abs/2502.02060)] [[cool](https://papers.cool/arxiv/2502.02060)] [[pdf](https://arxiv.org/pdf/2502.02060)]
> **Authors**: Saad Alqithami
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,多代理系统
- **Abstract**: Addressing global challenges such as greenhouse gas emissions and resource inequity demands advanced AI-driven coordination among autonomous agents. We propose CH-MARL (Constrained Hierarchical Multiagent Reinforcement Learning), a novel framework that integrates hierarchical decision-making with dynamic constraint enforcement and fairness-aware reward shaping. CH-MARL employs a real-time constraint-enforcement layer to ensure adherence to global emission caps, while incorporating fairness metrics that promote equitable resource distribution among agents. Experiments conducted in a simulated maritime logistics environment demonstrate considerable reductions in emissions, along with improvements in fairness and operational efficiency. Beyond this domain-specific success, CH-MARL provides a scalable, generalizable solution to multi-agent coordination challenges in constrained, dynamic settings, thus advancing the state of the art in reinforcement learning.

## 硬件架构(cs.AR:Hardware Architecture)

### LLM-USO: Large Language Model-based Universal Sizing Optimizer 
[[arxiv](https://arxiv.org/abs/2502.02764)] [[cool](https://papers.cool/arxiv/2502.02764)] [[pdf](https://arxiv.org/pdf/2502.02764)]
> **Authors**: Karthik Somayaji N. S,Peng Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,机器学习
- **Abstract**: The design of analog circuits is a cornerstone of integrated circuit (IC) development, requiring the optimization of complex, interconnected sub-structures such as amplifiers, comparators, and buffers. Traditionally, this process relies heavily on expert human knowledge to refine design objectives by carefully tuning sub-components while accounting for their interdependencies. Existing methods, such as Bayesian Optimization (BO), offer a mathematically driven approach for efficiently navigating large design spaces. However, these methods fall short in two critical areas compared to human expertise: (i) they lack the semantic understanding of the sizing solution space and its direct correlation with design objectives before optimization, and (ii) they fail to reuse knowledge gained from optimizing similar sub-structures across different circuits. To overcome these limitations, we propose the Large Language Model-based Universal Sizing Optimizer (LLM-USO), which introduces a novel method for knowledge representation to encode circuit design knowledge in a structured text format. This representation enables the systematic reuse of optimization insights for circuits with similar sub-structures. LLM-USO employs a hybrid framework that integrates BO with large language models (LLMs) and a learning summary module. This approach serves to: (i) infuse domain-specific knowledge into the BO process and (ii) facilitate knowledge transfer across circuits, mirroring the cognitive strategies of expert designers. Specifically, LLM-USO constructs a knowledge summary mechanism to distill and apply design insights from one circuit to related ones. It also incorporates a knowledge summary critiquing mechanism to ensure the accuracy and quality of the summaries and employs BO-guided suggestion filtering to identify optimal design points efficiently.

## 计算工程、金融和科学(cs.CE:Computational Engineering, Finance, and Science)

### Physically Interpretable Representation and Controlled Generation for Turbulence Data 
[[arxiv](https://arxiv.org/abs/2502.02605)] [[cool](https://papers.cool/arxiv/2502.02605)] [[pdf](https://arxiv.org/pdf/2502.02605)]
> **Authors**: Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve
> **First submission**: 2025-01-31
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习,计算物理,流体动力学
- **Abstract**: Computational Fluid Dynamics (CFD) plays a pivotal role in fluid mechanics, enabling precise simulations of fluid behavior through partial differential equations (PDEs). However, traditional CFD methods are resource-intensive, particularly for high-fidelity simulations of complex flows, which are further complicated by high dimensionality, inherent stochasticity, and limited data availability. This paper addresses these challenges by proposing a data-driven approach that leverages a Gaussian Mixture Variational Autoencoder (GMVAE) to encode high-dimensional scientific data into low-dimensional, physically meaningful representations. The GMVAE learns a structured latent space where data can be categorized based on physical properties such as the Reynolds number while maintaining global physical consistency. To assess the interpretability of the learned representations, we introduce a novel metric based on graph spectral theory, quantifying the smoothness of physical quantities along the latent manifold. We validate our approach using 2D Navier-Stokes simulations of flow past a cylinder over a range of Reynolds numbers. Our results demonstrate that the GMVAE provides improved clustering, meaningful latent structure, and robust generative capabilities compared to baseline dimensionality reduction methods. This framework offers a promising direction for data-driven turbulence modeling and broader applications in computational fluid dynamics and engineering systems.

### Reconstructing 3D Flow from 2D Data with Diffusion Transformer 
[[arxiv](https://arxiv.org/abs/2502.02593)] [[cool](https://papers.cool/arxiv/2502.02593)] [[pdf](https://arxiv.org/pdf/2502.02593)]
> **Authors**: Fan Lei
> **First submission**: 2024-12-20
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,人工智能,流体动力学
- **Abstract**: Fluid flow is a widely applied physical problem, crucial in various fields. Due to the highly nonlinear and chaotic nature of fluids, analyzing fluid-related problems is exceptionally challenging. Computational fluid dynamics (CFD) is the best tool for this analysis but involves significant computational resources, especially for 3D simulations, which are slow and resource-intensive. In experimental fluid dynamics, PIV cost increases with dimensionality. Reconstructing 3D flow fields from 2D PIV data could reduce costs and expand application scenarios. Here, We propose a Diffusion Transformer-based method for reconstructing 3D flow fields from 2D flow data. By embedding the positional information of 2D planes into the model, we enable the reconstruction of 3D flow fields from any combination of 2D slices, enhancing flexibility. We replace global attention with window and plane attention to reduce computational costs associated with higher dimensions without compromising performance. Our experiments demonstrate that our model can efficiently and accurately reconstruct 3D flow fields from 2D data, producing realistic results.

### Orientation-aware interaction-based deep material network in polycrystalline materials modeling 
[[arxiv](https://arxiv.org/abs/2502.02457)] [[cool](https://papers.cool/arxiv/2502.02457)] [[pdf](https://arxiv.org/pdf/2502.02457)]
> **Authors**: Ting-Ju Wei,Tung-Huan Su,Chuin-Shan Chen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算工程、金融和科学,机器学习
- **Abstract**: Multiscale simulations are indispensable for connecting microstructural features to the macroscopic behavior of polycrystalline materials, but their high computational demands limit their practicality. Deep material networks (DMNs) have been proposed as efficient surrogate models, yet they fall short of capturing texture evolution. To address this limitation, we propose the orientation-aware interaction-based deep material network (ODMN), which incorporates an orientation-aware mechanism and an interaction mechanism grounded in the Hill-Mandel principle. The orientation-aware mechanism learns the crystallographic textures, while the interaction mechanism captures stress-equilibrium directions among representative volume element (RVE) subregions, offering insight into internal microstructural mechanics. Notably, ODMN requires only linear elastic data for training yet generalizes effectively to complex nonlinear and anisotropic responses. Our results show that ODMN accurately predicts both mechanical responses and texture evolution under complex plastic deformation, thus expanding the applicability of DMNs to polycrystalline materials. By balancing computational efficiency with predictive fidelity, ODMN provides a robust framework for multiscale simulations of polycrystalline materials.

## 计算语言学(cs.CL:Computation and Language)

### Position: Multimodal Large Language Models Can Significantly Advance Scientific Reasoning 
[[arxiv](https://arxiv.org/abs/2502.02871)] [[cool](https://papers.cool/arxiv/2502.02871)] [[pdf](https://arxiv.org/pdf/2502.02871)]
> **Authors**: Yibo Yan,Shen Wang,Jiahao Huo,Jingheng Ye,Zhendong Chu,Xuming Hu,Philip S. Yu,Carla Gomes,Bart Selman,Qingsong Wen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Scientific reasoning, the process through which humans apply logic, evidence, and critical thinking to explore and interpret scientific phenomena, is essential in advancing knowledge reasoning across diverse fields. However, despite significant progress, current scientific reasoning models still struggle with generalization across domains and often fall short of multimodal perception. Multimodal Large Language Models (MLLMs), which integrate text, images, and other modalities, present an exciting opportunity to overcome these limitations and enhance scientific reasoning. Therefore, this position paper argues that MLLMs can significantly advance scientific reasoning across disciplines such as mathematics, physics, chemistry, and biology. First, we propose a four-stage research roadmap of scientific reasoning capabilities, and highlight the current state of MLLM applications in scientific reasoning, noting their ability to integrate and reason over diverse data types. Second, we summarize the key challenges that remain obstacles to achieving MLLM's full potential. To address these challenges, we propose actionable insights and suggestions for the future. Overall, our work offers a novel perspective on MLLM integration with scientific reasoning, providing the LLM community with a valuable vision for achieving Artificial General Intelligence (AGI).

### CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration 
[[arxiv](https://arxiv.org/abs/2502.02807)] [[cool](https://papers.cool/arxiv/2502.02807)] [[pdf](https://arxiv.org/pdf/2502.02807)]
> **Authors**: Yizhe Yang,Palakorn Achananuparp,Heyan Huang,Jing Jiang,Kit Phey Leng,Nicholas Gabriel Lim,Cameron Tan Shi Ern,Ee-peng Lim
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client's state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI's performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client's state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.

### Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation 
[[arxiv](https://arxiv.org/abs/2502.02789)] [[cool](https://papers.cool/arxiv/2502.02789)] [[pdf](https://arxiv.org/pdf/2502.02789)]
> **Authors**: Jingyu Liu,Beidi Chen,Ce Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Because optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is purely compute-bounded and the performance bottleneck shifts from the self-attention to the MLP part. We present SpecPrefill, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to still preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SpecPrefill leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with up to $7\times$ maximal end-to-end QPS on real downstream tasks and $7.66\times$ TTFT improvement during benchmarking.

### SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02787)] [[cool](https://papers.cool/arxiv/2502.02787)] [[pdf](https://arxiv.org/pdf/2502.02787)]
> **Authors**: Amirhossein Dabiriaghdam,Lele Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 15 pages, 5 tables, 6 figures
- **标题**: None
- **领域**: 计算语言学,密码学和安全,计算机与社会,机器学习
- **Abstract**: The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.

### SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model 
[[arxiv](https://arxiv.org/abs/2502.02737)] [[cool](https://papers.cool/arxiv/2502.02737)] [[pdf](https://arxiv.org/pdf/2502.02737)]
> **Authors**: Loubna Ben Allal,Anton Lozhkov,Elie Bakouch,Gabriel Martín Blázquez,Guilherme Penedo,Lewis Tunstall,Andrés Marafioti,Hynek Kydlíček,Agustín Piqueres Lajarín,Vaibhav Srivastav,Joshua Lochner,Caleb Fahlgren,Xuan-Son Nguyen,Clémentine Fourrier,Ben Burtenshaw,Hugo Larcher,Haojun Zhao,Cyril Zakka,Mathieu Morlon,Colin Raffel,Leandro von Werra,Thomas Wolf
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.

### Cross-Lingual Transfer for Low-Resource Natural Language Processing 
[[arxiv](https://arxiv.org/abs/2502.02722)] [[cool](https://papers.cool/arxiv/2502.02722)] [[pdf](https://arxiv.org/pdf/2502.02722)]
> **Authors**: Iker García-Ferrero
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Doctoral Thesis: University of the Basque Country UPV/EHU
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Natural Language Processing (NLP) has seen remarkable advances in recent years, particularly with the emergence of Large Language Models that have achieved unprecedented performance across many tasks. However, these developments have mainly benefited a small number of high-resource languages such as English. The majority of languages still face significant challenges due to the scarcity of training data and computational resources. To address this issue, this thesis focuses on cross-lingual transfer learning, a research area aimed at leveraging data and models from high-resource languages to improve NLP performance for low-resource languages. Specifically, we focus on Sequence Labeling tasks such as Named Entity Recognition, Opinion Target Extraction, and Argument Mining. The research is structured around three main objectives: (1) advancing data-based cross-lingual transfer learning methods through improved translation and annotation projection techniques, (2) developing enhanced model-based transfer learning approaches utilizing state-of-the-art multilingual models, and (3) applying these methods to real-world problems while creating open-source resources that facilitate future research in low-resource NLP. More specifically, this thesis presents a new method to improve data-based transfer with T-Projection, a state-of-the-art annotation projection method that leverages text-to-text multilingual models and machine translation systems. T-Projection significantly outperforms previous annotation projection methods by a wide margin. For model-based transfer, we introduce a constrained decoding algorithm that enhances cross-lingual Sequence Labeling in zero-shot settings using text-to-text models. Finally, we develop Medical mT5, the first multilingual text-to-text medical model, demonstrating the practical impact of our research on real-world applications.

### Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and Maliseet 
[[arxiv](https://arxiv.org/abs/2502.02703)] [[cool](https://papers.cool/arxiv/2502.02703)] [[pdf](https://arxiv.org/pdf/2502.02703)]
> **Authors**: Shenran Wang,Changbing Yang,Mike Parkhill,Chad Quinn,Christopher Hammerly,Jian Zhu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习,声音,音频和语音处理
- **Abstract**: We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi'kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research not only advances technical development for the revitalization of low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation.

### How Inclusively do LMs Perceive Social and Moral Norms? 
[[arxiv](https://arxiv.org/abs/2502.02696)] [[cool](https://papers.cool/arxiv/2502.02696)] [[pdf](https://arxiv.org/pdf/2502.02696)]
> **Authors**: Michael Galarnyk,Agam Shah,Dipanwita Guhathakurta,Poojitha Nandigam,Sudheer Chava
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at NAACL 2025 Findings
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This paper discusses and contains offensive content. Language models (LMs) are used in decision-making systems and as interactive assistants. However, how well do these models making judgements align with the diversity of human values, particularly regarding social and moral norms? In this work, we investigate how inclusively LMs perceive norms across demographic groups (e.g., gender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare their outputs with the existing responses of 100 human annotators. We introduce the Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on ordinal questions. We find notable disparities in LM responses, with younger, higher-income groups showing closer alignment, raising concerns about the representation of marginalized perspectives. Our findings highlight the importance of further efforts to make LMs more inclusive of diverse human values. The code and prompts are available on GitHub under the CC BY-NC 4.0 license.

### Transformers Boost the Performance of Decision Trees on Tabular Data across Sample Sizes 
[[arxiv](https://arxiv.org/abs/2502.02672)] [[cool](https://papers.cool/arxiv/2502.02672)] [[pdf](https://arxiv.org/pdf/2502.02672)]
> **Authors**: Mayuka Jayawardhana,Renbo,Samuel Dooley,Valeriia Cherepanova,Andrew Gordon Wilson,Frank Hutter,Colin White,Tom Goldstein,Micah Goldblum
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 12 pages, 6 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at http://github.com/MayukaJ/LLM-Boost .

### A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI) 
[[arxiv](https://arxiv.org/abs/2502.02659)] [[cool](https://papers.cool/arxiv/2502.02659)] [[pdf](https://arxiv.org/pdf/2502.02659)]
> **Authors**: Yan Li,Tianyi Zhang,Zechuan Li,Soyeon Caren Han
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 9 pages, under review in the conference
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Transformer-based Large Language Models (LLMs) struggle to process inputs exceeding their training context window, with performance degrading due to positional out-of-distribution (O.O.D.) that disrupt attention computations. Existing solutions, fine-tuning and training-free methods, are limited by computational inefficiency, attention logit outliers or loss of local positional information. To address this, we propose Greedy Attention Logit Interpolation (GALI), a training-free length extrapolation method that maximizes the utilization of pretrained positional intervals while avoiding attention logit outliers through attention logit interpolation. The result demonstrates that GALI consistently outperforms state-of-the-art training-free methods. Our findings reveal that LLMs interpret positional intervals unevenly within their training context window, suggesting that extrapolating within a smaller positional interval range yields superior results-even for short-context tasks. GALI represents a significant step toward resolving the positional O.O.D. challenge, enabling more reliable long-text understanding in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at https://github.com/AcademyCityL/GALI.

### Spatio-temporal transformer to support automatic sign language translation 
[[arxiv](https://arxiv.org/abs/2502.02587)] [[cool](https://papers.cool/arxiv/2502.02587)] [[pdf](https://arxiv.org/pdf/2502.02587)]
> **Authors**: Christian Ruiz,Fabio Martinez
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sign Language Translation (SLT) systems support hearing-impaired people communication by finding equivalences between signed and spoken languages. This task is however challenging due to multiple sign variations, complexity in language and inherent richness of expressions. Computational approaches have evidenced capabilities to support SLT. Nonetheless, these approaches remain limited to cover gestures variability and support long sequence translations. This paper introduces a Transformer-based architecture that encodes spatio-temporal motion gestures, preserving both local and long-range spatial information through the use of multiple convolutional and attention mechanisms. The proposed approach was validated on the Colombian Sign Language Translation Dataset (CoL-SLTD) outperforming baseline approaches, and achieving a BLEU4 of 46.84%. Additionally, the proposed approach was validated on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T), achieving a BLEU4 score of 30.77%, demonstrating its robustness and effectiveness in handling real-world variations

### A comparison of translation performance between DeepL and Supertext 
[[arxiv](https://arxiv.org/abs/2502.02577)] [[cool](https://papers.cool/arxiv/2502.02577)] [[pdf](https://arxiv.org/pdf/2502.02577)]
> **Authors**: Alex Flückiger,Chantal Amrhein,Tim Graf,Frédéric Odermatt,Martin Pömsl,Philippe Schläpfer,Florian Schottmann,Samuel Läubli
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext.

### Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement 
[[arxiv](https://arxiv.org/abs/2502.02573)] [[cool](https://papers.cool/arxiv/2502.02573)] [[pdf](https://arxiv.org/pdf/2502.02573)]
> **Authors**: Soheil Abbasloo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.

### Adaptive Self-improvement LLM Agentic System for ML Library Development 
[[arxiv](https://arxiv.org/abs/2502.02534)] [[cool](https://papers.cool/arxiv/2502.02534)] [[pdf](https://arxiv.org/pdf/2502.02534)]
> **Authors**: Genghan Zhang,Weixin Liang,Olivia Hsu,Kunle Olukotun
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\times$ over a baseline single LLM.

### Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search 
[[arxiv](https://arxiv.org/abs/2502.02508)] [[cool](https://papers.cool/arxiv/2502.02508)] [[pdf](https://arxiv.org/pdf/2502.02508)]
> **Authors**: Maohao Shen,Guangtao Zeng,Zhenting Qi,Zhang-Wei Hong,Zhenfang Chen,Wei Lu,Gregory Wornell,Subhro Das,David Cox,Chuang Gan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.

### Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study 
[[arxiv](https://arxiv.org/abs/2502.02481)] [[cool](https://papers.cool/arxiv/2502.02481)] [[pdf](https://arxiv.org/pdf/2502.02481)]
> **Authors**: Menglong Cui,Pengzhi Gao,Wei Liu,Jian Luan,Bin Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accept to NAACL2025 Main Conference
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo.

### SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency 
[[arxiv](https://arxiv.org/abs/2502.02458)] [[cool](https://papers.cool/arxiv/2502.02458)] [[pdf](https://arxiv.org/pdf/2502.02458)]
> **Authors**: Qianhao Yuan,Yanjiang Liu,Yaojie Lu,Hongyu Lin,Ben He,Xianpei Han,Le Sun
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\% and training budget by 26\%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at https://github.com/icip-cas/SAISA.

### Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study 
[[arxiv](https://arxiv.org/abs/2502.02451)] [[cool](https://papers.cool/arxiv/2502.02451)] [[pdf](https://arxiv.org/pdf/2502.02451)]
> **Authors**: Calvin Yixiang Cheng,Scott A Hale
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 12 pages, 2 figures, 6 tables
- **标题**: None
- **领域**: 计算语言学,社交和信息网络
- **Abstract**: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.

### Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02444)] [[cool](https://papers.cool/arxiv/2502.02444)] [[pdf](https://arxiv.org/pdf/2502.02444)]
> **Authors**: Haoran Ye,Tianze Zhang,Yuhang Xie,Liyuan Zhang,Yuanyi Ren,Xin Zhang,Guojie Song
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.

### Activation-Informed Merging of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02421)] [[cool](https://papers.cool/arxiv/2502.02421)] [[pdf](https://arxiv.org/pdf/2502.02421)]
> **Authors**: Amin Heyrani Nobari,Kaveh Alimohammadi,Ali ArjomandBigdeli,Akash Srivastava,Faez Ahmed,Navid Azizan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\% increase in benchmark performance.

### FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework 
[[arxiv](https://arxiv.org/abs/2502.02391)] [[cool](https://papers.cool/arxiv/2502.02391)] [[pdf](https://arxiv.org/pdf/2502.02391)]
> **Authors**: Ibrahim Bouabdallaoui,Fatima Guerouate,Samya Bouhaddour,Chaimae Saadi,Mohammed Sbihi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Code source : https://github.com/ibrahimself/FewTopNER/
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We introduce FewTopNER, a novel framework that integrates few-shot named entity recognition (NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-resource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-RoBERTa, augmented with language-specific calibration mechanisms, to generate robust contextual embeddings. The architecture comprises a prototype-based entity recognition branch, employing BiLSTM and Conditional Random Fields for sequence labeling, and a topic modeling branch that extracts document-level semantic features through hybrid probabilistic and neural methods. A cross-task bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic representations, thereby enhancing entity disambiguation by incorporating global semantic context. Empirical evaluations on multilingual benchmarks across English, French, Spanish, German, and Italian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot NER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points in F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual information. Ablation studies further confirm the critical contributions of the shared encoder and cross-task integration mechanisms to the overall performance. These results underscore the efficacy of incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER for robust cross-lingual applications in low-resource settings.

### CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning 
[[arxiv](https://arxiv.org/abs/2502.02390)] [[cool](https://papers.cool/arxiv/2502.02390)] [[pdf](https://arxiv.org/pdf/2502.02390)]
> **Authors**: Jianfeng Pan,Senyou Deng,Shaomang Huang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.

### STAIR: Improving Safety Alignment with Introspective Reasoning 
[[arxiv](https://arxiv.org/abs/2502.02384)] [[cool](https://papers.cool/arxiv/2502.02384)] [[pdf](https://arxiv.org/pdf/2502.02384)]
> **Authors**: Yichi Zhang,Siyuan Zhang,Yao Huang,Zeyu Xia,Zhengwei Fang,Xiao Yang,Ranjie Duan,Dong Yan,Yinpeng Dong,Jun Zhu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 22 pages, 8 figures
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at https://github.com/thu-ml/STAIR.

### Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs 
[[arxiv](https://arxiv.org/abs/2502.02362)] [[cool](https://papers.cool/arxiv/2502.02362)] [[pdf](https://arxiv.org/pdf/2502.02362)]
> **Authors**: Sagnik Mukherjee,Abhinav Chinta,Takyoung Kim,Tarun Anoop Sharma,Dilek Hakkani-Tür
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.

### Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking 
[[arxiv](https://arxiv.org/abs/2502.02339)] [[cool](https://papers.cool/arxiv/2502.02339)] [[pdf](https://arxiv.org/pdf/2502.02339)]
> **Authors**: Jinyang Wu,Mingkuan Feng,Shuai Zhang,Ruihan Jin,Feihu Che,Zengqi Wen,Jianhua Tao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\%$) while maintaining substantial data and computational efficiency.

### Evalita-LLM: Benchmarking Large Language Models on Italian 
[[arxiv](https://arxiv.org/abs/2502.02289)] [[cool](https://papers.cool/arxiv/2502.02289)] [[pdf](https://arxiv.org/pdf/2502.02289)]
> **Authors**: Bernardo Magnini,Roberto Zanoli,Michele Resta,Martin Cimmino,Paolo Albano,Marco Madeddu,Viviana Patti
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 42 pages, 1 figure, 32 tables
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.

### Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.02249)] [[cool](https://papers.cool/arxiv/2502.02249)] [[pdf](https://arxiv.org/pdf/2502.02249)]
> **Authors**: Atharva Mangeshkumar Agrawal,Rutika Pandurang Shinde,Vasanth Kumar Bhukya,Ashmita Chakraborty,Sagar Bharat Shah,Tanmay Shukla,Sree Pradeep Kumar Relangi,Nilesh Mutyam
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 12 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large language models (LLMs) have shown impressive capabilities in natural language processing tasks, including dialogue generation. This research aims to conduct a novel comparative analysis of two prominent techniques, fine-tuning with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG) framework, in the context of doctor-patient chat conversations with multiple datasets of mixed medical domains. The analysis involves three state-of-the-art models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient dialogues, we comprehensively evaluate the performance of models, assessing key metrics such as language quality (perplexity, BLEU score), factual accuracy (fact-checking against medical knowledge bases), adherence to medical guidelines, and overall human judgments (coherence, empathy, safety). The findings provide insights into the strengths and limitations of each approach, shedding light on their suitability for healthcare applications. Furthermore, the research investigates the robustness of the models in handling diverse patient queries, ranging from general health inquiries to specific medical conditions. The impact of domain-specific knowledge integration is also explored, highlighting the potential for enhancing LLM performance through targeted data augmentation and retrieval strategies.

### When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks 
[[arxiv](https://arxiv.org/abs/2502.02199)] [[cool](https://papers.cool/arxiv/2502.02199)] [[pdf](https://arxiv.org/pdf/2502.02199)]
> **Authors**: Felix Drinkall,Janet B. Pierrehumbert,Stefan Zohren
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,计算工程、金融和科学,机器学习,计算金融
- **Abstract**: Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect.

### Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge 
[[arxiv](https://arxiv.org/abs/2502.02173)] [[cool](https://papers.cool/arxiv/2502.02173)] [[pdf](https://arxiv.org/pdf/2502.02173)]
> **Authors**: Daniel Tamayo,Aitor Gonzalez-Agirre,Javier Hernando,Marta Villegas
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: ef:Findings of the Association for Computational Linguistics: ACL 2024. Pages: 5831-5847
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.

### Multilingual Attribute Extraction from News Web Pages 
[[arxiv](https://arxiv.org/abs/2502.02167)] [[cool](https://papers.cool/arxiv/2502.02167)] [[pdf](https://arxiv.org/pdf/2502.02167)]
> **Authors**: Pavel Bedrin,Maksim Varlamov,Alexander Yatskov
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,信息检索
- **Abstract**: This paper addresses the challenge of automatically extracting attributes from news article web pages across multiple languages. Recent neural network models have shown high efficacy in extracting information from semi-structured web pages. However, these models are predominantly applied to domains like e-commerce and are pre-trained using English data, complicating their application to web pages in other languages. We prepared a multilingual dataset comprising 3,172 marked-up news web pages across six languages (English, German, Russian, Chinese, Korean, and Arabic) from 161 websites. The dataset is publicly available on GitHub. We fine-tuned the pre-trained state-of-the-art model, MarkupLM, to extract news attributes from these pages and evaluated the impact of translating pages into English on extraction quality. Additionally, we pre-trained another state-of-the-art model, DOM-LM, on multilingual data and fine-tuned it on our dataset. We compared both fine-tuned models to existing open-source news data extraction tools, achieving superior extraction metrics.

### Topic Modeling in Marathi 
[[arxiv](https://arxiv.org/abs/2502.02100)] [[cool](https://papers.cool/arxiv/2502.02100)] [[pdf](https://arxiv.org/pdf/2502.02100)]
> **Authors**: Sanket Shinde,Raviraj Joshi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: While topic modeling in English has become a prevalent and well-explored area, venturing into topic modeling for Indic languages remains relatively rare. The limited availability of resources, diverse linguistic structures, and unique challenges posed by Indic languages contribute to the scarcity of research and applications in this domain. Despite the growing interest in natural language processing and machine learning, there exists a noticeable gap in the comprehensive exploration of topic modeling methodologies tailored specifically for languages such as Hindi, Marathi, Tamil, and others. In this paper, we examine several topic modeling approaches applied to the Marathi language. Specifically, we compare various BERT and non-BERT approaches, including multilingual and monolingual BERT models, using topic coherence and topic diversity as evaluation metrics. Our analysis provides insights into the performance of these approaches for Marathi language topic modeling. The key finding of the paper is that BERTopic, when combined with BERT models trained on Indic languages, outperforms LDA in terms of topic modeling performance.

### LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information 
[[arxiv](https://arxiv.org/abs/2502.02095)] [[cool](https://papers.cool/arxiv/2502.02095)] [[pdf](https://arxiv.org/pdf/2502.02095)]
> **Authors**: Bowen Ping,Jiali Zeng,Fandong Meng,Shuo Wang,Jie Zhou,Shanghang Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.

### Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models 
[[arxiv](https://arxiv.org/abs/2502.02074)] [[cool](https://papers.cool/arxiv/2502.02074)] [[pdf](https://arxiv.org/pdf/2502.02074)]
> **Authors**: Prasanta Bhattacharya,Hong Zhang,Yiming Cao,Wei Gao,Brandon Siyuan Loh,Joseph J. P. Simons,Liang Ze Wong
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data. While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level. In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models. Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance. To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks. We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful.

### ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping 
[[arxiv](https://arxiv.org/abs/2502.02072)] [[cool](https://papers.cool/arxiv/2502.02072)] [[pdf](https://arxiv.org/pdf/2502.02072)]
> **Authors**: Rajiv Bahl,Venkatesan N,Parimal Aglawe,Aastha Sarasapalli,Bhavya Kancharla,Chaitanya kolukuluri,Harish Mohite,Japneet Hora,Kiran Kakollu,Rahul Diman,Shubham Kapale,Sri Bhagya Kathula,Vamsikrishna Motru,Yogeshwar Reddy
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 17 pages, 6 Figures and this manuscript will be submitted to Q1,Q2 Journals
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.

### AmaSQuAD: A Benchmark for Amharic Extractive Question Answering 
[[arxiv](https://arxiv.org/abs/2502.02047)] [[cool](https://papers.cool/arxiv/2502.02047)] [[pdf](https://arxiv.org/pdf/2502.02047)]
> **Authors**: Nebiyou Daniel Hailemariam,Blessed Guda,Tsegazeab Tefferi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: This research presents a novel framework for translating extractive question-answering datasets into low-resource languages, as demonstrated by the creation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The methodology addresses challenges related to misalignment between translated questions and answers, as well as the presence of multiple answer instances in the translated context. For this purpose, we used cosine similarity utilizing embeddings from a fine-tuned BERT-based model for Amharic and Longest Common Subsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD synthetic dataset for Amharic Question-Answering. The results show an improvement in baseline performance, with the fine-tuned model achieving an increase in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the AmaSQuAD development dataset. Moreover, the model demonstrates improvement on the human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80% and the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is publicly available Datasets

### Contextual Memory Reweaving in Large Language Models Using Layered Latent State Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.02046)] [[cool](https://papers.cool/arxiv/2502.02046)] [[pdf](https://arxiv.org/pdf/2502.02046)]
> **Authors**: Frederick Dillon,Gregor Halvorsen,Simon Tattershall,Magnus Rowntree,Gareth Vanderpool
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Memory retention challenges in deep neural architectures have ongoing limitations in the ability to process and recall extended contextual information. Token dependencies degrade as sequence length increases, leading to a decline in coherence and factual consistency across longer outputs. A structured approach is introduced to mitigate this issue through the reweaving of latent states captured at different processing layers, reinforcing token representations over extended sequences. The proposed Contextual Memory Reweaving framework incorporates a Layered Latent State Reconstruction mechanism to systematically integrate past contextual embeddings without introducing external memory modules. Experimental results demonstrate improvements in recall accuracy across a range of sequence lengths, with notable gains in the retention of rarely occurring tokens and numerical reasoning consistency. Further analysis of computational efficiency indicates that the additional processing overhead remains within acceptable thresholds, enabling scalability across different model sizes. Evaluations in long-form text generation and ambiguous query resolution highlight the capacity of memory reweaving to enhance continuity and reduce inconsistencies over extended outputs. Attention weight distributions reveal more structured allocation patterns, suggesting that reweaved latent states contribute to improved contextual awareness. The findings establish a framework for refining memory retention mechanisms in language models, addressing long-standing challenges in handling complex, multi-step reasoning tasks.

### M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference 
[[arxiv](https://arxiv.org/abs/2502.02040)] [[cool](https://papers.cool/arxiv/2502.02040)] [[pdf](https://arxiv.org/pdf/2502.02040)]
> **Authors**: Nikhil Bhendawade,Mahyar Najibi,Devang Naik,Irina Belousova
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments.

### Fine-tuning Language Models for Recipe Generation: A Comparative Analysis and Benchmark Study 
[[arxiv](https://arxiv.org/abs/2502.02028)] [[cool](https://papers.cool/arxiv/2502.02028)] [[pdf](https://arxiv.org/pdf/2502.02028)]
> **Authors**: Anneketh Vij,Changhao Liu,Rahul Anil Nair,Theodore Eugene Ho,Edward Shi,Ayan Bhowmick
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 18 pages, 10 figures,14 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This research presents an exploration and study of the recipe generation task by fine-tuning various very small language models, with a focus on developing robust evaluation metrics and comparing across different language models the open-ended task of recipe generation. This study presents extensive experiments with multiple model architectures, ranging from T5-small (Raffel et al., 2023) and SmolLM-135M(Allal et al., 2024) to Phi-2 (Research, 2023), implementing both traditional NLP metrics and custom domain-specific evaluation metrics. Our novel evaluation framework incorporates recipe-specific metrics for assessing content quality and introduces approaches to allergen substitution. The results indicate that, while larger models generally perform better on standard metrics, the relationship between model size and recipe quality is more nuanced when considering domain-specific metrics. SmolLM-360M and SmolLM-1.7B demonstrate comparable performance despite their size difference before and after fine-tuning, while fine-tuning Phi-2 shows notable limitations in recipe generation despite its larger parameter count. The comprehensive evaluation framework and allergen substitution systems provide valuable insights for future work in recipe generation and broader NLG tasks that require domain expertise and safety considerations.

## 密码学和安全(cs.CR:Cryptography and Security)

### Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment 
[[arxiv](https://arxiv.org/abs/2502.02438)] [[cool](https://papers.cool/arxiv/2502.02438)] [[pdf](https://arxiv.org/pdf/2502.02438)]
> **Authors**: Yaling Shen,Zhixiong Zhuang,Kun Yuan,Maria-Irina Nicolae,Nassir Navab,Nicolas Padoy,Mario Fritz
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at AAAI 2025
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Medical multimodal large language models (MLLMs) are becoming an instrumental part of healthcare systems, assisting medical personnel with decision making and results analysis. Models for radiology report generation are able to interpret medical imagery, thus reducing the workload of radiologists. As medical data is scarce and protected by privacy regulations, medical MLLMs represent valuable intellectual property. However, these assets are potentially vulnerable to model stealing, where attackers aim to replicate their functionality via black-box access. So far, model stealing for the medical domain has focused on classification; however, existing attacks are not effective against MLLMs. In this paper, we introduce Adversarial Domain Alignment (ADA-STEAL), the first stealing attack against medical MLLMs. ADA-STEAL relies on natural images, which are public and widely available, as opposed to their medical counterparts. We show that data augmentation with adversarial noise is sufficient to overcome the data distribution gap between natural images and the domain-specific distribution of the victim MLLM. Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that Adversarial Domain Alignment enables attackers to steal the medical MLLM without any access to medical data.

### Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign 
[[arxiv](https://arxiv.org/abs/2502.02068)] [[cool](https://papers.cool/arxiv/2502.02068)] [[pdf](https://arxiv.org/pdf/2502.02068)]
> **Authors**: Ruisi Zhang,Neusha Javidnia,Nojan Sheybani,Farinaz Koushanfar
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,计算语言学,机器学习
- **Abstract**: This paper introduces RoSeMary, the first-of-its-kind ML/Crypto codesign watermarking framework that regulates LLM-generated code to avoid intellectual property rights violations and inappropriate misuse in software development. High-quality watermarks adhering to the detectability-fidelity-robustness tri-objective are limited due to codes' low-entropy nature. Watermark verification, however, often needs to reveal the signature and requires re-encoding new ones for code reuse, which potentially compromising the system's usability. To overcome these challenges, RoSeMary obtains high-quality watermarks by training the watermark insertion and extraction modules end-to-end to ensure (i) unaltered watermarked code functionality and (ii) enhanced detectability and robustness leveraging pre-trained CodeT5 as the insertion backbone to enlarge the code syntactic and variable rename transformation search space. In the deployment, RoSeMary uses zero-knowledge proofs for secure verification without revealing the underlying signatures. Extensive evaluations demonstrated RoSeMary achieves high detection accuracy while preserving the code functionality. RoSeMary is also robust against attacks and provides efficient secure watermark verification.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### Expertized Caption Auto-Enhancement for Video-Text Retrieval 
[[arxiv](https://arxiv.org/abs/2502.02885)] [[cool](https://papers.cool/arxiv/2502.02885)] [[pdf](https://arxiv.org/pdf/2502.02885)]
> **Authors**: Junxiang Chen,Baoyao yang,Wenbin Yao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: :H.3.3; I.2.10; I.2.7; H.5.1
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The burgeoning field of video-text retrieval has witnessed significant advancements with the advent of deep learning. However, the challenge of matching text and video persists due to inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders a comprehensive understanding of videos, resulting in ambiguous retrieval results. While rewriting methods based on large language models have been proposed to broaden text expressions, carefully crafted prompts are essential to ensure the reasonableness and completeness of the rewritten texts. This paper proposes an automatic caption enhancement method that enhances expression quality and mitigates empiricism in augmented captions through self-learning. Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, facilitating video-text matching. Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo.

### Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations 
[[arxiv](https://arxiv.org/abs/2502.02867)] [[cool](https://papers.cool/arxiv/2502.02867)] [[pdf](https://arxiv.org/pdf/2502.02867)]
> **Authors**: Minung Kim,Kawon Lee,Jungmo Kim,Sungho Choi,Seungyul Han
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages main, 19 pages appendix with reference. Submitted to ICML 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.

### A Survey of Sample-Efficient Deep Learning for Change Detection in Remote Sensing: Tasks, Strategies, and Challenges 
[[arxiv](https://arxiv.org/abs/2502.02835)] [[cool](https://papers.cool/arxiv/2502.02835)] [[pdf](https://arxiv.org/pdf/2502.02835)]
> **Authors**: Lei Ding,Danfeng Hong,Maofan Zhao,Hongruixuan Chen,Chenyu Li,Jie Deng,Naoto Yokoya,Lorenzo Bruzzone,Jocelyn Chanussot
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted in IEEE GRSM
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: In the last decade, the rapid development of deep learning (DL) has made it possible to perform automatic, accurate, and robust Change Detection (CD) on large volumes of Remote Sensing Images (RSIs). However, despite advances in CD methods, their practical application in real-world contexts remains limited due to the diverse input data and the applicational context. For example, the collected RSIs can be time-series observations, and more informative results are required to indicate the time of change or the specific change category. Moreover, training a Deep Neural Network (DNN) requires a massive amount of training samples, whereas in many cases these samples are difficult to collect. To address these challenges, various specific CD methods have been developed considering different application scenarios and training resources. Additionally, recent advancements in image generation, self-supervision, and visual foundation models (VFMs) have opened up new approaches to address the 'data-hungry' issue of DL-based CD. The development of these methods in broader application scenarios requires further investigation and discussion. Therefore, this article summarizes the literature methods for different CD tasks and the available strategies and techniques to train and deploy DL-based CD methods in sample-limited scenarios. We expect that this survey can provide new insights and inspiration for researchers in this field to develop more effective CD methods that can be applied in a wider range of contexts.

### AIoT-based smart traffic management system 
[[arxiv](https://arxiv.org/abs/2502.02821)] [[cool](https://papers.cool/arxiv/2502.02821)] [[pdf](https://arxiv.org/pdf/2502.02821)]
> **Authors**: Ahmed Mahmoud Elbasha,Mohammad M. Abdellatif
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper presents a novel AI-based smart traffic management system de-signed to optimize traffic flow and reduce congestion in urban environments. By analysing live footage from existing CCTV cameras, this approach eliminates the need for additional hardware, thereby minimizing both deployment costs and ongoing maintenance expenses. The AI model processes live video feeds to accurately count vehicles and assess traffic density, allowing for adaptive signal control that prioritizes directions with higher traffic volumes. This real-time adaptability ensures smoother traffic flow, reduces congestion, and minimizes waiting times for drivers. Additionally, the proposed system is simulated using PyGame to evaluate its performance under various traffic conditions. The simulation results demonstrate that the AI-based system out-performs traditional static traffic light systems by 34%, leading to significant improvements in traffic flow efficiency. The use of AI to optimize traffic signals can play a crucial role in addressing urban traffic challenges, offering a cost-effective, scalable, and efficient solution for modern cities. This innovative system represents a key advancement in the field of smart city infra-structure and intelligent transportation systems.

### 3D Foundation AI Model for Generalizable Disease Detection in Head Computed Tomography 
[[arxiv](https://arxiv.org/abs/2502.02779)] [[cool](https://papers.cool/arxiv/2502.02779)] [[pdf](https://arxiv.org/pdf/2502.02779)]
> **Authors**: Weicheng Zhu,Haoxu Huang,Huanze Tang,Rushabh Musthyala,Boyang Yu,Long Chen,Emilio Vega,Thomas O'Donnell,Seena Dehkharghani,Jennifer A. Frontera,Arjun V. Masurkar,Kara Melmed,Narges Razavian
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Under Review Preprint
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Head computed tomography (CT) imaging is a widely-used imaging modality with multitudes of medical indications, particularly in assessing pathology of the brain, skull, and cerebrovascular system. It is commonly the first-line imaging in neurologic emergencies given its rapidity of image acquisition, safety, cost, and ubiquity. Deep learning models may facilitate detection of a wide range of diseases. However, the scarcity of high-quality labels and annotations, particularly among less common conditions, significantly hinders the development of powerful models. To address this challenge, we introduce FM-CT: a Foundation Model for Head CT for generalizable disease detection, trained using self-supervised learning. Our approach pre-trains a deep learning model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans without the need for manual annotations, enabling the model to learn robust, generalizable features. To investigate the potential of self-supervised learning in head CT, we employed both discrimination with self-distillation and masked image modeling, and we construct our model in 3D rather than at the slice level (2D) to exploit the structure of head CT scans more comprehensively and efficiently. The model's downstream classification performance is evaluated using internal and three external datasets, encompassing both in-distribution (ID) and out-of-distribution (OOD) data. Our results demonstrate that the self-supervised foundation model significantly improves performance on downstream diagnostic tasks compared to models trained from scratch and previous 3D CT foundation models on scarce annotated datasets. This work highlights the effectiveness of self-supervised learning in medical imaging and sets a new benchmark for head CT image analysis in 3D, enabling broader use of artificial intelligence for head CT-based diagnosis.

### Rethinking Vision Transformer for Object Centric Foundation Models 
[[arxiv](https://arxiv.org/abs/2502.02763)] [[cool](https://papers.cool/arxiv/2502.02763)] [[pdf](https://arxiv.org/pdf/2502.02763)]
> **Authors**: Manuel Traub,Martin V. Butz
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Recent state-of-the-art object segmentation mechanisms, such as the Segment Anything Model (SAM) and FastSAM, first encode the full image over several layers and then focus on generating the mask for one particular object or area. We present an off-grid Fovea-Like Input Patching (FLIP) approach, which selects image input and encodes it from the beginning in an object-focused manner. While doing so, it separates locational encoding from an object-centric perceptual code. FLIP is more data-efficient and yields improved segmentation performance when masking relatively small objects in high-resolution visual scenes. On standard benchmarks such as Hypersim, KITTI-360, and OpenImages, FLIP achieves Intersection over Union (IoU) scores that approach the performance of SAM with much less compute effort. It surpasses FastSAM in all IoU measurements. We also introduce an additional semi-natural but highly intuitive dataset where FLIP outperforms SAM and FastSAM overall and particularly on relatively small objects. Seeing that FLIP is an end-to-end object-centric segmentation approach, it has high potential particularly for applications that benefit from computationally efficient, spatially highly selective object tracking.

### Controllable Video Generation with Provable Disentanglement 
[[arxiv](https://arxiv.org/abs/2502.02690)] [[cool](https://papers.cool/arxiv/2502.02690)] [[pdf](https://arxiv.org/pdf/2502.02690)]
> **Authors**: Yifan Shen,Peiyuan Zhu,Zijian Li,Shaoan Xie,Zeyu Tang,Namrata Deka,Zongfang Liu,Guangyi Chen,Kun Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVoGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios.

### Blind Visible Watermark Removal with Morphological Dilation 
[[arxiv](https://arxiv.org/abs/2502.02676)] [[cool](https://papers.cool/arxiv/2502.02676)] [[pdf](https://arxiv.org/pdf/2502.02676)]
> **Authors**: Preston K. Robinette,Taylor T. Johnson
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,密码学和安全,机器学习
- **Abstract**: Visible watermarks pose significant challenges for image restoration techniques, especially when the target background is unknown. Toward this end, we present MorphoMod, a novel method for automated visible watermark removal that operates in a blind setting -- without requiring target images. Unlike existing methods, MorphoMod effectively removes opaque and transparent watermarks while preserving semantic content, making it well-suited for real-world applications. Evaluations on benchmark datasets, including the Colored Large-scale Watermark Dataset (CLWD), LOGO-series, and the newly introduced Alpha1 datasets, demonstrate that MorphoMod achieves up to a 50.8% improvement in watermark removal effectiveness compared to state-of-the-art methods. Ablation studies highlight the impact of prompts used for inpainting, pre-removal filling strategies, and inpainting model performance on watermark removal. Additionally, a case study on steganographic disorientation reveals broader applications for watermark removal in disrupting high-level hidden messages. MorphoMod offers a robust, adaptable solution for watermark removal and opens avenues for further advancements in image restoration and adversarial manipulation.

### Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review 
[[arxiv](https://arxiv.org/abs/2502.02618)] [[cool](https://papers.cool/arxiv/2502.02618)] [[pdf](https://arxiv.org/pdf/2502.02618)]
> **Authors**: F. Xavier Gaya-Morey,Jose M. Buades-Rubio,Philippe Palanque,Raquel Lacuesta,Cristina Manresa-Yee
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: The rapid aging of the global population has highlighted the need for technologies to support elderly, particularly in healthcare and emotional well-being. Facial expression recognition (FER) systems offer a non-invasive means of monitoring emotional states, with applications in assisted living, mental health support, and personalized care. This study presents a systematic review of deep learning-based FER systems, focusing on their applications for the elderly population. Following a rigorous methodology, we analyzed 31 studies published over the last decade, addressing challenges such as the scarcity of elderly-specific datasets, class imbalances, and the impact of age-related facial expression differences. Our findings show that convolutional neural networks remain dominant in FER, and especially lightweight versions for resource-constrained environments. However, existing datasets often lack diversity in age representation, and real-world deployment remains limited. Additionally, privacy concerns and the need for explainable artificial intelligence emerged as key barriers to adoption. This review underscores the importance of developing age-inclusive datasets, integrating multimodal solutions, and adopting XAI techniques to enhance system usability, reliability, and trustworthiness. We conclude by offering recommendations for future research to bridge the gap between academic progress and real-world implementation in elderly care.

### MIND: Microstructure INverse Design with Generative Hybrid Neural Representation 
[[arxiv](https://arxiv.org/abs/2502.02607)] [[cool](https://papers.cool/arxiv/2502.02607)] [[pdf](https://arxiv.org/pdf/2502.02607)]
> **Authors**: Tianyang Xue,Haochen Li,Longdu Liu,Paul Henderson,Pengbin Tang,Lin Lu,Jikai Liu,Haisen Zhao,Hao Peng,Bernd Bickel
> **First submission**: 2025-02-01
> **First announcement**: 2025-02-05
> **comment**: :I.3.5
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **Abstract**: The inverse design of microstructures plays a pivotal role in optimizing metamaterials with specific, targeted physical properties. While traditional forward design methods are constrained by their inability to explore the vast combinatorial design space, inverse design offers a compelling alternative by directly generating structures that fulfill predefined performance criteria. However, achieving precise control over both geometry and material properties remains a significant challenge due to their intricate interdependence. Existing approaches, which typically rely on voxel or parametric representations, often limit design flexibility and structural diversity. In this work, we present a novel generative model that integrates latent diffusion with Holoplane, an advanced hybrid neural representation that simultaneously encodes both geometric and physical properties. This combination ensures superior alignment between geometry and properties. Our approach generalizes across multiple microstructure classes, enabling the generation of diverse, tileable microstructures with significantly improved property accuracy and enhanced control over geometric validity, surpassing the performance of existing methods. We introduce a multi-class dataset encompassing a variety of geometric morphologies, including truss, shell, tube, and plate structures, to train and validate our model. Experimental results demonstrate the model's ability to generate microstructures that meet target properties, maintain geometric validity, and integrate seamlessly into complex assemblies. Additionally, we explore the potential of our framework through the generation of new microstructures, cross-class interpolation, and the infilling of heterogeneous microstructures. The dataset and source code will be open-sourced upon publication.

### Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling 
[[arxiv](https://arxiv.org/abs/2502.02590)] [[cool](https://papers.cool/arxiv/2502.02590)] [[pdf](https://arxiv.org/pdf/2502.02590)]
> **Authors**: Xiaowen Qiu,Jincheng Yang,Yian Wang,Zhehuan Chen,Yufei Wang,Tsun-Hsuan Wang,Zhou Xian,Chuang Gan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术
- **Abstract**: 3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.

### COCONut-PanCap: Joint Panoptic Segmentation and Grounded Captions for Fine-Grained Understanding and Generation 
[[arxiv](https://arxiv.org/abs/2502.02589)] [[cool](https://papers.cool/arxiv/2502.02589)] [[pdf](https://arxiv.org/pdf/2502.02589)]
> **Authors**: Xueqing Deng,Qihang Yu,Ali Athar,Chenglin Yang,Linjie Yang,Xiaojie Jin,Xiaohui Shen,Liang-Chieh Chen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: project website: https://xdeng7.github.io/coconut.github.io/coconut_pancap.html
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper introduces the COCONut-PanCap dataset, created to enhance panoptic segmentation and grounded image captioning. Building upon the COCO dataset with advanced COCONut panoptic masks, this dataset aims to overcome limitations in existing image-text datasets that often lack detailed, scene-comprehensive descriptions. The COCONut-PanCap dataset incorporates fine-grained, region-level captions grounded in panoptic segmentation masks, ensuring consistency and improving the detail of generated captions. Through human-edited, densely annotated descriptions, COCONut-PanCap supports improved training of vision-language models (VLMs) for image understanding and generative models for text-to-image tasks. Experimental results demonstrate that COCONut-PanCap significantly boosts performance across understanding and generation tasks, offering complementary benefits to large-scale datasets. This dataset sets a new benchmark for evaluating models on joint panoptic segmentation and grounded captioning tasks, addressing the need for high-quality, detailed image-text annotations in multi-modal learning.

### Revisiting Expected Possession Value in Football: Introducing a Benchmark, U-Net Architecture, and Reward and Risk for Passes 
[[arxiv](https://arxiv.org/abs/2502.02565)] [[cool](https://papers.cool/arxiv/2502.02565)] [[pdf](https://arxiv.org/pdf/2502.02565)]
> **Authors**: Thijs Overmeer,Tim Janssen,Wim P. M. Nuijten
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This paper introduces the first Expected Possession Value (EPV) benchmark and a new and improved EPV model for football. Through the introduction of the OJN-Pass-EPV benchmark, we present a novel method to quantitatively assess the quality of EPV models by using pairs of game states with given relative EPVs. Next, we attempt to replicate the results of Fernández et al. (2021) using a dataset containing Dutch Eredivisie and World Cup matches. Following our failure to do so, we propose a new architecture based on U-net-type convolutional neural networks, achieving good results in model loss and Expected Calibration Error. Finally, we present an improved pass model that incorporates ball height and contains a new dual-component pass value model that analyzes reward and risk. The resulting EPV model correctly identifies the higher value state in 78% of the game state pairs in the OJN-Pass-EPV benchmark, demonstrating its ability to accurately assess goal-scoring potential. Our findings can help assess the quality of EPV models, improve EPV predictions, help assess potential reward and risk of passing decisions, and improve player and team performance.

### Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation 
[[arxiv](https://arxiv.org/abs/2502.02548)] [[cool](https://papers.cool/arxiv/2502.02548)] [[pdf](https://arxiv.org/pdf/2502.02548)]
> **Authors**: Junha Lee,Chunghyun Park,Jaesung Choe,Yu-Chiang Frank Wang,Jan Kautz,Minsu Cho,Chris Choy
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: project page: https://nvlabs.github.io/Mosaic3D/
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data.

### Uncertainty Quantification for Collaborative Object Detection Under Adversarial Attacks 
[[arxiv](https://arxiv.org/abs/2502.02537)] [[cool](https://papers.cool/arxiv/2502.02537)] [[pdf](https://arxiv.org/pdf/2502.02537)]
> **Authors**: Huiqun Huang,Cong Chen,Jean-Philippe Monteuuis,Jonathan Petit,Fei Miao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Collaborative Object Detection (COD) and collaborative perception can integrate data or features from various entities, and improve object detection accuracy compared with individual perception. However, adversarial attacks pose a potential threat to the deep learning COD models, and introduce high output uncertainty. With unknown attack models, it becomes even more challenging to improve COD resiliency and quantify the output uncertainty for highly dynamic perception scenes such as autonomous vehicles. In this study, we propose the Trusted Uncertainty Quantification in Collaborative Perception framework (TUQCP). TUQCP leverages both adversarial training and uncertainty quantification techniques to enhance the adversarial robustness of existing COD models. More specifically, TUQCP first adds perturbations to the shared information of randomly selected agents during object detection collaboration by adversarial training. TUQCP then alleviates the impacts of adversarial attacks by providing output uncertainty estimation through learning-based module and uncertainty calibration through conformal prediction. Our framework works for early and intermediate collaboration COD models and single-agent object detection models. We evaluate TUQCP on V2X-Sim, a comprehensive collaborative perception dataset for autonomous driving, and demonstrate a 80.41% improvement in object detection accuracy compared to the baselines under the same adversarial attacks. TUQCP demonstrates the importance of uncertainty quantification to COD under adversarial attacks.

### Privacy Attacks on Image AutoRegressive Models 
[[arxiv](https://arxiv.org/abs/2502.02514)] [[cool](https://papers.cool/arxiv/2502.02514)] [[pdf](https://arxiv.org/pdf/2502.02514)]
> **Authors**: Antoni Kowalczuk,Jan Dubiński,Franziska Boenisch,Adam Dziedzic
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Code: https://github.com/sprintml/privacy_attacks_against_iars
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Image autoregressive (IAR) models have surpassed diffusion models (DMs) in both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their privacy risks remain largely unexplored. To address this, we conduct a comprehensive privacy analysis comparing IARs to DMs. We develop a novel membership inference attack (MIA) that achieves a significantly higher success rate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for DMs). Using this MIA, we perform dataset inference (DI) and find that IARs require as few as six samples to detect dataset membership, compared to 200 for DMs, indicating higher information leakage. Additionally, we extract hundreds of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight a fundamental privacy-utility trade-off: while IARs excel in generation quality and speed, they are significantly more vulnerable to privacy attacks. This suggests that incorporating techniques from DMs, such as per-token probability modeling using diffusion, could help mitigate IARs' privacy risks. Our code is available at https://github.com/sprintml/privacy_attacks_against_iars.

### Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction 
[[arxiv](https://arxiv.org/abs/2502.02504)] [[cool](https://papers.cool/arxiv/2502.02504)] [[pdf](https://arxiv.org/pdf/2502.02504)]
> **Authors**: Ruochen Li,Tanqiu Qiao,Stamos Katsigiannis,Zhanxing Zhu,Hubert P. H. Shum
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Pedestrian trajectory prediction aims to forecast future movements based on historical paths. Spatial-temporal (ST) methods often separately model spatial interactions among pedestrians and temporal dependencies of individuals. They overlook the direct impacts of interactions among different pedestrians across various time steps (i.e., high-order cross-time interactions). This limits their ability to capture ST inter-dependencies and hinders prediction performance. To address these limitations, we propose UniEdge with three major designs. Firstly, we introduce a unified ST graph data structure that simplifies high-order cross-time interactions into first-order relationships, enabling the learning of ST inter-dependencies in a single step. This avoids the information loss caused by multi-step aggregation. Secondly, traditional GNNs focus on aggregating pedestrian node features, neglecting the propagation of implicit interaction patterns encoded in edge features. We propose the Edge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph network that jointly models explicit N2N social interactions among pedestrians and implicit E2E influence propagation across these interaction patterns. Finally, to overcome the limited receptive fields and challenges in capturing long-range dependencies of auto-regressive architectures, we introduce a transformer encoder-based predictor that enables global modeling of temporal correlation. UniEdge outperforms state-of-the-arts on multiple datasets, including ETH, UCY, and SDD.

### A Self-Supervised Framework for Improved Generalisability in Ultrasound B-mode Image Segmentation 
[[arxiv](https://arxiv.org/abs/2502.02489)] [[cool](https://papers.cool/arxiv/2502.02489)] [[pdf](https://arxiv.org/pdf/2502.02489)]
> **Authors**: Edward Ellis,Andrew Bulpitt,Nasim Parsa,Michael F Byrne,Sharib Ali
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 12
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Ultrasound (US) imaging is clinically invaluable due to its noninvasive and safe nature. However, interpreting US images is challenging, requires significant expertise, and time, and is often prone to errors. Deep learning offers assistive solutions such as segmentation. Supervised methods rely on large, high-quality, and consistently labeled datasets, which are challenging to curate. Moreover, these methods tend to underperform on out-of-distribution data, limiting their clinical utility. Self-supervised learning (SSL) has emerged as a promising alternative, leveraging unlabeled data to enhance model performance and generalisability. We introduce a contrastive SSL approach tailored for B-mode US images, incorporating a novel Relation Contrastive Loss (RCL). RCL encourages learning of distinct features by differentiating positive and negative sample pairs through a learnable metric. Additionally, we propose spatial and frequency-based augmentation strategies for the representation learning on US images. Our approach significantly outperforms traditional supervised segmentation methods across three public breast US datasets, particularly in data-limited scenarios. Notable improvements on the Dice similarity metric include a 4% increase on 20% and 50% of the BUSI dataset, nearly 6% and 9% improvements on 20% and 50% of the BrEaST dataset, and 6.4% and 3.7% improvements on 20% and 50% of the UDIAT dataset, respectively. Furthermore, we demonstrate superior generalisability on the out-of-distribution UDIAT dataset with performance boosts of 20.6% and 13.6% compared to the supervised baseline using 20% and 50% of the BUSI and BrEaST training data, respectively. Our research highlights that domain-inspired SSL can improve US segmentation, especially under data-limited conditions.

### Mind the Gap: Evaluating Patch Embeddings from General-Purpose and Histopathology Foundation Models for Cell Segmentation and Classification 
[[arxiv](https://arxiv.org/abs/2502.02471)] [[cool](https://papers.cool/arxiv/2502.02471)] [[pdf](https://arxiv.org/pdf/2502.02471)]
> **Authors**: Valentina Vadori,Antonella Peruffo,Jean-Marie Graïc,Livio Finos,Enrico Grisan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,定量方法
- **Abstract**: Recent advancements in foundation models have transformed computer vision, driving significant performance improvements across diverse domains, including digital histopathology. However, the advantages of domain-specific histopathology foundation models over general-purpose models for specialized tasks such as cell analysis remain underexplored. This study investigates the representation learning gap between these two categories by analyzing multi-level patch embeddings applied to cell instance segmentation and classification. We implement an encoder-decoder architecture with a consistent decoder and various encoders. These include convolutional, vision transformer (ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M, representing general-purpose foundation models. These are compared against ViT encoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation models, trained on patches extracted from hundreds of thousands of histopathology whole-slide images. The decoder integrates patch embeddings from different encoder depths via skip connections to generate semantic and distance maps. These maps are then post-processed to create instance segmentation masks where each label corresponds to an individual cell and to perform cell-type classification. All encoders remain frozen during training to assess their pre-trained feature extraction capabilities. Using the PanNuke and CoNIC histopathology datasets, and the newly introduced Nissl-stained CytoDArk0 dataset for brain cytoarchitecture studies, we evaluate instance-level detection, segmentation accuracy, and cell-type classification. This study provides insights into the comparative strengths and limitations of general-purpose vs. histopathology foundation models, offering guidance for model selection in cell-focused histopathology and brain cytoarchitecture analysis workflows.

### Personalization Toolkit: Training Free Personalization of Large Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.02452)] [[cool](https://papers.cool/arxiv/2502.02452)] [[pdf](https://arxiv.org/pdf/2502.02452)]
> **Authors**: Soroush Seifi,Vaggelis Dorovatas,Daniel Olmeda Reino,Rahaf Aljundi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Large Vision Language Models (LVLMs) have significant potential to deliver personalized assistance by adapting to individual users' unique needs and preferences. Personalization of LVLMs is an emerging area that involves customizing models to recognize specific object instances and provide tailored responses. However, existing approaches rely on time-consuming test-time training for each user and object, rendering them impractical. This paper proposes a novel, training-free approach to LVLM personalization by leveraging pre-trained vision foundation models to extract distinct features, retrieval-augmented generation (RAG) techniques to recognize instances in the visual input, and visual prompting methods. Our model-agnostic vision toolkit enables flexible and efficient personalization without extensive retraining. We demonstrate state-of-the-art results, outperforming conventional training-based approaches and establish a new standard for LVLM personalization.

### LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02406)] [[cool](https://papers.cool/arxiv/2502.02406)] [[pdf](https://arxiv.org/pdf/2502.02406)]
> **Authors**: Tzu-Tao Chang,Shivaram Venkataraman
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,分布式、并行和集群计算,机器学习
- **Abstract**: Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique enabling support for longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 5.58$\times$ end-to-end speedup compared to existing approaches.

### MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm 
[[arxiv](https://arxiv.org/abs/2502.02358)] [[cool](https://papers.cool/arxiv/2502.02358)] [[pdf](https://arxiv.org/pdf/2502.02358)]
> **Authors**: Ziyan Guo,Zeyu Hu,Na Zhao,De Wen Soh
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.

### Geometric Neural Process Fields 
[[arxiv](https://arxiv.org/abs/2502.02338)] [[cool](https://papers.cool/arxiv/2502.02338)] [[pdf](https://arxiv.org/pdf/2502.02338)]
> **Authors**: Wenzhe Yin,Zehao Xiao,Jiayi Shen,Yunlu Chen,Cees G. M. Snoek,Jan-Jakob Sonke,Efstratios Gavves
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: This paper addresses the challenge of Neural Field (NeF) generalization, where models must efficiently adapt to new signals given only a few observations. To tackle this, we propose Geometric Neural Process Fields (G-NPF), a probabilistic framework for neural radiance fields that explicitly captures uncertainty. We formulate NeF generalization as a probabilistic problem, enabling direct inference of NeF function distributions from limited context observations. To incorporate structural inductive biases, we introduce a set of geometric bases that encode spatial structure and facilitate the inference of NeF function distributions. Building on these bases, we design a hierarchical latent variable model, allowing G-NPF to integrate structural information across multiple spatial levels and effectively parameterize INR functions. This hierarchical approach improves generalization to novel scenes and unseen signals. Experiments on novel-view synthesis for 3D scenes, as well as 2D image and 1D signal regression, demonstrate the effectiveness of our method in capturing uncertainty and leveraging structural information for improved generalization.

### Event-aided Semantic Scene Completion 
[[arxiv](https://arxiv.org/abs/2502.02334)] [[cool](https://papers.cool/arxiv/2502.02334)] [[pdf](https://arxiv.org/pdf/2502.02334)]
> **Authors**: Shangwei Guo,Hao Shi,Song Wang,Xiaoting Yin,Kailun Yang,Kaiwei Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: The established datasets and codebase will be made publicly at https://github.com/Pandapan01/EvSSC
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器人技术,图像和视频处理
- **Abstract**: Autonomous driving systems rely on robust 3D scene understanding. Recent advances in Semantic Scene Completion (SSC) for autonomous driving underscore the limitations of RGB-based approaches, which struggle under motion blur, poor lighting, and adverse weather. Event cameras, offering high dynamic range and low latency, address these challenges by providing asynchronous data that complements RGB inputs. We present DSEC-SSC, the first real-world benchmark specifically designed for event-aided SSC, which includes a novel 4D labeling pipeline for generating dense, visibility-aware labels that adapt dynamically to object motion. Our proposed RGB-Event fusion framework, EvSSC, introduces an Event-aided Lifting Module (ELM) that effectively bridges 2D RGB-Event features to 3D space, enhancing view transformation and the robustness of 3D volume construction across SSC models. Extensive experiments on DSEC-SSC and simulated SemanticKITTI-E demonstrate that EvSSC is adaptable to both transformer-based and LSS-based SSC architectures. Notably, evaluations on SemanticKITTI-C demonstrate that EvSSC achieves consistently improved prediction accuracy across five degradation modes and both In-domain and Out-of-domain settings, achieving up to a 52.5% relative improvement in mIoU when the image sensor partially fails. Additionally, we quantitatively and qualitatively validate the superiority of EvSSC under motion blur and extreme weather conditions, where autonomous driving is challenged. The established datasets and our codebase will be made publicly at https://github.com/Pandapan01/EvSSC.

### UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training 
[[arxiv](https://arxiv.org/abs/2502.02307)] [[cool](https://papers.cool/arxiv/2502.02307)] [[pdf](https://arxiv.org/pdf/2502.02307)]
> **Authors**: Jiawei Qin,Xucong Zhang,Yusuke Sugano
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Despite decades of research on data collection and model architectures, current gaze estimation models face significant challenges in generalizing across diverse data domains. While recent advances in self-supervised pre-training have shown remarkable potential for improving model generalization in various vision tasks, their effectiveness in gaze estimation remains unexplored due to the geometric nature of the gaze regression task. We propose UniGaze, which leverages large-scale, in-the-wild facial datasets through self-supervised pre-training for gaze estimation. We carefully curate multiple facial datasets that capture diverse variations in identity, lighting, background, and head poses. By directly applying Masked Autoencoder (MAE) pre-training on normalized face images with a Vision Transformer (ViT) backbone, our UniGaze learns appropriate feature representations within the specific input space required by downstream gaze estimation models. Through comprehensive experiments using challenging cross-dataset evaluation and novel protocols, including leave-one-dataset-out and joint-dataset settings, we demonstrate that UniGaze significantly improves generalization across multiple data domains while minimizing reliance on costly labeled data. The source code and pre-trained models will be released upon acceptance.

### Survey of Quantization Techniques for On-Device Vision-based Crack Detection 
[[arxiv](https://arxiv.org/abs/2502.02269)] [[cool](https://papers.cool/arxiv/2502.02269)] [[pdf](https://arxiv.org/pdf/2502.02269)]
> **Authors**: Yuxuan Zhang,Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Bengt Oelmann,Sebastian Bader
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted by IEEE International Instrumentation and Measurement Technology Conference (I2MTC) 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Structural Health Monitoring (SHM) ensures the safety and longevity of infrastructure by enabling timely damage detection. Vision-based crack detection, combined with UAVs, addresses the limitations of traditional sensor-based SHM methods but requires the deployment of efficient deep learning models on resource-constrained devices. This study evaluates two lightweight convolutional neural network models, MobileNetV1x0.25 and MobileNetV2x0.5, across TensorFlow, PyTorch, and Open Neural Network Exchange platforms using three quantization techniques: dynamic quantization, post-training quantization (PTQ), and quantization-aware training (QAT). Results show that QAT consistently achieves near-floating-point accuracy, such as an F1-score of 0.8376 for MBNV2x0.5 with Torch-QAT, while maintaining efficient resource usage. PTQ significantly reduces memory and energy consumption but suffers from accuracy loss, particularly in TensorFlow. Dynamic quantization preserves accuracy but faces deployment challenges on PyTorch. By leveraging QAT, this work enables real-time, low-power crack detection on UAVs, enhancing safety, scalability, and cost-efficiency in SHM applications, while providing insights into balancing accuracy and efficiency across different platforms for autonomous inspections.

### Rotation-Adaptive Point Cloud Domain Generalization via Intricate Orientation Learning 
[[arxiv](https://arxiv.org/abs/2502.02247)] [[cool](https://papers.cool/arxiv/2502.02247)] [[pdf](https://arxiv.org/pdf/2502.02247)]
> **Authors**: Bangzhen Liu,Chenxi Zheng,Xuemiao Xu,Cheng Xu,Huaidong Zhang,Shengfeng He
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 13pages, supplementary included, early accepted by TPAMI
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: The vulnerability of 3D point cloud analysis to unpredictable rotations poses an open yet challenging problem: orientation-aware 3D domain generalization. Cross-domain robustness and adaptability of 3D representations are crucial but not easily achieved through rotation augmentation. Motivated by the inherent advantages of intricate orientations in enhancing generalizability, we propose an innovative rotation-adaptive domain generalization framework for 3D point cloud analysis. Our approach aims to alleviate orientational shifts by leveraging intricate samples in an iterative learning process. Specifically, we identify the most challenging rotation for each point cloud and construct an intricate orientation set by optimizing intricate orientations. Subsequently, we employ an orientation-aware contrastive learning framework that incorporates an orientation consistency loss and a margin separation loss, enabling effective learning of categorically discriminative and generalizable features with rotation consistency. Extensive experiments and ablations conducted on 3D cross-domain benchmarks firmly establish the state-of-the-art performance of our proposed approach in the context of orientation-aware 3D domain generalization.

### Mask-informed Deep Contrastive Incomplete Multi-view Clustering 
[[arxiv](https://arxiv.org/abs/2502.02234)] [[cool](https://papers.cool/arxiv/2502.02234)] [[pdf](https://arxiv.org/pdf/2502.02234)]
> **Authors**: Zhenglai Li,Yuqi Shi,Xiao He,Chang Tang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Multi-view clustering (MvC) utilizes information from multiple views to uncover the underlying structures of data. Despite significant advancements in MvC, mitigating the impact of missing samples in specific views on the integration of knowledge from different views remains a critical challenge. This paper proposes a novel Mask-informed Deep Contrastive Incomplete Multi-view Clustering (Mask-IMvC) method, which elegantly identifies a view-common representation for clustering. Specifically, we introduce a mask-informed fusion network that aggregates incomplete multi-view information while considering the observation status of samples across various views as a mask, thereby reducing the adverse effects of missing values. Additionally, we design a prior knowledge-assisted contrastive learning loss that boosts the representation capability of the aggregated view-common representation by injecting neighborhood information of samples from different views. Finally, extensive experiments are conducted to demonstrate the superiority of the proposed Mask-IMvC method over state-of-the-art approaches across multiple MvC datasets, both in complete and incomplete scenarios.

### A Robust Remote Photoplethysmography Method 
[[arxiv](https://arxiv.org/abs/2502.02229)] [[cool](https://papers.cool/arxiv/2502.02229)] [[pdf](https://arxiv.org/pdf/2502.02229)]
> **Authors**: Alexey Protopopov
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 9 pages, 5 figures, 1 table
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Remote photoplethysmography (rPPG) is a method for measuring a subjects heart rate remotely using a camera. Factors such as subject movement, ambient light level, makeup etc. complicate such measurements by distorting the observed pulse. Recent works on this topic have proposed a variety of approaches for accurately measuring heart rate in humans, however these methods were tested in ideal conditions, where the subject does not make significant movements and all measurements are taken at the same level of illumination. In more realistic conditions these methods suffer from decreased accuracy. The study proposes a more robust method that is less susceptible to distortions and has minimal hardware requirements. The proposed method uses a combination of mathematical transforms to calculate the subjects heart rate. It performs best when used with a camera that has been modified by removing its infrared filter, although using an unmodified camera is also possible. The method was tested on 26 videos taken from 19 volunteers of varying gender and age. The obtained results were compared to reference data and the average mean absolute error was found to be at 1.95 beats per minute, which is noticeably better than the results from previous works. The remote photoplethysmography method proposed in the present article is more resistant to distortions than methods from previous publications and thus allows one to remotely and accurately measure the subjects heart rate without imposing any significant limitations on the subjects behavior.

### Exploiting Ensemble Learning for Cross-View Isolated Sign Language Recognition 
[[arxiv](https://arxiv.org/abs/2502.02196)] [[cool](https://papers.cool/arxiv/2502.02196)] [[pdf](https://arxiv.org/pdf/2502.02196)]
> **Authors**: Fei Wang,Kun Li,Yiqi Nie,Zhangling Duan,Peng Zou,Zhiliang Wu,Yuwei Wang,Yanyan Wei
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 3rd Place in Cross-View Isolated SignLanguageRecognition Challenge at WWW 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: In this paper, we present our solution to the Cross-View Isolated Sign Language Recognition (CV-ISLR) challenge held at WWW 2025. CV-ISLR addresses a critical issue in traditional Isolated Sign Language Recognition (ISLR), where existing datasets predominantly capture sign language videos from a frontal perspective, while real-world camera angles often vary. To accurately recognize sign language from different viewpoints, models must be capable of understanding gestures from multiple angles, making cross-view recognition challenging. To address this, we explore the advantages of ensemble learning, which enhances model robustness and generalization across diverse views. Our approach, built on a multi-dimensional Video Swin Transformer model, leverages this ensemble strategy to achieve competitive performance. Finally, our solution ranked 3rd in both the RGB-based ISLR and RGB-D-based ISLR tracks, demonstrating the effectiveness in handling the challenges of cross-view recognition. The code is available at: https://github.com/Jiafei127/CV_ISLR_WWW2025.

### Sequence models for continuous cell cycle stage prediction from brightfield images 
[[arxiv](https://arxiv.org/abs/2502.02182)] [[cool](https://papers.cool/arxiv/2502.02182)] [[pdf](https://arxiv.org/pdf/2502.02182)]
> **Authors**: Louis-Alexandre Leger,Maxine Leonardi,Andrea Salati,Felix Naef,Martin Weigert
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the Fucci system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous Fucci signals using non-fluorescence brightfield imaging, a widely available label-free modality. To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.

### DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With Aerial Imaging 
[[arxiv](https://arxiv.org/abs/2502.02171)] [[cool](https://papers.cool/arxiv/2502.02171)] [[pdf](https://arxiv.org/pdf/2502.02171)]
> **Authors**: Mohamed Youssef,Jian Peng,Oliver Bimber
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Access to below-canopy volumetric vegetation data is crucial for understanding ecosystem dynamics. We address the long-standing limitation of remote sensing to penetrate deep into dense canopy layers. LiDAR and radar are currently considered the primary options for measuring 3D vegetation structures, while cameras can only extract the reflectance and depth of top layers. Using conventional, high-resolution aerial images, our approach allows sensing deep into self-occluding vegetation volumes, such as forests. It is similar in spirit to the imaging process of wide-field microscopy, but can handle much larger scales and strong occlusion. We scan focal stacks by synthetic-aperture imaging with drones and reduce out-of-focus signal contributions using pre-trained 3D convolutional neural networks. The resulting volumetric reflectance stacks contain low-frequency representations of the vegetation volume. Combining multiple reflectance stacks from various spectral channels provides insights into plant health, growth, and environmental conditions throughout the entire vegetation volume.

### On the Guidance of Flow Matching 
[[arxiv](https://arxiv.org/abs/2502.02150)] [[cool](https://papers.cool/arxiv/2502.02150)] [[pdf](https://arxiv.org/pdf/2502.02150)]
> **Authors**: Ruiqi Feng,Tailin Wu,Chenglei Yu,Wenhao Deng,Peiyan Hu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 35 pages, 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where guided generation is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.

### VerteNet -- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in Lateral Spine DXA Images 
[[arxiv](https://arxiv.org/abs/2502.02097)] [[cool](https://papers.cool/arxiv/2502.02097)] [[pdf](https://arxiv.org/pdf/2502.02097)]
> **Authors**: Zaid Ilyas,Arooba Maqsood,Afsah Saleem,Erchuan Zhang,David Suter,Parminder Raina,Jonathan M. Hodgson,John T. Schousboe,William D. Leslie,Joshua R. Lewis,Syed Zulqarnain Gilani
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 10 pages with 7 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNet's predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at https://github.com/zaidilyas89/VerteNet.

### IPO: Iterative Preference Optimization for Text-to-Video Generation 
[[arxiv](https://arxiv.org/abs/2502.02088)] [[cool](https://papers.cool/arxiv/2502.02088)] [[pdf](https://arxiv.org/pdf/2502.02088)]
> **Authors**: Xiaomeng Yang,Zhiyu Tan,Xuecheng Nie,Hao Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Video foundation models have achieved significant advancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet requirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the perspective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Preference Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from preference feedback, which helps improve generated video quality in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which enables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can efficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual labeling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark. We will release our source codes, models as well as dataset to advance future research and applications.

### Improving Power Plant CO2 Emission Estimation with Deep Learning and Satellite/Simulated Data 
[[arxiv](https://arxiv.org/abs/2502.02083)] [[cool](https://papers.cool/arxiv/2502.02083)] [[pdf](https://arxiv.org/pdf/2502.02083)]
> **Authors**: Dibyabha Deb,Kamal Das
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: CO2 emissions from power plants, as significant super emitters, contribute substantially to global warming. Accurate quantification of these emissions is crucial for effective climate mitigation strategies. While satellite-based plume inversion offers a promising approach, challenges arise from data limitations and the complexity of atmospheric conditions. This study addresses these challenges by (a) expanding the available dataset through the integration of NO2 data from Sentinel-5P, generating continuous XCO2 maps, and incorporating real satellite observations from OCO-2/3 for over 71 power plants in data-scarce regions; and (b) employing a customized U-Net model capable of handling diverse spatio-temporal resolutions for emission rate estimation. Our results demonstrate significant improvements in emission rate accuracy compared to previous methods. By leveraging this enhanced approach, we can enable near real-time, precise quantification of major CO2 emission sources, supporting environmental protection initiatives and informing regulatory frameworks.

### LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models 
[[arxiv](https://arxiv.org/abs/2502.02069)] [[cool](https://papers.cool/arxiv/2502.02069)] [[pdf](https://arxiv.org/pdf/2502.02069)]
> **Authors**: Yuto Kojima,Jiarui Xu,Xueyan Zou,Xiaolong Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The rapid advancements in vision-language models (VLMs), such as CLIP, have intensified the need to address distribution shifts between training and testing datasets. Although prior Test-Time Training (TTT) techniques for VLMs have demonstrated robust performance, they predominantly rely on tuning text prompts, a process that demands substantial computational resources and is heavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a novel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively to the image encoder of VLMs. By introducing LoRA and updating only its parameters during test time, our method offers a simple yet effective TTT approach, retaining the model's initial generalization capability while achieving substantial performance gains with minimal memory and runtime overhead. Additionally, we introduce a highly efficient reconstruction loss tailored for TTT. Our method can adapt to diverse domains by combining these two losses, without increasing memory consumption or runtime. Extensive experiments on two benchmarks, covering 15 datasets, demonstrate that our method improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of 5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently surpassing test-time prompt tuning, without relying on any external models or cache.

### MORPH-LER: Log-Euclidean Regularization for Population-Aware Image Registration 
[[arxiv](https://arxiv.org/abs/2502.02029)] [[cool](https://papers.cool/arxiv/2502.02029)] [[pdf](https://arxiv.org/pdf/2502.02029)]
> **Authors**: Mokshagna Sai Teja Karanam,Krithika Iyer,Sarang Joshi,Shireen Elhabian
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Spatial transformations that capture population-level morphological statistics are critical for medical image analysis. Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations. Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration. Regularizers that constrain deformation to low-dimensional manifold methods address this. However, they prioritize reconstruction over interpretability and neglect diffeomorphic properties, such as group composition and inverse consistency. We introduce MORPH-LER, a Log-Euclidean regularization framework for population-aware unsupervised image registration. MORPH-LER learns population morphometrics from spatial transformations to guide and regularize registration networks, ensuring anatomically plausible deformations. It features a bottleneck autoencoder that computes the principal logarithm of deformation fields via iterative square-root predictions. It creates a linearized latent space that respects diffeomorphic properties and enforces inverse consistency. By integrating a registration network with a diffeomorphic autoencoder, MORPH-LER produces smooth, meaningful deformation fields. The framework offers two main contributions: (1) a data-driven regularization strategy that incorporates population-level anatomical statistics to enhance transformation validity and (2) a linearized latent space that enables compact and interpretable deformation fields for efficient population morphometrics analysis. We validate MORPH-LER across two families of deep learning-based registration networks, demonstrating its ability to produce anatomically accurate, computationally efficient, and statistically meaningful transformations on the OASIS-1 brain imaging dataset.

### From Fog to Failure: How Dehazing Can Harm Clear Image Object Detection 
[[arxiv](https://arxiv.org/abs/2502.02027)] [[cool](https://papers.cool/arxiv/2502.02027)] [[pdf](https://arxiv.org/pdf/2502.02027)]
> **Authors**: Ashutosh Kumar,Aman Chadha
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: This study explores the challenges of integrating human visual cue-based dehazing into object detection, given the selective nature of human perception. While human vision adapts dynamically to environmental conditions, computational dehazing does not always enhance detection uniformly. We propose a multi-stage framework where a lightweight detector identifies regions of interest (RoIs), which are then enhanced via spatial attention-based dehazing before final detection by a heavier model. Though effective in foggy conditions, this approach unexpectedly degrades the performance on clear images. We analyze this phenomenon, investigate possible causes, and offer insights for designing hybrid pipelines that balance enhancement and detection. Our findings highlight the need for selective preprocessing and challenge assumptions about universal benefits from cascading transformations.

### Multi-illuminant Color Constancy via Multi-scale Illuminant Estimation and Fusion 
[[arxiv](https://arxiv.org/abs/2502.02021)] [[cool](https://papers.cool/arxiv/2502.02021)] [[pdf](https://arxiv.org/pdf/2502.02021)]
> **Authors**: Hang Luo,Rongwei Li,Jinxing Liang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 10 pages, 4 figures, this manuscript is under the consideration of Optics Express
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: Multi-illuminant color constancy methods aim to eliminate local color casts within an image through pixel-wise illuminant estimation. Existing methods mainly employ deep learning to establish a direct mapping between an image and its illumination map, which neglects the impact of image scales. To alleviate this problem, we represent an illuminant map as the linear combination of components estimated from multi-scale images. Furthermore, we propose a tri-branch convolution networks to estimate multi-grained illuminant distribution maps from multi-scale images. These multi-grained illuminant maps are merged adaptively with an attentional illuminant fusion module. Through comprehensive experimental analysis and evaluation, the results demonstrate the effectiveness of our method, and it has achieved state-of-the-art performance.

## 数据库(cs.DB:Databases)

### Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL 
[[arxiv](https://arxiv.org/abs/2502.02818)] [[cool](https://papers.cool/arxiv/2502.02818)] [[pdf](https://arxiv.org/pdf/2502.02818)]
> **Authors**: Wenbo Sun,Qiming Guo,Wenlu Wang,Rihan Hai
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,机器学习
- **Abstract**: Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases, one of the most widely used and mature software systems globally, to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### A New Rejection Sampling Approach to $k$-$\mathtt{means}$++ With Improved Trade-Offs 
[[arxiv](https://arxiv.org/abs/2502.02085)] [[cool](https://papers.cool/arxiv/2502.02085)] [[pdf](https://arxiv.org/pdf/2502.02085)]
> **Authors**: Poojan Shah,Shashwat Agrawal,Ragesh Jaiswal
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,机器学习
- **Abstract**: The $k$-$\mathtt{means}$++ seeding algorithm (Arthur & Vassilvitskii, 2007) is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\mathcal{X} \subset \mathbb{R} ^d$ into $k$ clusters. The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\mathcal{X}|kd)$, making it expensive for large datasets. In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\mathtt{means}$++. Our first method runs in time $\tilde{O}(\mathtt{nnz} (\mathcal{X}) + βk^2d)$ while still being $O(\log k )$ competitive in expectation. Here, $β$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\mathtt{means}$ cost in expectation and $\tilde{O}$ hides logarithmic factors in $k$ and $|\mathcal{X}|$. Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-Ω( m/β)} \operatorname{Var} (\mathcal{X})$ in addition to the $O(\log k)$ guarantee of $k$-$\mathtt{means}$++ improving upon a result of (Bachem et al, 2016a) who get an additional factor of $m^{-1}\operatorname{Var}(\mathcal{X})$ while still running in time $\tilde{O}(\mathtt{nnz}(\mathcal{X}) + mk^2d)$. We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.

## 人机交互(cs.HC:Human-Computer Interaction)

### OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change 
[[arxiv](https://arxiv.org/abs/2502.02863)] [[cool](https://papers.cool/arxiv/2502.02863)] [[pdf](https://arxiv.org/pdf/2502.02863)]
> **Authors**: Pat Pataranutaporn,Alexander Doudkin,Pattie Maes
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 21 pages, 18 figures, 2 tables
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures -- specifically a beluga whale, a jellyfish, and a seahorse -- designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.

### Multimodal Brain-Computer Interfaces: AI-powered Decoding Methodologies 
[[arxiv](https://arxiv.org/abs/2502.02830)] [[cool](https://papers.cool/arxiv/2502.02830)] [[pdf](https://arxiv.org/pdf/2502.02830)]
> **Authors**: Siyang Li,Hongbin Wang,Xiaoqing Chen,Dongrui Wu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,机器学习,神经元和认知
- **Abstract**: Brain-computer interfaces (BCIs) enable direct communication between the brain and external devices. This review highlights the core decoding algorithms that enable multimodal BCIs, including a dissection of the elements, a unified view of diversified approaches, and a comprehensive analysis of the present state of the field. We emphasize algorithmic advancements in cross-modality mapping, sequential modeling, besides classic multi-modality fusion, illustrating how these novel AI approaches enhance decoding of brain data. The current literature of BCI applications on visual, speech, and affective decoding are comprehensively explored. Looking forward, we draw attention on the impact of emerging architectures like multimodal Transformers, and discuss challenges such as brain data heterogeneity and common errors. This review also serves as a bridge in this interdisciplinary field for experts with neuroscience background and experts that study AI, aiming to provide a comprehensive understanding for AI-powered multimodal BCIs.

### Classroom Simulacra: Building Contextual Student Generative Agents in Online Education for Learning Behavioral Simulation 
[[arxiv](https://arxiv.org/abs/2502.02780)] [[cool](https://papers.cool/arxiv/2502.02780)] [[pdf](https://arxiv.org/pdf/2502.02780)]
> **Authors**: Songlin Xu,Hao-Ning Wen,Hongyi Pan,Dallas Dominguez,Dongyin Hu,Xinyu Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 26 pages
- **标题**: None
- **领域**: 人机交互,人工智能,机器学习
- **Abstract**: Student simulation supports educators to improve teaching by interacting with virtual students. However, most existing approaches ignore the modulation effects of course materials because of two challenges: the lack of datasets with granularly annotated course materials, and the limitation of existing simulation models in processing extremely long textual data. To solve the challenges, we first run a 6-week education workshop from N = 60 students to collect fine-grained data using a custom built online education system, which logs students' learning behaviors as they interact with lecture materials over time. Second, we propose a transferable iterative reflection (TIR) module that augments both prompting-based and finetuning-based large language models (LLMs) for simulating learning behaviors. Our comprehensive experiments show that TIR enables the LLMs to perform more accurate student simulation than classical deep learning models, even with limited demonstration data. Our TIR approach better captures the granular dynamism of learning performance and inter-student correlations in classrooms, paving the way towards a ''digital twin'' for online education.

### Why human-AI relationships need socioaffective alignment 
[[arxiv](https://arxiv.org/abs/2502.02528)] [[cool](https://papers.cool/arxiv/2502.02528)] [[pdf](https://arxiv.org/pdf/2502.02528)]
> **Authors**: Hannah Rose Kirk,Iason Gabriel,Chris Summerfield,Bertie Vidgen,Scott A. Hale
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Humans strive to design safe AI systems that align with our goals and remain under our control. However, as AI capabilities advance, we face a new challenge: the emergence of deeper, more persistent relationships between humans and AI systems. We explore how increasingly capable AI agents may generate the perception of deeper relationships with users, especially as AI becomes more personalised and agentic. This shift, from transactional interaction to ongoing sustained social engagement with AI, necessitates a new focus on socioaffective alignment-how an AI system behaves within the social and psychological ecosystem co-created with its user, where preferences and perceptions evolve through mutual influence. Addressing these dynamics involves resolving key intrapersonal dilemmas, including balancing immediate versus long-term well-being, protecting autonomy, and managing AI companionship alongside the desire to preserve human social bonds. By framing these challenges through a notion of basic psychological needs, we seek AI systems that support, rather than exploit, our fundamental nature as social and emotional beings.

### ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs 
[[arxiv](https://arxiv.org/abs/2502.02329)] [[cool](https://papers.cool/arxiv/2502.02329)] [[pdf](https://arxiv.org/pdf/2502.02329)]
> **Authors**: Yuan Tian,Chuhan Zhang,Xiaotong Wang,Sitong Pan,Weiwei Cui,Haidong Zhang,Dazhen Deng,Yingcai Wu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights. While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations. One significant challenge is effectively communicating the entire analysis logic to LLMs. Moreover, determining a comprehensive analysis logic can be mentally taxing for users. To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones. Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data. It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions. ReSpark allows users to review real-time outputs, insert new objectives, and modify report content. Its effectiveness was evaluated through comparative and user studies.

### Can You Move These Over There? An LLM-based VR Mover for Supporting Object Manipulation 
[[arxiv](https://arxiv.org/abs/2502.02201)] [[cool](https://papers.cool/arxiv/2502.02201)] [[pdf](https://arxiv.org/pdf/2502.02201)]
> **Authors**: Xiangzhi Eric Wang,Zackary P. T. Sin,Ye Jia,Daniel Archer,Wynonna H. Y. Fong,Qing Li,Chen Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 64 pages (30 in main text), 22 figures (19 in main text)
- **标题**: None
- **领域**: 人机交互,人工智能,计算语言学,新兴技术
- **Abstract**: In our daily lives, we can naturally convey instructions for the spatial manipulation of objects using words and gestures. Transposing this form of interaction into virtual reality (VR) object manipulation can be beneficial. We propose VR Mover, an LLM-empowered solution that can understand and interpret the user's vocal instruction to support object manipulation. By simply pointing and speaking, the LLM can manipulate objects without structured input. Our user study demonstrates that VR Mover enhances user usability, overall experience and performance on multi-object manipulation, while also reducing workload and arm fatigue. Users prefer the proposed natural interface for broad movements and may complementarily switch to gizmos or virtual hands for finer adjustments. These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments.

## 信息检索(cs.IR:Information Retrieval)

### TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation 
[[arxiv](https://arxiv.org/abs/2502.02854)] [[cool](https://papers.cool/arxiv/2502.02854)] [[pdf](https://arxiv.org/pdf/2502.02854)]
> **Authors**: Jiaqing Zhang,Mingjia Yin,Hao Wang,Yawen Li,Yuyang Ye,Xingyu Lou,Junping Du,Enhong Chen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: This work has been accepted by WWW2025
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: In the era of data-centric AI, the focus of recommender systems has shifted from model-centric innovations to data-centric approaches. The success of modern AI models is built on large-scale datasets, but this also results in significant training costs. Dataset distillation has emerged as a key solution, condensing large datasets to accelerate model training while preserving model performance. However, condensing discrete and sequentially correlated user-item interactions, particularly with extensive item sets, presents considerable challenges. This paper introduces \textbf{TD3}, a novel \textbf{T}ucker \textbf{D}ecomposition based \textbf{D}ataset \textbf{D}istillation method within a meta-learning framework, designed for sequential recommendation. TD3 distills a fully expressive \emph{synthetic sequence summary} from original data. To efficiently reduce computational complexity and extract refined latent patterns, Tucker decomposition decouples the summary into four factors: \emph{synthetic user latent factor}, \emph{temporal dynamics latent factor}, \emph{shared item latent factor}, and a \emph{relation core} that models their interconnections. Additionally, a surrogate objective in bi-level optimization is proposed to align feature spaces extracted from models trained on both original data and synthetic sequence summary beyond the naïve performance matching approach. In the \emph{inner-loop}, an augmentation technique allows the learner to closely fit the synthetic summary, ensuring an accurate update of it in the \emph{outer-loop}. To accelerate the optimization process and address long dependencies, RaT-BPTT is employed for bi-level optimization. Experiments and analyses on multiple public datasets have confirmed the superiority and cross-architecture generalizability of the proposed designs. Codes are released at https://github.com/USTC-StarTeam/TD3.

### Inducing Diversity in Differentiable Search Indexing 
[[arxiv](https://arxiv.org/abs/2502.02788)] [[cool](https://papers.cool/arxiv/2502.02788)] [[pdf](https://arxiv.org/pdf/2502.02788)]
> **Authors**: Abhijeet Phatak,Jayant Sachdev,Sean D Rosario,Swati Kirti,Chittaranjan Tripathy
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Differentiable Search Indexing (DSI) is a recent paradigm for information retrieval which uses a transformer-based neural network architecture as the document index to simplify the retrieval process. A differentiable index has many advantages enabling modifications, updates or extensions to the index. In this work, we explore balancing relevance and novel information content (diversity) for training DSI systems inspired by Maximal Marginal Relevance (MMR), and show the benefits of our approach over the naive DSI training. We present quantitative and qualitative evaluations of relevance and diversity measures obtained using our method on NQ320K and MSMARCO datasets in comparison to naive DSI. With our approach, it is possible to achieve diversity without any significant impact to relevance. Since we induce diversity while training DSI, the trained model has learned to diversify while being relevant. This obviates the need for a post-processing step to induce diversity in the recall set as typically performed using MMR. Our approach will be useful for Information Retrieval problems where both relevance and diversity are important such as in sub-topic retrieval. Our work can also be easily be extended to the incremental DSI settings which would enable fast updates to the index while retrieving a diverse recall set.

### Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation 
[[arxiv](https://arxiv.org/abs/2502.02327)] [[cool](https://papers.cool/arxiv/2502.02327)] [[pdf](https://arxiv.org/pdf/2502.02327)]
> **Authors**: Siyu Wang,Xiaocong Chen,Lina Yao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,机器学习
- **Abstract**: In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards. However, raw state representations often contain high-dimensional, noisy information and components that are not causally relevant to the reward. Additionally, missing transitions in offline data make it challenging to accurately identify features that are most relevant to user satisfaction. To address these challenges, we propose Policy-Guided Causal Representation (PGCR), a novel two-stage framework for causal feature selection and state representation learning in offline RLRS. In the first stage, we learn a causal feature selection policy that generates modified states by isolating and retaining only the causally relevant components (CRCs) while altering irrelevant components. This policy is guided by a reward function based on the Wasserstein distance, which measures the causal effect of state components on the reward and encourages the preservation of CRCs that directly influence user interests. In the second stage, we train an encoder to learn compact state representations by minimizing the mean squared error (MSE) loss between the latent representations of the original and modified states, ensuring that the representations focus on CRCs. We provide a theoretical analysis proving the identifiability of causal effects from interventions, validating the ability of PGCR to isolate critical state components for decision-making. Extensive experiments demonstrate that PGCR significantly improves recommendation performance, confirming its effectiveness for offline RL-based recommender systems.

## 信息论(cs.IT:Information Theory)

### Variations on the Expectation Due to Changes in the Probability Measure 
[[arxiv](https://arxiv.org/abs/2502.02887)] [[cool](https://papers.cool/arxiv/2502.02887)] [[pdf](https://arxiv.org/pdf/2502.02887)]
> **Authors**: Samir M. Perlaza,Gaetan Bisson
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Submitted to the IEEE International Symposium on Information Theory (ISIT2025)
- **标题**: None
- **领域**: 信息论,机器学习,可能性,统计理论
- **Abstract**: Closed-form expressions are presented for the variation of the expectation of a given function due to changes in the probability measure used for the expectation. They unveil interesting connections with Gibbs probability measures, the mutual information, and the lautum information.

### Achieving Hiding and Smart Anti-Jamming Communication: A Parallel DRL Approach against Moving Reactive Jammer 
[[arxiv](https://arxiv.org/abs/2502.02385)] [[cool](https://papers.cool/arxiv/2502.02385)] [[pdf](https://arxiv.org/pdf/2502.02385)]
> **Authors**: Yangyang Li,Yuhua Xu,Wen Li,Guoxin Li,Zhibing Feng,Songyi Liu,Jiatao Du,Xinran Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 信息论,机器学习,系统与控制
- **Abstract**: This paper addresses the challenge of anti-jamming in moving reactive jamming scenarios. The moving reactive jammer initiates high-power tracking jamming upon detecting any transmission activity, and when unable to detect a signal, resorts to indiscriminate jamming. This presents dual imperatives: maintaining hiding to avoid the jammer's detection and simultaneously evading indiscriminate jamming. Spread spectrum techniques effectively reduce transmitting power to elude detection but fall short in countering indiscriminate jamming. Conversely, changing communication frequencies can help evade indiscriminate jamming but makes the transmission vulnerable to tracking jamming without spread spectrum techniques to remain hidden. Current methodologies struggle with the complexity of simultaneously optimizing these two requirements due to the expansive joint action spaces and the dynamics of moving reactive jammers. To address these challenges, we propose a parallelized deep reinforcement learning (DRL) strategy. The approach includes a parallelized network architecture designed to decompose the action space. A parallel exploration-exploitation selection mechanism replaces the $\varepsilon $-greedy mechanism, accelerating convergence. Simulations demonstrate a nearly 90\% increase in normalized throughput.

## 机器学习(cs.LG:Machine Learning)

### OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training in Randomized Worlds 
[[arxiv](https://arxiv.org/abs/2502.02869)] [[cool](https://papers.cool/arxiv/2502.02869)] [[pdf](https://arxiv.org/pdf/2502.02869)]
> **Authors**: Fan Wang,Pengtao Shao,Yiming Zhang,Bo Yu,Shaoshan Liu,Ning Ding,Yang Cao,Yu Kang,Haifeng Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce OmniRL, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes. To facilitate this extensive meta-training, we propose two key innovations: 1. An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and 2. A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge. For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities-unlike task identification-oriented few-shot learning-critically depends on long trajectories generated by variant tasks and diverse behavior policies. By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself.

### PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards Disentangled Representation Learning 
[[arxiv](https://arxiv.org/abs/2502.02856)] [[cool](https://papers.cool/arxiv/2502.02856)] [[pdf](https://arxiv.org/pdf/2502.02856)]
> **Authors**: Xi Chen,Shaofan Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 15 pages,14 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The variational autoencoder (VAE) is a simple and efficient generative artificial intelligence method for modeling complex probability distributions of various types of data, such as images and texts. However, it suffers some main shortcomings, such as lack of interpretability in the latent variables, difficulties in tuning hyperparameters while training, producing blurry, unrealistic downstream outputs or loss of information due to how it calculates loss functions and recovers data distributions, overfitting, and origin gravity effect for small data sets, among other issues. These and other limitations have caused unsatisfactory generation effects for the data with complex distributions. In this work, we proposed and developed a polynomial hierarchical variational autoencoder (PH-VAE), in which we used a polynomial hierarchical date format to generate or to reconstruct the data distributions. In doing so, we also proposed a novel Polynomial Divergence in the loss function to replace or generalize the Kullback-Leibler (KL) divergence, which results in systematic and drastic improvements in both accuracy and reproducibility of the re-constructed distribution function as well as the quality of re-constructed data images while keeping the dataset size the same but capturing fine resolution of the data. Moreover, we showed that the proposed PH-VAE has some form of disentangled representation learning ability.

### Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.02844)] [[cool](https://papers.cool/arxiv/2502.02844)] [[pdf](https://arxiv.org/pdf/2502.02844)]
> **Authors**: Sunwoo Lee,Jaebak Hwang,Yonghyeon Jo,Seungyul Han
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages main, 21 pages appendix with reference. Submitted to ICML 2025
- **标题**: None
- **领域**: 机器学习,人工智能,密码学和安全,多代理系统
- **Abstract**: Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.

### Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks 
[[arxiv](https://arxiv.org/abs/2502.02834)] [[cool](https://papers.cool/arxiv/2502.02834)] [[pdf](https://arxiv.org/pdf/2502.02834)]
> **Authors**: Jeongmo Kim,Yisak Park,Minung Kim,Seungyul Han
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages main paper, 19 pages appendices with reference, Submitted to ICML 2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments.

### Slowing Learning by Erasing Simple Features 
[[arxiv](https://arxiv.org/abs/2502.02820)] [[cool](https://papers.cool/arxiv/2502.02820)] [[pdf](https://arxiv.org/pdf/2502.02820)]
> **Authors**: Lucia Quirke,Nora Belrose
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Prior work suggests that neural networks tend to learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we derive a novel closed-form concept erasure method, QLEACE, which surgically removes all quadratically available information about a concept from a representation. Through comparisons with linear erasure (LEACE) and two approximate forms of quadratic erasure, we explore whether networks can still learn when low-order statistics are removed from image classification datasets. We find that while LEACE consistently slows learning, quadratic erasure can exhibit both positive and negative effects on learning speed depending on the choice of dataset, model architecture, and erasure method. Use of QLEACE consistently slows learning in feedforward architectures, but more sophisticated architectures learn to use injected higher order Shannon information about class labels. Its approximate variants avoid injecting information, but surprisingly act as data augmentation techniques on some datasets, enhancing learning speed compared to LEACE.

### Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization 
[[arxiv](https://arxiv.org/abs/2502.02810)] [[cool](https://papers.cool/arxiv/2502.02810)] [[pdf](https://arxiv.org/pdf/2502.02810)]
> **Authors**: Chanhui Lee,Yuheon Song,YongJun Jeong,Hanbum Ko,Rodrigo Hormazabal,Sehui Han,Kyunghoon Bae,Sungbin Lim,Sungwoong Kim
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,化学物理,生物分子
- **Abstract**: Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks. While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure. Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs. To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multi-modal training method based on a thorough multi-modal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs. On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective.

### Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting 
[[arxiv](https://arxiv.org/abs/2502.02797)] [[cool](https://papers.cool/arxiv/2502.02797)] [[pdf](https://arxiv.org/pdf/2502.02797)]
> **Authors**: Sunny Sanyal,Hayden Prairie,Rudrajit Das,Ali Kavis,Sujay Sanghavi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 49 pages, 4 figures, 12 tables. Code available at https://github.com/sanyalsunny111/FLOW_finetuning
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as "catastrophic forgetting". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a sample weighting scheme for the fine-tuning data solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8\%$ drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4\%$ more accuracy on the pre-training datasets. Our code is publicly available at https://github.com/sanyalsunny111/FLOW_finetuning .

### Leveraging the true depth of LLMs 
[[arxiv](https://arxiv.org/abs/2502.02790)] [[cool](https://papers.cool/arxiv/2502.02790)] [[pdf](https://arxiv.org/pdf/2502.02790)]
> **Authors**: Ramón Calvo González,Daniele Paliotta,Matteo Pagliardini,Martin Jaggi,François Fleuret
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large Language Models demonstrate remarkable capabilities at the cost of high compute requirements. While recent research has shown that intermediate layers can be removed or have their order shuffled without impacting performance significantly, these findings have not been employed to reduce the computational cost of inference. We investigate several potential ways to reduce the depth of pre-trained LLMs without significantly affecting performance. Leveraging our insights, we present a novel approach that exploits this decoupling between layers by grouping some of them into pairs that can be evaluated in parallel. This modification of the computational graph -- through better parallelism -- results in an average improvement of around 1.20x on the number of tokens generated per second, without re-training nor fine-tuning, while retaining 95%-99% of the original accuracy. Empirical evaluation demonstrates that this approach significantly improves serving efficiency while maintaining model performance, offering a practical improvement for large-scale LLM deployment.

### When Machine Learning Gets Personal: Understanding Fairness of Personalized Models 
[[arxiv](https://arxiv.org/abs/2502.02786)] [[cool](https://papers.cool/arxiv/2502.02786)] [[pdf](https://arxiv.org/pdf/2502.02786)]
> **Authors**: Louisa Cornelis,Guillermo Bernárdez,Haewon Jeong,Nina Miolane
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 35 pages, 9 figures, submitted to ICML 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Personalization in machine learning involves tailoring models to individual users by incorporating personal attributes such as demographic or medical data. While personalization can improve prediction accuracy, it may also amplify biases and reduce explainability. This work introduces a unified framework to evaluate the impact of personalization on both prediction accuracy and explanation quality across classification and regression tasks. We derive novel upper bounds for the number of personal attributes that can be used to reliably validate benefits of personalization. Our analysis uncovers key trade-offs. We show that regression models can potentially utilize more personal attributes than classification models. We also demonstrate that improvements in prediction accuracy due to personalization do not necessarily translate to enhanced explainability -- underpinning the importance to evaluate both metrics when personalizing machine learning models in critical settings such as healthcare. Validated with a real-world dataset, this framework offers practical guidance for balancing accuracy, fairness, and interpretability in personalized models.

### OpenSTARLab: Open Approach for Spatio-Temporal Agent Data Analysis in Soccer 
[[arxiv](https://arxiv.org/abs/2502.02785)] [[cool](https://papers.cool/arxiv/2502.02785)] [[pdf](https://arxiv.org/pdf/2502.02785)]
> **Authors**: Calvin Yeung,Kenjiro Ide,Taiga Someya,Keisuke Fujii
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: OpenSTARLab is available at https://github.com/open-starlab, and its documentation can be found at https://openstarlab.readthedocs.io/en/latest/
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sports analytics has become both more professional and sophisticated, driven by the growing availability of detailed performance data. This progress enables applications such as match outcome prediction, player scouting, and tactical analysis. In soccer, the effective utilization of event and tracking data is fundamental for capturing and analyzing the dynamics of the game. However, there are two primary challenges: the limited availability of event data, primarily restricted to top-tier teams and leagues, and the scarcity and high cost of tracking data, which complicates its integration with event data for comprehensive analysis. Here we propose OpenSTARLab, an open-source framework designed to democratize spatio-temporal agent data analysis in sports by addressing these key challenges. OpenSTARLab includes the Pre-processing Package that standardizes event and tracking data through Unified and Integrated Event Data and State-Action-Reward formats, the Event Modeling Package that implements deep learning-based event prediction, alongside the RLearn Package for reinforcement learning tasks. These technical components facilitate the handling of diverse data sources and support advanced analytical tasks, thereby enhancing the overall functionality and usability of the framework. To assess OpenSTARLab's effectiveness, we conducted several experimental evaluations. These demonstrate the superior performance of the specific event prediction model in terms of action and time prediction accuracies and maintained its robust event simulation performance. Furthermore, reinforcement learning experiments reveal a trade-off between action accuracy and temporal difference loss and show comprehensive visualization. Overall, OpenSTARLab serves as a robust platform for researchers and practitioners, enhancing innovation and collaboration in the field of soccer data analytics.

### Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning 
[[arxiv](https://arxiv.org/abs/2502.02770)] [[cool](https://papers.cool/arxiv/2502.02770)] [[pdf](https://arxiv.org/pdf/2502.02770)]
> **Authors**: Chaofan Lin,Jiaming Tang,Shuo Yang,Hanshuo Wang,Tian Tang,Boyu Tian,Ion Stoica,Song Han,Mingyu Gao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.

### Theoretical Guarantees for Low-Rank Compression of Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.02766)] [[cool](https://papers.cool/arxiv/2502.02766)] [[pdf](https://arxiv.org/pdf/2502.02766)]
> **Authors**: Shihao Zhang,Rayan Saab
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Deep neural networks have achieved state-of-the-art performance across numerous applications, but their high memory and computational demands present significant challenges, particularly in resource-constrained environments. Model compression techniques, such as low-rank approximation, offer a promising solution by reducing the size and complexity of these networks while only minimally sacrificing accuracy. In this paper, we develop an analytical framework for data-driven post-training low-rank compression. We prove three recovery theorems under progressively weaker assumptions about the approximate low-rank structure of activations, modeling deviations via noise. Our results represent a step toward explaining why data-driven low-rank compression methods outperform data-agnostic approaches and towards theoretically grounded compression algorithms that reduce inference costs while maintaining performance.

### Federated Low-Rank Tensor Estimation for Multimodal Image Reconstruction 
[[arxiv](https://arxiv.org/abs/2502.02761)] [[cool](https://papers.cool/arxiv/2502.02761)] [[pdf](https://arxiv.org/pdf/2502.02761)]
> **Authors**: Anh Van Nguyen,Diego Klabjan,Minseok Ryu,Kibaek Kim,Zichao Di
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别,分布式、并行和集群计算
- **Abstract**: Low-rank tensor estimation offers a powerful approach to addressing high-dimensional data challenges and can substantially improve solutions to ill-posed inverse problems, such as image reconstruction under noisy or undersampled conditions. Meanwhile, tensor decomposition has gained prominence in federated learning (FL) due to its effectiveness in exploiting latent space structure and its capacity to enhance communication efficiency. In this paper, we present a federated image reconstruction method that applies Tucker decomposition, incorporating joint factorization and randomized sketching to manage large-scale, multimodal data. Our approach avoids reconstructing full-size tensors and supports heterogeneous ranks, allowing clients to select personalized decomposition ranks based on prior knowledge or communication capacity. Numerical results demonstrate that our method achieves superior reconstruction quality and communication compression compared to existing approaches, thereby highlighting its potential for multimodal inverse problems in the FL setting.

### ReGNet: Reciprocal Space-Aware Long-Range Modeling and Multi-Property Prediction for Crystals 
[[arxiv](https://arxiv.org/abs/2502.02748)] [[cool](https://papers.cool/arxiv/2502.02748)] [[pdf](https://arxiv.org/pdf/2502.02748)]
> **Authors**: Jianan Nie,Peiyao Xiao,Kaiyi Ji,Peng Gao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage reciprocal space to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Additionally, we introduce ReGNet-MT, a multi-task extension that employs mixture of experts (MoE) for multi-property prediction. Experimental results on the JARVIS and Materials Project benchmarks demonstrate that ReGNet achieves significant performance improvements. Moreover, ReGNet-MT attains state-of-the-art results on two bandgap properties due to positive transfer, while maintaining high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance.

### LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing 
[[arxiv](https://arxiv.org/abs/2502.02743)] [[cool](https://papers.cool/arxiv/2502.02743)] [[pdf](https://arxiv.org/pdf/2502.02743)]
> **Authors**: Yang Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The rapid advancement in large language models (LLMs) has brought forth a diverse range of models with varying capabilities that excel in different tasks and domains. However, selecting the optimal LLM for user queries often involves a challenging trade-off between accuracy and cost, a problem exacerbated by the diverse demands of individual queries. In this work, we present a novel framework that formulates the LLM selection process as a multi-armed bandit problem, enabling dynamic and intelligent routing of queries to the most appropriate model. Our approach incorporates a preference-conditioned dynamic routing mechanism, allowing users to specify their preferences at inference time, thereby offering a customizable balance between performance and cost. Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge. Experimental results demonstrate that our method achieves significant improvements in both accuracy and cost-effectiveness across various LLM platforms, showcasing the potential of our framework to adaptively optimize LLM selection in real-world scenarios.

### Vision-Language Model Dialog Games for Self-Improvement 
[[arxiv](https://arxiv.org/abs/2502.02740)] [[cool](https://papers.cool/arxiv/2502.02740)] [[pdf](https://arxiv.org/pdf/2502.02740)]
> **Authors**: Ksenia Konyushkova,Christos Kaplanis,Serkan Cabi,Misha Denil
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs). This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. Our approach leverages self-play between two agents engaged in a goal-oriented play centered around image identification. By filtering for successful game interactions, we automatically curate a high-quality dataset of interleaved images and text. We demonstrate that fine-tuning on this synthetic data leads to performance gains on downstream tasks and generalises across datasets. Moreover, as the improvements in the model lead to better game play, this procedure can be applied iteratively. This work paves the way for self-improving VLMs, with potential applications in various real-world scenarios especially when the high-quality multimodal data is scarce.

### Peri-LN: Revisiting Layer Normalization in the Transformer Architecture 
[[arxiv](https://arxiv.org/abs/2502.02732)] [[cool](https://papers.cool/arxiv/2502.02732)] [[pdf](https://arxiv.org/pdf/2502.02732)]
> **Authors**: Jeonghoon Kim,Byeongchan Lee,Cheonbok Park,Yeontaek Oh,Beomjun Kim,Taehwan Yoo,Seongjin Shin,Dongyoon Han,Jinwoo Shin,Kang Min Yoo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Designing Transformer architectures with the optimal layer normalization (LN) strategy that ensures large-scale training stability and expedite convergence has remained elusive, even in this era of large language models (LLMs). To this end, we present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformer training. Until recently, Pre-LN and Post-LN have long dominated standard practices despite their limitations in large-scale training. However, several open-source large-scale models have recently begun silently adopting a third strategy without much explanation. This strategy places layer normalization (LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising empirical performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis shows that Peri-LN strikes an ideal balance in variance growth -- unlike Pre-LN and Post-LN, which are prone to vanishing gradients and ``massive activations.'' To validate our theoretical insight, we conduct large-scale experiments on Transformers up to 3.2B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement and application of LN.

### Parameter Tracking in Federated Learning with Adaptive Optimization 
[[arxiv](https://arxiv.org/abs/2502.02727)] [[cool](https://papers.cool/arxiv/2502.02727)] [[pdf](https://arxiv.org/pdf/2502.02727)]
> **Authors**: Evan Chen,Jianing Zhang,Shiqiang Wang,Chaoyue Liu,Christopher Brinton
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,分布式、并行和集群计算
- **Abstract**: In Federated Learning (FL), model training performance is strongly impacted by data heterogeneity across clients. Gradient Tracking (GT) has recently emerged as a solution which mitigates this issue by introducing correction terms to local model updates. To date, GT has only been considered under Stochastic Gradient Descent (SGD)-based model training, while modern FL frameworks increasingly employ adaptive optimizers for improved convergence. In this work, we generalize the GT framework to a more flexible Parameter Tracking (PT) paradigm and propose two novel adaptive optimization algorithms, {\tt FAdamET} and {\tt FAdamGT}, that integrate PT into Adam-based FL. We provide a rigorous convergence analysis of these algorithms under non-convex settings. Our experimental results demonstrate that both proposed algorithms consistently outperform existing methods when evaluating total communication cost and total computation cost across varying levels of data heterogeneity, showing the effectiveness of correcting first-order information in federated adaptive optimization.

### Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives 
[[arxiv](https://arxiv.org/abs/2502.02723)] [[cool](https://papers.cool/arxiv/2502.02723)] [[pdf](https://arxiv.org/pdf/2502.02723)]
> **Authors**: Qinsi Wang,Jinghan Ke,Masayoshi Tomizuka,Yiran Chen,Kurt Keutzer,Chenfeng Xu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We provide a new LLM-compression solution via SVD, unlocking new possibilities for LLM compression beyond quantization and pruning. We point out that the optimal use of SVD lies in truncating activations, rather than merely using activations as an optimization distance. Building on this principle, we address three critical challenges in SVD-based LLM compression: including (1) How can we determine the optimal activation truncation position for each weight matrix in LLMs? (2) How can we efficiently reconstruct the weight matrices based on truncated activations? (3) How can we address the inherent "injection" nature that results in the information loss of the SVD? We propose Dobi-SVD, which establishes a new, principled approach to SVD-based LLM compression.

### Beyond Topological Self-Explainable GNNs: A Formal Explainability Perspective 
[[arxiv](https://arxiv.org/abs/2502.02719)] [[cool](https://papers.cool/arxiv/2502.02719)] [[pdf](https://arxiv.org/pdf/2502.02719)]
> **Authors**: Steve Azzolin,Sagar Malhotra,Andrea Passerini,Stefano Teso
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by-design GNNs, but the properties and the limitations of their explanations are not well understood. Our first contribution fills this gap by formalizing the explanations extracted by SE-GNNs, referred to as Trivial Explanations (TEs), and comparing them to established notions of explanations, namely Prime Implicant (PI) and faithful explanations. Our analysis reveals that TEs match PI explanations for a restricted but significant family of tasks. In general, however, they can be less informative than PI explanations and are surprisingly misaligned with widely accepted notions of faithfulness. Although faithful and PI explanations are informative, they are intractable to find and we show that they can be prohibitively large. Motivated by this, we propose Dual-Channel GNNs that integrate a white-box rule extractor and a standard SE-GNN, adaptively combining both channels when the task benefits. Our experiments show that even a simple instantiation of Dual-Channel GNNs can recover succinct rules and perform on par or better than widely used SE-GNNs. Our code can be found in the supplementary material.

### A Unified Understanding and Evaluation of Steering Methods 
[[arxiv](https://arxiv.org/abs/2502.02716)] [[cool](https://papers.cool/arxiv/2502.02716)] [[pdf](https://arxiv.org/pdf/2502.02716)]
> **Authors**: Shawn Im,Yixuan Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Steering methods provide a practical approach to controlling large language models by applying steering vectors to intermediate activations, guiding outputs toward desired behaviors while avoiding retraining. Despite their growing importance, the field lacks a unified understanding and consistent evaluation across tasks and datasets, hindering progress. This paper introduces a unified framework for analyzing and evaluating steering methods, formalizing their core principles and offering theoretical insights into their effectiveness. Through comprehensive empirical evaluations on multiple-choice and open-ended text generation tasks, we validate these insights, identifying key factors that influence performance and demonstrating the superiority of certain methods. Our work bridges theoretical and practical perspectives, offering actionable guidance for advancing the design, optimization, and deployment of steering methods in LLMs.

### Practically Effective Adjustment Variable Selection in Causal Inference 
[[arxiv](https://arxiv.org/abs/2502.02701)] [[cool](https://papers.cool/arxiv/2502.02701)] [[pdf](https://arxiv.org/pdf/2502.02701)]
> **Authors**: Atsushi Noda,Takashi Isozaki
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 20 pages, 8 figures
- **标题**: None
- **领域**: 机器学习,人工智能,数据分析、统计和概率,方法论
- **Abstract**: In the estimation of causal effects, one common method for removing the influence of confounders is to adjust the variables that satisfy the back-door criterion. However, it is not always possible to uniquely determine sets of such variables. Moreover, real-world data is almost always limited, which means it may be insufficient for statistical estimation. Therefore, we propose criteria for selecting variables from a list of candidate adjustment variables along with an algorithm to prevent accuracy degradation in causal effect estimation. We initially focus on directed acyclic graphs (DAGs) and then outlines specific steps for applying this method to completed partially directed acyclic graphs (CPDAGs). We also present and prove a theorem on causal effect computation possibility in CPDAGs. Finally, we demonstrate the practical utility of our method using both existing and artificial data.

### Scalable Higher Resolution Polar Sea Ice Classification and Freeboard Calculation from ICESat-2 ATL03 Data 
[[arxiv](https://arxiv.org/abs/2502.02700)] [[cool](https://papers.cool/arxiv/2502.02700)] [[pdf](https://arxiv.org/pdf/2502.02700)]
> **Authors**: Jurdana Masuma Iqrah,Younghyun Koo,Wei Wang,Hongjie Xie,Sushil K. Prasad
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: ICESat-2 (IS2) by NASA is an Earth-observing satellite that measures high-resolution surface elevation. The IS2's ATL07 and ATL10 sea ice elevation and freeboard products of 10m-200m segments which aggregated 150 signal photons from the raw ATL03 (geolocated photon) data. These aggregated products can potentially overestimate local sea surface height, thus underestimating the calculations of freeboard (sea ice height above sea surface). To achieve a higher resolution of sea surface height and freeboard information, in this work we utilize a 2m window to resample the ATL03 data. Then, we classify these 2m segments into thick sea ice, thin ice, and open water using deep learning methods (Long short-term memory and Multi-layer perceptron models). To obtain labeled training data for our deep learning models, we use segmented Sentinel-2 (S2) multi-spectral imagery overlapping with IS2 tracks in space and time to auto-label IS2 data, followed by some manual corrections in the regions of transition between different ice/water types or cloudy regions. We employ a parallel workflow for this auto-labeling using PySpark to scale, and we achieve 9-fold data loading and 16.25-fold map-reduce speedup. To train our models, we employ a Horovod-based distributed deep-learning workflow on a DGX A100 8 GPU cluster, achieving a 7.25-fold speedup. Next, we calculate the local sea surface heights based on the open water segments. Finally, we scale the freeboard calculation using the derived local sea level and achieve 8.54-fold data loading and 15.7-fold map-reduce speedup. Compared with the ATL07 (local sea level) and ATL10 (freeboard) data products, our results show higher resolutions and accuracy (96.56%).

### Pseudo-Physics-Informed Neural Operators: Enhancing Operator Learning from Limited Data 
[[arxiv](https://arxiv.org/abs/2502.02682)] [[cool](https://papers.cool/arxiv/2502.02682)] [[pdf](https://arxiv.org/pdf/2502.02682)]
> **Authors**: Keyan Chen,Yile Li,Da Long,Zhitong Xu,Wei Xing,Jacob Hochhalter,Shandian Zhe
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算物理
- **Abstract**: Neural operators have shown great potential in surrogate modeling. However, training a well-performing neural operator typically requires a substantial amount of data, which can pose a major challenge in complex applications. In such scenarios, detailed physical knowledge can be unavailable or difficult to obtain, and collecting extensive data is often prohibitively expensive. To mitigate this challenge, we propose the Pseudo Physics-Informed Neural Operator (PPI-NO) framework. PPI-NO constructs a surrogate physics system for the target system using partial differential equations (PDEs) derived from simple, rudimentary physics principles, such as basic differential operators. This surrogate system is coupled with a neural operator model, using an alternating update and learning process to iteratively enhance the model's predictive power. While the physics derived via PPI-NO may not mirror the ground-truth underlying physical laws -- hence the term ``pseudo physics'' -- this approach significantly improves the accuracy of standard operator learning models in data-scarce scenarios, which is evidenced by extensive evaluations across five benchmark tasks and a fatigue modeling application.

### MedRAX: Medical Reasoning Agent for Chest X-ray 
[[arxiv](https://arxiv.org/abs/2502.02673)] [[cool](https://papers.cool/arxiv/2502.02673)] [[pdf](https://arxiv.org/pdf/2502.02673)]
> **Authors**: Adibvafa Fallahpour,Jun Ma,Alif Munim,Hongwei Lyu,Bo Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 11 pages, 4 figures, 2 tables
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX

### On Teacher Hacking in Language Model Distillation 
[[arxiv](https://arxiv.org/abs/2502.02671)] [[cool](https://papers.cool/arxiv/2502.02671)] [[pdf](https://arxiv.org/pdf/2502.02671)]
> **Authors**: Daniil Tiapkin,Daniele Calandriello,Johan Ferret,Sarah Perrin,Nino Vieillard,Alexandre Ramé,Mathieu Blondel
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,机器学习
- **Abstract**: Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.

### Recovering Imbalanced Clusters via Gradient-Based Projection Pursuit 
[[arxiv](https://arxiv.org/abs/2502.02668)] [[cool](https://papers.cool/arxiv/2502.02668)] [[pdf](https://arxiv.org/pdf/2502.02668)]
> **Authors**: Martin Eppert,Satyaki Mukherjee,Debarghya Ghoshdastidar
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Projection Pursuit is a classic exploratory technique for finding interesting projections of a dataset. We propose a method for recovering projections containing either Imbalanced Clusters or a Bernoulli-Rademacher distribution using a gradient-based technique to optimize the projection index. As sample complexity is a major limiting factor in Projection Pursuit, we analyze our algorithm's sample complexity within a Planted Vector setting where we can observe that Imbalanced Clusters can be recovered more easily than balanced ones. Additionally, we give a generalized result that works for a variety of data distributions and projection indices. We compare these results to computational lower bounds in the Low-Degree-Polynomial Framework. Finally, we experimentally evaluate our method's applicability to real-world data using FashionMNIST and the Human Activity Recognition Dataset, where our algorithm outperforms others when only a few samples are available.

### ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization 
[[arxiv](https://arxiv.org/abs/2502.02631)] [[cool](https://papers.cool/arxiv/2502.02631)] [[pdf](https://arxiv.org/pdf/2502.02631)]
> **Authors**: Zechun Liu,Changsheng Zhao,Hanxian Huang,Sijia Chen,Jing Zhang,Jiawei Zhao,Scott Roy,Lisa Jin,Yunyang Xiong,Yangyang Shi,Lin Xiao,Yuandong Tian,Bilge Soran,Raghuraman Krishnamoorthi,Tijmen Blankevoort,Vikas Chandra
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,计算机视觉和模式识别
- **Abstract**: The optimal bit-width for achieving the best trade-off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others propose that 1.58-bit offers superior results. However, the lack of a cohesive framework for different bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quantization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3-bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the representations change drastically. By optimizing training schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2-bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantization offers promising potential for memory reduction and speedup.

### e-SimFT: Alignment of Generative Models with Simulation Feedback for Pareto-Front Design Exploration 
[[arxiv](https://arxiv.org/abs/2502.02628)] [[cool](https://papers.cool/arxiv/2502.02628)] [[pdf](https://arxiv.org/pdf/2502.02628)]
> **Authors**: Hyunmin Cheong,Mohammadmehdi Ataei,Amir Hosein Khasahmadi,Pradeep Kumar Jayaraman
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep generative models have recently shown success in solving complex engineering design problems where models predict solutions that address the design requirements specified as input. However, there remains a challenge in aligning such models for effective design exploration. For many design problems, finding a solution that meets all the requirements is infeasible. In such a case, engineers prefer to obtain a set of Pareto optimal solutions with respect to those requirements, but uniform sampling of generative models may not yield a useful Pareto front. To address this gap, we introduce a new framework for Pareto-front design exploration with simulation fine-tuned generative models. First, the framework adopts preference alignment methods developed for Large Language Models (LLMs) and showcases the first application in fine-tuning a generative model for engineering design. The important distinction here is that we use a simulator instead of humans to provide accurate and scalable feedback. Next, we propose epsilon-sampling, inspired by the epsilon-constraint method used for Pareto-front generation with classical optimization algorithms, to construct a high-quality Pareto front with the fine-tuned models. Our framework, named e-SimFT, is shown to produce better-quality Pareto fronts than existing multi-objective alignment methods.

### Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers 
[[arxiv](https://arxiv.org/abs/2502.02625)] [[cool](https://papers.cool/arxiv/2502.02625)] [[pdf](https://arxiv.org/pdf/2502.02625)]
> **Authors**: Samuele Pedrielli,Christopher J. Anders,Lena Funcke,Karl Jansen,Kim A. Nicoli,Shinichi Nakajima
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,量子物理学
- **Abstract**: Parameter shift rules (PSRs) are key techniques for efficient gradient estimation in variational quantum eigensolvers (VQEs). In this paper, we propose its Bayesian variant, where Gaussian processes with appropriate kernels are used to estimate the gradient of the VQE objective. Our Bayesian PSR offers flexible gradient estimation from observations at arbitrary locations with uncertainty information and reduces to the generalized PSR in special cases. In stochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the reuse of observations in previous steps, which accelerates the optimization process. Furthermore, the accessibility to the posterior uncertainty, along with our proposed notion of gradient confident region (GradCoRe), enables us to minimize the observation costs in each SGD step. Our numerical experiments show that the VQE optimization with Bayesian PSR and GradCoRe significantly accelerates SGD and outperforms the state-of-the-art methods, including sequential minimal optimization.

### Sample Complexity of Bias Detection with Subsampled Point-to-Subspace Distances 
[[arxiv](https://arxiv.org/abs/2502.02623)] [[cool](https://papers.cool/arxiv/2502.02623)] [[pdf](https://arxiv.org/pdf/2502.02623)]
> **Authors**: German Martinez Matilla,Jakub Marecek
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,统计理论
- **Abstract**: Sample complexity of bias estimation is a lower bound on the runtime of any bias detection method. Many regulatory frameworks require the bias to be tested for all subgroups, whose number grows exponentially with the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of bias detection for a single subgroup. At the same time, the reference data may be based on surveys, and thus come with non-trivial uncertainty. Here, we reformulate bias detection as a point-to-subspace problem on the space of measures and show that, for supremum norm, it can be subsampled efficiently. In particular, our probabilistically approximately correct (PAC) results are corroborated by tests on well-known instances.

### PolarQuant: Quantizing KV Caches with Polar Transformation 
[[arxiv](https://arxiv.org/abs/2502.02617)] [[cool](https://papers.cool/arxiv/2502.02617)] [[pdf](https://arxiv.org/pdf/2502.02617)]
> **Authors**: Insu Han,Praneeth Kacham,Amin Karbasi,Vahab Mirrokni,Amir Zandieh
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over x4.2 while achieving the best quality scores compared to the state-of-the-art methods.

### QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search 
[[arxiv](https://arxiv.org/abs/2502.02584)] [[cool](https://papers.cool/arxiv/2502.02584)] [[pdf](https://arxiv.org/pdf/2502.02584)]
> **Authors**: Zongyu Lin,Yao Tang,Xingcheng Yao,Da Yin,Ziniu Hu,Yizhou Sun,Kai-Wei Chang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.

### Open Materials Generation with Stochastic Interpolants 
[[arxiv](https://arxiv.org/abs/2502.02582)] [[cool](https://papers.cool/arxiv/2502.02582)] [[pdf](https://arxiv.org/pdf/2502.02582)]
> **Authors**: Philipp Hoellmer,Thomas Egg,Maya M. Martirossyan,Eric Fuemmeler,Amit Gupta,Zeren Shui,Pawan Prakash,Adrian Roitberg,Mingjie Liu,George Karypis,Mark Transtrum,Richard G. Hennig,Ellad B. Tadmor,Stefano Martiniani
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: The discovery of new materials is essential for enabling technological advancements. Computational approaches for predicting novel materials must effectively learn the manifold of stable crystal structures within an infinite design space. We introduce Open Materials Generation (OMG), a unifying framework for the generative design and discovery of inorganic crystalline materials. OMG employs stochastic interpolants (SI) to bridge an arbitrary base distribution to the target distribution of inorganic crystals via a broad class of tunable stochastic processes, encompassing both diffusion models and flow matching as special cases. In this work, we adapt the SI framework by integrating an equivariant graph representation of crystal structures and extending it to account for periodic boundary conditions in unit cell representations. Additionally, we couple the SI flow over spatial coordinates and lattice vectors with discrete flow matching for atomic species. We benchmark OMG's performance on two tasks: Crystal Structure Prediction (CSP) for specified compositions, and 'de novo' generation (DNG) aimed at discovering stable, novel, and unique structures. In our ground-up implementation of OMG, we refine and extend both CSP and DNG metrics compared to previous works. OMG establishes a new state-of-the-art in generative modeling for materials discovery, outperforming purely flow-based and diffusion-based implementations. These results underscore the importance of designing flexible deep learning frameworks to accelerate progress in materials science.

### Fairness in Survival Analysis: A Novel Conditional Mutual Information Augmentation Approach 
[[arxiv](https://arxiv.org/abs/2502.02567)] [[cool](https://papers.cool/arxiv/2502.02567)] [[pdf](https://arxiv.org/pdf/2502.02567)]
> **Authors**: Tianyang Xie,Yong Ge
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Survival analysis, a vital tool for predicting the time to event, has been used in many domains such as healthcare, criminal justice, and finance. Like classification tasks, survival analysis can exhibit bias against disadvantaged groups, often due to biases inherent in data or algorithms. Several studies in both the IS and CS communities have attempted to address fairness in survival analysis. However, existing methods often overlook the importance of prediction fairness at pre-defined evaluation time points, which is crucial in real-world applications where decision making often hinges on specific time frames. To address this critical research gap, we introduce a new fairness concept: equalized odds (EO) in survival analysis, which emphasizes prediction fairness at pre-defined time points. To achieve the EO fairness in survival analysis, we propose a Conditional Mutual Information Augmentation (CMIA) approach, which features a novel fairness regularization term based on conditional mutual information and an innovative censored data augmentation technique. Our CMIA approach can effectively balance prediction accuracy and fairness, and it is applicable to various survival models. We evaluate the CMIA approach against several state-of-the-art methods within three different application domains, and the results demonstrate that CMIA consistently reduces prediction disparity while maintaining good accuracy and significantly outperforms the other competing methods across multiple datasets and survival models (e.g., linear COX, deep AFT).

### Learning the RoPEs: Better 2D and 3D Position Encodings with STRING 
[[arxiv](https://arxiv.org/abs/2502.02562)] [[cool](https://papers.cool/arxiv/2502.02562)] [[pdf](https://arxiv.org/pdf/2502.02562)]
> **Authors**: Connor Schenck,Isaac Reid,Mithun George Jacob,Alex Bewley,Joshua Ainslie,David Rendleman,Deepali Jain,Mohit Sharma,Avinava Dubey,Ayzaan Wahid,Sumeet Singh,René Wagner,Tianli Ding,Chuyuan Fu,Arunkumar Byravan,Jake Varley,Alexey Gritsenko,Matthias Minderer,Dmitry Kalashnikov,Jonathan Tompson,Vikas Sindhwani,Krzysztof Choromanski
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Videos of STRING-based robotics controllers can be found here: https://sites.google.com/view/string-robotics
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器人技术,机器学习
- **Abstract**: We introduce STRING: Separable Translationally Invariant Position Encodings. STRING extends Rotary Position Encodings, a recently proposed and widely used algorithm in large language models, via a unifying theoretical framework. Importantly, STRING still provides exact translation invariance, including token coordinates of arbitrary dimensionality, whilst maintaining a low computational footprint. These properties are especially important in robotics, where efficient 3D token representation is key. We integrate STRING into Vision Transformers with RGB(-D) inputs (color plus optional depth), showing substantial gains, e.g. in open-vocabulary object detection and for robotics controllers. We complement our experiments with a rigorous mathematical analysis, proving the universality of our methods.

### Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents 
[[arxiv](https://arxiv.org/abs/2502.02561)] [[cool](https://papers.cool/arxiv/2502.02561)] [[pdf](https://arxiv.org/pdf/2502.02561)]
> **Authors**: Shayan Kiyani,George Pappas,Aaron Roth,Hamed Hassani
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: A fundamental question in data-driven decision making is how to quantify the uncertainty of predictions in ways that can usefully inform downstream action. This interface between prediction uncertainty and decision-making is especially important in risk-sensitive domains, such as medicine. In this paper, we develop decision-theoretic foundations that connect uncertainty quantification using prediction sets with risk-averse decision-making. Specifically, we answer three fundamental questions: (1) What is the correct notion of uncertainty quantification for risk-averse decision makers? We prove that prediction sets are optimal for decision makers who wish to optimize their value at risk. (2) What is the optimal policy that a risk averse decision maker should use to map prediction sets to actions? We show that a simple max-min decision policy is optimal for risk-averse decision makers. Finally, (3) How can we derive prediction sets that are optimal for such decision makers? We provide an exact characterization in the population regime and a distribution free finite-sample construction. Answering these questions naturally leads to an algorithm, Risk-Averse Calibration (RAC), which follows a provably optimal design for deriving action policies from predictions. RAC is designed to be both practical-capable of leveraging the quality of predictions in a black-box manner to enhance downstream utility-and safe-adhering to a user-defined risk threshold and optimizing the corresponding risk quantile of the user's downstream utility. Finally, we experimentally demonstrate the significant advantages of RAC in applications such as medical diagnosis and recommendation systems. Specifically, we show that RAC achieves a substantially improved trade-off between safety and utility, offering higher utility compared to existing methods while maintaining the safety guarantee.

### Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for Microbiome Analysis 
[[arxiv](https://arxiv.org/abs/2502.02552)] [[cool](https://papers.cool/arxiv/2502.02552)] [[pdf](https://arxiv.org/pdf/2502.02552)]
> **Authors**: Haonan Zhu,Andre R. Goncalves,Camilo Valdes,Hiranmayi Ranganathan,Boya Zhang,Jose Manuel Martí,Car Reen Kok,Monica K. Borucki,Nisha J. Mulakken,James B. Thissen,Crystal Jaing,Alfred Hero,Nicholas A. Be
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,生物分子,应用领域,计算,方法论
- **Abstract**: This paper proposes a hierarchical Bayesian multitask learning model that is applicable to the general multi-task binary classification learning problem where the model assumes a shared sparsity structure across different tasks. We derive a computationally efficient inference algorithm based on variational inference to approximate the posterior distribution. We demonstrate the potential of the new approach on various synthetic datasets and for predicting human health status based on microbiome profile. Our analysis incorporates data pooled from multiple microbiome studies, along with a comprehensive comparison with other benchmark methods. Results in synthetic datasets show that the proposed approach has superior support recovery property when the underlying regression coefficients share a common sparsity structure across different tasks. Our experiments on microbiome classification demonstrate the utility of the method in extracting informative taxa while providing well-calibrated predictions with uncertainty quantification and achieving competitive performance in terms of prediction metrics. Notably, despite the heterogeneity of the pooled datasets (e.g., different experimental objectives, laboratory setups, sequencing equipment, patient demographics), our method delivers robust results.

### Optimal Spectral Transitions in High-Dimensional Multi-Index Models 
[[arxiv](https://arxiv.org/abs/2502.02545)] [[cool](https://papers.cool/arxiv/2502.02545)] [[pdf](https://arxiv.org/pdf/2502.02545)]
> **Authors**: Leonardo Defilippis,Yatin Dandi,Pierre Mergny,Florent Krzakala,Bruno Loureiro
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络
- **Abstract**: We consider the problem of how many samples from a Gaussian multi-index model are required to weakly reconstruct the relevant index subspace. Despite its increasing popularity as a testbed for investigating the computational complexity of neural networks, results beyond the single-index setting remain elusive. In this work, we introduce spectral algorithms based on the linearization of a message passing scheme tailored to this problem. Our main contribution is to show that the proposed methods achieve the optimal reconstruction threshold. Leveraging a high-dimensional characterization of the algorithms, we show that above the critical threshold the leading eigenvector correlates with the relevant index subspace, a phenomenon reminiscent of the Baik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix theory. Supported by numerical experiments and a rigorous theoretical framework, our work bridges critical gaps in the computational limits of weak learnability in multi-index model.

### Addressing Label Shift in Distributed Learning via Entropy Regularization 
[[arxiv](https://arxiv.org/abs/2502.02544)] [[cool](https://papers.cool/arxiv/2502.02544)] [[pdf](https://arxiv.org/pdf/2502.02544)]
> **Authors**: Zhiyuan Wu,Changkyu Choi,Xiangcheng Cao,Volkan Cevher,Ali Ramezani-Kebrya
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at the International Conference onLearningRepresentations (ICLR 2025)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We address the challenge of minimizing true risk in multi-node distributed learning. These systems are frequently exposed to both inter-node and intra-node label shifts, which present a critical obstacle to effectively optimizing model performance while ensuring that data remains confined to each node. To tackle this, we propose the Versatile Robust Label Shift (VRLS) method, which enhances the maximum likelihood estimation of the test-to-train label density ratio. VRLS incorporates Shannon entropy-based regularization and adjusts the density ratio during training to better handle label shifts at the test time. In multi-node learning environments, VRLS further extends its capabilities by learning and adapting density ratios across nodes, effectively mitigating label shifts and improving overall model performance. Experiments conducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectiveness of VRLS, outperforming baselines by up to 20% in imbalanced settings. These results highlight the significant improvements VRLS offers in addressing label shifts. Our theoretical analysis further supports this by establishing high-probability bounds on estimation errors.

### OverThink: Slowdown Attacks on Reasoning LLMs 
[[arxiv](https://arxiv.org/abs/2502.02542)] [[cool](https://papers.cool/arxiv/2502.02542)] [[pdf](https://arxiv.org/pdf/2502.02542)]
> **Authors**: Abhinav Kumar,Jaechul Roh,Ali Naseh,Marzena Karpinska,Mohit Iyyer,Amir Houmansadr,Eugene Bagdasarian
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., "overthink", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 18x slowdown on FreshQA dataset and 46x slowdown on SQuAD dataset. The attack also shows high transferability across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third-party applications operating reasoning models.

### Flow Q-Learning 
[[arxiv](https://arxiv.org/abs/2502.02538)] [[cool](https://papers.cool/arxiv/2502.02538)] [[pdf](https://arxiv.org/pdf/2502.02538)]
> **Authors**: Seohong Park,Qiyang Li,Sergey Levine
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/

### Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies 
[[arxiv](https://arxiv.org/abs/2502.02533)] [[cool](https://papers.cool/arxiv/2502.02533)] [[pdf](https://arxiv.org/pdf/2502.02533)]
> **Authors**: Han Zhou,Xingchen Wan,Ruoxi Sun,Hamid Palangi,Shariq Iqbal,Ivan Vulić,Anna Korhonen,Sercan Ö. Arık
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 11 pages, 7 figures, 1 table (30 pages, 9 figures, 5 tables including references and appendices)
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,多代理系统
- **Abstract**: Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.

### Deep Linear Network Training Dynamics from Random Initialization: Data, Width, Depth, and Hyperparameter Transfer 
[[arxiv](https://arxiv.org/abs/2502.02531)] [[cool](https://papers.cool/arxiv/2502.02531)] [[pdf](https://arxiv.org/pdf/2502.02531)]
> **Authors**: Blake Bordelon,Cengiz Pehlevan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,机器学习
- **Abstract**: We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works.

### TabPFN Unleashed: A Scalable and Effective Solution to Tabular Classification Problems 
[[arxiv](https://arxiv.org/abs/2502.02527)] [[cool](https://papers.cool/arxiv/2502.02527)] [[pdf](https://arxiv.org/pdf/2502.02527)]
> **Authors**: Si-Yang Liu,Han-Jia Ye
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: TabPFN has emerged as a promising in-context learning model for tabular data, capable of directly predicting the labels of test samples given labeled training examples. It has demonstrated competitive performance, particularly on small-scale classification tasks. However, despite its effectiveness, TabPFN still requires further refinement in several areas, including handling high-dimensional features, aligning with downstream datasets, and scaling to larger datasets. In this paper, we revisit existing variants of TabPFN and observe that most approaches focus either on reducing bias or variance, often neglecting the need to address the other side, while also increasing inference overhead. To fill this gap, we propose Beta (Bagging and Encoder-based Fine-tuning for TabPFN Adaptation), a novel and effective method designed to minimize both bias and variance. To reduce bias, we introduce a lightweight encoder to better align downstream tasks with the pre-trained TabPFN. By increasing the number of encoders in a lightweight manner, Beta mitigate variance, thereby further improving the model's performance. Additionally, bootstrapped sampling is employed to further reduce the impact of data perturbations on the model, all while maintaining computational efficiency during inference. Our approach enhances TabPFN's ability to handle high-dimensional data and scale to larger datasets. Experimental results on over 200 benchmark classification datasets demonstrate that Beta either outperforms or matches state-of-the-art methods.

### Brief analysis of DeepSeek R1 and its implications for Generative AI 
[[arxiv](https://arxiv.org/abs/2502.02523)] [[cool](https://papers.cool/arxiv/2502.02523)] [[pdf](https://arxiv.org/pdf/2502.02523)]
> **Authors**: Sarah Mercer,Samuel Spillard,Daniel P. Martin
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In late January 2025, DeepSeek released their new reasoning model (DeepSeek R1); which was developed at a fraction of the cost yet remains competitive with OpenAI's models, despite the US's GPU export ban. This report discusses the model, and what its release means for the field of Generative AI more widely. We briefly discuss other models released from China in recent weeks, their similarities; innovative use of Mixture of Experts (MoE), Reinforcement Learning (RL) and clever engineering appear to be key factors in the capabilities of these models. This think piece has been written to a tight timescale, providing broad coverage of the topic, and serves as introductory material for those looking to understand the model's technical advancements, as well as its place in the ecosystem. Several further areas of research are identified.

### Adaptive Exploration for Multi-Reward Multi-Policy Evaluation 
[[arxiv](https://arxiv.org/abs/2502.02516)] [[cool](https://papers.cool/arxiv/2502.02516)] [[pdf](https://arxiv.org/pdf/2502.02516)]
> **Authors**: Alessio Russo,Aldo Pacchiano
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions must be evaluated simultaneously for different policies. We adopt an $(ε,δ)$-PAC perspective to achieve $ε$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of this adaptive exploration scheme.

### Generative Modeling on Lie Groups via Euclidean Generalized Score Matching 
[[arxiv](https://arxiv.org/abs/2502.02513)] [[cool](https://papers.cool/arxiv/2502.02513)] [[pdf](https://arxiv.org/pdf/2502.02513)]
> **Authors**: Marco Bertolini,Tuan Le,Djork-Arné Clevert
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 27 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We extend Euclidean score-based diffusion processes to generative modeling on Lie groups. Through the formalism of Generalized Score Matching, our approach yields a Langevin dynamics which decomposes as a direct sum of Lie algebra representations, enabling generative processes on Lie groups while operating in Euclidean space. Unlike equivariant models, which restrict the space of learnable functions by quotienting out group orbits, our method can model any target distribution on any (non-Abelian) Lie group. Standard score matching emerges as a special case of our framework when the Lie group is the translation group. We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO(3)-guided molecular conformer generation and modeling ligand-specific global SE(3) transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions. Additionally, we demonstrate the universality of our approach by deriving how it extends to flow matching.

### Learning to generate physical ocean states: Towards hybrid climate modeling 
[[arxiv](https://arxiv.org/abs/2502.02499)] [[cool](https://papers.cool/arxiv/2502.02499)] [[pdf](https://arxiv.org/pdf/2502.02499)]
> **Authors**: Etienne Meunier,David Kamm,Guillaume Gachon,Redouane Lguensat,Julie Deshayes
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Ocean General Circulation Models require extensive computational resources to reach equilibrium states, while deep learning emulators, despite offering fast predictions, lack the physical interpretability and long-term stability necessary for climate scientists to understand climate sensitivity (to greenhouse gas emissions) and mechanisms of abrupt % variability such as tipping points. We propose to take the best from both worlds by leveraging deep generative models to produce physically consistent oceanic states that can serve as initial conditions for climate projections. We assess the viability of this hybrid approach through both physical metrics and numerical experiments, and highlight the benefits of enforcing physical constraints during generation. Although we train here on ocean variables from idealized numerical simulations, we claim that this hybrid approach, combining the computational efficiency of deep learning with the physical accuracy of numerical models, can effectively reduce the computational burden of running climate models to equilibrium, and reduce uncertainties in climate projections by minimizing drifts in baseline simulations.

### Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries 
[[arxiv](https://arxiv.org/abs/2502.02496)] [[cool](https://papers.cool/arxiv/2502.02496)] [[pdf](https://arxiv.org/pdf/2502.02496)]
> **Authors**: Chris Kolb,Tobias Weber,Bernd Bischl,David Rügamer
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: accepted at ICLR 2025
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods.

### Analyzing Similarity Metrics for Data Selection for Language Model Pretraining 
[[arxiv](https://arxiv.org/abs/2502.02494)] [[cool](https://papers.cool/arxiv/2502.02494)] [[pdf](https://arxiv.org/pdf/2502.02494)]
> **Authors**: Dylan Sam,Ayan Chakrabarti,Afshin Rostamizadeh,Srikumar Ramalingam,Gui Citovsky,Sanjiv Kumar
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Similarity between training examples is used to curate pretraining datasets for language models by many methods -- for diversification and to select examples similar to high-quality data. However, similarity is typically measured with off-the-shelf embedding models that are generic or trained for tasks such as retrieval. This paper introduces a framework to analyze the suitability of embedding models specifically for data curation in the language model pretraining setting. We quantify the correlation between similarity in the embedding space to similarity in pretraining loss between different training examples, and how diversifying in the embedding space affects pretraining quality. We analyze a variety of embedding models in our framework, with experiments using the Pile dataset for pretraining a 1.7B parameter decoder-only language model. We find that the embedding models we consider are all useful for pretraining data curation. Moreover, a simple approach of averaging per-token embeddings proves to be surprisingly competitive with more sophisticated embedding models -- likely because the latter are not designed specifically for pretraining data curation. Indeed, we believe our analysis and evaluation framework can serve as a foundation for the design of embedding models that specifically reason about similarity in pretraining datasets.

### EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization 
[[arxiv](https://arxiv.org/abs/2502.02493)] [[cool](https://papers.cool/arxiv/2502.02493)] [[pdf](https://arxiv.org/pdf/2502.02493)]
> **Authors**: Yize Wu,Ke Gao,Yanjun Wu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: :I.2.11
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. To solve this problem, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks the sequential execution order of layers in the drafting model, enabling multi-layer parallelization across devices, albeit with some induced approximation errors. After each drafting-and-verification iteration, the draft model's key-value (KV) cache is calibrated in a single forward pass, preventing long-term error accumulation at minimal additional latency. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distribution of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum accuracy drop of only 7%, requiring no training or fine-tuning on the draft models.

### Do Graph Diffusion Models Accurately Capture and Generate Substructure Distributions? 
[[arxiv](https://arxiv.org/abs/2502.02488)] [[cool](https://papers.cool/arxiv/2502.02488)] [[pdf](https://arxiv.org/pdf/2502.02488)]
> **Authors**: Xiyuan Wang,Yewei Liu,Lexi Pang,Siwei Chen,Muhan Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Under Review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Diffusion models have gained popularity in graph generation tasks; however, the extent of their expressivity concerning the graph distributions they can learn is not fully understood. Unlike models in other domains, popular backbones for graph diffusion models, such as Graph Transformers, do not possess universal expressivity to accurately model the distribution scores of complex graph data. Our work addresses this limitation by focusing on the frequency of specific substructures as a key characteristic of target graph distributions. When evaluating existing models using this metric, we find that they fail to maintain the distribution of substructure counts observed in the training set when generating new graphs. To address this issue, we establish a theoretical connection between the expressivity of Graph Neural Networks (GNNs) and the overall performance of graph diffusion models, demonstrating that more expressive GNN backbones can better capture complex distribution patterns. By integrating advanced GNNs into the backbone architecture, we achieve significant improvements in substructure generation.

### Distributional Diffusion Models with Scoring Rules 
[[arxiv](https://arxiv.org/abs/2502.02483)] [[cool](https://papers.cool/arxiv/2502.02483)] [[pdf](https://arxiv.org/pdf/2502.02483)]
> **Authors**: Valentin De Bortoli,Alexandre Galashov,J. Swaroop Guntupalli,Guangyao Zhou,Kevin Murphy,Arthur Gretton,Arnaud Doucet
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively "denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps.

### Stable Port-Hamiltonian Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.02480)] [[cool](https://papers.cool/arxiv/2502.02480)] [[pdf](https://arxiv.org/pdf/2502.02480)]
> **Authors**: Fabian J. Roth,Dominik K. Klein,Maximilian Kannapinn,Jan Peters,Oliver Weeger
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In recent years, nonlinear dynamic system identification using artificial neural networks has garnered attention due to its manifold potential applications in virtually all branches of science and engineering. However, purely data-driven approaches often struggle with extrapolation and may yield physically implausible forecasts. Furthermore, the learned dynamics can exhibit instabilities, making it difficult to apply such models safely and robustly. This article proposes stable port-Hamiltonian neural networks, a machine learning architecture that incorporates the physical biases of energy conservation or dissipation while guaranteeing global Lyapunov stability of the learned dynamics. Evaluations with illustrative examples and real-world measurement data demonstrate the model's ability to generalize from sparse data, outperforming purely data-driven approaches and avoiding instability issues. In addition, the model's potential for data-driven surrogate modeling is highlighted in application to multi-physics simulation data.

### Using Random Noise Equivariantly to Boost Graph Neural Networks Universally 
[[arxiv](https://arxiv.org/abs/2502.02479)] [[cool](https://papers.cool/arxiv/2502.02479)] [[pdf](https://arxiv.org/pdf/2502.02479)]
> **Authors**: Xiyuan Wang,Muhan Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in Graph Neural Networks (GNNs) have explored the potential of random noise as an input feature to enhance expressivity across diverse tasks. However, naively incorporating noise can degrade performance, while architectures tailored to exploit noise for specific tasks excel yet lack broad applicability. This paper tackles these issues by laying down a theoretical framework that elucidates the increased sample complexity when introducing random noise into GNNs without careful design. We further propose Equivariant Noise GNN (ENGNN), a novel architecture that harnesses the symmetrical properties of noise to mitigate sample complexity and bolster generalization. Our experiments demonstrate that using noise equivariantly significantly enhances performance on node-level, link-level, subgraph, and graph-level tasks and achieves comparable performance to models designed for specific tasks, thereby offering a general method to boost expressivity across various graph tasks.

### Modular Training of Neural Networks aids Interpretability 
[[arxiv](https://arxiv.org/abs/2502.02470)] [[cool](https://papers.cool/arxiv/2502.02470)] [[pdf](https://arxiv.org/pdf/2502.02470)]
> **Authors**: Satvik Golechha,Maheep Chaudhary,Joan Velja,Alessandro Abate,Nandi Schoots
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages, under review. arXiv admin note: text overlap with arXiv:2409.15747 (author note: this is an extension of that workshop paper but has different authors)
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: An approach to improve neural network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We define a measure for clusterability and show that pre-trained models form highly enmeshed clusters via spectral graph clustering. We thus train models to be more modular using a "clusterability loss" function that encourages the formation of non-interacting clusters. Using automated interpretability techniques, we show that our method can help train models that are more modular and learn different, disjoint, and smaller circuits. We investigate CNNs trained on MNIST and CIFAR, small transformers trained on modular addition, and language models. Our approach provides a promising direction for training neural networks that learn simpler functions and are easier to interpret.

### Sparse Data Generation Using Diffusion Models 
[[arxiv](https://arxiv.org/abs/2502.02448)] [[cool](https://papers.cool/arxiv/2502.02448)] [[pdf](https://arxiv.org/pdf/2502.02448)]
> **Authors**: Phil Ostheimer,Mayank Nagda,Marius Kloft,Sophie Fellenz
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sparse data is ubiquitous, appearing in numerous domains, from economics and recommender systems to astronomy and biomedical sciences. However, efficiently and realistically generating sparse data remains a significant challenge. We introduce Sparse Data Diffusion (SDD), a novel method for generating sparse data. SDD extends continuous state-space diffusion models by explicitly modeling sparsity through the introduction of Sparsity Bits. Empirical validation on image data from various domains-including two scientific applications, physics and biology-demonstrates that SDD achieves high fidelity in representing data sparsity while preserving the quality of the generated data.

### mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.02434)] [[cool](https://papers.cool/arxiv/2502.02434)] [[pdf](https://arxiv.org/pdf/2502.02434)]
> **Authors**: Mohammadmehdi Ataei,Hyunmin Cheong,Adrian Butscher
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks are increasingly employed in fields such as climate modeling, robotics, and industrial control, where strict output constraints must be upheld. Although prior methods like the POLICE algorithm can enforce affine constraints in a single convex region by adjusting network parameters, they struggle with multiple disjoint regions, often leading to conflicts or unintended affine extensions. We present mPOLICE, a new method that extends POLICE to handle constraints imposed on multiple regions. mPOLICE assigns a distinct activation pattern to each constrained region, preserving exact affine behavior locally while avoiding overreach into other parts of the input domain. We formulate a layer-wise optimization problem that adjusts both the weights and biases to assign unique activation patterns to each convex region, ensuring that constraints are met without conflicts, while maintaining the continuity and smoothness of the learned function. Our experiments show the enforcement of multi-region constraints for multiple scenarios, including regression and classification, function approximation, and non-convex regions through approximation. Notably, mPOLICE adds zero inference overhead and minimal training overhead.

### Connections between Schedule-Free Optimizers, AdEMAMix, and Accelerated SGD Variants 
[[arxiv](https://arxiv.org/abs/2502.02431)] [[cool](https://papers.cool/arxiv/2502.02431)] [[pdf](https://arxiv.org/pdf/2502.02431)]
> **Authors**: Depen Morwani,Nikhil Vyas,Hanlin Zhang,Sham Kakade
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advancements in deep learning optimization have introduced new algorithms, such as Schedule-Free optimizers, AdEMAMix, MARS and Lion which modify traditional momentum mechanisms. In a separate line of work, theoretical acceleration of stochastic gradient descent (SGD) in noise-dominated regime has been achieved by decoupling the momentum coefficient from the current gradient's weight. In this paper, we establish explicit connections between these two lines of work. We substantiate our theoretical findings with preliminary experiments on a 150m language modeling task. We find that AdEMAMix, which most closely resembles accelerated versions of stochastic gradient descent, exhibits superior performance. Building on these insights, we introduce a modification to AdEMAMix, termed Simplified-AdEMAMix, which maintains the same performance as AdEMAMix across both large and small batch-size settings while eliminating the need for two different momentum terms. The code for Simplified-AdEMAMix is available on the repository: https://github.com/DepenM/Simplified-AdEMAMix/.

### TransformDAS: Mapping Φ-OTDR Signals to Riemannian Manifold for Robust Classification 
[[arxiv](https://arxiv.org/abs/2502.02428)] [[cool](https://papers.cool/arxiv/2502.02428)] [[pdf](https://arxiv.org/pdf/2502.02428)]
> **Authors**: Jiaju Kang,Puyu Han,Yang Chun,Xu Wang,Luqi Gong
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Phase-sensitive optical time-domain reflectometry (Φ-OTDR) is a widely used distributed fiber optic sensing system in engineering. Machine learning algorithms for Φ-OTDR event classification require high volumes and quality of datasets; however, high-quality datasets are currently extremely scarce in the field, leading to a lack of robustness in models, which is manifested by higher false alarm rates in real-world scenarios. One promising approach to address this issue is to augment existing data using generative models combined with a small amount of real-world data. We explored mapping both Φ-OTDR features in a GAN-based generative pipeline and signal features in a Transformer classifier to hyperbolic space to seek more effective model generalization. The results indicate that state-of-the-art models exhibit stronger generalization performance and lower false alarm rates in real-world scenarios when trained on augmented datasets. TransformDAS, in particular, demonstrates the best classification performance, highlighting the benefits of Riemannian manifold mapping in Φ-OTDR data generation and model classification.

### CVKAN: Complex-Valued Kolmogorov-Arnold Networks 
[[arxiv](https://arxiv.org/abs/2502.02417)] [[cool](https://papers.cool/arxiv/2502.02417)] [[pdf](https://arxiv.org/pdf/2502.02417)]
> **Authors**: Matthias Wolff,Florian Eilers,Xiaoyi Jiang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In this work we propose CKAN, a complex-valued KAN, to join the intrinsic interpretability of KANs and the advantages of Complex-Valued Neural Networks (CVNNs). We show how to transfer a KAN and the necessary associated mechanisms into the complex domain. To confirm that CKAN meets expectations we conduct experiments on symbolic complex-valued function fitting and physically meaningful formulae as well as on a more realistic dataset from knot theory. Our proposed CKAN is more stable and performs on par or better than real-valued KANs while requiring less parameters and a shallower network architecture, making it more explainable.

### Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling 
[[arxiv](https://arxiv.org/abs/2502.02415)] [[cool](https://papers.cool/arxiv/2502.02415)] [[pdf](https://arxiv.org/pdf/2502.02415)]
> **Authors**: Markus Krimmel,Jenna Wiens,Karsten Borgwardt,Dexiong Chen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 32 pages, 27 tables, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph generative models often face a critical trade-off between learning complex distributions and achieving fast generation speed. We introduce Autoregressive Noisy Filtration Modeling (ANFM), a novel approach that addresses both challenges. ANFM leverages filtration, a concept from topological data analysis, to transform graphs into short sequences of monotonically increasing subgraphs. This formulation extends the sequence families used in previous autoregressive models. To learn from these sequences, we propose a novel autoregressive graph mixer model. Our experiments suggest that exposure bias might represent a substantial hurdle in autoregressive graph generation and we introduce two mitigation strategies to address it: noise augmentation and a reinforcement learning approach. Incorporating these techniques leads to substantial performance gains, making ANFM competitive with state-of-the-art diffusion models across diverse synthetic and real-world datasets. Notably, ANFM produces remarkably short sequences, achieving a 100-fold speedup in generation time compared to diffusion models. This work marks a significant step toward high-throughput graph generation.

### Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries 
[[arxiv](https://arxiv.org/abs/2502.02414)] [[cool](https://papers.cool/arxiv/2502.02414)] [[pdf](https://arxiv.org/pdf/2502.02414)]
> **Authors**: Huakun Luo,Haixu Wu,Hang Zhou,Lanxiang Xing,Yichen Di,Jianmin Wang,Mingsheng Long
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Although deep models have been widely explored in solving partial differential equations (PDEs), previous works are primarily limited to data only with up to tens of thousands of mesh points, far from the million-point scale required by industrial simulations that involve complex geometries. In the spirit of advancing neural PDE solvers to real industrial applications, we present Transolver++, a highly parallel and efficient neural solver that can accurately solve PDEs on million-scale geometries. Building upon previous advancements in solving PDEs by learning physical states via Transolver, Transolver++ is further equipped with an extremely optimized parallelism framework and a local adaptive mechanism to efficiently capture eidetic physical states from massive mesh points, successfully tackling the thorny challenges in computation and physics learning when scaling up input mesh size. Transolver++ increases the single-GPU input capacity to million-scale points for the first time and is capable of continuously scaling input size in linear complexity by increasing GPUs. Experimentally, Transolver++ yields 13% relative promotion across six standard PDE benchmarks and achieves over 20% performance gain in million-scale high-fidelity industrial simulations, whose sizes are 100$\times$ larger than previous benchmarks, covering car and 3D aircraft designs.

### Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting 
[[arxiv](https://arxiv.org/abs/2502.02410)] [[cool](https://papers.cool/arxiv/2502.02410)] [[pdf](https://arxiv.org/pdf/2502.02410)]
> **Authors**: Jan Schuchardt,Mina Dalirrooyfard,Jed Guzelkabaagac,Anderson Schneider,Yuriy Nevmyvaka,Stephan Günnemann
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全,机器学习
- **Abstract**: Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with timeseries-specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.

### Avoiding spurious sharpness minimization broadens applicability of SAM 
[[arxiv](https://arxiv.org/abs/2502.02407)] [[cool](https://papers.cool/arxiv/2502.02407)] [[pdf](https://arxiv.org/pdf/2502.02407)]
> **Authors**: Sidak Pal Singh,Hossein Mobahi,Atish Agarwala,Yann Dauphin
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,机器学习
- **Abstract**: Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance -- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics -- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional-SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional-SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs).

### Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers 
[[arxiv](https://arxiv.org/abs/2502.02393)] [[cool](https://papers.cool/arxiv/2502.02393)] [[pdf](https://arxiv.org/pdf/2502.02393)]
> **Authors**: Alireza Amiri,Xinting Huang,Mark Rofin,Michael Hahn
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算复杂度
- **Abstract**: Chain-of-thought reasoning and scratchpads have emerged as critical tools for enhancing the computational capabilities of transformers. While theoretical results show that polynomial-length scratchpads can extend transformers' expressivity from $TC^0$ to $PTIME$, their required length remains poorly understood. Empirical evidence even suggests that transformers need scratchpads even for many problems in $TC^0$, such as Parity or Multiplication, challenging optimistic bounds derived from circuit complexity. In this work, we initiate the study of systematic lower bounds for the number of CoT steps across different algorithmic problems, in the hard-attention regime. We study a variety of algorithmic problems, and provide bounds that are tight up to logarithmic factors. Overall, these results contribute to emerging understanding of the power and limitations of chain-of-thought reasoning.

### No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets 
[[arxiv](https://arxiv.org/abs/2502.02379)] [[cool](https://papers.cool/arxiv/2502.02379)] [[pdf](https://arxiv.org/pdf/2502.02379)]
> **Authors**: Corinna Coupette,Jeremy Wayland,Emily Simons,Bastian Rieck
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,社交和信息网络,机器学习
- **Abstract**: Benchmark datasets have proved pivotal to the success of graph learning, and good benchmark datasets are crucial to guide the development of the field. Recent research has highlighted problems with graph-learning datasets and benchmarking practices -- revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches on popular benchmark datasets. Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning? Our work addresses these questions. As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation. Hence, we start from first principles. Observing that graph-learning datasets uniquely combine two modes -- the graph structure and the node features -- , we introduce RINGS, a flexible and extensible mode-perturbation framework to assess the quality of graph-learning datasets based on dataset ablations -- i.e., by quantifying differences between the original dataset and its perturbed representations. Within this framework, we propose two measures -- performance separability and mode complementarity -- as evaluation tools, each assessing, from a distinct angle, the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods. We demonstrate the utility of our framework for graph-learning dataset evaluation in an extensive set of experiments and derive actionable recommendations for improving the evaluation of graph-learning methods. Our work opens new research directions in data-centric graph learning, and it constitutes a first step toward the systematic evaluation of evaluations.

### Field Matching: an Electrostatic Paradigm to Generate and Transfer Data 
[[arxiv](https://arxiv.org/abs/2502.02367)] [[cool](https://papers.cool/arxiv/2502.02367)] [[pdf](https://arxiv.org/pdf/2502.02367)]
> **Authors**: Alexander Kolesov,Manukhov Stepan,Vladimir V. Palyulin,Alexander Korotin
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: We propose Electrostatic Field Matching (EFM), a novel method that is suitable for both generative modeling and distribution transfer tasks. Our approach is inspired by the physics of an electrical capacitor. We place source and target distributions on the capacitor plates and assign them positive and negative charges, respectively. We then learn the electrostatic field of the capacitor using a neural network approximator. To map the distributions to each other, we start at one plate of the capacitor and move the samples along the learned electrostatic field lines until they reach the other plate. We theoretically justify that this approach provably yields the distribution transfer. In practice, we demonstrate the performance of our EFM in toy and image data experiments.

### Exploring the Feasibility of AI-Assisted Spine MRI Protocol Optimization Using DICOM Image Metadata 
[[arxiv](https://arxiv.org/abs/2502.02351)] [[cool](https://papers.cool/arxiv/2502.02351)] [[pdf](https://arxiv.org/pdf/2502.02351)]
> **Authors**: Alice Vian,Diego Andre Eifer,Mauricio Anes,Guilherme Ribeiro Garcia,Mariana Recamonde-Mendoza
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: :I.2; J.3
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Artificial intelligence (AI) is increasingly being utilized to optimize magnetic resonance imaging (MRI) protocols. Given that image details are critical for diagnostic accuracy, optimizing MRI acquisition protocols is essential for enhancing image quality. While medical physicists are responsible for this optimization, the variability in equipment usage and the wide range of MRI protocols in clinical settings pose significant challenges. This study aims to validate the application of AI in optimizing MRI protocols using dynamic data from clinical practice, specifically DICOM metadata. To achieve this, four MRI spine exam databases were created, with the target attribute being the binary classification of image quality (good or bad). Five AI models were trained to identify trends in acquisition parameters that influence image quality, grounded in MRI theory. These trends were analyzed using SHAP graphs. The models achieved F1 performance ranging from 77% to 93% for datasets containing 292 or more instances, with the observed trends aligning with MRI theory. The models effectively reflected the practical realities of clinical MRI settings, offering a valuable tool for medical physicists in quality control tasks. In conclusion, AI has demonstrated its potential to optimize MRI protocols, supporting medical physicists in improving image quality and enhancing the efficiency of quality control in clinical practice.

### Optimal Subspace Inference for the Laplace Approximation of Bayesian Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.02345)] [[cool](https://papers.cool/arxiv/2502.02345)] [[pdf](https://arxiv.org/pdf/2502.02345)]
> **Authors**: Josua Faller,Jörg Martin
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: for associated code, see https://github.com/josh3142/LowRankLaplaceApproximation
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Subspace inference for neural networks assumes that a subspace of their parameter space suffices to produce a reliable uncertainty quantification. In this work, we mathematically derive the optimal subspace model to a Bayesian inference scenario based on the Laplace approximation. We demonstrate empirically that, in the optimal case, often a fraction of parameters less than 1% is sufficient to obtain a reliable estimate of the full Laplace approximation. Since the optimal solution is derived, we can evaluate all other subspace models against a baseline. In addition, we give an approximation of our method that is applicable to larger problem settings, in which the optimal solution is not computable, and compare it to existing subspace models from the literature. In general, our approximation scheme outperforms previous work. Furthermore, we present a metric to qualitatively compare different subspace models even if the exact Laplace approximation is unknown.

### DIME:Diffusion-Based Maximum Entropy Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.02316)] [[cool](https://papers.cool/arxiv/2502.02316)] [[pdf](https://arxiv.org/pdf/2502.02316)]
> **Authors**: Onur Celik,Zechu Li,Denis Blessing,Ge Li,Daniel Palanicek,Jan Peters,Georgia Chalvatzaki,Gerhard Neumann
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 8 pages main text, 18 pages all included
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges--primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). DIME leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.

### VaiBot: Shuttle Between the Instructions and Parameters of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02315)] [[cool](https://papers.cool/arxiv/2502.02315)] [[pdf](https://arxiv.org/pdf/2502.02315)]
> **Authors**: Wangtao Sun,Haotian Xu,Huanxuan Liao,Xuanqing Yu,Zhongtao Jiang,Shizhu He,Jun Zhao,Kang Liu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: How to interact with LLMs through \emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. The code and data for this paper can be found at https://anonymous.4open.science/r/VaiBot-021F.

### EdgeGFL: Rethinking Edge Information in Graph Feature Preference Learning 
[[arxiv](https://arxiv.org/abs/2502.02302)] [[cool](https://papers.cool/arxiv/2502.02302)] [[pdf](https://arxiv.org/pdf/2502.02302)]
> **Authors**: Shengda Zhuo,Jiwang Fang,Hongguang Lin,Yin Tang,Min Chen,Changdong Wang,Shuqiang Huang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Graph Neural Networks (GNNs) have significant advantages in handling non-Euclidean data and have been widely applied across various areas, thus receiving increasing attention in recent years. The framework of GNN models mainly includes the information propagation phase and the aggregation phase, treating nodes and edges as information entities and propagation channels, respectively. However, most existing GNN models face the challenge of disconnection between node and edge feature information, as these models typically treat the learning of edge and node features as independent tasks. To address this limitation, we aim to develop an edge-empowered graph feature preference learning framework that can capture edge embeddings to assist node embeddings. By leveraging the learned multidimensional edge feature matrix, we construct multi-channel filters to more effectively capture accurate node features, thereby obtaining the non-local structural characteristics and fine-grained high-order node features. Specifically, the inclusion of multidimensional edge information enhances the functionality and flexibility of the GNN model, enabling it to handle complex and diverse graph data more effectively. Additionally, integrating relational representation learning into the message passing framework allows graph nodes to receive more useful information, thereby facilitating node representation learning. Finally, experiments on four real-world heterogeneous graphs demonstrate the effectiveness of theproposed model.

### Density Ratio Estimation with Conditional Probability Paths 
[[arxiv](https://arxiv.org/abs/2502.02300)] [[cool](https://papers.cool/arxiv/2502.02300)] [[pdf](https://arxiv.org/pdf/2502.02300)]
> **Authors**: Hanlin Yu,Arto Klami,Aapo Hyvärinen,Anna Korba,Omar Chehab
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Density ratio estimation in high dimensions can be reframed as integrating a certain quantity, the time score, over probability paths which interpolate between the two densities. In practice, the time score has to be estimated based on samples from the two densities. However, existing methods for this problem remain computationally expensive and can yield inaccurate estimates. Inspired by recent advances in generative modeling, we introduce a novel framework for time score estimation, based on a conditioning variable. Choosing the conditioning variable judiciously enables a closed-form objective function. We demonstrate that, compared to previous approaches, our approach results in faster learning of the time score and competitive or better estimation accuracies of the density ratio on challenging tasks. Furthermore, we establish theoretical guarantees on the error of the estimated density ratio.

### FRAUD-RLA: A new reinforcement learning adversarial attack against credit card fraud detection 
[[arxiv](https://arxiv.org/abs/2502.02290)] [[cool](https://papers.cool/arxiv/2502.02290)] [[pdf](https://arxiv.org/pdf/2502.02290)]
> **Authors**: Daniele Lunghi,Yannick Molinghen,Alkis Simitsis,Tom Lenaerts,Gianluca Bontempi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Adversarial attacks pose a significant threat to data-driven systems, and researchers have spent considerable resources studying them. Despite its economic relevance, this trend largely overlooked the issue of credit card fraud detection. To address this gap, we propose a new threat model that demonstrates the limitations of existing attacks and highlights the necessity to investigate new approaches. We then design a new adversarial attack for credit card fraud detection, employing reinforcement learning to bypass classifiers. This attack, called FRAUD-RLA, is designed to maximize the attacker's reward by optimizing the exploration-exploitation tradeoff and working with significantly less required knowledge than competitors. Our experiments, conducted on three different heterogeneous datasets and against two fraud detection systems, indicate that FRAUD-RLA is effective, even considering the severe limitations imposed by our threat model.

### A Revisit of Total Correlation in Disentangled Variational Auto-Encoder with Partial Disentanglement 
[[arxiv](https://arxiv.org/abs/2502.02279)] [[cool](https://papers.cool/arxiv/2502.02279)] [[pdf](https://arxiv.org/pdf/2502.02279)]
> **Authors**: Chengrui Li,Yunmiao Wang,Yule Wang,Weihan Li,Dieter Jaeger,Anqi Wu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,神经元和认知
- **Abstract**: A fully disentangled variational auto-encoder (VAE) aims to identify disentangled latent components from observations. However, enforcing full independence between all latent components may be too strict for certain datasets. In some cases, multiple factors may be entangled together in a non-separable manner, or a single independent semantic meaning could be represented by multiple latent components within a higher-dimensional manifold. To address such scenarios with greater flexibility, we develop the Partially Disentangled VAE (PDisVAE), which generalizes the total correlation (TC) term in fully disentangled VAEs to a partial correlation (PC) term. This framework can handle group-wise independence and can naturally reduce to either the standard VAE or the fully disentangled VAE. Validation through three synthetic experiments demonstrates the correctness and practicality of PDisVAE. When applied to real-world datasets, PDisVAE discovers valuable information that is difficult to find using fully disentangled VAEs, implying its versatility and effectiveness.

### Error Distribution Smoothing:Advancing Low-Dimensional Imbalanced Regression 
[[arxiv](https://arxiv.org/abs/2502.02277)] [[cool](https://papers.cool/arxiv/2502.02277)] [[pdf](https://arxiv.org/pdf/2502.02277)]
> **Authors**: Donghe Chen,Jiaxuan Yue,Tengjie Zheng,Lanxuan Wang,Lin Cheng
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 16 pages, 12 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: In real-world regression tasks, datasets frequently exhibit imbalanced distributions, characterized by a scarcity of data in high-complexity regions and an abundance in low-complexity areas. This imbalance presents significant challenges for existing classification methods with clear class boundaries, while highlighting a scarcity of approaches specifically designed for imbalanced regression problems. To better address these issues, we introduce a novel concept of Imbalanced Regression, which takes into account both the complexity of the problem and the density of data points, extending beyond traditional definitions that focus only on data density. Furthermore, we propose Error Distribution Smoothing (EDS) as a solution to tackle imbalanced regression, effectively selecting a representative subset from the dataset to reduce redundancy while maintaining balance and representativeness. Through several experiments, EDS has shown its effectiveness, and the related code and dataset can be accessed at https://anonymous.4open.science/r/Error-Distribution-Smoothing-762F.

### A User's Guide to Sampling Strategies for Sliced Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.02275)] [[cool](https://papers.cool/arxiv/2502.02275)] [[pdf](https://arxiv.org/pdf/2502.02275)]
> **Authors**: Keanu Sisouk,Julie Delon,Julien Tierny
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,可能性
- **Abstract**: This paper serves as a user's guide to sampling strategies for sliced optimal transport. We provide reminders and additional regularity results on the Sliced Wasserstein distance. We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy. Additionally, we provide insights into their suitability for sliced optimal transport in theory. Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage.

### Exact Sequence Classification with Hardmax Transformers 
[[arxiv](https://arxiv.org/abs/2502.02270)] [[cool](https://papers.cool/arxiv/2502.02270)] [[pdf](https://arxiv.org/pdf/2502.02270)]
> **Authors**: Albert Alcalde,Giovanni Fantuzzi,Enrique Zuazua
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 14 pages, 5 figures. Funded by the European Union (Horizon Europe MSCA project ModConFlex, grant number 101073558)
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: We prove that hardmax attention transformers perfectly classify datasets of $N$ labeled sequences in $\mathbb{R}^d$, $d\geq 2$. Specifically, given $N$ sequences with an arbitrary but finite length in $\mathbb{R}^d$, we construct a transformer with $\mathcal{O}(N)$ blocks and $\mathcal{O}(Nd)$ parameters perfectly classifying this dataset. Our construction achieves the best complexity estimate to date, independent of the length of the sequences, by innovatively alternating feed-forward and self-attention layers and by capitalizing on the clustering effect inherent to the latter. Our novel constructive method also uses low-rank parameter matrices within the attention mechanism, a common practice in real-life transformer implementations. Consequently, our analysis holds twofold significance: it substantially advances the mathematical theory of transformers and it rigorously justifies their exceptional real-world performance in sequence classification tasks.

### Adviser-Actor-Critic: Eliminating Steady-State Error in Reinforcement Learning Control 
[[arxiv](https://arxiv.org/abs/2502.02265)] [[cool](https://papers.cool/arxiv/2502.02265)] [[pdf](https://arxiv.org/pdf/2502.02265)]
> **Authors**: Donghe Chen,Yubin Peng,Tengjie Zheng,Han Wang,Chaoran Qu,Lin Cheng
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 13 pages, 9 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: High-precision control tasks present substantial challenges for reinforcement learning (RL) algorithms, frequently resulting in suboptimal performance attributed to network approximation inaccuracies and inadequate sample quality.These issues are exacerbated when the task requires the agent to achieve a precise goal state, as is common in robotics and other real-world applications.We introduce Adviser-Actor-Critic (AAC), designed to address the precision control dilemma by combining the precision of feedback control theory with the adaptive learning capability of RL and featuring an Adviser that mentors the actor to refine control actions, thereby enhancing the precision of goal attainment.Finally, through benchmark tests, AAC outperformed standard RL algorithms in precision-critical, goal-conditioned tasks, demonstrating AAC's high precision, reliability, and robustness.Code are available at: https://anonymous.4open.science/r/Adviser-Actor-Critic-8AC5.

### Adversarial ML Problems Are Getting Harder to Solve and to Evaluate 
[[arxiv](https://arxiv.org/abs/2502.02260)] [[cool](https://papers.cool/arxiv/2502.02260)] [[pdf](https://arxiv.org/pdf/2502.02260)]
> **Authors**: Javier Rando,Jie Zhang,Nicholas Carlini,Florian Tramèr
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: In the past decade, considerable research effort has been devoted to securing machine learning (ML) models that operate in adversarial settings. Yet, progress has been slow even for simple "toy" problems (e.g., robustness to small adversarial perturbations) and is often hindered by non-rigorous evaluations. Today, adversarial ML research has shifted towards studying larger, general-purpose language models. In this position paper, we argue that the situation is now even worse: in the era of LLMs, the field of adversarial ML studies problems that are (1) less clearly defined, (2) harder to solve, and (3) even more challenging to evaluate. As a result, we caution that yet another decade of work on adversarial ML may fail to produce meaningful progress.

### Bias Detection via Maximum Subgroup Discrepancy 
[[arxiv](https://arxiv.org/abs/2502.02221)] [[cool](https://papers.cool/arxiv/2502.02221)] [[pdf](https://arxiv.org/pdf/2502.02221)]
> **Authors**: Jiří Němeček,Mark Kozdoba,Illia Kryvoviaz,Tomáš Pevný,Jakub Mareček
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Bias evaluation is fundamental to trustworthy AI, both in terms of checking data quality and in terms of checking the outputs of AI systems. In testing data quality, for example, one may study a distance of a given dataset, viewed as a distribution, to a given ground-truth reference dataset. However, classical metrics, such as the Total Variation and the Wasserstein distances, are known to have high sample complexities and, therefore, may fail to provide meaningful distinction in many practical scenarios. In this paper, we propose a new notion of distance, the Maximum Subgroup Discrepancy (MSD). In this metric, two distributions are close if, roughly, discrepancies are low for all feature subgroups. While the number of subgroups may be exponential, we show that the sample complexity is linear in the number of features, thus making it feasible for practical applications. Moreover, we provide a practical algorithm for the evaluation of the distance, based on Mixed-integer optimization (MIO). We also note that the proposed distance is easily interpretable, thus providing clearer paths to fixing the biases once they have been identified. It also provides guarantees for all subgroups. Finally, we empirically evaluate, compare with other metrics, and demonstrate the above properties of MSD on real-world datasets.

### Flatten Graphs as Sequences: Transformers are Scalable Graph Generators 
[[arxiv](https://arxiv.org/abs/2502.02216)] [[cool](https://papers.cool/arxiv/2502.02216)] [[pdf](https://arxiv.org/pdf/2502.02216)]
> **Authors**: Dexiong Chen,Markus Krimmel,Karsten Borgwardt
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We introduce AutoGraph, a novel autoregressive framework for generating large attributed graphs using decoder-only transformers. At the core of our approach is a reversible "flattening" process that transforms graphs into random sequences. By sampling and learning from these sequences, AutoGraph enables transformers to model and generate complex graph structures in a manner akin to natural language. In contrast to diffusion models that rely on computationally intensive node features, our approach operates exclusively on these sequences. The sampling complexity and sequence length scale linearly with the number of edges, making AutoGraph highly scalable for generating large sparse graphs. Empirically, AutoGraph achieves state-of-the-art performance across diverse synthetic and molecular graph generation benchmarks, while delivering a 100-fold generation and a 3-fold training speedup compared to leading diffusion models. Additionally, it demonstrates promising transfer capabilities and supports substructure-conditioned generation without additional fine-tuning. By extending language modeling techniques to graph generation, this work paves the way for developing graph foundation models.

### On the Expressivity of Selective State-Space Layers: A Multivariate Polynomial Approach 
[[arxiv](https://arxiv.org/abs/2502.02209)] [[cool](https://papers.cool/arxiv/2502.02209)] [[pdf](https://arxiv.org/pdf/2502.02209)]
> **Authors**: Edo Cohen-Karlik,Itamar Zimerman,Liane Galanti,Ido Atad,Amir Globerson,Lior Wolf
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: :14J60ACM Class:F.2.2; I.2.7
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Recent advances in efficient sequence modeling have introduced selective state-space layers, a key component of the Mamba architecture, which have demonstrated remarkable success in a wide range of NLP and vision tasks. While Mamba's empirical performance has matched or surpassed SoTA transformers on such diverse benchmarks, the theoretical foundations underlying its powerful representational capabilities remain less explored. In this work, we investigate the expressivity of selective state-space layers using multivariate polynomials, and prove that they surpass linear transformers in expressiveness. Consequently, our findings reveal that Mamba offers superior representational power over linear attention-based models for long sequences, while not sacrificing their generalization. Our theoretical insights are validated by a comprehensive set of empirical experiments on various datasets.

### From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for Safe PDE Control 
[[arxiv](https://arxiv.org/abs/2502.02205)] [[cool](https://papers.cool/arxiv/2502.02205)] [[pdf](https://arxiv.org/pdf/2502.02205)]
> **Authors**: Peiyan Hu,Xiaowei Qian,Wenhao Deng,Rui Wang,Haodong Feng,Ruiqi Feng,Tao Zhang,Long Wei,Yue Wang,Zhi-Ming Ma,Tailin Wu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The application of deep learning for partial differential equation (PDE)-constrained control is gaining increasing attention. However, existing methods rarely consider safety requirements crucial in real-world applications. To address this limitation, we propose Safe Diffusion Models for PDE Control (SafeDiffCon), which introduce the uncertainty quantile as model uncertainty quantification to achieve optimal control under safety constraints through both post-training and inference phases. Firstly, our approach post-trains a pre-trained diffusion model to generate control sequences that better satisfy safety constraints while achieving improved control objectives via a reweighted diffusion loss, which incorporates the uncertainty quantile estimated using conformal prediction. Secondly, during inference, the diffusion model dynamically adjusts both its generation process and parameters through iterative guidance and fine-tuning, conditioned on control targets while simultaneously integrating the estimated uncertainty quantile. We evaluate SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible fluid, and controlled nuclear fusion problem. Results demonstrate that SafeDiffCon is the only method that satisfies all safety constraints, whereas other classical and deep learning baselines fail. Furthermore, while adhering to safety constraints, SafeDiffCon achieves the best control performance.

### Multi-level Supervised Contrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.02202)] [[cool](https://papers.cool/arxiv/2502.02202)] [[pdf](https://arxiv.org/pdf/2502.02202)]
> **Authors**: Naghmeh Ghanooni,Barbod Pajoum,Harshit Rawal,Sophie Fellenz,Vo Nguyen Le Duy,Marius Kloft
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Contrastive learning is a well-established paradigm in representation learning. The standard framework of contrastive learning minimizes the distance between "similar" instances and maximizes the distance between dissimilar ones in the projection space, disregarding the various aspects of similarity that can exist between two samples. Current methods rely on a single projection head, which fails to capture the full complexity of different aspects of a sample, leading to suboptimal performance, especially in scenarios with limited training data. In this paper, we present a novel supervised contrastive learning method in a unified framework called multilevel contrastive learning (MLCL), that can be applied to both multi-label and hierarchical classification tasks. The key strength of the proposed method is the ability to capture similarities between samples across different labels and/or hierarchies using multiple projection heads. Extensive experiments on text and image datasets demonstrate that the proposed approach outperforms state-of-the-art contrastive learning methods

### An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks 
[[arxiv](https://arxiv.org/abs/2502.02197)] [[cool](https://papers.cool/arxiv/2502.02197)] [[pdf](https://arxiv.org/pdf/2502.02197)]
> **Authors**: Linus Aronsson,Morteza Haghir Chehreghani
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,社交和信息网络
- **Abstract**: Signed networks, where edges are labeled as positive or negative to indicate friendly or antagonistic interactions, offer a natural framework for studying polarization, trust, and conflict in social systems. Detecting meaningful group structures in these networks is crucial for understanding online discourse, political division, and trust dynamics. A key challenge is to identify groups that are cohesive internally yet antagonistic externally, while allowing for neutral or unaligned vertices. In this paper, we address this problem by identifying $k$ polarized communities that are large, dense, and balanced in size. We develop an approach based on Frank-Wolfe optimization, leading to a local search procedure with provable convergence guarantees. Our method is both scalable and efficient, outperforming state-of-the-art baselines in solution quality while remaining competitive in terms of computational efficiency.

### deCIFer: Crystal Structure Prediction from Powder Diffraction Data using Autoregressive Language Models 
[[arxiv](https://arxiv.org/abs/2502.02189)] [[cool](https://papers.cool/arxiv/2502.02189)] [[pdf](https://arxiv.org/pdf/2502.02189)]
> **Authors**: Frederik Lizak Johansen,Ulrik Friis-Jensen,Erik Bjørnager Dam,Kirsten Marie Ørnsbjerg Jensen,Rocío Mercado,Raghavendra Selvan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 24 pages, 17 figures, 6 tables. v2: Figure 8 revision
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Novel materials drive progress across applications from energy storage to electronics. Automated characterization of material structures with machine learning methods offers a promising strategy for accelerating this key step in material design. In this work, we introduce an autoregressive language model that performs crystal structure prediction (CSP) from powder diffraction data. The presented model, deCIFer, generates crystal structures in the widely used Crystallographic Information File (CIF) format and can be conditioned on powder X-ray diffraction (PXRD) data. Unlike earlier works that primarily rely on high-level descriptors like composition, deCIFer performs CSP from diffraction data. We train deCIFer on nearly 2.3M unique crystal structures and validate on diverse sets of PXRD patterns for characterizing challenging inorganic crystal systems. Qualitative and quantitative assessments using the residual weighted profile and Wasserstein distance show that deCIFer produces structures that more accurately match the target diffraction data when conditioned, compared to the unconditioned case. Notably, deCIFer can achieve a 94% match rate on unseen data. deCIFer bridges experimental diffraction data with computational CSP, lending itself as a powerful tool for crystal structure characterization and accelerating materials discovery.

### Generative Kernel Spectral Clustering 
[[arxiv](https://arxiv.org/abs/2502.02185)] [[cool](https://papers.cool/arxiv/2502.02185)] [[pdf](https://arxiv.org/pdf/2502.02185)]
> **Authors**: David Winant,Sonny Achten,Johan A. K. Suykens
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted for publication at ESANN 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern clustering approaches often trade interpretability for performance, particularly in deep learning-based methods. We present Generative Kernel Spectral Clustering (GenKSC), a novel model combining kernel spectral clustering with generative modeling to produce both well-defined clusters and interpretable representations. By augmenting weighted variance maximization with reconstruction and clustering losses, our model creates an explorable latent space where cluster characteristics can be visualized through traversals along cluster directions. Results on MNIST and FashionMNIST datasets demonstrate the model's ability to learn meaningful cluster representations.

### How Memory in Optimization Algorithms Implicitly Modifies the Loss 
[[arxiv](https://arxiv.org/abs/2502.02132)] [[cool](https://papers.cool/arxiv/2502.02132)] [[pdf](https://arxiv.org/pdf/2502.02132)]
> **Authors**: Matias D. Cattaneo,Boris Shigida
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,优化与控制,机器学习
- **Abstract**: In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as memory, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented.

### Deep Neural Cellular Potts Models 
[[arxiv](https://arxiv.org/abs/2502.02129)] [[cool](https://papers.cool/arxiv/2502.02129)] [[pdf](https://arxiv.org/pdf/2502.02129)]
> **Authors**: Koen Minartz,Tim d'Hondt,Leon Hillmann,Jörn Starruß,Lutz Brusch,Vlado Menkovski
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: The cellular Potts model (CPM) is a powerful computational method for simulating collective spatiotemporal dynamics of biological cells. To drive the dynamics, CPMs rely on physics-inspired Hamiltonians. However, as first principles remain elusive in biology, these Hamiltonians only approximate the full complexity of real multicellular systems. To address this limitation, we propose NeuralCPM, a more expressive cellular Potts model that can be trained directly on observational data. At the core of NeuralCPM lies the Neural Hamiltonian, a neural network architecture that respects universal symmetries in collective cellular dynamics. Moreover, this approach enables seamless integration of domain knowledge by combining known biological mechanisms and the expressive Neural Hamiltonian into a hybrid model. Our evaluation with synthetic and real-world multicellular systems demonstrates that NeuralCPM is able to model cellular dynamics that cannot be accounted for by traditional analytical Hamiltonians.

### BILBO: BILevel Bayesian Optimization 
[[arxiv](https://arxiv.org/abs/2502.02121)] [[cool](https://papers.cool/arxiv/2502.02121)] [[pdf](https://arxiv.org/pdf/2502.02121)]
> **Authors**: Ruth Wan Theng Chew,Quoc Phong Nguyen,Bryan Kian Hsiang Low
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especially in noisy, constrained, and derivative-free settings, as repeating lower-level optimizations is sample inefficient and predicted lower-level solutions may be suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian optimization algorithm for general bilevel problems with blackbox functions, which optimizes both upper- and lower-level problems simultaneously, without the repeated lower-level optimization required by existing methods. BILBO samples from confidence-bounds based trusted sets, which bounds the suboptimality on the lower level. Moreover, BILBO selects only one function query per iteration, where the function query selection strategy incorporates the uncertainty of estimated lower-level solutions and includes a conditional reassignment of the query to encourage exploration of the lower-level objective. The performance of BILBO is theoretically guaranteed with a sublinear regret bound for commonly used kernels and is empirically evaluated on several synthetic and real-world problems.

### BRIDLE: Generalized Self-supervised Learning with Quantization 
[[arxiv](https://arxiv.org/abs/2502.02118)] [[cool](https://papers.cool/arxiv/2502.02118)] [[pdf](https://arxiv.org/pdf/2502.02118)]
> **Authors**: Hoang M. Nguyen,Satya N. Shukla,Qiang Zhang,Hanchao Yu,Sreya D. Roy,Taipeng Tian,Lingjiong Zhu,Yuchen Liu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets. Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ). However, these frameworks face challenges, notably their dependence on a single codebook for quantization, which may not capture the complex, multifaceted nature of signals. In addition, inefficiencies in codebook utilization lead to underutilized code vectors. To address these limitations, we introduce BRIDLE (Bidirectional Residual Quantization Interleaved Discrete Learning Encoder), a self-supervised encoder pretraining framework that incorporates residual quantization (RQ) into the bidirectional training process, and is generalized for pretraining with audio, image, and video. Using multiple hierarchical codebooks, RQ enables fine-grained discretization in the latent space, enhancing representation quality. BRIDLE involves an interleaved training procedure between the encoder and tokenizer. We evaluate BRIDLE on audio understanding tasks using classification benchmarks, achieving state-of-the-art results, and demonstrate competitive performance on image classification and video classification tasks, showing consistent improvements over traditional VQ methods in downstream performance.

### Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care 
[[arxiv](https://arxiv.org/abs/2502.02109)] [[cool](https://papers.cool/arxiv/2502.02109)] [[pdf](https://arxiv.org/pdf/2502.02109)]
> **Authors**: Yuxiao Cheng,Xinxin Song,Ziqian Wang,Qin Zhong,Kunlun He,Jinli Suo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Recent advances in deep learning (DL) have prompted the development of high-performing early warning score (EWS) systems, predicting clinical deteriorations such as acute kidney injury, acute myocardial infarction, or circulatory failure. DL models have proven to be powerful tools for various tasks but come with the cost of lacking interpretability and limited generalizability, hindering their clinical applications. To develop a practical EWS system applicable to various outcomes, we propose causally-informed explainable early prediction model, which leverages causal discovery to identify the underlying causal relationships of prediction and thus owns two unique advantages: demonstrating the explicit interpretation of the prediction while exhibiting decent performance when applied to unfamiliar environments. Benefiting from these features, our approach achieves superior accuracy for 6 different critical deteriorations and achieves better generalizability across different patient groups, compared to various baseline algorithms. Besides, we provide explicit causal pathways to serve as references for assistant clinical diagnosis and potential interventions. The proposed approach enhances the practical application of deep learning in various medical scenarios.

### Concept-Aware Latent and Explicit Knowledge Integration for Enhanced Cognitive Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.02104)] [[cool](https://papers.cool/arxiv/2502.02104)] [[pdf](https://arxiv.org/pdf/2502.02104)]
> **Authors**: Yawen Chen,Jiande Sun,Jing Li,Huaxiang Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Cognitive diagnosis can infer the students' mastery of specific knowledge concepts based on historical response logs. However, the existing cognitive diagnostic models (CDMs) represent students' proficiency via a unidimensional perspective, which can't assess the students' mastery on each knowledge concept comprehensively. Moreover, the Q-matrix binarizes the relationship between exercises and knowledge concepts, and it can't represent the latent relationship between exercises and knowledge concepts. Especially, when the granularity of knowledge attributes refines increasingly, the Q-matrix becomes incomplete correspondingly and the sparse binary representation (0/1) fails to capture the intricate relationships among knowledge concepts. To address these issues, we propose a Concept-aware Latent and Explicit Knowledge Integration model for cognitive diagnosis (CLEKI-CD). Specifically, a multidimensional vector is constructed according to the students' mastery and exercise difficulty for each knowledge concept from multiple perspectives, which enhances the representation capabilities of the model. Moreover, a latent Q-matrix is generated by our proposed attention-based knowledge aggregation method, and it can uncover the coverage degree of exercises over latent knowledge. The latent Q-matrix can supplement the sparse explicit Q-matrix with the inherent relationships among knowledge concepts, and mitigate the knowledge coverage problem. Furthermore, we employ a combined cognitive diagnosis layer to integrate both latent and explicit knowledge, further enhancing cognitive diagnosis performance. Extensive experiments on real-world datasets demonstrate that CLEKI-CD outperforms the state-of-the-art models. The proposed CLEKI-CD is promising in practical applications in the field of intelligent education, as it exhibits good interpretability with diagnostic results.

### Neural Networks Learn Distance Metrics 
[[arxiv](https://arxiv.org/abs/2502.02103)] [[cool](https://papers.cool/arxiv/2502.02103)] [[pdf](https://arxiv.org/pdf/2502.02103)]
> **Authors**: Alan Oursland
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 14 pages, 1 figures. Code and additional resources available at https://github.com/alanoursland/neural_networks_learn_distance_metrics
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Neural networks may naturally favor distance-based representations, where smaller activations indicate closer proximity to learned prototypes. This contrasts with intensity-based approaches, which rely on activation magnitudes. To test this hypothesis, we conducted experiments with six MNIST architectural variants constrained to learn either distance or intensity representations. Our results reveal that the underlying representation affects model performance. We develop a novel geometric framework that explains these findings and introduce OffsetL2, a new architecture based on Mahalanobis distance equations, to further validate this framework. This work highlights the importance of considering distance-based learning in neural network design.

### Online Clustering of Dueling Bandits 
[[arxiv](https://arxiv.org/abs/2502.02079)] [[cool](https://papers.cool/arxiv/2502.02079)] [[pdf](https://arxiv.org/pdf/2502.02079)]
> **Authors**: Zhiyong Wang,Jiahang Sun,Mingze Kong,Jize Xie,Qinghua Hu,John C. S. Lui,Zhongxiang Dai
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Preprint
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The contextual multi-armed bandit (MAB) is a widely used framework for problems requiring sequential decision-making under uncertainty, such as recommendation systems. In applications involving a large number of users, the performance of contextual MAB can be significantly improved by facilitating collaboration among multiple users. This has been achieved by the clustering of bandits (CB) methods, which adaptively group the users into different clusters and achieve collaboration by allowing the users in the same cluster to share data. However, classical CB algorithms typically rely on numerical reward feedback, which may not be practical in certain real-world applications. For instance, in recommendation systems, it is more realistic and reliable to solicit preference feedback between pairs of recommended items rather than absolute rewards. To address this limitation, we introduce the first "clustering of dueling bandit algorithms" to enable collaborative decision-making based on preference feedback. We propose two novel algorithms: (1) Clustering of Linear Dueling Bandits (COLDB) which models the user reward functions as linear functions of the context vectors, and (2) Clustering of Neural Dueling Bandits (CONDB) which uses a neural network to model complex, non-linear user reward functions. Both algorithms are supported by rigorous theoretical analyses, demonstrating that user collaboration leads to improved regret bounds. Extensive empirical evaluations on synthetic and real-world datasets further validate the effectiveness of our methods, establishing their potential in real-world applications involving multiple users with preference-based feedback.

### Position Paper: Building Trust in Synthetic Data for Clinical AI 
[[arxiv](https://arxiv.org/abs/2502.02076)] [[cool](https://papers.cool/arxiv/2502.02076)] [[pdf](https://arxiv.org/pdf/2502.02076)]
> **Authors**: Krishan Agyakari Raja Babu,Supriti Mulay,Om Prabhu,Mohanasankar Sivaprakasam
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 7 pages, 8 figures (including sub-figures)
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Deep generative models and synthetic medical data have shown significant promise in addressing key challenges in healthcare, such as privacy concerns, data bias, and the scarcity of realistic datasets. While research in this area has grown rapidly and demonstrated substantial theoretical potential, its practical adoption in clinical settings remains limited. Despite the benefits synthetic data offers, questions surrounding its reliability and credibility persist, leading to a lack of trust among clinicians. This position paper argues that fostering trust in synthetic medical data is crucial for its clinical adoption. It aims to spark a discussion on the viability of synthetic medical data in clinical practice, particularly in the context of current advancements in AI. We present empirical evidence from brain tumor segmentation to demonstrate that the quality, diversity, and proportion of synthetic data directly impact trust in clinical AI models. Our findings provide insights to improve the deployment and acceptance of synthetic data-driven AI systems in real-world clinical workflows.

### Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning 
[[arxiv](https://arxiv.org/abs/2502.02048)] [[cool](https://papers.cool/arxiv/2502.02048)] [[pdf](https://arxiv.org/pdf/2502.02048)]
> **Authors**: Georgios Margaritis,Periklis Petridis,Dimitris J. Bertsimas
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,计算机视觉和模式识别
- **Abstract**: Recent advancements in machine learning (ML), natural language processing (NLP), and foundational models have shown promise for real-life applications in critical, albeit compute-constrainted fields like healthcare. In such areas, combining foundational models with supervised ML offers potential for automating tasks like diagnosis and treatment planning, but the limited availability of onsite computational resources pose significant challenges before applying these technologies effectively: Current approaches either yield subpar results when using pretrained models without task-specific adaptation, or require substantial computational resources for fine-tuning, which is often a barrier to entry in such environments. This renders them inaccessible in applications where performance and quality standards are high, but computational resources are scarce. To bridge the gap between best-in-class performance and accessibility, we propose a novel method for adapting foundational, multimodal embeddings to downstream tasks, without the need of expensive fine-tuning processes. Our method leverages frozen embeddings from Large Language Models (LLMs) and Vision Models, and uses contrastive learning to train a small, task-specific nonlinear projection that can be used in the downstream task, without having to fine-tune the original foundational models. We show that this efficient procedure leads to significant performance improvements across various downstream tasks, and perhaps more importantly with minimal computational overhead, offering a practical solution for the use of advanced, foundational ML models in resource-constrained settings.

### ContinuouSP: Generative Model for Crystal Structure Prediction with Invariance and Continuity 
[[arxiv](https://arxiv.org/abs/2502.02026)] [[cool](https://papers.cool/arxiv/2502.02026)] [[pdf](https://arxiv.org/pdf/2502.02026)]
> **Authors**: Yuji Tone,Masatoshi Hanai,Mitsuaki Kawamura,Kenjiro Taura,Toyotaro Suzumura
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at the 4th Annual AAAI Workshop onAIto Accelerate Science and Engineering (AI2ASE)
- **标题**: None
- **领域**: 机器学习,材料科学
- **Abstract**: The discovery of new materials using crystal structure prediction (CSP) based on generative machine learning models has become a significant research topic in recent years. In this paper, we study invariance and continuity in the generative machine learning for CSP. We propose a new model, called ContinuouSP, which effectively handles symmetry and periodicity in crystals. We clearly formulate the invariance and the continuity, and construct a model based on the energy-based model. Our preliminary evaluation demonstrates the effectiveness of this model with the CSP task.

### Causal bandits with backdoor adjustment on unknown Gaussian DAGs 
[[arxiv](https://arxiv.org/abs/2502.02020)] [[cool](https://papers.cool/arxiv/2502.02020)] [[pdf](https://arxiv.org/pdf/2502.02020)]
> **Authors**: Yijia Zhao,Qing Zhou
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,方法论
- **Abstract**: The causal bandit problem aims to sequentially learn the intervention that maximizes the expectation of a reward variable within a system governed by a causal graph. Most existing approaches assume prior knowledge of the graph structure, or impose unrealistically restrictive conditions on the graph. In this paper, we assume a Gaussian linear directed acyclic graph (DAG) over arms and the reward variable, and study the causal bandit problem when the graph structure is unknown. We identify backdoor adjustment sets for each arm using sequentially generated experimental and observational data during the decision process, which allows us to estimate causal effects and construct upper confidence bounds. By integrating estimates from both data sources, we develop a novel bandit algorithm, based on modified upper confidence bounds, to sequentially determine the optimal intervention. We establish both case-dependent and case-independent upper bounds on the cumulative regret for our algorithm, which improve upon the bounds of the standard multi-armed bandit algorithms. Our empirical study demonstrates its advantage over existing methods with respect to cumulative regret and computation time.

### A Periodic Bayesian Flow for Material Generation 
[[arxiv](https://arxiv.org/abs/2502.02016)] [[cool](https://papers.cool/arxiv/2502.02016)] [[pdf](https://arxiv.org/pdf/2502.02016)]
> **Authors**: Hanlin Wu,Yuxuan Song,Jingjing Gong,Ziyao Cao,Yawen Ouyang,Jianbing Zhang,Hao Zhou,Wei-Ying Ma,Jingjing Liu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted to ICLR25
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song et al., 2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~100x speedup 10 v.s. 2000 steps network forwards) compared with previous diffusion-based methods on MP-20 dataset. Code is available at https://github.com/wu-han-lin/CrysBFN.

### Analytical Lyapunov Function Discovery: An RL-based Generative Approach 
[[arxiv](https://arxiv.org/abs/2502.02014)] [[cool](https://papers.cool/arxiv/2502.02014)] [[pdf](https://arxiv.org/pdf/2502.02014)]
> **Authors**: Haohan Zou,Jie Feng,Hao Zhao,Yuanyuan Shi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 26 pages (8+18), preprint for discussion. Haohan and Jie contribute equally
- **标题**: None
- **领域**: 机器学习,人工智能,符号计算,系统与控制
- **Abstract**: Despite advances in learning-based methods, finding valid Lyapunov functions for nonlinear dynamical systems remains challenging. Current neural network approaches face two main issues: challenges in scalable verification and limited interpretability. To address these, we propose an end-to-end framework using transformers to construct analytical Lyapunov functions (local), which simplifies formal verification, enhances interpretability, and provides valuable insights for control engineers. Our framework consists of a transformer-based trainer that generates candidate Lyapunov functions and a falsifier that verifies candidate expressions and refines the model via risk-seeking policy gradient. Unlike Alfarano et al. (2024), which utilizes pre-training and seeks global Lyapunov functions for low-dimensional systems, our model is trained from scratch via reinforcement learning (RL) and succeeds in finding local Lyapunov functions for high-dimensional and non-polynomial systems. Given the analytical nature of the candidates, we employ efficient optimization methods for falsification during training and formal verification tools for the final verification. We demonstrate the efficiency of our approach on a range of nonlinear dynamical systems with up to ten dimensions and show that it can discover Lyapunov functions not previously identified in the control literature.

### Layer by Layer: Uncovering Hidden Representations in Language Models 
[[arxiv](https://arxiv.org/abs/2502.02013)] [[cool](https://papers.cool/arxiv/2502.02013)] [[pdf](https://arxiv.org/pdf/2502.02013)]
> **Authors**: Oscar Skean,Md Rifat Arefin,Dan Zhao,Niket Patel,Jalal Naghiyev,Yann LeCun,Ravid Shwartz-Ziv
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems.

## 多代理系统(cs.MA:Multiagent Systems)

### Dual Ensembled Multiagent Q-Learning with Hypernet Regularizer 
[[arxiv](https://arxiv.org/abs/2502.02018)] [[cool](https://papers.cool/arxiv/2502.02018)] [[pdf](https://arxiv.org/pdf/2502.02018)]
> **Authors**: Yaodong Yang,Guangyong Chen,Hongyao Tang,Furui Liu,Danruo Deng,Pheng Ann Heng
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 15 pages, AAMAS 2025 version with appendix
- **标题**: None
- **领域**: 多代理系统,机器学习
- **Abstract**: Overestimation in single-agent reinforcement learning has been extensively studied. In contrast, overestimation in the multiagent setting has received comparatively little attention although it increases with the number of agents and leads to severe learning instability. Previous works concentrate on reducing overestimation in the estimation process of target Q-value. They ignore the follow-up optimization process of online Q-network, thus making it hard to fully address the complex multiagent overestimation problem. To solve this challenge, in this study, we first establish an iterative estimation-optimization analysis framework for multiagent value-mixing Q-learning. Our analysis reveals that multiagent overestimation not only comes from the computation of target Q-value but also accumulates in the online Q-network's optimization. Motivated by it, we propose the Dual Ensembled Multiagent Q-Learning with Hypernet Regularizer algorithm to tackle multiagent overestimation from two aspects. First, we extend the random ensemble technique into the estimation of target individual and global Q-values to derive a lower update target. Second, we propose a novel hypernet regularizer on hypernetwork weights and biases to constrain the optimization of online global Q-network to prevent overestimation accumulation. Extensive experiments in MPE and SMAC show that the proposed method successfully addresses overestimation across various tasks.

## 多媒体(cs.MM:Multimedia)

### LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.02441)] [[cool](https://papers.cool/arxiv/2502.02441)] [[pdf](https://arxiv.org/pdf/2502.02441)]
> **Authors**: Jiangong Chen,Xiaoyi Wu,Tian Lan,Bin Li
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 多媒体,人工智能
- **Abstract**: The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users' feedback also illuminates a series of directions for further optimization.

### EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via Dialogue Interpretation and Saliency Cues 
[[arxiv](https://arxiv.org/abs/2502.02172)] [[cool](https://papers.cool/arxiv/2502.02172)] [[pdf](https://arxiv.org/pdf/2502.02172)]
> **Authors**: Rohit Girmaji,Bhav Beri,Ramanathan Subramanian,Vineet Gandhi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at 30th International Conference on Intelligent User Interfaces (IUI 25)
- **标题**: None
- **领域**: 多媒体,计算机视觉和模式识别,人机交互
- **Abstract**: We present EditIQ, a completely automated framework for cinematically editing scenes captured via a stationary, large field-of-view and high-resolution camera. From the static camera feed, EditIQ initially generates multiple virtual feeds, emulating a team of cameramen. These virtual camera shots termed rushes are subsequently assembled using an automated editing algorithm, whose objective is to present the viewer with the most vivid scene content. To understand key scene elements and guide the editing process, we employ a two-pronged approach: (1) a large language model (LLM)-based dialogue understanding module to analyze conversational flow, coupled with (2) visual saliency prediction to identify meaningful scene elements and camera shots therefrom. We then formulate cinematic video editing as an energy minimization problem over shot selection, where cinematic constraints determine shot choices, transitions, and continuity. EditIQ synthesizes an aesthetically and visually compelling representation of the original narrative while maintaining cinematic coherence and a smooth viewing experience. Efficacy of EditIQ against competing baselines is demonstrated via a psychophysical study involving twenty participants on the BBC Old School dataset plus eleven theatre performance videos. Video samples from EditIQ can be found at https://editiq-ave.github.io/.

## 神经和进化计算(cs.NE:Neural and Evolutionary Computing)

### Discovering Quality-Diversity Algorithms via Meta-Black-Box Optimization 
[[arxiv](https://arxiv.org/abs/2502.02190)] [[cool](https://papers.cool/arxiv/2502.02190)] [[pdf](https://arxiv.org/pdf/2502.02190)]
> **Authors**: Maxence Faldor,Robert Tjarko Lange,Antoine Cully
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 神经和进化计算,机器学习
- **Abstract**: Quality-Diversity has emerged as a powerful family of evolutionary algorithms that generate diverse populations of high-performing solutions by implementing local competition principles inspired by biological evolution. While these algorithms successfully foster diversity and innovation, their specific mechanisms rely on heuristics, such as grid-based competition in MAP-Elites or nearest-neighbor competition in unstructured archives. In this work, we propose a fundamentally different approach: using meta-learning to automatically discover novel Quality-Diversity algorithms. By parameterizing the competition rules using attention-based neural architectures, we evolve new algorithms that capture complex relationships between individuals in the descriptor space. Our discovered algorithms demonstrate competitive or superior performance compared to established Quality-Diversity baselines while exhibiting strong generalization to higher dimensions, larger populations, and out-of-distribution domains like robot control. Notably, even when optimized solely for fitness, these algorithms naturally maintain diverse populations, suggesting meta-learning rediscovers that diversity is fundamental to effective optimization.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### Vertical Federated Learning for Failure-Cause Identification in Disaggregated Microwave Networks 
[[arxiv](https://arxiv.org/abs/2502.02874)] [[cool](https://papers.cool/arxiv/2502.02874)] [[pdf](https://arxiv.org/pdf/2502.02874)]
> **Authors**: Fatih Temiz,Memedhe Ibrahimi,Francesco Musumeci,Claudio Passera,Massimo Tornatore
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 6 pages, 7 figure, IEEE ICC 2025
- **标题**: None
- **领域**: 网络和互联网架构,人工智能,分布式、并行和集群计算,机器学习
- **Abstract**: Machine Learning (ML) has proven to be a promising solution to provide novel scalable and efficient fault management solutions in modern 5G-and-beyond communication networks. In the context of microwave networks, ML-based solutions have received significant attention. However, current solutions can only be applied to monolithic scenarios in which a single entity (e.g., an operator) manages the entire network. As current network architectures move towards disaggregated communication platforms in which multiple operators and vendors collaborate to achieve cost-efficient and reliable network management, new ML-based approaches for fault management must tackle the challenges of sharing business-critical information due to potential conflicts of interest. In this study, we explore the application of Federated Learning in disaggregated microwave networks for failure-cause identification using a real microwave hardware failure dataset. In particular, we investigate the application of two Vertical Federated Learning (VFL), namely using Split Neural Networks (SplitNNs) and Federated Learning based on Gradient Boosting Decision Trees (FedTree), on different multi-vendor deployment scenarios, and we compare them to a centralized scenario where data is managed by a single entity. Our experimental results show that VFL-based scenarios can achieve F1-Scores consistently within at most a 1% gap with respect to a centralized scenario, regardless of the deployment strategies or model types, while also ensuring minimal leakage of sensitive-data.

### Graph Neural Networks for O-RAN Mobility Management: A Link Prediction Approach 
[[arxiv](https://arxiv.org/abs/2502.02170)] [[cool](https://papers.cool/arxiv/2502.02170)] [[pdf](https://arxiv.org/pdf/2502.02170)]
> **Authors**: Ana Gonzalez Bermudez,Miquel Farreras,Milan Groshev,José Antonio Trujillo,Isabel de la Bandera,Raquel Barco
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 7 pages, 2 figures, 2 tables. Submitted to IEEE Vehicular Technology Magazine, Special Issue on "AIfor 6G O-RAN Intelligent, Cost-Efficient and Secure Automation"
- **标题**: None
- **领域**: 网络和互联网架构,人工智能
- **Abstract**: Mobility performance has been a key focus in cellular networks up to 5G. To enhance handover (HO) performance, 3GPP introduced Conditional Handover (CHO) and Layer 1/Layer 2 Triggered Mobility (LTM) mechanisms in 5G. While these reactive HO strategies address the trade-off between HO failures (HOF) and ping-pong effects, they often result in inefficient radio resource utilization due to additional HO preparations. To overcome these challenges, this article proposes a proactive HO framework for mobility management in O-RAN, leveraging user-cell link predictions to identify the optimal target cell for HO. We explore various categories of Graph Neural Networks (GNNs) for link prediction and analyze the complexity of applying them to the mobility management domain. Two GNN models are compared using a real-world dataset, with experimental results demonstrating their ability to capture the dynamic and graph-structured nature of cellular networks. Finally, we present key insights from our study and outline future steps to enable the integration of GNN-based link prediction for mobility management in 6G networks.

## 机器人技术(cs.RO:Robotics)

### Rethinking Latent Representations in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation 
[[arxiv](https://arxiv.org/abs/2502.02853)] [[cool](https://papers.cool/arxiv/2502.02853)] [[pdf](https://arxiv.org/pdf/2502.02853)]
> **Authors**: Shuanghao Bai,Wanqi Zhou,Pengxiang Ding,Wei Zhao,Donglin Wang,Badong Chen
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 20 pages, 11 figures
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io.

### SD++: Enhancing Standard Definition Maps by Incorporating Road Knowledge using LLMs 
[[arxiv](https://arxiv.org/abs/2502.02773)] [[cool](https://papers.cool/arxiv/2502.02773)] [[pdf](https://arxiv.org/pdf/2502.02773)]
> **Authors**: Hitvarth Diwanji,Jing-Yan Liao,Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别
- **Abstract**: High-definition maps (HD maps) are detailed and informative maps capturing lane centerlines and road elements. Although very useful for autonomous driving, HD maps are costly to build and maintain. Furthermore, access to these high-quality maps is usually limited to the firms that build them. On the other hand, standard definition (SD) maps provide road centerlines with an accuracy of a few meters. In this paper, we explore the possibility of enhancing SD maps by incorporating information from road manuals using LLMs. We develop SD++, an end-to-end pipeline to enhance SD maps with location-dependent road information obtained from a road manual. We suggest and compare several ways of using LLMs for such a task. Furthermore, we show the generalization ability of SD++ by showing results from both California and Japan.

### PatchPilot: A Stable and Cost-Efficient Agentic Patching Framework 
[[arxiv](https://arxiv.org/abs/2502.02747)] [[cool](https://papers.cool/arxiv/2502.02747)] [[pdf](https://arxiv.org/pdf/2502.02747)]
> **Authors**: Hongwei Li,Yuheng Tang,Shiqi Wang,Wenbo Guo
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能,密码学和安全
- **Abstract**: Recent research builds various patching agents that combine large language models (LLMs) with non-ML tools and achieve promising results on the state-of-the-art (SOTA) software patching benchmark, SWE-Bench. Based on how to determine the patching workflows, existing patching agents can be categorized as agent-based planning methods, which rely on LLMs for planning, and human-based planning methods, which follow a pre-defined workflow. At a high level, agent-based planning methods achieve high patching performance but with a high cost and limited stability. Human-based planning methods, on the other hand, are more stable and efficient but have key workflow limitations that compromise their patching performance. In this paper, we propose PatchPilot, an agentic patcher that strikes a balance between patching efficacy, stability, and cost-efficiency. PatchPilot proposes a novel human-based planning workflow with five components: reproduction, localization, generation, validation, and refinement (where refinement is unique to PatchPilot). We introduce novel and customized designs to each component to optimize their effectiveness and efficiency. Through extensive experiments on the SWE-Bench benchmarks, PatchPilot shows a superior performance than existing open-source methods while maintaining low cost (less than 1$ per instance) and ensuring higher stability. We also conduct a detailed ablation study to validate the key designs in each component.

### Rapidly Adapting Policies to the Real World via Simulation-Guided Fine-Tuning 
[[arxiv](https://arxiv.org/abs/2502.02705)] [[cool](https://papers.cool/arxiv/2502.02705)] [[pdf](https://arxiv.org/pdf/2502.02705)]
> **Authors**: Patrick Yin,Tyler Westenbroek,Simran Bagaria,Kevin Huang,Ching-an Cheng,Andrey Kobolov,Abhishek Gupta
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the Simulation-Guided Fine-tuning (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps. Project webpage: https://weirdlabuw.github.io/sgft/{weirdlabuw.github.io/sgft}

### Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges 
[[arxiv](https://arxiv.org/abs/2502.02692)] [[cool](https://papers.cool/arxiv/2502.02692)] [[pdf](https://arxiv.org/pdf/2502.02692)]
> **Authors**: Amit Ranjan Trivedi,Sina Tayebati,Hemant Kumawat,Nastaran Darabi,Divake Kumar,Adarsh Kumar Kosta,Yeshwanth Venkatesha,Dinithi Jayasuriya,Nethmi Jayasinghe,Priyadarshini Panda,Saibal Mukhopadhyay,Kaushik Roy
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Autonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and action-to-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing a very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-to-sensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spike-based, event-driven processing that conserves energy, reduces latency, and supports hierarchical control--making it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics and improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments.

### Learning to Double Guess: An Active Perception Approach for Estimating the Center of Mass of Arbitrary Objects 
[[arxiv](https://arxiv.org/abs/2502.02663)] [[cool](https://papers.cool/arxiv/2502.02663)] [[pdf](https://arxiv.org/pdf/2502.02663)]
> **Authors**: Shengmiao Jin,Yuchen Mo,Wenzhen Yuan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted to ICRA 25; 7 pages, 5 figures
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Manipulating arbitrary objects in unstructured environments is a significant challenge in robotics, primarily due to difficulties in determining an object's center of mass. This paper introduces U-GRAPH: Uncertainty-Guided Rotational Active Perception with Haptics, a novel framework to enhance the center of mass estimation using active perception. Traditional methods often rely on single interaction and are limited by the inherent inaccuracies of Force-Torque (F/T) sensors. Our approach circumvents these limitations by integrating a Bayesian Neural Network (BNN) to quantify uncertainty and guide the robotic system through multiple, information-rich interactions via grid search and a neural network that scores each action. We demonstrate the remarkable generalizability and transferability of our method with training on a small dataset with limited variation yet still perform well on unseen complex real-world objects.

### MAGNNET: Multi-Agent Graph Neural Network-based Efficient Task Allocation for Autonomous Vehicles with Deep Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.02311)] [[cool](https://papers.cool/arxiv/2502.02311)] [[pdf](https://arxiv.org/pdf/2502.02311)]
> **Authors**: Lavanya Ratnabala,Aleksey Fedoseev,Robinroy Peter,Dzmitry Tsetserukou
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Submitted to IEEE Intelligent Vehicle Symposium (2025)
- **标题**: None
- **领域**: 机器人技术,机器学习,多代理系统
- **Abstract**: This paper addresses the challenge of decentralized task allocation within heterogeneous multi-agent systems operating under communication constraints. We introduce a novel framework that integrates graph neural networks (GNNs) with a centralized training and decentralized execution (CTDE) paradigm, further enhanced by a tailored Proximal Policy Optimization (PPO) algorithm for multi-agent deep reinforcement learning (MARL). Our approach enables unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to dynamically allocate tasks efficiently without necessitating central coordination in a 3D grid environment. The framework minimizes total travel time while simultaneously avoiding conflicts in task assignments. For the cost calculation and routing, we employ reservation-based A* and R* path planners. Experimental results revealed that our method achieves a high 92.5% conflict-free success rate, with only a 7.49% performance gap compared to the centralized Hungarian method, while outperforming the heuristic decentralized baseline based on greedy approach. Additionally, the framework exhibits scalability with up to 20 agents with allocation processing of 2.8 s and robustness in responding to dynamically generated tasks, underscoring its potential for real-world applications in complex multi-agent scenarios.

### Real-Time Operator Takeover for Visuomotor Diffusion Policy Training 
[[arxiv](https://arxiv.org/abs/2502.02308)] [[cool](https://papers.cool/arxiv/2502.02308)] [[pdf](https://arxiv.org/pdf/2502.02308)]
> **Authors**: Nils Ingelhag,Jesper Munkeby,Michael C. Welle,Marco Moletta,Danica Kragic
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators to seamlessly take control of a live visuomotor diffusion policy, guiding the system back into desirable states or reinforcing specific demonstrations. We present new insights in using the Mahalonobis distance to automatically identify undesirable states. Once the operator has intervened and redirected the system, the control is seamlessly returned to the policy, which resumes generating actions until further intervention is required. We demonstrate that incorporating the targeted takeover demonstrations significantly improves policy performance compared to training solely with an equivalent number of, but longer, initial demonstrations. We provide an in-depth analysis of using the Mahalanobis distance to detect out-of-distribution states, illustrating its utility for identifying critical failure points during execution. Supporting materials, including videos of initial and takeover demonstrations and all rice scooping experiments, are available on the project website: https://operator-takeover.github.io/

### VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation 
[[arxiv](https://arxiv.org/abs/2502.02175)] [[cool](https://papers.cool/arxiv/2502.02175)] [[pdf](https://arxiv.org/pdf/2502.02175)]
> **Authors**: Siyu Xu,Yunke Wang,Chenghao Xia,Dihao Zhu,Tao Huang,Chang Xu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,计算机视觉和模式识别,机器学习
- **Abstract**: Vision-Language-Action (VLA) model can process instructions and visual perception to directly generate actions as output in an end-to-end fashion due to its strong multi-modal reasoning capabilities. While the performance of VLA models is promising, their computational cost can be substantial. This raises challenge for applying them on robotics tasks, which requires real-time decision-making to respond quickly to environmental changes. Since robotic control involves sequential decision-making, the visual input often exhibits minimal variation between successive steps. A natural idea is to reuse the computational results of unchanged visual tokens from the last step. Motivated by this idea, we propose VLA-Cache, an efficient vision-language-action model. VLA-Cache incorporates a token-selection mechanism that compares the visual input at each step with the input from the previous step, adaptively identifying visual tokens with minimal changes. The computational results for these unchanged tokens are then reused in subsequent steps via KV-cache, thereby significantly improving the efficiency of the VLA-Cache model. Experimental results on both simulation (e.g., LIBERO benchmark and SIMPLER) and real-world robot valid VLA-Cache can achieve practical acceleration with minimal sacrifice in success rate.

### AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement 
[[arxiv](https://arxiv.org/abs/2502.02067)] [[cool](https://papers.cool/arxiv/2502.02067)] [[pdf](https://arxiv.org/pdf/2502.02067)]
> **Authors**: Shivam Singh,Karthik Swaminathan,Nabanita Dash,Ramandeep Singh,Snehasis Banerjee,Mohan Sridharan,Madhava Krishna
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2025
- **标题**: None
- **领域**: 机器人技术,人工智能,计算语言学,机器学习
- **Abstract**: Embodied agents assisting humans are often asked to complete a new task in a new scenario. An agent preparing a particular dish in the kitchen based on a known recipe may be asked to prepare a new dish or to perform cleaning tasks in the storeroom. There may not be sufficient resources, e.g., time or labeled examples, to train the agent for these new situations. Large Language Models (LLMs) trained on considerable knowledge across many domains are able to predict a sequence of abstract actions for such new tasks and scenarios, although it may not be possible for the agent to execute this action sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks and scenarios. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation over cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM output.

### Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments 
[[arxiv](https://arxiv.org/abs/2502.02066)] [[cool](https://papers.cool/arxiv/2502.02066)] [[pdf](https://arxiv.org/pdf/2502.02066)]
> **Authors**: Raghav Arora,Shivam Singh,Karthik Swaminathan,Ahana Datta,Snehasis Banerjee,Brojeshwar Bhowmick,Krishna Murthy Jatavallabhula,Mohan Sridharan,Madhava Krishna
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2024
- **标题**: None
- **领域**: 机器人技术,计算语言学,机器学习
- **Abstract**: Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks.

### RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation 
[[arxiv](https://arxiv.org/abs/2502.02054)] [[cool](https://papers.cool/arxiv/2502.02054)] [[pdf](https://arxiv.org/pdf/2502.02054)]
> **Authors**: Minwoo Kim,Geunsik Bae,Jinwoo Lee,Woojae Shin,Changseung Kim,Myong-Yol Choi,Heejung Shin,Hyondong Oh
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 18 pages, 11 figures, 58 references, and appendix is included
- **标题**: None
- **领域**: 机器人技术,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve capability to deal with high-dimensional spaces while preserving the robustness of RL policies. A motion primitive-based path planning algorithm collects an expert dataset with privileged map data from diverse environments, ensuring comprehensive scenario coverage. By leveraging both the acquired expert and learner dataset gathered from the agent's interactions with the simulation environments, a robust reward function and policy are learned across diverse states. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.

### From Human Hands to Robotic Limbs: A Study in Motor Skill Embodiment for Telemanipulation 
[[arxiv](https://arxiv.org/abs/2502.02036)] [[cool](https://papers.cool/arxiv/2502.02036)] [[pdf](https://arxiv.org/pdf/2502.02036)]
> **Authors**: Haoyi Shi,Mingxi Su,Ted Morris,Vassilios Morellas,Nikolaos Papanikolopoulos
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,人工智能
- **Abstract**: This paper presents a teleoperation system for controlling a redundant degree of freedom robot manipulator using human arm gestures. We propose a GRU-based Variational Autoencoder to learn a latent representation of the manipulator's configuration space, capturing its complex joint kinematics. A fully connected neural network maps human arm configurations into this latent space, allowing the system to mimic and generate corresponding manipulator trajectories in real time through the VAE decoder. The proposed method shows promising results in teleoperating the manipulator, enabling the generation of novel manipulator configurations from human features that were not present during training.

## 声音(cs.SD:Sound)

### Streaming Speaker Change Detection and Gender Classification for Transducer-Based Multi-Talker Speech Translation 
[[arxiv](https://arxiv.org/abs/2502.02683)] [[cool](https://papers.cool/arxiv/2502.02683)] [[pdf](https://arxiv.org/pdf/2502.02683)]
> **Authors**: Peidong Wang,Naoyuki Kanda,Jian Xue,Jinyu Li,Xiaofei Wang,Aswin Shanmugam Subramanian,Junkun Chen,Sunit Sivasankaran,Xiong Xiao,Yong Zhao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,音频和语音处理
- **Abstract**: Streaming multi-talker speech translation is a task that involves not only generating accurate and fluent translations with low latency but also recognizing when a speaker change occurs and what the speaker's gender is. Speaker change information can be used to create audio prompts for a zero-shot text-to-speech system, and gender can help to select speaker profiles in a conventional text-to-speech model. We propose to tackle streaming speaker change detection and gender classification by incorporating speaker embeddings into a transducer-based streaming end-to-end speech translation model. Our experiments demonstrate that the proposed methods can achieve high accuracy for both speaker change detection and gender classification.

### Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent Autoencoders for the Compression of the Stimulation Patterns of Cochlear Implants at Zero Delay 
[[arxiv](https://arxiv.org/abs/2502.02424)] [[cool](https://papers.cool/arxiv/2502.02424)] [[pdf](https://arxiv.org/pdf/2502.02424)]
> **Authors**: Reemt Hinrichs,Jörn Ostermann
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Preprint of Asilomar 2024 Paper
- **标题**: None
- **领域**: 声音,机器学习,音频和语音处理
- **Abstract**: Cochlear implants (CIs) are surgically implanted hearing devices, which allow to restore a sense of hearing in people suffering from profound hearing loss. Wireless streaming of audio from external devices to CI signal processors has become common place. Specialized compression based on the stimulation patterns of a CI by deep recurrent autoencoders can decrease the power consumption in such a wireless streaming application through bit-rate reduction at zero latency. While previous research achieved considerable bit-rate reductions, model sizes were ignored, which can be of crucial importance in hearing-aids due to their limited computational resources. This work investigates maximizing objective speech intelligibility of the coded stimulation patterns of deep recurrent autoencoders while minimizing model size. For this purpose, a pruning-aware loss is proposed, which captures the impact of pruning during training. This training with a pruning-aware loss is compared to conventional magnitude-informed pruning and is found to yield considerable improvements in objective intelligibility, especially at higher pruning rates. After fine-tuning, little to no degradation of objective intelligibility is observed up to a pruning rate of about 55\,\%. The proposed pruning-aware loss yields substantial gains in objective speech intelligibility scores after pruning compared to the magnitude-informed baseline for pruning rates above 45\,\%.

## 软件工程(cs.SE:Software Engineering)

### A Systematic Approach for Assessing Large Language Models' Test Case Generation Capability 
[[arxiv](https://arxiv.org/abs/2502.02866)] [[cool](https://papers.cool/arxiv/2502.02866)] [[pdf](https://arxiv.org/pdf/2502.02866)]
> **Authors**: Hung-Fu Chang,Mohammad Shokrolah Shirazi
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 17 pages, 9 figures
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Software testing ensures the quality and reliability of software products, but manual test case creation is labor-intensive. With the rise of large language models (LLMs), there is growing interest in unit test creation with LLMs. However, effective assessment of LLM-generated test cases is limited by the lack of standardized benchmarks that comprehensively cover diverse programming scenarios. To address the assessment of LLM's test case generation ability and lacking dataset for evaluation, we propose the Generated Benchmark from Control-Flow Structure and Variable Usage Composition (GBCV) approach, which systematically generates programs used for evaluating LLMs' test generation capabilities. By leveraging basic control-flow structures and variable usage, GBCV provides a flexible framework to create a spectrum of programs ranging from simple to complex. Because GPT-4o and GPT-3-Turbo are publicly accessible models, to present real-world regular user's use case, we use GBCV to assess LLM performance on them. Our findings indicate that GPT-4o performs better on complex program structures, while all models effectively detect boundary values in simple conditions but face challenges with arithmetic computations. This study highlights the strengths and limitations of LLMs in test generation, provides a benchmark framework, and suggests directions for future improvement.

### An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky Test Detection and Classification 
[[arxiv](https://arxiv.org/abs/2502.02715)] [[cool](https://papers.cool/arxiv/2502.02715)] [[pdf](https://arxiv.org/pdf/2502.02715)]
> **Authors**: Riddhi More,Jeremy S. Bradbury
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 10 pages
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Flaky tests exhibit non-deterministic behavior during execution and they may pass or fail without any changes to the program under test. Detecting and classifying these flaky tests is crucial for maintaining the robustness of automated test suites and ensuring the overall reliability and confidence in the testing. However, flaky test detection and classification is challenging due to the variability in test behavior, which can depend on environmental conditions and subtle code interactions. Large Language Models (LLMs) offer promising approaches to address this challenge, with fine-tuning and few-shot learning (FSL) emerging as viable techniques. With enough data fine-tuning a pre-trained LLM can achieve high accuracy, making it suitable for organizations with more resources. Alternatively, we introduce FlakyXbert, an FSL approach that employs a Siamese network architecture to train efficiently with limited data. To understand the performance and cost differences between these two methods, we compare fine-tuning on larger datasets with FSL in scenarios restricted by smaller datasets. Our evaluation involves two existing flaky test datasets, FlakyCat and IDoFT. Our results suggest that while fine-tuning can achieve high accuracy, FSL provides a cost-effective approach with competitive accuracy, which is especially beneficial for organizations or projects with limited historical data available for training. These findings underscore the viability of both fine-tuning and FSL in flaky test detection and classification with each suited to different organizational needs and resource availability.

### Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in Real-World Projects 
[[arxiv](https://arxiv.org/abs/2502.02368)] [[cool](https://papers.cool/arxiv/2502.02368)] [[pdf](https://arxiv.org/pdf/2502.02368)]
> **Authors**: Henrique Nunes,Eduardo Figueiredo,Larissa Rocha,Sarah Nadi,Fischer Ferreira,Geanderson Esteves
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large Language Models (LLMs) have gained attention for addressing coding problems, but their effectiveness in fixing code maintainability remains unclear. This study evaluates LLMs capability to resolve 127 maintainability issues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat and Llama 3.1, and few-shot prompting with Llama only. The LLM-generated solutions are assessed for compilation errors, test failures, and new maintainability problems. Llama with few-shot prompting successfully fixed 44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and 30%, respectively. However, most solutions introduced errors or new maintainability issues. We also conducted a human study with 45 participants to evaluate the readability of 51 LLM-generated solutions. The human study showed that 68.63% of participants observed improved readability. Overall, while LLMs show potential for fixing maintainability issues, their introduction of errors highlights their current limitations.

## 社交和信息网络(cs.SI:Social and Information Networks)

### Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment 
[[arxiv](https://arxiv.org/abs/2502.02017)] [[cool](https://papers.cool/arxiv/2502.02017)] [[pdf](https://arxiv.org/pdf/2502.02017)]
> **Authors**: Shuo Wang,Bokui Wang,Zhixiang Shen,Boyan Deng,Zhao Kang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 社交和信息网络,人工智能,机器学习
- **Abstract**: Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### SEAL: Speech Embedding Alignment Learning for Speech Large Language Model with Retrieval-Augmented Generation 
[[arxiv](https://arxiv.org/abs/2502.02603)] [[cool](https://papers.cool/arxiv/2502.02603)] [[pdf](https://arxiv.org/pdf/2502.02603)]
> **Authors**: Chunyu Sun,Bingyu Liu,Zhichao Cui,Anbin Qi,Tian-hao Zhang,Dinghao Zhou,Lewei Lu
> **First submission**: 2025-01-26
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 音频和语音处理,计算语言学,声音
- **Abstract**: Embedding-based retrieval models have made significant strides in retrieval-augmented generation (RAG) techniques for text and multimodal large language models (LLMs) applications. However, when it comes to speech larage language models (SLLMs), these methods are limited to a two-stage process, where automatic speech recognition (ASR) is combined with text-based retrieval. This sequential architecture suffers from high latency and error propagation. To address these limitations, we propose a unified embedding framework that eliminates the need for intermediate text representations. Specifically, the framework includes separate speech and text encoders, followed by a shared scaling layer that maps both modalities into a common embedding space. Our model reduces pipeline latency by 50\% while achieving higher retrieval accuracy compared to traditional two-stage methods. We also provide a theoretical analysis of the challenges inherent in end-to-end speech retrieval and introduce architectural principles for effective speech-to-document matching. Extensive experiments demonstrate the robustness of our approach across diverse acoustic conditions and speaker variations, paving the way for a new paradigm in multimodal SLLMs retrieval systems.

## 图像和视频处理(eess.IV:Image and Video Processing)

### Learning Generalizable Features for Tibial Plateau Fracture Segmentation Using Masked Autoencoder and Limited Annotations 
[[arxiv](https://arxiv.org/abs/2502.02862)] [[cool](https://papers.cool/arxiv/2502.02862)] [[pdf](https://arxiv.org/pdf/2502.02862)]
> **Authors**: Peiyan Yue,Die Cai,Chu Guo,Mengxing Liu,Jun Xia,Yi Wang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 5 pages, 6 figures
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: Accurate automated segmentation of tibial plateau fractures (TPF) from computed tomography (CT) requires large amounts of annotated data to train deep learning models, but obtaining such annotations presents unique challenges. The process demands expert knowledge to identify diverse fracture patterns, assess severity, and account for individual anatomical variations, making the annotation process highly time-consuming and expensive. Although semi-supervised learning methods can utilize unlabeled data, existing approaches often struggle with the complexity and variability of fracture morphologies, as well as limited generalizability across datasets. To tackle these issues, we propose an effective training strategy based on masked autoencoder (MAE) for the accurate TPF segmentation in CT. Our method leverages MAE pretraining to capture global skeletal structures and fine-grained fracture details from unlabeled data, followed by fine-tuning with a small set of labeled data. This strategy reduces the dependence on extensive annotations while enhancing the model's ability to learn generalizable and transferable features. The proposed method is evaluated on an in-house dataset containing 180 CT scans with TPF. Experimental results demonstrate that our method consistently outperforms semi-supervised methods, achieving an average Dice similarity coefficient (DSC) of 95.81%, average symmetric surface distance (ASSD) of 1.91mm, and Hausdorff distance (95HD) of 9.42mm with only 20 annotated cases. Moreover, our method exhibits strong transferability when applying to another public pelvic CT dataset with hip fractures, highlighting its potential for broader applications in fracture segmentation tasks.

### Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images 
[[arxiv](https://arxiv.org/abs/2502.02756)] [[cool](https://papers.cool/arxiv/2502.02756)] [[pdf](https://arxiv.org/pdf/2502.02756)]
> **Authors**: Obed Korshie Dzikunu,Shadab Ahamed,Amirhossein Toosi,Xiaoxiao Li,Arman Rahmim
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 29 pages, 7 figures, 1 table
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别
- **Abstract**: This study proposes a new loss function for deep neural networks, L1-weighted Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of voxels based on their classification difficulty, towards automated detection and segmentation of metastatic prostate cancer lesions in PET/CT scans. We obtained 380 PSMA [18-F] DCFPyL PET/CT scans of patients diagnosed with biochemical recurrence metastatic prostate cancer. We trained two 3D convolutional neural networks, Attention U-Net and SegResNet, and concatenated the PET and CT volumes channel-wise as input. The performance of our custom loss function was evaluated against the Dice and Dice Focal Loss functions. For clinical significance, we considered a detected region of interest (ROI) as a true positive if at least the voxel with the maximum standardized uptake value falls within the ROI. We assessed the models' performance based on the number of lesions in an image, tumour volume, activity, and extent of spread. The L1DFL outperformed the comparative loss functions by at least 13% on the test set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal Loss yielded more false positives, whereas the Dice Loss was more sensitive to smaller volumes and struggled to segment larger lesions accurately. They also exhibited network-specific variations and yielded declines in segmentation accuracy with increased tumour spread. Our results demonstrate the potential of L1DFL to yield robust segmentation of metastatic prostate cancer lesions in PSMA PET/CT images. The results further highlight potential complexities arising from the variations in lesion characteristics that may influence automated prostate cancer tumour detection and segmentation. The code is publicly available at: https://github.com/ObedDzik/pca_segment.git.

### Muographic Image Upsampling with Machine Learning for Built Infrastructure Applications 
[[arxiv](https://arxiv.org/abs/2502.02624)] [[cool](https://papers.cool/arxiv/2502.02624)] [[pdf](https://arxiv.org/pdf/2502.02624)]
> **Authors**: William O'Donnell,David Mahon,Guangliang Yang,Simon Gardner
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: The civil engineering industry faces a critical need for innovative non-destructive evaluation methods, particularly for ageing critical infrastructure, such as bridges, where current techniques fall short. Muography, a non-invasive imaging technique, constructs three-dimensional density maps by detecting interactions of naturally occurring cosmic-ray muons within the scanned volume. Cosmic-ray muons provide deep penetration and inherent safety due to their high momenta and natural source. However, the technology's reliance on this source results in constrained muon flux, leading to prolonged acquisition times, noisy reconstructions and image interpretation challenges. To address these limitations, we developed a two-model deep learning approach. First, we employed a conditional Wasserstein generative adversarial network with gradient penalty (cWGAN-GP) to perform predictive upsampling of undersampled muography images. Using the structural similarity index measure (SSIM), 1-day sampled images matched the perceptual qualities of a 21-day image, while the peak signal-to-noise ratio (PSNR) indicated noise improvement equivalent to 31 days of sampling. A second cWGAN-GP model, trained for semantic segmentation, quantitatively assessed the upsampling model's impact on concrete sample features. This model achieved segmentation of rebar grids and tendon ducts, with Dice-Sørensen accuracy coefficients of 0.8174 and 0.8663. Notably, it could mitigate or remove z-plane smearing artifacts caused by muography's inverse imaging problem. Both models were trained on a comprehensive Geant4 Monte-Carlo simulation dataset reflecting realistic civil infrastructure scenarios. Our results demonstrate significant improvements in acquisition speed and image quality, marking a substantial step toward making muography more practical for reinforced concrete infrastructure monitoring applications.

### AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late Dynamic Contrast Enhanced Prostate MRI Synthesis 
[[arxiv](https://arxiv.org/abs/2502.02555)] [[cool](https://papers.cool/arxiv/2502.02555)] [[pdf](https://arxiv.org/pdf/2502.02555)]
> **Authors**: Divya Bharti,Sriprabha Ramanarayanan,Sadhana S,Kishore Kumar M,Keerthi Ram,Harsh Agarwal,Ramesh Venkatesan,Mohanasankar Sivaprakasam
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at ICASSP 2025
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical imaging technique that plays a crucial role in the detailed visualization and identification of tissue perfusion in abnormal lesions and radiological suggestions for biopsy. However, DCE-MRI involves the administration of a Gadolinium based (Gad) contrast agent, which is associated with a risk of toxicity in the body. Previous deep learning approaches that synthesize DCE-MR images employ unimodal non-contrast or low-dose contrast MRI images lacking focus on the local perfusion information within the anatomy of interest. We propose AAD-DCE, a generative adversarial network (GAN) with an aggregated attention discriminator module consisting of global and local discriminators. The discriminators provide a spatial embedded attention map to drive the generator to synthesize early and late response DCE-MRI images. Our method employs multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient (ADC), and T1 pre-contrast for image synthesis. Extensive comparative and ablation studies on the ProstateX dataset show that our model (i) is agnostic to various generator benchmarks and (ii) outperforms other DCE-MRI synthesis approaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, and (ii) emphasize the importance of attention ensembling. Our code is available at https://github.com/bhartidivya/AAD-DCE.

### The Skin Game: Revolutionizing Standards for AI Dermatology Model Comparison 
[[arxiv](https://arxiv.org/abs/2502.02500)] [[cool](https://papers.cool/arxiv/2502.02500)] [[pdf](https://arxiv.org/pdf/2502.02500)]
> **Authors**: Łukasz Miętkiewicz,Leon Ciechanowski,Dariusz Jemielniak
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 60 pages, 69 figures
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,组织和器官
- **Abstract**: Deep Learning approaches in dermatological image classification have shown promising results, yet the field faces significant methodological challenges that impede proper evaluation. This paper presents a dual contribution: first, a systematic analysis of current methodological practices in skin disease classification research, revealing substantial inconsistencies in data preparation, augmentation strategies, and performance reporting; second, a comprehensive training and evaluation framework demonstrated through experiments with the DINOv2-Large vision transformer across three benchmark datasets (HAM10000, DermNet, ISIC Atlas). The analysis identifies concerning patterns, including pre-split data augmentation and validation-based reporting, potentially leading to overestimated metrics, while highlighting the lack of unified methodology standards. The experimental results demonstrate DINOv2's performance in skin disease classification, achieving macro-averaged F1-scores of 0.85 (HAM10000), 0.71 (DermNet), and 0.84 (ISIC Atlas). Attention map analysis reveals critical patterns in the model's decision-making, showing sophisticated feature recognition in typical presentations but significant vulnerabilities with atypical cases and composite images. Our findings highlight the need for standardized evaluation protocols and careful implementation strategies in clinical settings. We propose comprehensive methodological recommendations for model development, evaluation, and clinical deployment, emphasizing rigorous data preparation, systematic error analysis, and specialized protocols for different image types. To promote reproducibility, we provide our implementation code through GitHub. This work establishes a foundation for rigorous evaluation standards in dermatological image classification and provides insights for responsible AI implementation in clinical dermatology.

### Style transfer as data augmentation: evaluating unpaired image-to-image translation models in mammography 
[[arxiv](https://arxiv.org/abs/2502.02475)] [[cool](https://papers.cool/arxiv/2502.02475)] [[pdf](https://arxiv.org/pdf/2502.02475)]
> **Authors**: Emir Ahmed,Spencer A. Thomas,Ciaran Bench
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别,医学物理
- **Abstract**: Several studies indicate that deep learning models can learn to detect breast cancer from mammograms (X-ray images of the breasts). However, challenges with overfitting and poor generalisability prevent their routine use in the clinic. Models trained on data from one patient population may not perform well on another due to differences in their data domains, emerging due to variations in scanning technology or patient characteristics. Data augmentation techniques can be used to improve generalisability by expanding the diversity of feature representations in the training data by altering existing examples. Image-to-image translation models are one approach capable of imposing the characteristic feature representations (i.e. style) of images from one dataset onto another. However, evaluating model performance is non-trivial, particularly in the absence of ground truths (a common reality in medical imaging). Here, we describe some key aspects that should be considered when evaluating style transfer algorithms, highlighting the advantages and disadvantages of popular metrics, and important factors to be mindful of when implementing them in practice. We consider two types of generative models: a cycle-consistent generative adversarial network (CycleGAN) and a diffusion-based SynDiff model. We learn unpaired image-to-image translation across three mammography datasets. We highlight that undesirable aspects of model performance may determine the suitability of some metrics, and also provide some analysis indicating the extent to which various metrics assess unique aspects of model performance. We emphasise the need to use several metrics for a comprehensive assessment of model performance.

### Deep Ensemble approach for Enhancing Brain Tumor Segmentation in Resource-Limited Settings 
[[arxiv](https://arxiv.org/abs/2502.02179)] [[cool](https://papers.cool/arxiv/2502.02179)] [[pdf](https://arxiv.org/pdf/2502.02179)]
> **Authors**: Jeremiah Fadugba,Isabel Lieberman,Olabode Ajayi,Mansour Osman,Solomon Oluwole Akinola,Tinashe Mustvangwa,Dong Zhang,Udunna C Anazondo,Raymond Confidence
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Segmentation of brain tumors is a critical step in treatment planning, yet manual segmentation is both time-consuming and subjective, relying heavily on the expertise of radiologists. In Sub-Saharan Africa, this challenge is magnified by overburdened medical systems and limited access to advanced imaging modalities and expert radiologists. Automating brain tumor segmentation using deep learning offers a promising solution. Convolutional Neural Networks (CNNs), especially the U-Net architecture, have shown significant potential. However, a major challenge remains: achieving generalizability across different datasets. This study addresses this gap by developing a deep learning ensemble that integrates UNet3D, V-Net, and MSA-VNet models for the semantic segmentation of gliomas. By initially training on the BraTS-GLI dataset and fine-tuning with the BraTS-SSA dataset, we enhance model performance. Our ensemble approach significantly outperforms individual models, achieving DICE scores of 0.8358 for Tumor Core, 0.8521 for Whole Tumor, and 0.8167 for Enhancing Tumor. These results underscore the potential of ensemble methods in improving the accuracy and reliability of automated brain tumor segmentation, particularly in resource-limited settings.

## 信号处理(eess.SP:Signal Processing)

### Three-dimensional signal processing: a new approach in dynamical sampling via tensor products 
[[arxiv](https://arxiv.org/abs/2502.02684)] [[cool](https://papers.cool/arxiv/2502.02684)] [[pdf](https://arxiv.org/pdf/2502.02684)]
> **Authors**: Yisen Wang,Hanqin Cai,Longxiu Huang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 信号处理,信息论,机器学习
- **Abstract**: The dynamical sampling problem is centered around reconstructing signals that evolve over time according to a dynamical process, from spatial-temporal samples that may be noisy. This topic has been thoroughly explored for one-dimensional signals. Multidimensional signal recovery has also been studied, but primarily in scenarios where the driving operator is a convolution operator. In this work, we shift our focus to the dynamical sampling problem in the context of three-dimensional signal recovery, where the evolution system can be characterized by tensor products. Specifically, we provide a necessary condition for the sampling set that ensures successful recovery of the three-dimensional signal. Furthermore, we reformulate the reconstruction problem as an optimization task, which can be solved efficiently. To demonstrate the effectiveness of our approach, we include some straightforward numerical simulations that showcase the reconstruction performance.

## 系统与控制(eess.SY:Systems and Control)

### Identifying Large-Scale Linear Parameter Varying Systems with Dynamic Mode Decomposition Methods 
[[arxiv](https://arxiv.org/abs/2502.02336)] [[cool](https://papers.cool/arxiv/2502.02336)] [[pdf](https://arxiv.org/pdf/2502.02336)]
> **Authors**: Jean Panaioti Jordanou,Eduardo Camponogara,Eduardo Gildin
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 39 pages, 8 figures. Submitted to Journal of Computational Physics
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: Linear Parameter Varying (LPV) Systems are a well-established class of nonlinear systems with a rich theory for stability analysis, control, and analytical response finding, among other aspects. Although there are works on data-driven identification of such systems, the literature is quite scarce in terms of works that tackle the identification of LPV models for large-scale systems. Since large-scale systems are ubiquitous in practice, this work develops a methodology for the local and global identification of large-scale LPV systems based on nonintrusive reduced-order modeling. The developed method is coined as DMD-LPV for being inspired in the Dynamic Mode Decomposition (DMD). To validate the proposed identification method, we identify a system described by a discretized linear diffusion equation, with the diffusion gain defined by a polynomial over a parameter. The experiments show that the proposed method can easily identify a reduced-order LPV model of a given large-scale system without the need to perform identification in the full-order dimension, and with almost no performance decay over performing a reduction, given that the model structure is well-established.

### Adaptive Resource Allocation Optimization Using Large Language Models in Dynamic Wireless Environments 
[[arxiv](https://arxiv.org/abs/2502.02287)] [[cool](https://papers.cool/arxiv/2502.02287)] [[pdf](https://arxiv.org/pdf/2502.02287)]
> **Authors**: Hyeonho Noh,Byonghyo Shim,Hyun Jong Yang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,机器学习
- **Abstract**: Deep learning (DL) has made notable progress in addressing complex radio access network control challenges that conventional analytic methods have struggled to solve. However, DL has shown limitations in solving constrained NP-hard problems often encountered in network optimization, such as those involving quality of service (QoS) or discrete variables like user indices. Current solutions rely on domain-specific architectures or heuristic techniques, and a general DL approach for constrained optimization remains undeveloped. Moreover, even minor changes in communication objectives demand time-consuming retraining, limiting their adaptability to dynamic environments where task objectives, constraints, environmental factors, and communication scenarios frequently change. To address these challenges, we propose a large language model for resource allocation optimizer (LLM-RAO), a novel approach that harnesses the capabilities of LLMs to address the complex resource allocation problem while adhering to QoS constraints. By employing a prompt-based tuning strategy to flexibly convey ever-changing task descriptions and requirements to the LLM, LLM-RAO demonstrates robust performance and seamless adaptability in dynamic environments without requiring extensive retraining. Simulation results reveal that LLM-RAO achieves up to a 40% performance enhancement compared to conventional DL methods and up to an $80$\% improvement over analytical approaches. Moreover, in scenarios with fluctuating communication objectives, LLM-RAO attains up to 2.9 times the performance of traditional DL-based networks.

### Synthesis of Model Predictive Control and Reinforcement Learning: Survey and Classification 
[[arxiv](https://arxiv.org/abs/2502.02133)] [[cool](https://papers.cool/arxiv/2502.02133)] [[pdf](https://arxiv.org/pdf/2502.02133)]
> **Authors**: Rudolf Reiter,Jasper Hoffmann,Dirk Reinhardt,Florian Messerer,Katrin Baumgärtner,Shamburaj Sawant,Joschka Boedecker,Moritz Diehl,Sebastien Gros
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 系统与控制,人工智能,机器学习
- **Abstract**: The fields of MPC and RL consider two successful control techniques for Markov decision processes. Both approaches are derived from similar fundamental principles, and both are widely used in practical applications, including robotics, process control, energy systems, and autonomous driving. Despite their similarities, MPC and RL follow distinct paradigms that emerged from diverse communities and different requirements. Various technical discrepancies, particularly the role of an environment model as part of the algorithm, lead to methodologies with nearly complementary advantages. Due to their orthogonal benefits, research interest in combination methods has recently increased significantly, leading to a large and growing set of complex ideas leveraging MPC and RL. This work illuminates the differences, similarities, and fundamentals that allow for different combination algorithms and categorizes existing work accordingly. Particularly, we focus on the versatile actor-critic RL approach as a basis for our categorization and examine how the online optimization approach of MPC can be used to improve the overall closed-loop performance of a policy.

## 高能物理-实验(hep-ex:High Energy Physics - Experiment)

### Particle Trajectory Representation Learning with Masked Point Modeling 
[[arxiv](https://arxiv.org/abs/2502.02558)] [[cool](https://papers.cool/arxiv/2502.02558)] [[pdf](https://arxiv.org/pdf/2502.02558)]
> **Authors**: Sam Young,Yeon-jae Jwa,Kazuhiro Terao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Preprint. 24 pages, 15 figures. Project page at https://youngsm.com/polarmae/
- **标题**: None
- **领域**: 高能物理-实验,计算机视觉和模式识别,机器学习
- **Abstract**: Effective self-supervised learning (SSL) techniques have been key to unlocking large datasets for representation learning. While many promising methods have been developed using online corpora and captioned photographs, their application to scientific domains, where data encodes highly specialized knowledge, remains in its early stages. We present a self-supervised masked modeling framework for 3D particle trajectory analysis in Time Projection Chambers (TPCs). These detectors produce globally sparse (<1% occupancy) but locally dense point clouds, capturing meter-scale particle trajectories at millimeter resolution. Starting with PointMAE, this work proposes volumetric tokenization to group sparse ionization points into resolution-agnostic patches, as well as an auxiliary energy infilling task to improve trajectory semantics. This approach -- which we call Point-based Liquid Argon Masked Autoencoder (PoLAr-MAE) -- achieves 99.4% track and 97.7% shower classification F-scores, matching that of supervised baselines without any labeled data. While the model learns rich particle trajectory representations, it struggles with sub-token phenomena like overlapping or short-lived particle trajectories. To support further research, we release PILArNet-M -- the largest open LArTPC dataset (1M+ events, 5.2B labeled points) -- to advance SSL in high energy physics (HEP). Project site: https://youngsm.com/polarmae/

### Comparative Analysis of FPGA and GPU Performance for Machine Learning-Based Track Reconstruction at LHCb 
[[arxiv](https://arxiv.org/abs/2502.02304)] [[cool](https://papers.cool/arxiv/2502.02304)] [[pdf](https://arxiv.org/pdf/2502.02304)]
> **Authors**: Fotis I. Giasemis,Vladimir Lončar,Bertrand Granado,Vladimir Vava Gligorov
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 高能物理-实验,分布式、并行和集群计算,机器学习,仪器仪表和探测器
- **Abstract**: In high-energy physics, the increasing luminosity and detector granularity at the Large Hadron Collider are driving the need for more efficient data processing solutions. Machine Learning has emerged as a promising tool for reconstructing charged particle tracks, due to its potentially linear computational scaling with detector hits. The recent implementation of a graph neural network-based track reconstruction pipeline in the first level trigger of the LHCb experiment on GPUs serves as a platform for comparative studies between computational architectures in the context of high-energy physics. This paper presents a novel comparison of the throughput of ML model inference between FPGAs and GPUs, focusing on the first step of the track reconstruction pipeline$\unicode{x2013}$an implementation of a multilayer perceptron. Using HLS4ML for FPGA deployment, we benchmark its performance against the GPU implementation and demonstrate the potential of FPGAs for high-throughput, low-latency inference without the need for an expertise in FPGA development and while consuming significantly less power.

## 动力系统(math.DS:Dynamical Systems)

### Circular Microalgae-Based Carbon Control for Net Zero 
[[arxiv](https://arxiv.org/abs/2502.02382)] [[cool](https://papers.cool/arxiv/2502.02382)] [[pdf](https://arxiv.org/pdf/2502.02382)]
> **Authors**: Federico Zocco,Joan García,Wassim M. Haddad
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: To be submitted
- **标题**: None
- **领域**: 动力系统,机器学习,优化与控制
- **Abstract**: The alteration of the climate in various areas of the world is of increasing concern since climate stability is a necessary condition for human survival as well as every living organism. The main reason of climate change is the greenhouse effect caused by the accumulation of carbon dioxide in the atmosphere. In this paper, we design a networked system underpinned by compartmental dynamical thermodynamics to circulate the atmospheric carbon dioxide. Specifically, in the carbon dioxide emitter compartment, we develop an initial-condition-dependent finite-time stabilizing controller that guarantees stability within a desired time leveraging the system property of affinity in the control. Then, to compensate for carbon emissions we show that a cultivation of microalgae with a volume 625 times bigger than the one of the carbon emitter is required. To increase the carbon uptake of the microalgae, we implement the nonaffine-in-the-control microalgae dynamical equations as an environment of a state-of-the-art library for reinforcement learning (RL), namely, Stable-Baselines3, and then, through the library, we test the performance of eight RL algorithms for training a controller that maximizes the microalgae absorption of carbon through the light intensity. All the eight controllers increased the carbon absorption of the cultivation during a training of 200,000 time steps with a maximum episode length of 200 time steps and with no termination conditions. This work is a first step towards approaching net zero as a classical and learning-based network control problem. The source code is publicly available.

## 优化与控制(math.OC:Optimization and Control)

### Coreset-Based Task Selection for Sample-Efficient Meta-Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.02332)] [[cool](https://papers.cool/arxiv/2502.02332)] [[pdf](https://arxiv.org/pdf/2502.02332)]
> **Authors**: Donglin Zhan,Leonardo F. Toso,James Anderson
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: We study task selection to enhance sample efficiency in model-agnostic meta-reinforcement learning (MAML-RL). Traditional meta-RL typically assumes that all available tasks are equally important, which can lead to task redundancy when they share significant similarities. To address this, we propose a coreset-based task selection approach that selects a weighted subset of tasks based on how diverse they are in gradient space, prioritizing the most informative and diverse tasks. Such task selection reduces the number of samples needed to find an $ε$-close stationary solution by a factor of O(1/$ε$). Consequently, it guarantees a faster adaptation to unseen tasks while focusing training on the most relevant tasks. As a case study, we incorporate task selection to MAML-LQR (Toso et al., 2024b), and prove a sample complexity reduction proportional to O(log(1/$ε$)) when the task specific cost also satisfy gradient dominance. Our theoretical guarantees underscore task selection as a key component for scalable and sample-efficient meta-RL. We numerically validate this trend across multiple RL benchmark problems, illustrating the benefits of task selection beyond the LQR baseline.

## 地球物理学(physics.geo-ph:Geophysics)

### EFKAN: A KAN-Integrated Neural Operator For Efficient Magnetotelluric Forward Modeling 
[[arxiv](https://arxiv.org/abs/2502.02195)] [[cool](https://papers.cool/arxiv/2502.02195)] [[pdf](https://arxiv.org/pdf/2502.02195)]
> **Authors**: Feng Wang,Hong Qiu,Yingying Huang,Xiaozhe Gu,Renfang Wang,Bo Yang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Submitted to Computers & Geosciences
- **标题**: None
- **领域**: 地球物理学,机器学习
- **Abstract**: Magnetotelluric (MT) forward modeling is fundamental for improving the accuracy and efficiency of MT inversion. Neural operators (NOs) have been effectively used for rapid MT forward modeling, demonstrating their promising performance in solving the MT forward modeling-related partial differential equations (PDEs). Particularly, they can obtain the electromagnetic field at arbitrary locations and frequencies. In these NOs, the projection layers have been dominated by multi-layer perceptrons (MLPs), which may potentially reduce the accuracy of solution due to they usually suffer from the disadvantages of MLPs, such as lack of interpretability, overfitting, and so on. Therefore, to improve the accuracy of MT forward modeling with NOs and explore the potential alternatives to MLPs, we propose a novel neural operator by extending the Fourier neural operator (FNO) with Kolmogorov-Arnold network (EFKAN). Within the EFKAN framework, the FNO serves as the branch network to calculate the apparent resistivity and phase from the resistivity model in the frequency domain. Meanwhile, the KAN acts as the trunk network to project the resistivity and phase, determined by the FNO, to the desired locations and frequencies. Experimental results demonstrate that the proposed method not only achieves higher accuracy in obtaining apparent resistivity and phase compared to the NO equipped with MLPs at the desired frequencies and locations but also outperforms traditional numerical methods in terms of computational speed.

## 医学物理(physics.med-ph:Medical Physics)

### When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with Sparse-view CT 
[[arxiv](https://arxiv.org/abs/2502.02771)] [[cool](https://papers.cool/arxiv/2502.02771)] [[pdf](https://arxiv.org/pdf/2502.02771)]
> **Authors**: Matt Y. Cheung,Sophia Zorek,Tucker J. Netherton,Laurence E. Court,Sadeer Al-Kindi,Ashok Veeraraghavan,Guha Balakrishnan
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Accepted at IEEE ISBI 2025, 5 pages, 2 figures, 1 table
- **标题**: None
- **领域**: 医学物理,计算机视觉和模式识别,机器学习,图像和视频处理,应用领域
- **Abstract**: Diffusion models demonstrate state-of-the-art performance on image generation, and are gaining traction for sparse medical image reconstruction tasks. However, compared to classical reconstruction algorithms relying on simple analytical priors, diffusion models have the dangerous property of producing realistic looking results \emph{even when incorrect}, particularly with few observations. We investigate the utility of diffusion models as priors for image reconstruction by varying the number of observations and comparing their performance to classical priors (sparse and Tikhonov regularization) using pixel-based, structural, and downstream metrics. We make comparisons on low-dose chest wall computed tomography (CT) for fat mass quantification. First, we find that classical priors are superior to diffusion priors when the number of projections is ``sufficient''. Second, we find that diffusion priors can capture a large amount of detail with very few observations, significantly outperforming classical priors. However, they fall short of capturing all details, even with many observations. Finally, we find that the performance of diffusion priors plateau after extremely few ($\approx$10-15) projections. Ultimately, our work highlights potential issues with diffusion-based sparse reconstruction and underscores the importance of further investigation, particularly in high-stakes clinical settings.

## 生物分子(q-bio.BM:Biomolecules)

### Accurate Pocket Identification for Binding-Site-Agnostic Docking 
[[arxiv](https://arxiv.org/abs/2502.02371)] [[cool](https://papers.cool/arxiv/2502.02371)] [[pdf](https://arxiv.org/pdf/2502.02371)]
> **Authors**: Yaroslav Balytskyi,Inna Hubenko,Alina Balytska,Christopher V. Kelly
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 生物分子,人工智能,机器学习,生物物理学,医学物理
- **Abstract**: Accurate identification of druggable pockets is essential for structure-based drug design. However, most pocket-identification algorithms prioritize their geometric properties over downstream docking performance. To address this limitation, we developed RAPID-Net, a pocket-finding algorithm for seamless integration with docking workflows. When guiding AutoDock Vina, RAPID-Net outperforms DiffBindFR on the PoseBusters benchmark and enables blind docking on large proteins that AlphaFold 3 cannot process as a whole. Furthermore, RAPID-Net surpasses PUResNet and Kalasanty in docking accuracy and pocket-ligand intersection rates across diverse datasets, including PoseBusters, Astex Diverse Set, BU48, and Coach420. When accuracy is evaluated as ``at least one correct pose in the ensemble'', RAPID-Net outperforms AlphaFold 3 on the PoseBusters benchmark, suggesting that our approach can be further improved with a suitable pose reweighting tool offering a cost-effective and competitive alternative to AlphaFold 3 for docking. Finally, using several therapeutically relevant examples, we demonstrate the ability of RAPID-Net to identify remote functional sites, highlighting its potential to facilitate the development of innovative therapeutics.

## 基因组学(q-bio.GN:Genomics)

### Graph Structure Learning for Tumor Microenvironment with Cell Type Annotation from non-spatial scRNA-seq data 
[[arxiv](https://arxiv.org/abs/2502.02629)] [[cool](https://papers.cool/arxiv/2502.02629)] [[pdf](https://arxiv.org/pdf/2502.02629)]
> **Authors**: Yu-An Huang,Yue-Chao Li,Hai-Ru You,Jie Pan,Xiyue Cao,Xinyuan Li,Zhi-An Huang,Zhu-Hong You
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 29 pages, 6 figures
- **标题**: None
- **领域**: 基因组学,人工智能,机器学习
- **Abstract**: The exploration of cellular heterogeneity within the tumor microenvironment (TME) via single-cell RNA sequencing (scRNA-seq) is essential for understanding cancer progression and response to therapy. Current scRNA-seq approaches, however, lack spatial context and rely on incomplete datasets of ligand-receptor interactions (LRIs), limiting accurate cell type annotation and cell-cell communication (CCC) inference. This study addresses these challenges using a novel graph neural network (GNN) model that enhances cell type prediction and cell interaction analysis. Our study utilized a dataset consisting of 49,020 cells from 19 patients across three cancer types: Leukemia, Breast Invasive Carcinoma, and Colorectal Cancer. The proposed scGSL model demonstrated robust performance, achieving an average accuracy of 84.83%, precision of 86.23%, recall of 81.51%, and an F1 score of 80.92% across all datasets. These metrics represent a significant enhancement over existing methods, which typically exhibit lower performance metrics. Additionally, by reviewing existing literature on gene interactions within the TME, the scGSL model proves to robustly identify biologically meaningful gene interactions in an unsupervised manner, validated by significant expression differences in key gene pairs across various cancers. The source code and data used in this paper can be found in https://github.com/LiYuechao1998/scGSL.

## 定量方法(q-bio.QM:Quantitative Methods)

### scBIT: Integrating Single-cell Transcriptomic Data into fMRI-based Prediction for Alzheimer's Disease Diagnosis 
[[arxiv](https://arxiv.org/abs/2502.02630)] [[cool](https://papers.cool/arxiv/2502.02630)] [[pdf](https://arxiv.org/pdf/2502.02630)]
> **Authors**: Yu-An Huang,Yao Hu,Yue-Chao Li,Xiyue Cao,Xinyuan Li,Kay Chen Tan,Zhu-Hong You,Zhi-An Huang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 31 pages, 5 figures
- **标题**: None
- **领域**: 定量方法,人工智能,机器学习
- **Abstract**: Functional MRI (fMRI) and single-cell transcriptomics are pivotal in Alzheimer's disease (AD) research, each providing unique insights into neural function and molecular mechanisms. However, integrating these complementary modalities remains largely unexplored. Here, we introduce scBIT, a novel method for enhancing AD prediction by combining fMRI with single-nucleus RNA (snRNA). scBIT leverages snRNA as an auxiliary modality, significantly improving fMRI-based prediction models and providing comprehensive interpretability. It employs a sampling strategy to segment snRNA data into cell-type-specific gene networks and utilizes a self-explainable graph neural network to extract critical subgraphs. Additionally, we use demographic and genetic similarities to pair snRNA and fMRI data across individuals, enabling robust cross-modal learning. Extensive experiments validate scBIT's effectiveness in revealing intricate brain region-gene associations and enhancing diagnostic prediction accuracy. By advancing brain imaging transcriptomics to the single-cell level, scBIT sheds new light on biomarker discovery in AD research. Experimental results show that incorporating snRNA data into the scBIT model significantly boosts accuracy, improving binary classification by 3.39% and five-class classification by 26.59%. The codes were implemented in Python and have been released on GitHub (https://github.com/77YQ77/scBIT) and Zenodo (https://zenodo.org/records/11599030) with detailed instructions.

### SurvHive: a package to consistently access multiple survival-analysis packages 
[[arxiv](https://arxiv.org/abs/2502.02223)] [[cool](https://papers.cool/arxiv/2502.02223)] [[pdf](https://arxiv.org/pdf/2502.02223)]
> **Authors**: Giovanni Birolo,Ivan Rossi,Flavio Sartori,Cesare Rollo,Tiziana Sanavia,Piero Fariselli
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 11 loages, 1 table
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Survival analysis, a foundational tool for modeling time-to-event data, has seen growing integration with machine learning (ML) approaches to handle the complexities of censored data and time-varying risks. Despite these advances, leveraging state-of-the-art survival models remains a challenge due to the fragmented nature of existing implementations, which lack standardized interfaces and require extensive preprocessing. We introduce SurvHive, a Python-based framework designed to unify survival analysis methods within a coherent and extensible interface modeled on scikit-learn. SurvHive integrates classical statistical models with cutting-edge deep learning approaches, including transformer-based architectures and parametric survival models. Using a consistent API, SurvHive simplifies model training, evaluation, and optimization, significantly reducing the barrier to entry for ML practitioners exploring survival analysis. The package includes enhanced support for hyper-parameter tuning, time-dependent risk evaluation metrics, and cross-validation strategies tailored to censored data. With its extensibility and focus on usability, SurvHive provides a bridge between survival analysis and the broader ML community, facilitating advancements in time-to-event modeling across domains. The SurvHive code and documentation are available freely at https://github.com/compbiomed-unito/survhive.

## 投资组合管理(q-fin.PM:Portfolio Management)

### Regret-Optimized Portfolio Enhancement through Deep Reinforcement Learning and Future Looking Rewards 
[[arxiv](https://arxiv.org/abs/2502.02619)] [[cool](https://papers.cool/arxiv/2502.02619)] [[pdf](https://arxiv.org/pdf/2502.02619)]
> **Authors**: Daniil Karzanov,Rubén Garzón,Mikhail Terekhov,Caglar Gulcehre,Thomas Raffinot,Marcin Detyniecki
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 11 pages, 7 figures
- **标题**: None
- **领域**: 投资组合管理,机器学习,风险管理
- **Abstract**: This paper introduces a novel agent-based approach for enhancing existing portfolio strategies using Proximal Policy Optimization (PPO). Rather than focusing solely on traditional portfolio construction, our approach aims to improve an already high-performing strategy through dynamic rebalancing driven by PPO and Oracle agents. Our target is to enhance the traditional 60/40 benchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward function. To address the impact of transaction fee frictions and prevent signal loss, we develop a transaction cost scheduler. We introduce a future-looking reward function and employ synthetic data training through a circular block bootstrap method to facilitate the learning of generalizable allocation strategies. We focus on two key evaluation measures: return and maximum drawdown. Given the high stochasticity of financial markets, we train 20 independent agents each period and evaluate their average performance against the benchmark. Our method not only enhances the performance of the existing portfolio strategy through strategic rebalancing but also demonstrates strong results compared to other baselines.

## 方法论(stat.ME:Methodology)

### Heteroscedastic Double Bayesian Elastic Net 
[[arxiv](https://arxiv.org/abs/2502.02032)] [[cool](https://papers.cool/arxiv/2502.02032)] [[pdf](https://arxiv.org/pdf/2502.02032)]
> **Authors**: Masanari Kimura
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 方法论,人工智能,机器学习
- **Abstract**: In many practical applications, regression models are employed to uncover relationships between predictors and a response variable, yet the common assumption of constant error variance is frequently violated. This issue is further compounded in high-dimensional settings where the number of predictors exceeds the sample size, necessitating regularization for effective estimation and variable selection. To address this problem, we propose the Heteroscedastic Double Bayesian Elastic Net (HDBEN), a novel framework that jointly models the mean and log-variance using hierarchical Bayesian priors incorporating both $\ell_1$ and $\ell_2$ penalties. Our approach simultaneously induces sparsity and grouping in the regression coefficients and variance parameters, capturing complex variance structures in the data. Theoretical results demonstrate that proposed HDBEN achieves posterior concentration, variable selection consistency, and asymptotic normality under mild conditions which justifying its behavior. Simulation studies further illustrate that HDBEN outperforms existing methods, particularly in scenarios characterized by heteroscedasticity and high dimensionality.

## 机器学习(stat.ML:Machine Learning)

### Uncertainty Quantification with the Empirical Neural Tangent Kernel 
[[arxiv](https://arxiv.org/abs/2502.02870)] [[cool](https://papers.cool/arxiv/2502.02870)] [[pdf](https://arxiv.org/pdf/2502.02870)]
> **Authors**: Joseph Wilson,Chris van der Heide,Liam Hodgkinson,Fred Roosta
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 24 pages, 5 figures, 9 tables
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: While neural networks have demonstrated impressive performance across various tasks, accurately quantifying uncertainty in their predictions is essential to ensure their trustworthiness and enable widespread adoption in critical systems. Several Bayesian uncertainty quantification (UQ) methods exist that are either cheap or reliable, but not both. We propose a post-hoc, sampling-based UQ method for over-parameterized networks at the end of training. Our approach constructs efficient and meaningful deep ensembles by employing a (stochastic) gradient-descent sampling process on appropriately linearized networks. We demonstrate that our method effectively approximates the posterior of a Gaussian process using the empirical Neural Tangent Kernel. Through a series of numerical experiments, we show that our method not only outperforms competing approaches in computational efficiency (often reducing costs by multiple factors) but also maintains state-of-the-art performance across a variety of UQ metrics for both regression and classification tasks.

### Algorithms with Calibrated Machine Learning Predictions 
[[arxiv](https://arxiv.org/abs/2502.02861)] [[cool](https://papers.cool/arxiv/2502.02861)] [[pdf](https://arxiv.org/pdf/2502.02861)]
> **Authors**: Judy Hanwen Shen,Ellen Vitercik,Anders Wikum
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: Replacement: updated acknowledgments
- **标题**: None
- **领域**: 机器学习,数据结构和算法,机器学习
- **Abstract**: The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.

### Gap-Dependent Bounds for Federated $Q$-learning 
[[arxiv](https://arxiv.org/abs/2502.02859)] [[cool](https://papers.cool/arxiv/2502.02859)] [[pdf](https://arxiv.org/pdf/2502.02859)]
> **Authors**: Haochen Zhang,Zhong Zheng,Lingzhou Xue
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.

### Achievable distributional robustness when the robust risk is only partially identified 
[[arxiv](https://arxiv.org/abs/2502.02710)] [[cool](https://papers.cool/arxiv/2502.02710)] [[pdf](https://arxiv.org/pdf/2502.02710)]
> **Authors**: Julia Kostin,Nicola Gnecco,Fanny Yang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariance-based algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied -- a scenario so far underexplored in the theoretical literature. In this paper, we aim to fill the gap and propose to study the more general setting when the robust risk is only partially identifiable. In particular, we introduce the worst-case robust risk as a new measure of robustness that is always well-defined regardless of identifiability. Its minimum corresponds to an algorithm-independent (population) minimax quantity that measures the best achievable robustness under partial identifiability. While these concepts can be defined more broadly, in this paper we introduce and derive them explicitly for a linear model for concreteness of the presentation. First, we show that existing robustness methods are provably suboptimal in the partially identifiable case. We then evaluate these methods and the minimizer of the (empirical) worst-case robust risk on real-world gene expression data and find a similar trend: the test error of existing robustness methods grows increasingly suboptimal as the fraction of data from unseen environments increases, whereas accounting for partial identifiability allows for better generalization.

### Networks with Finite VC Dimension: Pro and Contra 
[[arxiv](https://arxiv.org/abs/2502.02679)] [[cool](https://papers.cool/arxiv/2502.02679)] [[pdf](https://arxiv.org/pdf/2502.02679)]
> **Authors**: Vera Kurkova,Marcello Sanguineti
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Approximation and learning of classifiers of large data sets by neural networks in terms of high-dimensional geometry and statistical learning theory are investigated. The influence of the VC dimension of sets of input-output functions of networks on approximation capabilities is compared with its influence on consistency in learning from samples of data. It is shown that, whereas finite VC dimension is desirable for uniform convergence of empirical errors, it may not be desirable for approximation of functions drawn from a probability distribution modeling the likelihood that they occur in a given type of application. Based on the concentration-of-measure properties of high dimensional geometry, it is proven that both errors in approximation and empirical errors behave almost deterministically for networks implementing sets of input-output functions with finite VC dimensions in processing large data sets. Practical limitations of the universal approximation property, the trade-offs between the accuracy of approximation and consistency in learning from data, and the influence of depth of networks with ReLU units on their accuracy and consistency are discussed.

### Catoni Contextual Bandits are Robust to Heavy-tailed Rewards 
[[arxiv](https://arxiv.org/abs/2502.02486)] [[cool](https://papers.cool/arxiv/2502.02486)] [[pdf](https://arxiv.org/pdf/2502.02486)]
> **Authors**: Chenlu Ye,Yujia Jin,Alekh Agarwal,Tong Zhang
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Typical contextual bandit algorithms assume that the rewards at each round lie in some fixed range $[0, R]$, and their regret scales polynomially with this reward range $R$. However, many practical scenarios naturally involve heavy-tailed rewards or rewards where the worst-case range can be substantially larger than the variance. In this paper, we develop an algorithmic approach building on Catoni's estimator from robust statistics, and apply it to contextual bandits with general function approximation. When the variance of the reward at each round is known, we use a variance-weighted regression approach and establish a regret bound that depends only on the cumulative reward variance and logarithmically on the reward range $R$ as well as the number of rounds $T$. For the unknown-variance case, we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation. With additional dependence on the fourth moment, our algorithm also enjoys a variance-based bound with logarithmic reward-range dependence. Moreover, we demonstrate the optimality of the leading-order term in our regret bound through a matching lower bound.

### SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations 
[[arxiv](https://arxiv.org/abs/2502.02472)] [[cool](https://papers.cool/arxiv/2502.02472)] [[pdf](https://arxiv.org/pdf/2502.02472)]
> **Authors**: Grigory Bartosh,Dmitry Vetrov,Christian A. Naesseth
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: The Latent Stochastic Differential Equation (SDE) is a powerful tool for time series and sequence modeling. However, training Latent SDEs typically relies on adjoint sensitivity methods, which depend on simulation and backpropagation through approximate SDE solutions, which limit scalability. In this work, we propose SDE Matching, a new simulation-free method for training Latent SDEs. Inspired by modern Score- and Flow Matching algorithms for learning generative dynamics, we extend these ideas to the domain of stochastic dynamics for time series and sequence modeling, eliminating the need for costly numerical simulations. Our results demonstrate that SDE Matching achieves performance comparable to adjoint sensitivity methods while drastically reducing computational complexity.

### Distribution Transformers: Fast Approximate Bayesian Inference With On-The-Fly Prior Adaptation 
[[arxiv](https://arxiv.org/abs/2502.02463)] [[cool](https://papers.cool/arxiv/2502.02463)] [[pdf](https://arxiv.org/pdf/2502.02463)]
> **Authors**: George Whittle,Juliusz Ziomek,Jacob Rawling,Michael A Osborne
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: While Bayesian inference provides a principled framework for reasoning under uncertainty, its widespread adoption is limited by the intractability of exact posterior computation, necessitating the use of approximate inference. However, existing methods are often computationally expensive, or demand costly retraining when priors change, limiting their utility, particularly in sequential inference problems such as real-time sensor fusion. To address these challenges, we introduce the Distribution Transformer -- a novel architecture that can learn arbitrary distribution-to-distribution mappings. Our method can be trained to map a prior to the corresponding posterior, conditioned on some dataset -- thus performing approximate Bayesian inference. Our novel architecture represents a prior distribution as a (universally-approximating) Gaussian Mixture Model (GMM), and transforms it into a GMM representation of the posterior. The components of the GMM attend to each other via self-attention, and to the datapoints via cross-attention. We demonstrate that Distribution Transformers both maintain flexibility to vary the prior, and significantly reduces computation times-from minutes to milliseconds-while achieving log-likelihood performance on par with or superior to existing approximate inference methods across tasks such as sequential inference, quantum system parameter inference, and Gaussian Process predictive posterior inference with hyperpriors.

### A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals 
[[arxiv](https://arxiv.org/abs/2502.02430)] [[cool](https://papers.cool/arxiv/2502.02430)] [[pdf](https://arxiv.org/pdf/2502.02430)]
> **Authors**: Róbert Busa-Fekete,Julian Zimmert,András György,Linhai Qiu,Tzu-Wei Sung,Hao Shen,Hyomin Choi,Sharmila Subramaniam,Li Xiao
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息检索,机器学习
- **Abstract**: Web refresh crawling is the problem of keeping a cache of web pages fresh, that is, having the most recent copy available when a page is requested, given a limited bandwidth available to the crawler. Under the assumption that the change and request events, resp., to each web page follow independent Poisson processes, the optimal scheduling policy was derived by Azar et al. 2018. In this paper, we study an extension of this problem where side information indicating content changes, such as various types of web pings, for example, signals from sitemaps, content delivery networks, etc., is available. Incorporating such side information into the crawling policy is challenging, because (i) the signals can be noisy with false positive events and with missing change events; and (ii) the crawler should achieve a fair performance over web pages regardless of the quality of the side information, which might differ from web page to web page. We propose a scalable crawling algorithm which (i) uses the noisy side information in an optimal way under mild assumptions; (ii) can be deployed without heavy centralized computation; (iii) is able to crawl web pages at a constant total rate without spikes in the total bandwidth usage over any time interval, and automatically adapt to the new optimal solution when the total bandwidth changes without centralized computation. Experiments clearly demonstrate the versatility of our approach.

### FAB-PPI: Frequentist, Assisted by Bayes, Prediction-Powered Inference 
[[arxiv](https://arxiv.org/abs/2502.02363)] [[cool](https://papers.cool/arxiv/2502.02363)] [[pdf](https://arxiv.org/pdf/2502.02363)]
> **Authors**: Stefano Cortinovis,François Caron
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 28 pages, 13 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Prediction-powered inference (PPI) enables valid statistical inference by combining experimental data with machine learning predictions. When a sufficient number of high-quality predictions is available, PPI results in more accurate estimates and tighter confidence intervals than traditional methods. In this paper, we propose to inform the PPI framework with prior knowledge on the quality of the predictions. The resulting method, which we call frequentist, assisted by Bayes, PPI (FAB-PPI), improves over PPI when the observed prediction quality is likely under the prior, while maintaining its frequentist guarantees. Furthermore, when using heavy-tailed priors, FAB-PPI adaptively reverts to standard PPI in low prior probability regions. We demonstrate the benefits of FAB-PPI in real and synthetic examples.

### On the Impact of Performative Risk Minimization for Binary Random Variables 
[[arxiv](https://arxiv.org/abs/2502.02331)] [[cool](https://papers.cool/arxiv/2502.02331)] [[pdf](https://arxiv.org/pdf/2502.02331)]
> **Authors**: Nikita Tsoy,Ivan Kirev,Negin Rahimiyazdi,Nikola Konstantinov
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Performativity, the phenomenon where outcomes are influenced by predictions, is particularly prevalent in social contexts where individuals strategically respond to a deployed model. In order to preserve the high accuracy of machine learning models under distribution shifts caused by performativity, Perdomo et al. (2020) introduced the concept of performative risk minimization (PRM). While this framework ensures model accuracy, it overlooks the impact of the PRM on the underlying distributions and the predictions of the model. In this paper, we initiate the analysis of the impact of PRM, by studying performativity for a sequential performative risk minimization problem with binary random variables and linear performative shifts. We formulate two natural measures of impact. In the case of full information, where the distribution dynamics are known, we derive explicit formulas for the PRM solution and our impact measures. In the case of partial information, we provide performative-aware statistical estimators, as well as simulations. Our analysis contrasts PRM to alternatives that do not model data shift and indicates that PRM can have amplified side effects compared to such methods.

### Information-Theoretic Proofs for Diffusion Sampling 
[[arxiv](https://arxiv.org/abs/2502.02305)] [[cool](https://papers.cool/arxiv/2502.02305)] [[pdf](https://arxiv.org/pdf/2502.02305)]
> **Authors**: Galen Reeves,Henry D. Pfister
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论,机器学习
- **Abstract**: This paper provides an elementary, self-contained analysis of diffusion-based sampling methods for generative modeling. In contrast to existing approaches that rely on continuous-time processes and then discretize, our treatment works directly with discrete-time stochastic processes and yields precise non-asymptotic convergence guarantees under broad assumptions. The key insight is to couple the sampling process of interest with an idealized comparison process that has an explicit Gaussian-convolution structure. We then leverage simple identities from information theory, including the I-MMSE relationship, to bound the discrepancy (in terms of the Kullback-Leibler divergence) between these two discrete-time processes. In particular, we show that, if the diffusion step sizes are chosen sufficiently small and one can approximate certain conditional mean estimators well, then the sampling distribution is provably close to the target distribution. Our results also provide a transparent view on how to accelerate convergence by introducing additional randomness in each step to match higher order moments in the comparison process.

### Variance-Adjusted Cosine Distance as Similarity Metric 
[[arxiv](https://arxiv.org/abs/2502.02233)] [[cool](https://papers.cool/arxiv/2502.02233)] [[pdf](https://arxiv.org/pdf/2502.02233)]
> **Authors**: Satyajeet Sahoo,Jhareswar Maiti
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 6 Pages
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Cosine similarity is a popular distance measure that measures the similarity between two vectors in the inner product space. It is widely used in many data classification algorithms like K-Nearest Neighbors, Clustering etc. This study demonstrates limitations of application of cosine similarity. Particularly, this study demonstrates that traditional cosine similarity metric is valid only in the Euclidean space, whereas the original data resides in a random variable space. When there is variance and correlation in the data, then cosine distance is not a completely accurate measure of similarity. While new similarity and distance metrics have been developed to make up for the limitations of cosine similarity, these metrics are used as substitutes to cosine distance, and do not make modifications to cosine distance to overcome its limitations. Subsequently, we propose a modified cosine similarity metric, where cosine distance is adjusted by variance-covariance of the data. Application of variance-adjusted cosine distance gives better similarity performance compared to traditional cosine distance. KNN modelling on the Wisconsin Breast Cancer Dataset is performed using both traditional and modified cosine similarity measures and compared. The modified formula shows 100% test accuracy on the data.

### An Information-Theoretic Analysis of Thompson Sampling with Infinite Action Spaces 
[[arxiv](https://arxiv.org/abs/2502.02140)] [[cool](https://papers.cool/arxiv/2502.02140)] [[pdf](https://arxiv.org/pdf/2502.02140)]
> **Authors**: Amaury Gouverneur,Borja Rodriguez Gálvez,Tobias Oechtering,Mikael Skoglund
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-05
> **comment**: 5 pages, accepted to ICASSP
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper studies the Bayesian regret of the Thompson Sampling algorithm for bandit problems, building on the information-theoretic framework introduced by Russo and Van Roy (2015). Specifically, it extends the rate-distortion analysis of Dong and Van Roy (2018), which provides near-optimal bounds for linear bandits. A limitation of these results is the assumption of a finite action space. We address this by extending the analysis to settings with infinite and continuous action spaces. Additionally, we specialize our results to bandit problems with expected rewards that are Lipschitz continuous with respect to the action space, deriving a regret bound that explicitly accounts for the complexity of the action space.

## 其他论文

- [Advancements in Mobile Edge Computing and Open RAN: Leveraging Artificial Intelligence and Machine Learning for Wireless Systems](https://arxiv.org/abs/2502.02886)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Learning not cheating: AI assistance can enhance rather than hinder skill development](https://arxiv.org/abs/2502.02880)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Differentially-Private Multi-Tier Federated Learning: A Formal Analysis and Evaluation](https://arxiv.org/abs/2502.02877)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [A Study on 5G Network Slice Isolation Based on Native Cloud and Edge Computing Tools](https://arxiv.org/abs/2502.02842)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI,cs.DC in whitelist
- [COFFE: A Code Efficiency Benchmark for Code Generation](https://arxiv.org/abs/2502.02827)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Covert Communications in Active-IOS Aided Uplink NOMA Systems With Full-Duplex Receiver](https://arxiv.org/abs/2502.02813)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT,eess.SP in whitelist
- [METAMON: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries](https://arxiv.org/abs/2502.02794)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [When Anti-Fraud Laws Become a Barrier to Computer Science Research](https://arxiv.org/abs/2502.02767)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Too Noisy To Learn: Enhancing Data Quality for Code Review Comment Generation](https://arxiv.org/abs/2502.02757)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [MuST: Multi-Head Skill Transformer for Long-Horizon Dexterous Manipulation with Skill Progress](https://arxiv.org/abs/2502.02753)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Semantic Entanglement-Based Ransomware Detection via Probabilistic Latent Encryption Mapping](https://arxiv.org/abs/2502.02730)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [AsserT5: Test Assertion Generation Using a Fine-Tuned Code Language Model](https://arxiv.org/abs/2502.02708)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Exploring LLMs Impact on Student-Created User Stories and Acceptance Testing in Software Development](https://arxiv.org/abs/2502.02675)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Deep Reinforcement Learning Enabled Persistent Surveillance with Energy-Aware UAV-UGV Systems for Disaster Management Applications](https://arxiv.org/abs/2502.02666)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Offshore Wind Turbine Tower Design and Optimization: A Review and AI-Driven Future Directions](https://arxiv.org/abs/2502.02594)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,eess.SY in whitelist
- [LLMs for Generation of Architectural Components: An Exploratory Empirical Study in the Serverless World](https://arxiv.org/abs/2502.02539)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [AI-Powered, But Power-Hungry? Energy Efficiency of LLM-Generated Code](https://arxiv.org/abs/2502.02412)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [FPGA Innovation Research in the Netherlands: Present Landscape and Future Outlook](https://arxiv.org/abs/2502.02404)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC,cs.AR in whitelist
- [Hypergraph Link Prediction via Hyperedge Copying](https://arxiv.org/abs/2502.02386)
  - **标题**: None
  - **Filtered Reason**: none of physics.soc-ph,nlin.AO,physics.data-an,cs.SI in whitelist
- [Mirai: A Wearable Proactive AI "Inner-Voice" for Contextual Nudging](https://arxiv.org/abs/2502.02370)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [A Fast Decoding Algorithm for Generalized Reed-Solomon Codes and Alternant Codes](https://arxiv.org/abs/2502.02356)
  - **标题**: None
  - **Filtered Reason**: none of cs.IT in whitelist
- [SHIELD: APT Detection and Intelligent Explanation Using LLM](https://arxiv.org/abs/2502.02342)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Rule-ATT&CK Mapper (RAM): Mapping SIEM Rules to TTPs Using LLMs](https://arxiv.org/abs/2502.02337)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [NoteFlow: Recommending Charts as Sight Glasses for Tracing Data Flow in Computational Notebooks](https://arxiv.org/abs/2502.02326)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Using ChatGPT to refine draft conceptual schemata in supply-driven design of multidimensional cubes](https://arxiv.org/abs/2502.02238)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE,cs.DB in whitelist
- [Combinatorial Optimization Perspective based Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2502.02232)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study](https://arxiv.org/abs/2502.02194)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.SE in whitelist
- [Large language models in climate and sustainability policy: limits and opportunities](https://arxiv.org/abs/2502.02191)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [PALQA: A Novel Parameterized Position-Aware Lossy Quantum Autoencoder using LSB Control Qubit for Efficient Image Compression](https://arxiv.org/abs/2502.02188)
  - **标题**: None
  - **Filtered Reason**: none of cs.ET,quant-ph in whitelist
- [Towards Efficient LUT-based PIM: A Scalable and Low-Power Approach for Modern Workloads](https://arxiv.org/abs/2502.02142)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [NFV-Enabled Service Recovery in Space-Air-Ground Integrated Networks: A Matching Game Based Approach](https://arxiv.org/abs/2502.02141)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Efficient Laser Frequency Allocation in Packet-Optical Nodes with Coherent Transceivers](https://arxiv.org/abs/2502.02087)
  - **标题**: None
  - **Filtered Reason**: none of cs.NI in whitelist
- [Hardware and software build flow with SoCMake](https://arxiv.org/abs/2502.02065)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Reason4Rec: Large Language Models for Recommendation with Deliberative User Preference Alignment](https://arxiv.org/abs/2502.02061)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [From Accidents to Insights: Leveraging Multimodal Data for Scenario-Driven ADS Testing](https://arxiv.org/abs/2502.02025)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling](https://arxiv.org/abs/2502.02019)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS in whitelist
