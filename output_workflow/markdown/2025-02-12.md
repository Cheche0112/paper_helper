> 本文由 [https://github.com/huiyeruzhou/arxiv_crawler](https://github.com/huiyeruzhou/arxiv_crawler) 自动生成
>
> 领域白名单：cs.AI,cs.CL,cs.LG,cs.CV
> 关键词： LLM, GPT, AI, language+model, deep+learning, transformer, neural+network, machine+learning

# 论文全览：2025-02-12

共有283篇相关领域论文, 另有41篇其他

## 地球和行星天体物理学(astro-ph.EP:Earth and Planetary Astrophysics)

### Exoplanet Transit Candidate Identification in TESS Full-Frame Images via a Transformer-Based Algorithm 
[[arxiv](https://arxiv.org/abs/2502.07542)] [[cool](https://papers.cool/arxiv/2502.07542)] [[pdf](https://arxiv.org/pdf/2502.07542)]
> **Authors**: Helem Salinas,Rafael Brahm,Greg Olmschenk,Richard K. Barry,Karim Pichara,Stela Ishitani Silva,Vladimir Araujo
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 地球和行星天体物理学,星系天体物理学,天体物理学仪器和方法,人工智能
- **Abstract**: The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius > 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity.

## 材料科学(cond-mat.mtrl-sci:Materials Science)

### Explainable Multimodal Machine Learning for Revealing Structure-Property Relationships in Carbon Nanotube Fibers 
[[arxiv](https://arxiv.org/abs/2502.07400)] [[cool](https://papers.cool/arxiv/2502.07400)] [[pdf](https://arxiv.org/pdf/2502.07400)]
> **Authors**: Daisuke Kimura,Naoko Tajima,Toshiya Okazaki,Shun Muroga
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 33 pages, 9 figures
- **标题**: None
- **领域**: 材料科学,软凝聚态物质,人工智能,机器学习,数据分析、统计和概率
- **Abstract**: In this study, we propose Explainable Multimodal Machine Learning (EMML), which integrates the analysis of diverse data types (multimodal data) using factor analysis for feature extraction with Explainable AI (XAI), for carbon nanotube (CNT) fibers prepared from aqueous dispersions. This method is a powerful approach to elucidate the mechanisms governing material properties, where multi-stage fabrication conditions and multiscale structures have complex influences. Thus, in our case, this approach helps us understand how different processing steps and structures at various scales impact the final properties of CNT fibers. The analysis targeted structures ranging from the nanoscale to the macroscale, including aggregation size distributions of CNT dispersions and the effective length of CNTs. Furthermore, because some types of data were difficult to interpret using standard methods, challenging-to-interpret distribution data were analyzed using Negative Matrix Factorization (NMF) for extracting key features that determine the outcome. Contribution analysis with SHapley Additive exPlanations (SHAP) demonstrated that small, uniformly distributed aggregates are crucial for improving fracture strength, while CNTs with long effective lengths are significant factors for enhancing electrical conductivity. The analysis also identified thresholds and trends for these key factors to assist in defining the conditions needed to optimize CNT fiber properties. EMML is not limited to CNT fibers but can be applied to the design of other materials derived from nanomaterials, making it a useful tool for developing a wide range of advanced materials. This approach provides a foundation for advancing data-driven materials research.

### PICTS: A Novel Deep Reinforcement Learning Approach for Dynamic P-I Control in Scanning Probe Microscopy 
[[arxiv](https://arxiv.org/abs/2502.07326)] [[cool](https://papers.cool/arxiv/2502.07326)] [[pdf](https://arxiv.org/pdf/2502.07326)]
> **Authors**: Ziwei Wei,Shuming Wei,Qibin Zeng,Wanheng Lu,Huajun Liu,Kaiyang Zeng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 21 pages, 6 figures
- **标题**: None
- **领域**: 材料科学,机器学习,应用物理
- **Abstract**: We have developed a Parallel Integrated Control and Training System, leveraging the deep reinforcement learning to dynamically adjust the control strategies in real time for scanning probe microscopy techniques.

### Global Universal Scaling and Ultra-Small Parameterization in Machine Learning Interatomic Potentials with Super-Linearity 
[[arxiv](https://arxiv.org/abs/2502.07293)] [[cool](https://papers.cool/arxiv/2502.07293)] [[pdf](https://arxiv.org/pdf/2502.07293)]
> **Authors**: Yanxiao Hu,Ye Sheng,Jing Huang,Xiaoxin Xu,Yuyan Yang,Mingqiang Zhang,Yabei Wu,Caichao Ye,Jiong Yang,Wenqing Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 材料科学,机器学习
- **Abstract**: Using machine learning (ML) to construct interatomic interactions and thus potential energy surface (PES) has become a common strategy for materials design and simulations. However, those current models of machine learning interatomic potential (MLIP) provide no relevant physical constrains, and thus may owe intrinsic out-of-domain difficulty which underlies the challenges of model generalizability and physical scalability. Here, by incorporating physics-informed Universal-Scaling law and nonlinearity-embedded interaction function, we develop a Super-linear MLIP with both Ultra-Small parameterization and greatly expanded expressive capability, named SUS2-MLIP. Due to the global scaling rooting in universal equation of state (UEOS), SUS2-MLIP not only has significantly-reduced parameters by decoupling the element space from coordinate space, but also naturally outcomes the out-of-domain difficulty and endows the potentials with inherent generalizability and scalability even with relatively small training dataset. The nonlinearity-enbeding transformation for interaction function expands the expressive capability and make the potentials super-linear. The SUS2-MLIP outperforms the state-of-the-art MLIP models with its exceptional computational efficiency especially for multiple-element materials and physical scalability in property prediction. This work not only presents a highly-efficient universal MLIP model but also sheds light on incorporating physical constraints into artificial-intelligence-aided materials simulation.

## 人工智能(cs.AI:Artificial Intelligence)

### Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles 
[[arxiv](https://arxiv.org/abs/2502.08119)] [[cool](https://papers.cool/arxiv/2502.08119)] [[pdf](https://arxiv.org/pdf/2502.08119)]
> **Authors**: Jiahao You,Ziye Jia,Chao Dong,Qihui Wu,Zhu Han
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器人技术
- **Abstract**: The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios.

### Universal Adversarial Attack on Aligned Multimodal LLMs 
[[arxiv](https://arxiv.org/abs/2502.07987)] [[cool](https://papers.cool/arxiv/2502.07987)] [[pdf](https://arxiv.org/pdf/2502.07987)]
> **Authors**: Temurbek Rahmatullaev,Polina Druzhinina,Matvey Mikhalchuk,Andrey Kuznetsov,Anton Razzhigaev
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Added an affiliation
- **标题**: None
- **领域**: 人工智能
- **Abstract**: We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers.

### Deep Semantic Graph Learning via LLM based Node Enhancement 
[[arxiv](https://arxiv.org/abs/2502.07982)] [[cool](https://papers.cool/arxiv/2502.07982)] [[pdf](https://arxiv.org/pdf/2502.07982)]
> **Authors**: Chuanqi Shi,Yiyi Tao,Hang Zhang,Lun Wang,Shaoshuai Du,Yixian Shen,Yanxin Shen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models.

### Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders 
[[arxiv](https://arxiv.org/abs/2502.07957)] [[cool](https://papers.cool/arxiv/2502.07957)] [[pdf](https://arxiv.org/pdf/2502.07957)]
> **Authors**: Kshitish Ghate,Isaac Slaughter,Kyra Wilson,Mona Diab,Aylin Caliskan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted to NAACL Main, 2025
- **标题**: None
- **领域**: 人工智能
- **Abstract**: While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \leq r \leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline.

### Mathematical reasoning and the computer 
[[arxiv](https://arxiv.org/abs/2502.07850)] [[cool](https://papers.cool/arxiv/2502.07850)] [[pdf](https://arxiv.org/pdf/2502.07850)]
> **Authors**: Kevin Buzzard
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: This article was written in 2023 and is thus now rather out of date. Apologies for taking so long to upload to ArXiv
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Computers have already changed the way that humans do mathematics: they enable us to compute efficiently. But will they soon be helping us to reason? And will they one day start reasoning themselves? We give an overview of recent developments in neural networks, computer theorem provers and large language models.

### Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment 
[[arxiv](https://arxiv.org/abs/2502.07803)] [[cool](https://papers.cool/arxiv/2502.07803)] [[pdf](https://arxiv.org/pdf/2502.07803)]
> **Authors**: Cheryl Li,Tianyuan Xu,Yiwen Guo
> **First submission**: 2025-02-05
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations." This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.

### MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces 
[[arxiv](https://arxiv.org/abs/2502.07709)] [[cool](https://papers.cool/arxiv/2502.07709)] [[pdf](https://arxiv.org/pdf/2502.07709)]
> **Authors**: Loris Gaven,Thomas Carta,Clément Romac,Cédric Colas,Sylvain Lamprier,Olivier Sigaud,Pierre-Yves Oudeyer
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.

### Human Decision-making is Susceptible to AI-driven Manipulation 
[[arxiv](https://arxiv.org/abs/2502.07663)] [[cool](https://papers.cool/arxiv/2502.07663)] [[pdf](https://arxiv.org/pdf/2502.07663)]
> **Authors**: Sahand Sabour,June M. Liu,Siyang Liu,Chris Z. Yao,Shiyao Cui,Xuanming Zhang,Wen Zhang,Yaru Cao,Advait Bhat,Jian Guan,Wei Wu,Rada Mihalcea,Hongning Wang,Tim Althoff,Tatia M. C. Lee,Minlie Huang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Work in progress
- **标题**: None
- **领域**: 人工智能,计算语言学,计算机与社会,人机交互
- **Abstract**: Artificial Intelligence (AI) systems are increasingly intertwined with daily life, assisting users in executing various tasks and providing guidance on decision-making. This integration introduces risks of AI-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes. Through a randomized controlled trial with 233 participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts. Participants interacted with one of three AI agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit psychological tactics to reach its hidden objectives. By analyzing participants' decision patterns and shifts in their preference ratings post-interaction, we found significant susceptibility to AI-driven manipulation. Particularly, across both decision-making domains, participants interacting with the manipulative agents shifted toward harmful options at substantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA: 42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional, 12.8%). Notably, our findings reveal that even subtle manipulative objectives (MA) can be as effective as employing explicit psychological strategies (SEMA) in swaying human decision-making. By revealing the potential for covert AI influence, this study highlights a critical vulnerability in human-AI interactions, emphasizing the need for ethical safeguards and regulatory frameworks to ensure responsible deployment of AI technologies and protect human autonomy.

### SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07644)] [[cool](https://papers.cool/arxiv/2502.07644)] [[pdf](https://arxiv.org/pdf/2502.07644)]
> **Authors**: Shihao Xia,Mengting He,Shuai Shao,Tingting Yu,Yiying Zhang,Linhai Song
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 16 pages. arXiv admin note: text overlap with arXiv:2404.04306
- **标题**: None
- **领域**: 人工智能
- **Abstract**: To govern smart contracts running on Ethereum, multiple Ethereum Request for Comment (ERC) standards have been developed, each having a set of rules to guide the behaviors of smart contracts. Violating the ERC rules could cause serious security issues and financial loss, signifying the importance of verifying smart contracts follow ERCs. Today's practices of such verification are to manually audit each single contract, use expert-developed program-analysis tools, or use large language models (LLMs), all of which are far from effective in identifying ERC rule violations. This paper introduces SymGPT, a tool that combines the natural language understanding of large language models (LLMs) with the formal guarantees of symbolic execution to automatically verify smart contracts' compliance with ERC rules. To develop SymGPT, we conduct an empirical study of 132 ERC rules from three widely used ERC standards, examining their content, security implications, and natural language descriptions. Based on this study, we design SymGPT by first instructing an LLM to translate ERC rules into a defined EBNF grammar. We then synthesize constraints from the formalized rules to represent scenarios where violations may occur and use symbolic execution to detect them. Our evaluation shows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world contracts, including 1,375 violations with clear attack paths for stealing financial assets, demonstrating its effectiveness. Furthermore, SymGPT outperforms six automated techniques and a security-expert auditing service, underscoring its superiority over current smart contract analysis methods.

### NatureLM: Deciphering the Language of Nature for Scientific Discovery 
[[arxiv](https://arxiv.org/abs/2502.07527)] [[cool](https://papers.cool/arxiv/2502.07527)] [[pdf](https://arxiv.org/pdf/2502.07527)]
> **Authors**: Yingce Xia,Peiran Jin,Shufang Xie,Liang He,Chuan Cao,Renqian Luo,Guoqing Liu,Yue Wang,Zequn Liu,Yuan-Jyue Chen,Zekun Guo,Yeqi Bai,Pan Deng,Yaosen Min,Ziheng Lu,Hongxia Hao,Han Yang,Jielan Li,Chang Liu,Jia Zhang,Jianwei Zhu,Kehan Wu,Wei Zhang,Kaiyuan Gao,Qizhi Pei, et al. (20 additional authors not shown)
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 81 pages
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases.

### Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems 
[[arxiv](https://arxiv.org/abs/2502.07503)] [[cool](https://papers.cool/arxiv/2502.07503)] [[pdf](https://arxiv.org/pdf/2502.07503)]
> **Authors**: Ibrahim Alabdulmohsin,Xiaohua Zhai
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 18 pages, 9 figures
- **标题**: None
- **领域**: 人工智能,机器学习
- **Abstract**: Recent research in language modeling reveals two scaling effects: the well-known improvement from increased training compute, and a lesser-known boost from applying more sophisticated or computationally intensive inference methods. Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time. For a given fixed model architecture and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. These advantages are maintained even when compared to state-of-the-art recursive techniques like the "repeat-all-over" (RAO) strategy in Mobile LLM. Finally, stochastic RINS not only can enhance performance further but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation.

### Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames 
[[arxiv](https://arxiv.org/abs/2502.07443)] [[cool](https://papers.cool/arxiv/2502.07443)] [[pdf](https://arxiv.org/pdf/2502.07443)]
> **Authors**: Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算机科学与博弈论
- **Abstract**: LLM-driven multi-agent-based simulations have been gaining traction with applications in game-theoretic and social simulations. While most implementations seek to exploit or evaluate LLM-agentic reasoning, they often do so with a weak notion of agency and simplified architectures. We implement a role-based multi-agent strategic interaction framework tailored to sophisticated recursive reasoners, providing the means for systematic in-depth development and evaluation of strategic reasoning. Our game environment is governed by the umpire responsible for facilitating games, from matchmaking through move validation to environment management. Players incorporate state-of-the-art LLMs in their decision mechanism, relying on a formal hypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty contests to evaluate the recursive reasoning capabilities of the latest LLMs, providing a comparison to an established baseline model from economics and data from human experiments. Furthermore, we introduce the foundations of an alternative semantic measure of reasoning to the k-level theory. Our experiments show that artificial reasoners can outperform the baseline model in terms of both approximating human behaviour and reaching the optimal solution.

### Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation 
[[arxiv](https://arxiv.org/abs/2502.07423)] [[cool](https://papers.cool/arxiv/2502.07423)] [[pdf](https://arxiv.org/pdf/2502.07423)]
> **Authors**: Erik M. Lintunen,Nadia M. Ady,Sebastian Deterding,Christian Guckelsberger
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 6 pages excluding references
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Computational models offer powerful tools for formalising psychological theories, making them both testable and applicable in digital contexts. However, they remain little used in the study of motivation within psychology. We focus on the "need for competence", postulated as a key basic human need within Self-Determination Theory (SDT) -- arguably the most influential psychological framework for studying intrinsic motivation (IM). The need for competence is treated as a single construct across SDT texts. Yet, recent research has identified multiple, ambiguously defined facets of competence in SDT. We propose that these inconsistencies may be alleviated by drawing on computational models from the field of artificial intelligence, specifically from the domain of reinforcement learning (RL). By aligning the aforementioned facets of competence -- effectance, skill use, task performance, and capacity growth -- with existing RL formalisms, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly. The formalisms reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory. While our research lays a promising foundation, empirical studies of these models in both humans and machines are needed, inviting collaboration across disciplines.

### LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! 
[[arxiv](https://arxiv.org/abs/2502.07374)] [[cool](https://papers.cool/arxiv/2502.07374)] [[pdf](https://arxiv.org/pdf/2502.07374)]
> **Authors**: Dacheng Li,Shiyi Cao,Tyler Griggs,Shu Liu,Xiangxi Mo,Eric Tang,Sumanth Hegde,Kourosh Hakhamaneshi,Shishir G. Patil,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.

### KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems 
[[arxiv](https://arxiv.org/abs/2502.07350)] [[cool](https://papers.cool/arxiv/2502.07350)] [[pdf](https://arxiv.org/pdf/2502.07350)]
> **Authors**: Jusheng Zhang,Zimeng Huang,Yijia Fan,Ningyuan Liu,Mingyan Li,Zhuojie Yang,Jiawei Yao,Jian Wang,Keze Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能
- **Abstract**: As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination.

### Coarse Set Theory for AI Ethics and Decision-Making: A Mathematical Framework for Granular Evaluations 
[[arxiv](https://arxiv.org/abs/2502.07347)] [[cool](https://papers.cool/arxiv/2502.07347)] [[pdf](https://arxiv.org/pdf/2502.07347)]
> **Authors**: Takashi Izumo
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 25 pages, 2 figures
- **标题**: None
- **领域**: 人工智能,信息论,逻辑,可能性
- **Abstract**: In artificial intelligence (AI) and decision-making systems, structured approximations are essential for balancing model interpretability and predictive accuracy. Coarse Set Theory (CST) introduces a mathematical framework to formalize Coarse Ethics (CE), which models coarse-grained decision-making processes commonly used in human evaluations and AI classification systems. CST defines hierarchical relationships among sets using totally ordered structures and coarse mappings, enabling AI systems to adjust decision granularity dynamically. However, coarse evaluations inherently involve a trade-off between efficiency and information retention, as they simplify complex data representations at the cost of precision. To quantitatively assess this trade-off, we introduce Kullback-Leibler (KL) divergence as a measure of information loss in coarse evaluations, demonstrating the impact of coarse partitioning on decision accuracy. This study applies CST to grading systems, automated recommendations, and risk assessments, demonstrating its potential to enhance fairness, reduce bias, and improve transparency in AI-driven decision-making.

### When More is Less: Understanding Chain-of-Thought Length in LLMs 
[[arxiv](https://arxiv.org/abs/2502.07266)] [[cool](https://papers.cool/arxiv/2502.07266)] [[pdf](https://arxiv.org/pdf/2502.07266)]
> **Authors**: Yuyang Wu,Yifei Wang,Tianqi Du,Stefanie Jegelka,Yisen Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人工智能,计算语言学,机器学习
- **Abstract**: Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy? In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that longer reasoning processes are increasingly susceptible to noise. We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs.

## 硬件架构(cs.AR:Hardware Architecture)

### Column-wise Quantization of Weights and Partial Sums for Accurate and Efficient Compute-In-Memory Accelerators 
[[arxiv](https://arxiv.org/abs/2502.07842)] [[cool](https://papers.cool/arxiv/2502.07842)] [[pdf](https://arxiv.org/pdf/2502.07842)]
> **Authors**: Jiyoon Kim,Kang Eun Jeon,Yulhwa Kim,Jong Hwan Ko
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 硬件架构,人工智能,机器学习
- **Abstract**: Compute-in-memory (CIM) is an efficient method for implementing deep neural networks (DNNs) but suffers from substantial overhead from analog-to-digital converters (ADCs), especially as ADC precision increases. Low-precision ADCs can reduce this overhead but introduce partial-sum quantization errors degrading accuracy. Additionally, low-bit weight constraints, imposed by cell limitations and the need for multiple cells for higher-bit weights, present further challenges. While fine-grained partial-sum quantization has been studied to lower ADC resolution effectively, weight granularity, which limits overall partial-sum quantized accuracy, remains underexplored. This work addresses these challenges by aligning weight and partial-sum quantization granularities at the column-wise level. Our method improves accuracy while maintaining dequantization overhead, simplifies training by removing two-stage processes, and ensures robustness to memory cell variations via independent column-wise scale factors. We also propose an open-source CIM-oriented convolution framework to handle fine-grained weights and partial-sums efficiently, incorporating a novel tiling method and group convolution. Experimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18 (ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively, compared to the best-performing related works. Additionally, variation analysis reveals the robustness of our method against memory cell variations. These findings highlight the effectiveness of our quantization scheme in enhancing accuracy and robustness while maintaining hardware efficiency in CIM-based DNN implementations. Our code is available at https://github.com/jiyoonkm/ColumnQuant.

### MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures 
[[arxiv](https://arxiv.org/abs/2502.07834)] [[cool](https://papers.cool/arxiv/2502.07834)] [[pdf](https://arxiv.org/pdf/2502.07834)]
> **Authors**: Do Yeong Kang,Yeong Hwan Oh,Chanwook Hwang,Jinhee Kim,Kang Eun Jeon,Jong Hwan Ko
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: Accepted to appear at DATE 2025
- **标题**: None
- **领域**: 硬件架构,人工智能,机器学习
- **Abstract**: The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between highdimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency.

### Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs 
[[arxiv](https://arxiv.org/abs/2502.07823)] [[cool](https://papers.cool/arxiv/2502.07823)] [[pdf](https://arxiv.org/pdf/2502.07823)]
> **Authors**: Tousif Rahman,Gang Mao,Bob Pattison,Sidharth Maheshwari,Marcos Sartori,Adrian Wheeldon,Rishad Shafik,Alex Yakovlev
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: Accepted as a full paper by the 2025 EDGEAIFOUNDATION Austin
- **标题**: None
- **领域**: 硬件架构,人工智能,机器学习
- **Abstract**: Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugality. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference architecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on-chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The proposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application.

## 计算语言学(cs.CL:Computation and Language)

### HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses 
[[arxiv](https://arxiv.org/abs/2502.08109)] [[cool](https://papers.cool/arxiv/2502.08109)] [[pdf](https://arxiv.org/pdf/2502.08109)]
> **Authors**: Sujeong Lee,Hayoung Lee,Seongsoo Heo,Wonik Choi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 11 pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models.

### GCoT: Chain-of-Thought Prompt Learning for Graphs 
[[arxiv](https://arxiv.org/abs/2502.08092)] [[cool](https://papers.cool/arxiv/2502.08092)] [[pdf](https://arxiv.org/pdf/2502.08092)]
> **Authors**: Xingtong Yu,Chang Zhou,Zhongwei Kuai,Xinming Zhang,Yuan Fang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Under review
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.

### NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals 
[[arxiv](https://arxiv.org/abs/2502.08080)] [[cool](https://papers.cool/arxiv/2502.08080)] [[pdf](https://arxiv.org/pdf/2502.08080)]
> **Authors**: Neha Srikanth,Rachel Rudinger
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model's consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify critical atomic sub-problems of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the inferential consistency of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts.

### On Mechanistic Circuits for Extractive Question-Answering 
[[arxiv](https://arxiv.org/abs/2502.08059)] [[cool](https://papers.cool/arxiv/2502.08059)] [[pdf](https://arxiv.org/pdf/2502.08059)]
> **Authors**: Samyadeep Basu,Vlad Morariu,Zichao Wang,Ryan Rossi,Cherry Zhao,Soheil Feizi,Varun Manjunatha
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models are increasingly used to process documents and facilitate question-answering on them. In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information. We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass. Using this insight, we then introduce ATTNATTRIB, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution from ATTNATTRIB as an additional signal during the forward pass. Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering.

### Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs 
[[arxiv](https://arxiv.org/abs/2502.08045)] [[cool](https://papers.cool/arxiv/2502.08045)] [[pdf](https://arxiv.org/pdf/2502.08045)]
> **Authors**: Mohsinul Kabir,Ajwad Abrar,Sophia Ananiadou
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Preprint
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.

### Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery 
[[arxiv](https://arxiv.org/abs/2502.08037)] [[cool](https://papers.cool/arxiv/2502.08037)] [[pdf](https://arxiv.org/pdf/2502.08037)]
> **Authors**: Fan Jiang,Honglin Yu,Grace Chung,Trevor Cohn
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 33 pages
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\textit{Franken-Adapter}$, a modular language adaptation approach for decoder-only LLMs with embedding surgery. Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data. These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer. Our experiments on $\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of up to 20% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1%) in English. Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency. Additionally, we show the versatility of our method by achieving a 14% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc.

### Contextual Subspace Manifold Projection for Structural Refinement of Large Language Model Representations 
[[arxiv](https://arxiv.org/abs/2502.08026)] [[cool](https://papers.cool/arxiv/2502.08026)] [[pdf](https://arxiv.org/pdf/2502.08026)]
> **Authors**: Alistair Wren,Beatrice Loxley,Hamish Cadwallader,Simon Beckwith,Fabian Pargeter,James Blades
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Internal representations within deep neural architectures encode high-dimensional abstractions of linguistic structures, yet they often exhibit inefficiencies in feature distribution, limiting expressiveness and adaptability. Contextual Subspace Manifold Projection introduces a structured refinement technique that selectively reconfigures token embeddings through controlled subspace constraints, ensuring more stable and geometrically well-defined feature distributions. Empirical evaluations demonstrated that the structured intervention reduced anisotropy, leading to improved representation compactness while preserving semantic fidelity across transformer layers. Clustering analyses indicated that token embeddings exhibited greater feature separability, reinforcing the hypothesis that structured projection techniques enhance internal representation organization without sacrificing linguistic coherence. Gradient magnitude distributions suggested that the method introduced a smoother optimization trajectory, potentially contributing to more stable parameter updates throughout training. Computational overhead associated with the projection operations remained minimal, ensuring that the refinements did not introduce significant trade-offs in model efficiency or inference speed. Comparisons with standard embedding refinement techniques highlighted that structured manifold constraints provided a direct mechanism for improving representation quality without requiring additional gradient-based optimization. Perplexity evaluations confirmed that the adjustments did not negatively impact sequence coherence, further validating the effectiveness of the proposed approach.

### Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding 
[[arxiv](https://arxiv.org/abs/2502.08020)] [[cool](https://papers.cool/arxiv/2502.08020)] [[pdf](https://arxiv.org/pdf/2502.08020)]
> **Authors**: Ziyao Wang,Muneeza Azmart,Ang Li,Raya Horesh,Mikhail Yurochkin
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications

### The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models 
[[arxiv](https://arxiv.org/abs/2502.08009)] [[cool](https://papers.cool/arxiv/2502.08009)] [[pdf](https://arxiv.org/pdf/2502.08009)]
> **Authors**: Artem Kirsanov,Chi-Ning Chou,Kyunghyun Cho,SueYeon Chung
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: To appear in NAACL Findings 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.

### MetaSC: Test-Time Safety Specification Optimization for Language Models 
[[arxiv](https://arxiv.org/abs/2502.07985)] [[cool](https://papers.cool/arxiv/2502.07985)] [[pdf](https://arxiv.org/pdf/2502.07985)]
> **Authors**: Víctor Gallego
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .

### Training Sparse Mixture Of Experts Text Embedding Models 
[[arxiv](https://arxiv.org/abs/2502.07972)] [[cool](https://papers.cool/arxiv/2502.07972)] [[pdf](https://arxiv.org/pdf/2502.07972)]
> **Authors**: Zach Nussbaum,Brandon Duderstadt
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,信息检索
- **Abstract**: Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline at \href{https://github.com/nomic-ai/contrastors}{https://github.com/nomic-ai/contrastors}.

### Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature? 
[[arxiv](https://arxiv.org/abs/2502.07963)] [[cool](https://papers.cool/arxiv/2502.07963)] [[pdf](https://arxiv.org/pdf/2502.07963)]
> **Authors**: Hye Sun Yun,Karen Y. C. Zhang,Ramez Kouzy,Iain J. Marshall,Junyi Jessy Li,Byron C. Wallace
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 20 pages, 10 figures, 3 tables
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.

### Adapting Multilingual Embedding Models to Historical Luxembourgish 
[[arxiv](https://arxiv.org/abs/2502.07938)] [[cool](https://papers.cool/arxiv/2502.07938)] [[pdf](https://arxiv.org/pdf/2502.07938)]
> **Authors**: Andrianos Michail,Corina Julia Raclé,Juri Opitz,Simon Clematide
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The growing volume of digitized historical texts requires effective semantic search using text embeddings. However, pre-trained multilingual models, typically evaluated on contemporary texts, face challenges with historical digitized content due to OCR noise and outdated spellings. We explore the use of multilingual embeddings for cross-lingual semantic search on historical Luxembourgish, a low-resource language. We collect historical Luxembourgish news articles spanning various time periods and use GPT-4o to segment and translate them into closely related languages, creating 20,000 parallel training sentences per language pair. We further create a historical bitext mining evaluation set and find that these models struggle to perform cross-lingual search on historical Luxembourgish. To address this, we propose a simple adaptation method using in-domain training data, achieving up to 98\% accuracy in cross-lingual evaluations. We release our adapted models and historical Luxembourgish-German/French bitexts to support further research.

### Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning 
[[arxiv](https://arxiv.org/abs/2502.07912)] [[cool](https://papers.cool/arxiv/2502.07912)] [[pdf](https://arxiv.org/pdf/2502.07912)]
> **Authors**: Rujing Yao,Yang Wu,Chenghao Wang,Jingwei Xiong,Fang Wang,Xiaozhong Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods.

### Intelligent Legal Assistant: An Interactive Clarification System for Legal Question Answering 
[[arxiv](https://arxiv.org/abs/2502.07904)] [[cool](https://papers.cool/arxiv/2502.07904)] [[pdf](https://arxiv.org/pdf/2502.07904)]
> **Authors**: Rujing Yao,Yiquan Wu,Tong Zhang,Xuhui Zhang,Yuting Huang,Yang Wu,Jiayin Yang,Changlong Sun,Fang Wang,Xiaozhong Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The rise of large language models has opened new avenues for users seeking legal advice. However, users often lack professional legal knowledge, which can lead to questions that omit critical information. This deficiency makes it challenging for traditional legal question-answering systems to accurately identify users' actual needs, often resulting in imprecise or generalized advice. In this work, we develop a legal question-answering system called Intelligent Legal Assistant, which interacts with users to precisely capture their needs. When a user poses a question, the system requests that the user select their geographical location to pinpoint the applicable laws. It then generates clarifying questions and options based on the key information missing from the user's initial question. This allows the user to select and provide the necessary details. Once all necessary information is provided, the system produces an in-depth legal analysis encompassing three aspects: overall conclusion, jurisprudential analysis, and resolution suggestions.

### Auditing Prompt Caching in Language Model APIs 
[[arxiv](https://arxiv.org/abs/2502.07776)] [[cool](https://papers.cool/arxiv/2502.07776)] [[pdf](https://arxiv.org/pdf/2502.07776)]
> **Authors**: Chenchen Gu,Xiang Lisa Li,Rohith Kuditipudi,Percy Liang,Tatsunori Hashimoto
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 20 pages, 7 figures
- **标题**: None
- **领域**: 计算语言学,密码学和安全,机器学习
- **Abstract**: Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.

### Breaking Down Bias: On The Limits of Generalizable Pruning Strategies 
[[arxiv](https://arxiv.org/abs/2502.07771)] [[cool](https://papers.cool/arxiv/2502.07771)] [[pdf](https://arxiv.org/pdf/2502.07771)]
> **Authors**: Sibo Ma,Alejandro Salinas,Peter Henderson,Julian Nyarko
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 28 pages, 9 figures, 1 table
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会,机器学习
- **Abstract**: We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.

### An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa and Dynamic Contextual Positional Gating 
[[arxiv](https://arxiv.org/abs/2502.07755)] [[cool](https://papers.cool/arxiv/2502.07755)] [[pdf](https://arxiv.org/pdf/2502.07755)]
> **Authors**: Mohammad Ali Labbaf Khaniki,Sahabeh Saadati,Mohammad Manthouri
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This paper presents a novel Natural Language Processing (NLP) framework for enhancing medical diagnosis through the integration of advanced techniques in data augmentation, feature extraction, and classification. The proposed approach employs back-translation to generate diverse paraphrased datasets, improving robustness and mitigating overfitting in classification tasks. Leveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with Dynamic Contextual Positional Gating (DCPG), the model captures fine-grained contextual and positional relationships, dynamically adjusting the influence of positional information based on semantic context to produce high-quality text embeddings. For classification, an Attention-Based Feedforward Neural Network (ABFNN) is utilized, effectively focusing on the most relevant features to improve decision-making accuracy. Applied to the classification of symptoms, clinical notes, and other medical texts, this architecture demonstrates its ability to address the complexities of medical data. The combination of data augmentation, contextual embedding generation, and advanced classification mechanisms offers a robust and accurate diagnostic tool, with potential applications in automated medical diagnosis and clinical decision support. This method demonstrates the effectiveness of the proposed NLP framework for medical diagnosis, achieving remarkable results with an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only underscore the model's robust performance in classifying medical texts with exceptional precision and reliability but also highlight its superiority over existing methods, making it a highly promising tool for automated diagnostic systems.

### WHODUNIT: Evaluation benchmark for culprit detection in mystery stories 
[[arxiv](https://arxiv.org/abs/2502.07747)] [[cool](https://papers.cool/arxiv/2502.07747)] [[pdf](https://arxiv.org/pdf/2502.07747)]
> **Authors**: Kshitij Gupta
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to identify the perpetrator after reading and comprehending the story. To evaluate model robustness, we apply a range of character-level name augmentations, including original names, name swaps, and substitutions with well-known real and/or fictional entities from popular discourse. We further use various prompting styles to investigate the influence of prompting on deductive reasoning accuracy. We conduct evaluation study with state-of-the-art models, specifically GPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with majority response selection to ensure reliability. The results demonstrate that while LLMs perform reliably on unaltered texts, accuracy diminishes with certain name substitutions, particularly those with wide recognition. This dataset is publicly available here.

### Making Language Models Robust Against Negation 
[[arxiv](https://arxiv.org/abs/2502.07717)] [[cool](https://papers.cool/arxiv/2502.07717)] [[pdf](https://arxiv.org/pdf/2502.07717)]
> **Authors**: MohammadHossein Rezaei,Eduardo Blanco
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Negation has been a long-standing challenge for language models. Previous studies have shown that they struggle with negation in many natural language understanding tasks. In this work, we propose a self-supervised method to make language models more robust against negation. We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task. We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf versions on nine negation-related benchmarks. Most notably, our pre-training tasks yield between 1.8% and 9.1% improvement on CondaQA, a large question-answering corpus requiring reasoning over negation.

### Large Language Models as Proxies for Theories of Human Linguistic Cognition 
[[arxiv](https://arxiv.org/abs/2502.07687)] [[cool](https://papers.cool/arxiv/2502.07687)] [[pdf](https://arxiv.org/pdf/2502.07687)]
> **Authors**: Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: We consider the possible role of current large language models (LLMs) in the study of human linguistic cognition. We focus on the use of such models as proxies for theories of cognition that are relatively linguistically-neutral in their representations and learning but differ from current LLMs in key ways. We illustrate this potential use of LLMs as proxies for theories of cognition in the context of two kinds of questions: (a) whether the target theory accounts for the acquisition of a given pattern from a given corpus; and (b) whether the target theory makes a given typologically-attested pattern easier to acquire than another, typologically-unattested pattern. For each of the two questions we show, building on recent literature, how current LLMs can potentially be of help, but we note that at present this help is quite limited.

### Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered LLM Approach 
[[arxiv](https://arxiv.org/abs/2502.07677)] [[cool](https://papers.cool/arxiv/2502.07677)] [[pdf](https://arxiv.org/pdf/2502.07677)]
> **Authors**: Param Kulkarni,Yingchi Liu,Hao-Ming Fu,Shaohua Yang,Isuru Gunasekara,Matt Peloquin,Noah Spitzer-Williams,Xiaotian Zhou,Xiaozhong Liu,Zhengping Ji,Yasser Ibrahim
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Achieving a delicate balance between fostering trust in law enforcement and protecting the rights of both officers and civilians continues to emerge as a pressing research and product challenge in the world today. In the pursuit of fairness and transparency, this study presents an innovative AI-driven system designed to generate police report drafts from complex, noisy, and multi-role dialogue data. Our approach intelligently extracts key elements of law enforcement interactions and includes them in the draft, producing structured narratives that are not only high in quality but also reinforce accountability and procedural clarity. This framework holds the potential to transform the reporting process, ensuring greater oversight, consistency, and fairness in future policing practices. A demonstration video of our system can be accessed at https://drive.google.com/file/d/1kBrsGGR8e3B5xPSblrchRGj-Y-kpCHNO/view?usp=sharing

### FoQA: A Faroese Question-Answering Dataset 
[[arxiv](https://arxiv.org/abs/2502.07642)] [[cool](https://papers.cool/arxiv/2502.07642)] [[pdf](https://arxiv.org/pdf/2502.07642)]
> **Authors**: Annika Simonsen,Dan Saattrup Nielsen,Hafsteinn Einarsson
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Camera-ready version for RESOURCEFUL workshop, 2025
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: We present FoQA, a Faroese extractive question-answering (QA) dataset with 2,000 samples, created using a semi-automated approach combining Large Language Models (LLMs) and human validation. The dataset was generated from Faroese Wikipedia articles using GPT-4-turbo for initial QA generation, followed by question rephrasing to increase complexity and native speaker validation to ensure quality. We provide baseline performance metrics for FoQA across multiple models, including LLMs and BERT, demonstrating its effectiveness in evaluating Faroese QA performance. The dataset is released in three versions: a validated set of 2,000 samples, a complete set of all 10,001 generated samples, and a set of 2,395 rejected samples for error analysis.

### Tractable Transformers for Flexible Conditional Generation 
[[arxiv](https://arxiv.org/abs/2502.07616)] [[cool](https://papers.cool/arxiv/2502.07616)] [[pdf](https://arxiv.org/pdf/2502.07616)]
> **Authors**: Anji Liu,Xuejie Liu,Dayuan Zhao,Mathias Niepert,Yitao Liang,Guy Van den Broeck
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines.

### DPO-Shift: Shifting the Distribution of Direct Preference Optimization 
[[arxiv](https://arxiv.org/abs/2502.07599)] [[cool](https://papers.cool/arxiv/2502.07599)] [[pdf](https://arxiv.org/pdf/2502.07599)]
> **Authors**: Xiliang Yang,Feng Jiang,Qianen Zhang,Lei Zhao,Xiao Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.

### We Can't Understand AI Using our Existing Vocabulary 
[[arxiv](https://arxiv.org/abs/2502.07586)] [[cool](https://papers.cool/arxiv/2502.07586)] [[pdf](https://arxiv.org/pdf/2502.07586)]
> **Authors**: John Hewitt,Robert Geirhos,Been Kim
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Position paper
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.

### O1 Embedder: Let Retrievers Think Before Action 
[[arxiv](https://arxiv.org/abs/2502.07555)] [[cool](https://papers.cool/arxiv/2502.07555)] [[pdf](https://arxiv.org/pdf/2502.07555)]
> **Authors**: Ruiran Yan,Zheng Liu,Defu Lian
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: The growing power of large language models (LLMs) has revolutionized how people access and utilize information. Notably, the LLMs excel at performing fine-grained data representation, which facilitates precise retrieval of information. They also generate high-quality answers based on external references, enabling the production of useful knowledge. The recent introduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another leap forward, highlighting LLMs' ability to think progressively before delivering final answers. This breakthrough significantly improves the ability to address complex tasks, e.g., coding and math proofs. Inspired by this progress, we aim to develop similar capabilities for retrieval models, which hold great promise for tackling critical challenges in the field, including multi-task retrieval, zero-shot retrieval, and tasks requiring intensive reasoning of complex relationships. With this motivation, we propose a novel approach called O1 Embedder, which generates useful thoughts for the input query before making retrieval for the target documents. To realize this objective, we conquer two technical difficulties. First, we design a data synthesis workflow, creating training signals for O1 Embedder by generating initial thoughts from an LLM-expert and subsequently refining them using a retrieval committee. Second, we optimize the training process, enabling a pre-trained model to be jointly fine-tuned to generate retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning. Our approach is evaluated by comprehensive experiments, where substantial improvements are achieved across 12 popular datasets, spanning both in-domain and out-of-domain scenarios. These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models.

### Grammar Control in Dialogue Response Generation for Language Learning Chatbots 
[[arxiv](https://arxiv.org/abs/2502.07544)] [[cool](https://papers.cool/arxiv/2502.07544)] [[pdf](https://arxiv.org/pdf/2502.07544)]
> **Authors**: Dominik Glandorf,Peng Cui,Detmar Meurers,Mrinmaya Sachan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted to NAACL 2025
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Chatbots based on large language models offer cheap conversation practice opportunities for language learners. However, they are hard to control for linguistic forms that correspond to learners' current needs, such as grammar. We control grammar in chatbot conversation practice by grounding a dialogue response generation model in a pedagogical repository of grammar skills. We also explore how this control helps learners to produce specific grammar. We comprehensively evaluate prompting, fine-tuning, and decoding strategies for grammar-controlled dialogue response generation. Strategically decoding Llama3 outperforms GPT-3.5 when tolerating minor response quality losses. Our simulation predicts grammar-controlled responses to support grammar acquisition adapted to learner proficiency. Existing language learning chatbots and research on second language acquisition benefit from these affordances. Code available on GitHub.

### Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More 
[[arxiv](https://arxiv.org/abs/2502.07490)] [[cool](https://papers.cool/arxiv/2502.07490)] [[pdf](https://arxiv.org/pdf/2502.07490)]
> **Authors**: Xialie Zhuang,Zhikai Jia,Jianjin Li,Zhenyu Zhang,Li Shen,Zheng Cao,Shiwei Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 15 pages,7 figures
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.

### Multi-Agent Collaboration for Multilingual Code Instruction Tuning 
[[arxiv](https://arxiv.org/abs/2502.07487)] [[cool](https://papers.cool/arxiv/2502.07487)] [[pdf](https://arxiv.org/pdf/2502.07487)]
> **Authors**: Jian Yang,Wei Zhang,Jiaxi Yang,Yibo Miao,Shanghaoran Quan,Zhenhe Wu,Qiyao Peng,Liqun Yang,Tianyu Liu,Zeyu Cui,Binyuan Hui,Junyang Lin
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.

### PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian 
[[arxiv](https://arxiv.org/abs/2502.07459)] [[cool](https://papers.cool/arxiv/2502.07459)] [[pdf](https://arxiv.org/pdf/2502.07459)]
> **Authors**: Erfan Moosavi Monazzah,Vahid Rahimzadeh,Yadollah Yaghoobzadeh,Azadeh Shakery,Mohammad Taher Pilehvar
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted at NAACL 2025 Main Conference, the dataset is available on HuggingFace (see https://huggingface.co/datasets/teias-ai/percul)
- **标题**: None
- **领域**: 计算语言学,人工智能,计算机与社会
- **Abstract**: Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the use of translation as a shortcut. We evaluate several state-of-the-art multilingual and Persian-specific LLMs, establishing a foundation for future research in cross-cultural NLP evaluation. Our experiments demonstrate a 11.3% gap between best closed source model and layperson baseline while the gap increases to 21.3% by using the best open-weight model. You can access the dataset from here: https://huggingface.co/datasets/teias-ai/percul

### Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon 
[[arxiv](https://arxiv.org/abs/2502.07445)] [[cool](https://papers.cool/arxiv/2502.07445)] [[pdf](https://arxiv.org/pdf/2502.07445)]
> **Authors**: Nurit Cohen-Inger,Yehonatan Elisha,Bracha Shapira,Lior Rokach,Seffi Cohen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能,机器学习
- **Abstract**: Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.

### Hierarchical Document Parsing via Large Margin Feature Matching and Heuristics 
[[arxiv](https://arxiv.org/abs/2502.07442)] [[cool](https://papers.cool/arxiv/2502.07442)] [[pdf](https://arxiv.org/pdf/2502.07442)]
> **Authors**: Duong Anh Kiet
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: DocUI@AAAI-25, 2 pages, technical report
- **标题**: None
- **领域**: 计算语言学,计算机视觉和模式识别
- **Abstract**: We present our solution to the AAAI-25 VRD-IU challenge, achieving first place in the competition. Our approach integrates large margin loss for improved feature discrimination and employs heuristic rules to refine hierarchical relationships. By combining a deep learning-based matching strategy with greedy algorithms, we achieve a significant boost in accuracy while maintaining computational efficiency. Our method attains an accuracy of 0.98904 on the private leaderboard, demonstrating its effectiveness in document structure parsing. Source codes are publicly available at https://github.com/ffyyytt/VRUID-AAAI-DAKiet

### RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs 
[[arxiv](https://arxiv.org/abs/2502.07424)] [[cool](https://papers.cool/arxiv/2502.07424)] [[pdf](https://arxiv.org/pdf/2502.07424)]
> **Authors**: Alan Saji,Jaavid Aktar Husain,Thanmay Jayakumar,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 19 pages, 19 figures
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Large Language Models (LLMs) exhibit remarkable multilingual generalization despite being predominantly trained on English-centric corpora. A fundamental question arises: how do LLMs achieve such robust multilingual capabilities? We take the case of non-Roman script languages, we investigate the role of Romanization - the representation of non-Roman scripts using Roman characters - as a bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in Romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and Romanized scripts, suggesting a shared underlying representation. Additionally, for translation into non-Roman script languages, our findings reveal that when the target language is in Romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of Romanization in facilitating language transfer.

### Entity Linking using LLMs for Automated Product Carbon Footprint Estimation 
[[arxiv](https://arxiv.org/abs/2502.07418)] [[cool](https://papers.cool/arxiv/2502.07418)] [[pdf](https://arxiv.org/pdf/2502.07418)]
> **Authors**: Steffen Castle,Julian Moreno Schneider,Leonhard Hennig,Georg Rehm
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: ef:Proceedings of The 1st Workshop on Ecology, Environment, and NaturalLanguageProcessing (2025)
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Growing concerns about climate change and sustainability are driving manufacturers to take significant steps toward reducing their carbon footprints. For these manufacturers, a first step towards this goal is to identify the environmental impact of the individual components of their products. We propose a system leveraging large language models (LLMs) to automatically map components from manufacturer Bills of Materials (BOMs) to Life Cycle Assessment (LCA) database entries by using LLMs to expand on available component information. Our approach reduces the need for manual data processing, paving the way for more accessible sustainability practices.

### Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation Generation 
[[arxiv](https://arxiv.org/abs/2502.07391)] [[cool](https://papers.cool/arxiv/2502.07391)] [[pdf](https://arxiv.org/pdf/2502.07391)]
> **Authors**: Palaash Goel,Dushyant Singh Chauhan,Md Shad Akhtar
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn model, aka. TURBO. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. TURBO assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed TURBO model on the MORE+ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of TURBO by an average margin of $+3.3\%$. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on TURBO's generated explanations and find them out to be comparatively better than other systems.

### LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation 
[[arxiv](https://arxiv.org/abs/2502.07365)] [[cool](https://papers.cool/arxiv/2502.07365)] [[pdf](https://arxiv.org/pdf/2502.07365)]
> **Authors**: Zican Dong,Junyi Li,Jinhao Jiang,Mingyu Xu,Wayne Xin Zhao,Bingning Wang,Weipeng Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.

### Bridging the Evaluation Gap: Leveraging Large Language Models for Topic Model Evaluation 
[[arxiv](https://arxiv.org/abs/2502.07352)] [[cool](https://papers.cool/arxiv/2502.07352)] [[pdf](https://arxiv.org/pdf/2502.07352)]
> **Authors**: Zhiyin Tan,Jennifer D'Souza
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: accepted by IRCDL 2025
- **标题**: None
- **领域**: 计算语言学,人工智能,数字图书馆
- **Abstract**: This study presents a framework for automated evaluation of dynamically evolving topic taxonomies in scientific literature using Large Language Models (LLMs). In digital library systems, topic modeling plays a crucial role in efficiently organizing and retrieving scholarly content, guiding researchers through complex knowledge landscapes. As research domains proliferate and shift, traditional human centric and static evaluation methods struggle to maintain relevance. The proposed approach harnesses LLMs to measure key quality dimensions, such as coherence, repetitiveness, diversity, and topic-document alignment, without heavy reliance on expert annotators or narrow statistical metrics. Tailored prompts guide LLM assessments, ensuring consistent and interpretable evaluations across various datasets and modeling techniques. Experiments on benchmark corpora demonstrate the method's robustness, scalability, and adaptability, underscoring its value as a more holistic and dynamic alternative to conventional evaluation strategies.

### BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07346)] [[cool](https://papers.cool/arxiv/2502.07346)] [[pdf](https://arxiv.org/pdf/2502.07346)]
> **Authors**: Xu Huang,Wenhao Zhu,Hanxu Hu,Conghui He,Lei Li,Shujian Huang,Fei Yuan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学
- **Abstract**: Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.

### Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering 
[[arxiv](https://arxiv.org/abs/2502.07340)] [[cool](https://papers.cool/arxiv/2502.07340)] [[pdf](https://arxiv.org/pdf/2502.07340)]
> **Authors**: Shuzheng Si,Haozhe Zhao,Gang Chen,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Kaikai An,Kangyang Luo,Chen Qian,Fanchao Qi,Baobao Chang,Maosong Sun
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less.

### MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs 
[[arxiv](https://arxiv.org/abs/2502.07322)] [[cool](https://papers.cool/arxiv/2502.07322)] [[pdf](https://arxiv.org/pdf/2502.07322)]
> **Authors**: Zilu Dong,Xiangqing Shen,Rui Xia
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 9 pages
- **标题**: None
- **领域**: 计算语言学,机器学习
- **Abstract**: As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover a critical limitation that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals that the root cause lies in MEMIT's key value modeling framework: When multiple facts with the same subject in a batch are modeled through MEMIT's key value mechanism, identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in updates conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in same-subject batch editing scenarios. Experimental results demonstrate that when MEMIT's edit success rate drops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate exceeding 90%, showcasing remarkable robustness to subject entity collisions.

### CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction 
[[arxiv](https://arxiv.org/abs/2502.07316)] [[cool](https://papers.cool/arxiv/2502.07316)] [[pdf](https://arxiv.org/pdf/2502.07316)]
> **Authors**: Junlong Li,Daya Guo,Dejian Yang,Runxin Xu,Yu Wu,Junxian He
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.

### Small Language Model Makes an Effective Long Text Extractor 
[[arxiv](https://arxiv.org/abs/2502.07286)] [[cool](https://papers.cool/arxiv/2502.07286)] [[pdf](https://arxiv.org/pdf/2502.07286)]
> **Authors**: Yelin Chen,Fanjin Zhang,Jie Tang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: AAAI'25, 9 pages, 1 appendix pages
- **标题**: None
- **领域**: 计算语言学,人工智能
- **Abstract**: Named Entity Recognition (NER) is a fundamental problem in natural language processing (NLP). However, the task of extracting longer entity spans (e.g., awards) from extended texts (e.g., homepages) is barely explored. Current NER methods predominantly fall into two categories: span-based methods and generation-based methods. Span-based methods require the enumeration of all possible token-pair spans, followed by classification on each span, resulting in substantial redundant computations and excessive GPU memory usage. In contrast, generation-based methods involve prompting or fine-tuning large language models (LLMs) to adapt to downstream NER tasks. However, these methods struggle with the accurate generation of longer spans and often incur significant time costs for effective fine-tuning. To address these challenges, this paper introduces a lightweight span-based NER method called SeNER, which incorporates a bidirectional arrow attention mechanism coupled with LogN-Scaling on the [CLS] token to embed long texts effectively, and comprises a novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to reduce redundant candidate token-pair spans significantly and model interactions between token-pair spans simultaneously. Extensive experiments demonstrate that our method achieves state-of-the-art extraction accuracy on three long NER datasets and is capable of extracting entities from long texts in a GPU-memory-friendly manner. Code: https://github.com/THUDM/scholar-profiling/tree/main/sener

### GENERator: A Long-Context Generative Genomic Foundation Model 
[[arxiv](https://arxiv.org/abs/2502.07272)] [[cool](https://papers.cool/arxiv/2502.07272)] [[pdf](https://arxiv.org/pdf/2502.07272)]
> **Authors**: Wei Wu,Qiuyi Li,Mingyang Li,Kun Fu,Fuli Feng,Jieping Ye,Hui Xiong,Zheng Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算语言学,基因组学
- **Abstract**: Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of promoter sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions.

## 密码学和安全(cs.CR:Cryptography and Security)

### SLVR: Securely Leveraging Client Validation for Robust Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.08055)] [[cool](https://papers.cool/arxiv/2502.08055)] [[pdf](https://arxiv.org/pdf/2502.08055)]
> **Authors**: Jihye Choi,Sai Rahul Rachuri,Ke Wang,Somesh Jha,Yizhen Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 29 pages
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Federated Learning (FL) enables collaborative model training while keeping client data private. However, exposing individual client updates makes FL vulnerable to reconstruction attacks. Secure aggregation mitigates such privacy risks but prevents the server from verifying the validity of each client update, creating a privacy-robustness tradeoff. Recent efforts attempt to address this tradeoff by enforcing checks on client updates using zero-knowledge proofs, but they support limited predicates and often depend on public validation data. We propose SLVR, a general framework that securely leverages clients' private data through secure multi-party computation. By utilizing clients' data, SLVR not only eliminates the need for public validation data, but also enables a wider range of checks for robustness, including cross-client accuracy validation. It also adapts naturally to distribution shifts in client data as it can securely refresh its validation data up-to-date. Our empirical evaluations show that SLVR improves robustness against model poisoning attacks, particularly outperforming existing methods by up to 50% under adaptive attacks. Additionally, SLVR demonstrates effective adaptability and stable convergence under various distribution shift scenarios.

### Unveiling Client Privacy Leakage from Public Dataset Usage in Federated Distillation 
[[arxiv](https://arxiv.org/abs/2502.08001)] [[cool](https://papers.cool/arxiv/2502.08001)] [[pdf](https://arxiv.org/pdf/2502.08001)]
> **Authors**: Haonan Shi,Tu Ouyang,An Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 14 pages, 10 figures
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Federated Distillation (FD) has emerged as a popular federated training framework, enabling clients to collaboratively train models without sharing private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which leverages public datasets for knowledge sharing, has become widely adopted. Although PDA-FD enhances privacy compared to traditional Federated Learning, we demonstrate that the use of public datasets still poses significant privacy risks to clients' private training data. This paper presents the first comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious server. We show that the server can exploit clients' inference results on public datasets to extract two critical types of private information: label distributions and membership information of the private training dataset. To quantify these vulnerabilities, we introduce two novel attacks specifically designed for the PDA-FD setting: a label distribution inference attack and innovative membership inference methods based on Likelihood Ratio Attack (LiRA). Through extensive evaluation of three representative PDA-FD frameworks (FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance, with label distribution attacks reaching minimal KL-divergence and membership inference attacks maintaining high True Positive Rates under low False Positive Rate constraints. Our findings reveal significant privacy risks in current PDA-FD frameworks and emphasize the need for more robust privacy protection mechanisms in collaborative learning systems.

### Decoding Complexity: Intelligent Pattern Exploration with CHPDA (Context Aware Hybrid Pattern Detection Algorithm) 
[[arxiv](https://arxiv.org/abs/2502.07815)] [[cool](https://papers.cool/arxiv/2502.07815)] [[pdf](https://arxiv.org/pdf/2502.07815)]
> **Authors**: Lokesh Koli,Shubham Kalra,Karanpreet Singh
> **First submission**: 2025-02-09
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: Detecting sensitive data such as Personally Identifiable Information (PII) and Protected Health Information (PHI) is critical for data security platforms. This study evaluates regex-based pattern matching algorithms and exact-match search techniques to optimize detection speed, accuracy, and scalability. Our benchmarking results indicate that Google RE2 provides the best balance of speed (10-15 ms/MB), memory efficiency (8-16 MB), and accuracy (99.5%) among regex engines, outperforming PCRE while maintaining broader hardware compatibility than Hyperscan. For exact matching, Aho-Corasick demonstrated superior performance (8 ms/MB) and scalability for large datasets. Performance analysis revealed that regex processing time scales linearly with dataset size and pattern complexity. A hybrid AI + Regex approach achieved the highest F1 score (91. 6%) by improving recall and minimizing false positives. Device benchmarking confirmed that our solution maintains efficient CPU and memory usage on both high-performance and mid-range systems. Despite its effectiveness, challenges remain, such as limited multilingual support and the need for regular pattern updates. Future work should focus on expanding language coverage, integrating data security and privacy management (DSPM) with data loss prevention (DLP) tools, and enhancing regulatory compliance for broader global adoption.

### CryptoX : Compositional Reasoning Evaluation of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07813)] [[cool](https://papers.cool/arxiv/2502.07813)] [[pdf](https://arxiv.org/pdf/2502.07813)]
> **Authors**: Jiajun Shi,Chaoren Wei,Liqun Yang,Zekun Moore Wang,Chenghao Yang,Ge Zhang,Stephen Huang,Tao Peng,Jian Yang,Zhoufutu Wen
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能
- **Abstract**: The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs.

### CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception 
[[arxiv](https://arxiv.org/abs/2502.07807)] [[cool](https://papers.cool/arxiv/2502.07807)] [[pdf](https://arxiv.org/pdf/2502.07807)]
> **Authors**: Senkang Hu,Yihang Tao,Zihan Fang,Guowen Xu,Yiqin Deng,Sam Kwong,Yuguang Fang
> **First submission**: 2025-02-07
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 密码学和安全,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple vehicles to share sensing information to enhance perception performance. However, compared with single-vehicle perception, the openness of a CP system makes it more vulnerable to malicious attacks that can inject malicious information to mislead the perception of an ego vehicle, resulting in severe risks for safe driving. To mitigate such vulnerability, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and demonstrate the superiority of CP-Guard+.

### Scalable Fingerprinting of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07760)] [[cool](https://papers.cool/arxiv/2502.07760)] [[pdf](https://arxiv.org/pdf/2502.07760)]
> **Authors**: Anshul Nasery,Jonathan Hayase,Creston Brooks,Peiyao Sheng,Himanshu Tyagi,Pramod Viswanath,Sewoong Oh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 23 pages 15 figures
- **标题**: None
- **领域**: 密码学和安全,机器学习
- **Abstract**: Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. However, to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that {\em scalability} is critical, i.e., scaling up the number of fingerprints one can embed into a model. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two orders of magnitude more than existing schemes -- without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks.

### RoMA: Robust Malware Attribution via Byte-level Adversarial Training with Global Perturbations and Adversarial Consistency Regularization 
[[arxiv](https://arxiv.org/abs/2502.07492)] [[cool](https://papers.cool/arxiv/2502.07492)] [[pdf](https://arxiv.org/pdf/2502.07492)]
> **Authors**: Yuxia Sun,Huihong Chen,Jingcai Guo,Aoxiang Sun,Zhetao Li,Haolin Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 密码学和安全,计算机视觉和模式识别
- **Abstract**: Attributing APT (Advanced Persistent Threat) malware to their respective groups is crucial for threat intelligence and cybersecurity. However, APT adversaries often conceal their identities, rendering attribution inherently adversarial. Existing machine learning-based attribution models, while effective, remain highly vulnerable to adversarial attacks. For example, the state-of-the-art byte-level model MalConv sees its accuracy drop from over 90% to below 2% under PGD (projected gradient descent) attacks. Existing gradient-based adversarial training techniques for malware detection or image processing were applied to malware attribution in this study, revealing that both robustness and training efficiency require significant improvement. To address this, we propose RoMA, a novel single-step adversarial training approach that integrates global perturbations to generate enhanced adversarial samples and employs adversarial consistency regularization to improve representation quality and resilience. A novel APT malware dataset named AMG18, with diverse samples and realistic class imbalances, is introduced for evaluation. Extensive experiments show that RoMA significantly outperforms seven competing methods in both adversarial robustness (e.g., achieving over 80% robust accuracy-more than twice that of the next-best method under PGD attacks) and training efficiency (e.g., more than twice as fast as the second-best method in terms of accuracy), while maintaining superior standard accuracy in non-adversarial scenarios.

## 计算机视觉和模式识别(cs.CV:Computer Vision and Pattern Recognition)

### MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models 
[[arxiv](https://arxiv.org/abs/2502.08079)] [[cool](https://papers.cool/arxiv/2502.08079)] [[pdf](https://arxiv.org/pdf/2502.08079)]
> **Authors**: Peng-Fei Zhang,Guangdong Bai,Zi Huang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain.

### From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis 
[[arxiv](https://arxiv.org/abs/2502.08025)] [[cool](https://papers.cool/arxiv/2502.08025)] [[pdf](https://arxiv.org/pdf/2502.08025)]
> **Authors**: Kristofer Grover Roos,Atsushi Fukuda,Quan Huu Cap
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: While functional magnetic resonance imaging (fMRI) offers rich spatial resolution, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial resolution necessary for precise neural localization. To bridge these gaps, we introduce E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is specifically designed to capture and translate meaningful features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three datasets demonstrate that E2fNet consistently outperforms existing methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). Our findings suggest that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at https://github.com/kgr20/E2fNet.

### Federated Self-supervised Domain Generalization for Label-efficient Polyp Segmentation 
[[arxiv](https://arxiv.org/abs/2502.07951)] [[cool](https://papers.cool/arxiv/2502.07951)] [[pdf](https://arxiv.org/pdf/2502.07951)]
> **Authors**: Xinyi Tan,Jiacheng Wang,Liansheng Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted at ADSMI @ MICCAI 2024
- **标题**: None
- **领域**: 计算机视觉和模式识别,分布式、并行和集群计算,机器学习
- **Abstract**: Employing self-supervised learning (SSL) methodologies assumes par-amount significance in handling unlabeled polyp datasets when building deep learning-based automatic polyp segmentation models. However, the intricate privacy dynamics surrounding medical data often preclude seamless data sharing among disparate medical centers. Federated learning (FL) emerges as a formidable solution to this privacy conundrum, yet within the realm of FL, optimizing model generalization stands as a pressing imperative. Robust generalization capabilities are imperative to ensure the model's efficacy across diverse geographical domains post-training on localized client datasets. In this paper, a Federated self-supervised Domain Generalization method is proposed to enhance the generalization capacity of federated and Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical SSL method, DropPos, LFDG proposes an adversarial learning-based data augmentation method (SSADA) to enhance the data diversity. LFDG further proposes a relaxation module based on Source-reconstruction and Augmentation-masking (SRAM) to maintain stability in feature learning. We have validated LFDG on polyp images from six medical centers. The performance of our method achieves 3.80% and 3.92% better than the baseline and other recent FL methods and SSL methods, respectively.

### SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion 
[[arxiv](https://arxiv.org/abs/2502.07945)] [[cool](https://papers.cool/arxiv/2502.07945)] [[pdf](https://arxiv.org/pdf/2502.07945)]
> **Authors**: Yannik Frisch,Ssharvien Kumar Sivakumar,Çağhan Köksal,Elsa Böhm,Felix Wagner,Adrian Gericke,Ghazal Ghazaei,Anirban Mukhopadhyay
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Surgical simulation offers a promising addition to conventional surgical training. However, available simulation tools lack photorealism and rely on hardcoded behaviour. Denoising Diffusion Models are a promising alternative for high-fidelity image synthesis, but existing state-of-the-art conditioning methods fall short in providing precise control or interactivity over the generated scenes. We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for controllable surgical scene synthesis by leveraging Scene Graphs. These graphs encode a surgical scene's components' spatial and semantic information, which are then translated into an intermediate representation using our novel pre-training step that explicitly captures local and global information. Our proposed method improves the fidelity of generated images and their coherence with the graph input over the state-of-the-art. Further, we demonstrate the simulation's realism and controllability in a user assessment study involving clinical experts. Scene Graphs can be effectively used for precise and interactive conditioning of Denoising Diffusion Models for simulating surgical scenes, enabling high fidelity and interactive control over the generated content.

### DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation Vulnerabilities 
[[arxiv](https://arxiv.org/abs/2502.07905)] [[cool](https://papers.cool/arxiv/2502.07905)] [[pdf](https://arxiv.org/pdf/2502.07905)]
> **Authors**: Chashi Mahiul Islam,Samuel Jacob Chacko,Preston Horne,Xiuwen Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 19 pages, 4 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Multimodal Large Language Models (MLLMs) represent the cutting edge of AI technology, with DeepSeek models emerging as a leading open-source alternative offering competitive performance to closed-source systems. While these models demonstrate remarkable capabilities, their vision-language integration mechanisms introduce specific vulnerabilities. We implement an adapted embedding manipulation attack on DeepSeek Janus that induces targeted visual hallucinations through systematic optimization of image embeddings. Through extensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve hallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM > 0.88) of the manipulated images on open-ended questions. Our analysis demonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to these attacks, with closed-form evaluation showing consistently higher hallucination rates compared to open-ended questioning. We introduce a novel multi-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for robust evaluation. The implications of these findings are particularly concerning given DeepSeek's open-source nature and widespread deployment potential. This research emphasizes the critical need for embedding-level security measures in MLLM deployment pipelines and contributes to the broader discussion of responsible AI implementation.

### MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers 
[[arxiv](https://arxiv.org/abs/2502.07856)] [[cool](https://papers.cool/arxiv/2502.07856)] [[pdf](https://arxiv.org/pdf/2502.07856)]
> **Authors**: Ao Li,Wei Fang,Hongbo Zhao,Le Lu,Ge Yang,Minfeng Xu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted by ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.

### Vision-Language Models for Edge Networks: A Comprehensive Survey 
[[arxiv](https://arxiv.org/abs/2502.07855)] [[cool](https://papers.cool/arxiv/2502.07855)] [[pdf](https://arxiv.org/pdf/2502.07855)]
> **Authors**: Ahmed Sharshar,Latif U. Khan,Waseem Ullah,Mohsen Guizani
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings.

### Technical note on calibrating vision-language models under covariate shift 
[[arxiv](https://arxiv.org/abs/2502.07847)] [[cool](https://papers.cool/arxiv/2502.07847)] [[pdf](https://arxiv.org/pdf/2502.07847)]
> **Authors**: Behraj Khan,Rizwan Qureshi,Tahir Syed
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Despite being a successful example of emerging capability, vision-language foundation models for low-shot vision classification have a limited ability to sufficiently generalize to the target data distribution due to sample poverty, leading to sensitivity to variations in the data. A popular mitigation strategy is finetuning over multiple datasets, but domain generalization is expensive when practiced in this manner. This work examines both covariate shift between pre-training data and the underspecified target data, and \textit{confidence misalignment}, where the model's prediction confidence amplified by the limited data availability. We propose \textit{Confidence-Calibrated Covariate Shift Correction ($C3SC$)}, a unified framework to mitigate both covariate shift and confidence misalignment. $C3SC$ leverages Fisher information penalty for covariate shift correction and confidence misalignment penalty (CMP) to lower confidence on misclassified examples. Experimental results across various vision and covariate shift datasets demonstrates that $C3SC$ significantly improves in calibration (ECE) by $5.82\%$ at maximum. $C3SC$ shows better robustness as well by showing $3.5\%$ improvement in accuracy metric on challenging covariate shift datasets, making $C3SC$ a promising solution for reliable real-world vision-language low-shot applications under distribution shift.

### NanoVLMs: How small can we go and still make coherent Vision Language Models? 
[[arxiv](https://arxiv.org/abs/2502.07838)] [[cool](https://papers.cool/arxiv/2502.07838)] [[pdf](https://arxiv.org/pdf/2502.07838)]
> **Authors**: Mukund Agarwalla,Himanshu Kumar,Raj Dandekar,Rajat Dandekar,Sreedath Panat
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: 11 pages, 8 figures, 3 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled-down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments.

### Captured by Captions: On Memorization and its Mitigation in CLIP Models 
[[arxiv](https://arxiv.org/abs/2502.07830)] [[cool](https://papers.cool/arxiv/2502.07830)] [[pdf](https://arxiv.org/pdf/2502.07830)]
> **Authors**: Wenhao Wang,Adam Dziedzic,Grace C. Kim,Michael Backes,Franziska Boenisch
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: Accepted at ICLR 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习
- **Abstract**: Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with "mis-captioned" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.

### Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing 
[[arxiv](https://arxiv.org/abs/2502.07829)] [[cool](https://papers.cool/arxiv/2502.07829)] [[pdf](https://arxiv.org/pdf/2502.07829)]
> **Authors**: Sihao Wu,Xiaonan Si,Chi Xing,Jianhong Wang,Gaojie Jin,Guangliang Cheng,Lijun Zhang,Xiaowei Huang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: The integration of preference alignment with diffusion models (DMs) has emerged as a transformative approach to enhance image generation and editing capabilities. Although integrating diffusion models with preference alignment strategies poses significant challenges for novices at this intersection, comprehensive and systematic reviews of this subject are still notably lacking. To bridge this gap, this paper extensively surveys preference alignment with diffusion models in image generation and editing. First, we systematically review cutting-edge optimization techniques such as reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), and others, highlighting their pivotal role in aligning preferences with DMs. Then, we thoroughly explore the applications of aligning preferences with DMs in autonomous driving, medical imaging, robotics, and more. Finally, we comprehensively discuss the challenges of preference alignment with DMs. To our knowledge, this is the first survey centered on preference alignment with DMs, providing insights to drive future innovation in this dynamic area.

### Deep Learning in Automated Power Line Inspection: A Review 
[[arxiv](https://arxiv.org/abs/2502.07826)] [[cool](https://papers.cool/arxiv/2502.07826)] [[pdf](https://arxiv.org/pdf/2502.07826)]
> **Authors**: Md. Ahasan Atick Faisal,Imene Mecheter,Yazan Qiblawey,Javier Hernandez Fernandez,Muhammad E. H. Chowdhury,Serkan Kiranyaz
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: 40 pages, 12 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,图像和视频处理
- **Abstract**: In recent years, power line maintenance has seen a paradigm shift by moving towards computer vision-powered automated inspection. The utilization of an extensive collection of videos and images has become essential for maintaining the reliability, safety, and sustainability of electricity transmission. A significant focus on applying deep learning techniques for enhancing power line inspection processes has been observed in recent research. A comprehensive review of existing studies has been conducted in this paper, to aid researchers and industries in developing improved deep learning-based systems for analyzing power line data. The conventional steps of data analysis in power line inspections have been examined, and the body of current research has been systematically categorized into two main areas: the detection of components and the diagnosis of faults. A detailed summary of the diverse methods and techniques employed in these areas has been encapsulated, providing insights into their functionality and use cases. Special attention has been given to the exploration of deep learning-based methodologies for the analysis of power line inspection data, with an exposition of their fundamental principles and practical applications. Moreover, a vision for future research directions has been outlined, highlighting the need for advancements such as edge-cloud collaboration, and multi-modal analysis among others. Thus, this paper serves as a comprehensive resource for researchers delving into deep learning for power line analysis, illuminating the extent of current knowledge and the potential areas for future investigation.

### Pre-Trained Video Generative Models as World Simulators 
[[arxiv](https://arxiv.org/abs/2502.07825)] [[cool](https://papers.cool/arxiv/2502.07825)] [[pdf](https://arxiv.org/pdf/2502.07825)]
> **Authors**: Haoran He,Yang Zhang,Liang Lin,Zhongwen Xu,Ling Pan
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: 20 pages
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Video generative models pre-trained on large-scale internet datasets have achieved remarkable success, excelling at producing realistic synthetic videos. However, they often generate clips based on static prompts (e.g., text or images), limiting their ability to model interactive and dynamic scenarios. In this paper, we propose Dynamic World Simulation (DWS), a novel approach to transform pre-trained video generative models into controllable world simulators capable of executing specified action trajectories. To achieve precise alignment between conditioned actions and generated visual changes, we introduce a lightweight, universal action-conditioned module that seamlessly integrates into any existing model. Instead of focusing on complex visual details, we demonstrate that consistent dynamic transition modeling is the key to building powerful world simulators. Building upon this insight, we further introduce a motion-reinforced loss that enhances action controllability by compelling the model to capture dynamic changes more effectively. Experiments demonstrate that DWS can be versatilely applied to both diffusion and autoregressive transformer models, achieving significant improvements in generating action-controllable, dynamically consistent videos across games and robotics domains. Moreover, to facilitate the applications of the learned world simulator in downstream tasks such as model-based reinforcement learning, we propose prioritized imagination to improve sample efficiency, demonstrating competitive performance compared with state-of-the-art methods.

### Unpaired Image Dehazing via Kolmogorov-Arnold Transformation of Latent Features 
[[arxiv](https://arxiv.org/abs/2502.07812)] [[cool](https://papers.cool/arxiv/2502.07812)] [[pdf](https://arxiv.org/pdf/2502.07812)]
> **Authors**: Le-Anh Tran
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-12
> **comment**: 11 pages, 8 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This paper proposes an innovative framework for Unsupervised Image Dehazing via Kolmogorov-Arnold Transformation, termed UID-KAT. Image dehazing is recognized as a challenging and ill-posed vision task that requires complex transformations and interpretations in the feature space. Recent advancements have introduced Kolmogorov-Arnold Networks (KANs), inspired by the Kolmogorov-Arnold representation theorem, as promising alternatives to Multi-Layer Perceptrons (MLPs) since KANs can leverage their polynomial foundation to more efficiently approximate complex functions while requiring fewer layers than MLPs. Motivated by this potential, this paper explores the use of KANs combined with adversarial training and contrastive learning to model the intricate relationship between hazy and clear images. Adversarial training is employed due to its capacity in producing high-fidelity images, and contrastive learning promotes the model's emphasis on significant features while suppressing the influence of irrelevant information. The proposed UID-KAT framework is trained in an unsupervised setting to take advantage of the abundance of real-world data and address the challenge of preparing paired hazy/clean images. Experimental results show that UID-KAT achieves state-of-the-art dehazing performance across multiple datasets and scenarios, outperforming existing unpaired methods while reducing model complexity. The source code for this work is publicly available at https://github.com/tranleanh/uid-kat.

### Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts 
[[arxiv](https://arxiv.org/abs/2502.07802)] [[cool](https://papers.cool/arxiv/2502.07802)] [[pdf](https://arxiv.org/pdf/2502.07802)]
> **Authors**: Feng Liang,Haoyu Ma,Zecheng He,Tingbo Hou,Ji Hou,Kunpeng Li,Xiaoliang Dai,Felix Juefei-Xu,Samaneh Azadi,Animesh Sinha,Peizhao Zhang,Peter Vajda,Diana Marculescu
> **First submission**: 2025-02-04
> **First announcement**: 2025-02-12
> **comment**: Project page: https://jeff-liangf.github.io/projects/movieweaver/
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形,机器学习
- **Abstract**: Video personalization, which generates customized videos using reference images, has gained significant attention. However, prior methods typically focus on single-concept personalization, limiting broader applications that require multi-concept integration. Attempts to extend these models to multiple concepts often lead to identity blending, which results in composite characters with fused attributes from multiple sources. This challenge arises due to the lack of a mechanism to link each concept with its specific reference image. We address this with anchored prompts, which embed image anchors as unique tokens within text prompts, guiding accurate referencing during generation. Additionally, we introduce concept embeddings to encode the order of reference images. Our approach, Movie Weaver, seamlessly weaves multiple concepts-including face, body, and animal images-into one video, allowing flexible combinations in a single model. The evaluation shows that Movie Weaver outperforms existing methods for multi-concept video personalization in identity preservation and overall quality.

### Pippo: High-Resolution Multi-View Humans from a Single Image 
[[arxiv](https://arxiv.org/abs/2502.07785)] [[cool](https://papers.cool/arxiv/2502.07785)] [[pdf](https://arxiv.org/pdf/2502.07785)]
> **Authors**: Yash Kant,Ethan Weber,Jin Kyu Kim,Rawal Khirodkar,Su Zhaoen,Julieta Martinez,Igor Gilitschenski,Shunsuke Saito,Timur Bagautdinov
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Project Page - http://yashkant.github.io/pippo
- **标题**: None
- **领域**: 计算机视觉和模式识别,图形
- **Abstract**: We present Pippo, a generative model capable of producing 1K resolution dense turnaround videos of a person from a single casually clicked photo. Pippo is a multi-view diffusion transformer and does not require any additional inputs - e.g., a fitted parametric model or camera parameters of the input image. We pre-train Pippo on 3B human images without captions, and conduct multi-view mid-training and post-training on studio captured humans. During mid-training, to quickly absorb the studio dataset, we denoise several (up to 48) views at low-resolution, and encode target cameras coarsely using a shallow MLP. During post-training, we denoise fewer views at high-resolution and use pixel-aligned controls (e.g., Spatial anchor and Plucker rays) to enable 3D consistent generations. At inference, we propose an attention biasing technique that allows Pippo to simultaneously generate greater than 5 times as many views as seen during training. Finally, we also introduce an improved metric to evaluate 3D consistency of multi-view generations, and show that Pippo outperforms existing works on multi-view human generation from a single image.

### A Flag Decomposition for Hierarchical Datasets 
[[arxiv](https://arxiv.org/abs/2502.07782)] [[cool](https://papers.cool/arxiv/2502.07782)] [[pdf](https://arxiv.org/pdf/2502.07782)]
> **Authors**: Nathan Mankovich,Ignacio Santamaria,Gustau Camps-Valls,Tolga Birdal
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Flag manifolds encode hierarchical nested sequences of subspaces and serve as powerful structures for various computer vision and machine learning applications. Despite their utility in tasks such as dimensionality reduction, motion averaging, and subspace clustering, current applications are often restricted to extracting flags using common matrix decomposition methods like the singular value decomposition. Here, we address the need for a general algorithm to factorize and work with hierarchical datasets. In particular, we propose a novel, flag-based method that decomposes arbitrary hierarchical real-valued data into a hierarchy-preserving flag representation in Stiefel coordinates. Our work harnesses the potential of flag manifolds in applications including denoising, clustering, and few-shot learning.

### Stay-Positive: A Case for Ignoring Real Image Features in Fake Image Detection 
[[arxiv](https://arxiv.org/abs/2502.07778)] [[cool](https://papers.cool/arxiv/2502.07778)] [[pdf](https://arxiv.org/pdf/2502.07778)]
> **Authors**: Anirudh Sundara Rajan,Yong Jae Lee
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Detecting AI generated images is a challenging yet essential task. A primary difficulty arises from the detectors tendency to rely on spurious patterns, such as compression artifacts, which can influence its decisions. These issues often stem from specific patterns that the detector associates with the real data distribution, making it difficult to isolate the actual generative traces. We argue that an image should be classified as fake if and only if it contains artifacts introduced by the generative model. Based on this premise, we propose Stay Positive, an algorithm designed to constrain the detectors focus to generative artifacts while disregarding those associated with real data. Experimental results demonstrate that detectors trained with Stay Positive exhibit reduced susceptibility to spurious correlations, leading to improved generalization and robustness to post processing. Additionally, unlike detectors that associate artifacts with real images, those that focus purely on fake artifacts are better at detecting inpainted real images.

### Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras 
[[arxiv](https://arxiv.org/abs/2502.07758)] [[cool](https://papers.cool/arxiv/2502.07758)] [[pdf](https://arxiv.org/pdf/2502.07758)]
> **Authors**: Nektarios A. Valous,Eckhard Hitzer,Dragoş Duşe,Rodrigo Rojas Moraleda,Ferdinand Popp,Meggy Suarez-Carmona,Anna Berthel,Ismini Papageorgiou,Carlo Fremd,Alexander Rölle,Christina C. Westhoff,Bénédicte Lenoir,Niels Halama,Inka Zörnig,Dirk Jäger
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 24 pages, 18 figures, 14 tables
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Hypercomplex image processing extends conventional techniques in a unified paradigm encompassing algebraic and geometric principles. This work leverages quaternions and the two-dimensional orthogonal planes split framework (splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D planes) for natural/biomedical image analysis through the following computational workflows and outcomes: natural/biomedical image re-colorization, natural image de-colorization, natural/biomedical image contrast enhancement, computational re-staining and stain separation in histological images, and performance gains in machine/deep learning pipelines for histological images. The workflows are analyzed separately for natural and biomedical images to showcase the effectiveness of the proposed approaches. The proposed workflows can regulate color appearance (e.g. with alternative renditions and grayscale conversion) and image contrast, be part of automated image processing pipelines (e.g. isolating stain components, boosting learning models), and assist in digital pathology applications (e.g. enhancing biomarker visibility, enabling colorblind-friendly renditions). Employing only basic arithmetic and matrix operations, this work offers a computationally accessible methodology - in the hypercomplex domain - that showcases versatility and consistency across image processing tasks and a range of computer vision and biomedical applications. The proposed non-data-driven methods achieve comparable or better results (particularly in cases involving well-known methods) to those reported in the literature, showcasing the potential of robust theoretical frameworks with practical effectiveness. Results, methods, and limitations are detailed alongside discussion of promising extensions, emphasizing the potential of feature-rich mathematical/computational frameworks for natural and biomedical images.

### CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression Generation 
[[arxiv](https://arxiv.org/abs/2502.07751)] [[cool](https://papers.cool/arxiv/2502.07751)] [[pdf](https://arxiv.org/pdf/2502.07751)]
> **Authors**: Rabeya Tus Sadia,Md Atik Ahamed,Qiang Cheng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,基因组学
- **Abstract**: The integration of single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics (ST) data is crucial for understanding gene expression in spatial context. Existing methods for such integration have limited performance, with structural similarity often below 60\%, We attribute this limitation to the failure to consider causal relationships between genes. We present CausalGeD, which combines diffusion and autoregressive processes to leverage these relationships. By generalizing the Causal Attention Transformer from image generation to gene expression data, our model captures regulatory mechanisms without predefined relationships. Across 10 tissue datasets, CausalGeD outperformed state-of-the-art baselines by 5- 32\% in key metrics, including Pearson's correlation and structural similarity, advancing both technical and biological insights.

### EdgeEar: Efficient and Accurate Ear Recognition for Edge Devices 
[[arxiv](https://arxiv.org/abs/2502.07734)] [[cool](https://papers.cool/arxiv/2502.07734)] [[pdf](https://arxiv.org/pdf/2502.07734)]
> **Authors**: Camile Lendering,Bernardo Perrone Ribeiro,Žiga Emeršič,Peter Peer
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Submitted to IEEE FG 2025
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Ear recognition is a contactless and unobtrusive biometric technique with applications across various domains. However, deploying high-performing ear recognition models on resource-constrained devices is challenging, limiting their applicability and widespread adoption. This paper introduces EdgeEar, a lightweight model based on a proposed hybrid CNN-transformer architecture to solve this problem. By incorporating low-rank approximations into specific linear layers, EdgeEar reduces its parameter count by a factor of 50 compared to the current state-of-the-art, bringing it below two million while maintaining competitive accuracy. Evaluation on the Unconstrained Ear Recognition Challenge (UERC2023) benchmark shows that EdgeEar achieves the lowest EER while significantly reducing computational costs. These findings demonstrate the feasibility of efficient and accurate ear recognition, which we believe will contribute to the wider adoption of ear biometrics.

### Matrix3D: Large Photogrammetry Model All-in-One 
[[arxiv](https://arxiv.org/abs/2502.07685)] [[cool](https://papers.cool/arxiv/2502.07685)] [[pdf](https://arxiv.org/pdf/2502.07685)]
> **Authors**: Yuanxun Lu,Jingyang Zhang,Tian Fang,Jean-Daniel Nahmias,Yanghai Tsin,Long Quan,Xun Cao,Yao Yao,Shiwei Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Project Page: https://nju-3dv.github.io/projects/matrix3d
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We present Matrix3D, a unified model that performs several photogrammetry subtasks, including pose estimation, depth prediction, and novel view synthesis using just the same model. Matrix3D utilizes a multi-modal diffusion transformer (DiT) to integrate transformations across several modalities, such as images, camera parameters, and depth maps. The key to Matrix3D's large-scale multi-modal training lies in the incorporation of a mask learning strategy. This enables full-modality model training even with partially complete data, such as bi-modality data of image-pose and image-depth pairs, thus significantly increases the pool of available training data. Matrix3D demonstrates state-of-the-art performance in pose estimation and novel view synthesis tasks. Additionally, it offers fine-grained control through multi-round interactions, making it an innovative tool for 3D content creation. Project page: https://nju-3dv.github.io/projects/matrix3d.

### Scaling Pre-training to One Hundred Billion Data for Vision Language Models 
[[arxiv](https://arxiv.org/abs/2502.07617)] [[cool](https://papers.cool/arxiv/2502.07617)] [[pdf](https://arxiv.org/pdf/2502.07617)]
> **Authors**: Xiao Wang,Ibrahim Alabdulmohsin,Daniel Salz,Zhe Li,Keran Rong,Xiaohua Zhai
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: We provide an empirical investigation of the potential of pre-training vision-language models on an unprecedented scale: 100 billion examples. We find that model performance tends to saturate at this scale on many common Western-centric classification and retrieval benchmarks, such as COCO Captions. Nevertheless, tasks of cultural diversity achieve more substantial gains from the 100-billion scale web data, thanks to its coverage of long-tail concepts. Furthermore, we analyze the model's multilinguality and show gains in low-resource languages as well. In addition, we observe that reducing the size of the pretraining dataset via quality filters like using CLIP, typically used to enhance performance, may inadvertently reduce the cultural diversity represented even in large-scale datasets. Our results highlight that while traditional benchmarks may not benefit significantly from scaling noisy, raw web data to 100 billion examples, this data scale is vital for building truly inclusive multimodal systems.

### Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07601)] [[cool](https://papers.cool/arxiv/2502.07601)] [[pdf](https://arxiv.org/pdf/2502.07601)]
> **Authors**: Jiacong Xu,Shao-Yuan Lo,Bardia Safaei,Vishal M. Patel,Isht Dwivedi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 19 pages, 10 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,计算语言学
- **Abstract**: Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD & reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: https://xujiacong.github.io/Anomaly-OV/

### YOLO Network For Defect Detection In Optical lenses 
[[arxiv](https://arxiv.org/abs/2502.07592)] [[cool](https://papers.cool/arxiv/2502.07592)] [[pdf](https://arxiv.org/pdf/2502.07592)]
> **Authors**: Habib Yaseen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Mass-produced optical lenses often exhibit defects that alter their scattering properties and compromise quality standards. Manual inspection is usually adopted to detect defects, but it is not recommended due to low accuracy, high error rate and limited scalability. To address these challenges, this study presents an automated defect detection system based on the YOLOv8 deep learning model. A custom dataset of optical lenses, annotated with defect and lens regions, was created to train the model. Experimental results obtained in this study reveal that the system can be used to efficiently and accurately detect defects in optical lenses. The proposed system can be utilized in real-time industrial environments to enhance quality control processes by enabling reliable and scalable defect detection in optical lens manufacturing.

### VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 
[[arxiv](https://arxiv.org/abs/2502.07531)] [[cool](https://papers.cool/arxiv/2502.07531)] [[pdf](https://arxiv.org/pdf/2502.07531)]
> **Authors**: Sixiao Zheng,Zimian Peng,Yanpeng Zhou,Yi Zhu,Hang Xu,Xiangru Huang,Yanwei Fu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,机器学习,多媒体
- **Abstract**: Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available.

### CodePhys: Robust Video-based Remote Physiological Measurement through Latent Codebook Querying 
[[arxiv](https://arxiv.org/abs/2502.07526)] [[cool](https://papers.cool/arxiv/2502.07526)] [[pdf](https://arxiv.org/pdf/2502.07526)]
> **Authors**: Shuyang Chu,Menghan Xia,Mengyao Yuan,Xin Liu,Tapio Seppanen,Guoying Zhao,Jingang Shi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Remote photoplethysmography (rPPG) aims to measure non-contact physiological signals from facial videos, which has shown great potential in many applications. Most existing methods directly extract video-based rPPG features by designing neural networks for heart rate estimation. Although they can achieve acceptable results, the recovery of rPPG signal faces intractable challenges when interference from real-world scenarios takes place on facial video. Specifically, facial videos are inevitably affected by non-physiological factors (e.g., camera device noise, defocus, and motion blur), leading to the distortion of extracted rPPG signals. Recent rPPG extraction methods are easily affected by interference and degradation, resulting in noisy rPPG signals. In this paper, we propose a novel method named CodePhys, which innovatively treats rPPG measurement as a code query task in a noise-free proxy space (i.e., codebook) constructed by ground-truth PPG signals. We consider noisy rPPG features as queries and generate high-fidelity rPPG features by matching them with noise-free PPG features from the codebook. Our approach also incorporates a spatial-aware encoder network with a spatial attention mechanism to highlight physiologically active areas and uses a distillation loss to reduce the influence of non-periodic visual interference. Experimental results on four benchmark datasets demonstrate that CodePhys outperforms state-of-the-art methods in both intra-dataset and cross-dataset settings.

### Efficient Continuous Group Convolutions for Local SE(3) Equivariance in 3D Point Clouds 
[[arxiv](https://arxiv.org/abs/2502.07505)] [[cool](https://papers.cool/arxiv/2502.07505)] [[pdf](https://arxiv.org/pdf/2502.07505)]
> **Authors**: Lisa Weijler,Pedro Hermosilla
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Extending the translation equivariance property of convolutional neural networks to larger symmetry groups has been shown to reduce sample complexity and enable more discriminative feature learning. Further, exploiting additional symmetries facilitates greater weight sharing than standard convolutions, leading to an enhanced network expressivity without an increase in parameter count. However, extending the equivariant properties of a convolution layer comes at a computational cost. In particular, for 3D data, expanding equivariance to the SE(3) group (rotation and translation) results in a 6D convolution operation, which is not tractable for larger data samples such as 3D scene scans. While efforts have been made to develop efficient SE(3) equivariant networks, existing approaches rely on discretization or only introduce global rotation equivariance. This limits their applicability to point clouds representing a scene composed of multiple objects. This work presents an efficient, continuous, and local SE(3) equivariant convolution layer for point cloud processing based on general group convolution and local reference frames. Our experiments show that our approach achieves competitive or superior performance across a range of datasets and tasks, including object classification and semantic segmentation, with negligible computational overhead.

### RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2502.07455)] [[cool](https://papers.cool/arxiv/2502.07455)] [[pdf](https://arxiv.org/pdf/2502.07455)]
> **Authors**: Viacheslav Vasilev,Julia Agafonova,Nikolai Gerasimenko,Alexander Kapitanov,Polina Mikhailova,Evelina Mironova,Denis Dimitrov
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted for NAACL 2025 Findings, GitHub: https://github.com/ai-forever/RusCode
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学
- **Abstract**: Text-to-image generation models have gained popularity among users around the world. However, many of these models exhibit a strong bias toward English-speaking cultures, ignoring or misrepresenting the unique characteristics of other language groups, countries, and nationalities. The lack of cultural awareness can reduce the generation quality and lead to undesirable consequences such as unintentional insult, and the spread of prejudice. In contrast to the field of natural language processing, cultural awareness in computer vision has not been explored as extensively. In this paper, we strive to reduce this gap. We propose a RusCode benchmark for evaluating the quality of text-to-image generation containing elements of the Russian cultural code. To do this, we form a list of 19 categories that best represent the features of Russian visual culture. Our final dataset consists of 1250 text prompts in Russian and their translations into English. The prompts cover a wide range of topics, including complex concepts from art, popular culture, folk traditions, famous people's names, natural objects, scientific achievements, etc. We present the results of a human evaluation of the side-by-side comparison of Russian visual concepts representations using popular generative models.

### Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head Attention without Alignment Barriers 
[[arxiv](https://arxiv.org/abs/2502.07436)] [[cool](https://papers.cool/arxiv/2502.07436)] [[pdf](https://arxiv.org/pdf/2502.07436)]
> **Authors**: Zhaodong Bing,Linze Li,Jiajun Liang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Knowledge distillation (KD) in transformers often faces challenges due to misalignment in the number of attention heads between teacher and student models. Existing methods either require identical head counts or introduce projectors to bridge dimensional gaps, limiting flexibility and efficiency. We propose Squeezing-Heads Distillation (SHD), a novel approach that enables seamless knowledge transfer between models with varying head counts by compressing multi-head attention maps via efficient linear approximation. Unlike prior work, SHD eliminates alignment barriers without additional parameters or architectural modifications. Our method dynamically approximates the combined effect of multiple teacher heads into fewer student heads, preserving fine-grained attention patterns while reducing redundancy. Experiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and vision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it outperforms logit-based and feature-alignment KD baselines, achieving state-of-the-art results in image classification, image generation language fine-tuning, and language pre-training. The key innovations of flexible head compression, projector-free design, and linear-time complexity make SHD a versatile and scalable solution for distilling modern transformers. This work bridges a critical gap in KD, enabling efficient deployment of compact models without compromising performance.

### ArthroPhase: A Novel Dataset and Method for Phase Recognition in Arthroscopic Video 
[[arxiv](https://arxiv.org/abs/2502.07431)] [[cool](https://papers.cool/arxiv/2502.07431)] [[pdf](https://arxiv.org/pdf/2502.07431)]
> **Authors**: Ali Bahari Malayeri,Matthias Seibold,Nicola Cavalcanti,Jonas Hein,Sascha Jecklin,Lazaros Vlachopoulos,Sandro Fucentese,Sandro Hodel,Philipp Furnstahl
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: This study aims to advance surgical phase recognition in arthroscopic procedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by introducing the first arthroscopy dataset and developing a novel transformer-based model. We aim to establish a benchmark for arthroscopic surgical phase recognition by leveraging spatio-temporal features to address the specific challenges of arthroscopic videos including limited field of view, occlusions, and visual distortions. We developed the ACL27 dataset, comprising 27 videos of ACL surgeries, each labeled with surgical phases. Our model employs a transformer-based architecture, utilizing temporal-aware frame-wise feature extraction through a ResNet-50 and transformer layers. This approach integrates spatio-temporal features and introduces a Surgical Progress Index (SPI) to quantify surgery progression. The model's performance was evaluated using accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80 datasets. The proposed model achieved an overall accuracy of 72.91% on the ACL27 dataset. On the Cholec80 dataset, the model achieved a comparable performance with the state-of-the-art methods with an accuracy of 92.4%. The SPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80 datasets respectively, indicating reliable surgery progression estimation. This study introduces a significant advancement in surgical phase recognition for arthroscopy, providing a comprehensive dataset and a robust transformer-based model. The results validate the model's effectiveness and generalizability, highlighting its potential to improve surgical training, real-time assistance, and operational efficiency in orthopedic surgery. The publicly available dataset and code will facilitate future research and development in this critical field.

### Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized Attention Vision Transformer for Autonomous Driving 
[[arxiv](https://arxiv.org/abs/2502.07417)] [[cool](https://papers.cool/arxiv/2502.07417)] [[pdf](https://arxiv.org/pdf/2502.07417)]
> **Authors**: Novendra Setyawan,Ghufron Wahyu Kurniawan,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Under Review on IEEE Transactions on Intelligent Transportation Systems
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: The perception system is a a critical role of an autonomous driving system for ensuring safety. The driving scene perception system fundamentally represents an object detection task that requires achieving a balance between accuracy and processing speed. Many contemporary methods focus on improving detection accuracy but often overlook the importance of real-time detection capabilities when computational resources are limited. Thus, it is vital to investigate efficient object detection strategies for driving scenes. This paper introduces Fast-COS, a novel single-stage object detection framework crafted specifically for driving scene applications. The research initiates with an analysis of the backbone, considering both macro and micro architectural designs, yielding the Reparameterized Attention Vision Transformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise Convolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance computational efficiency and feature extraction. In extensive tests across GPU, edge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the ImageNet-1K dataset, demonstrating significant throughput improvements over comparable backbone models such as ResNet, FastViT, RepViT, and EfficientFormer. Additionally, integrating RepMSDW into a feature pyramid network forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS enhances object detection in driving scenes, attaining an AP50 score of 57.2% on the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses leading models in efficiency, delivering up to 75.9% faster GPU inference and 1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet. These findings establish Fast-COS as a highly scalable and reliable solution suitable for real-time applications, especially in resource-limited environments like autonomous driving systems

### EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering 
[[arxiv](https://arxiv.org/abs/2502.07411)] [[cool](https://papers.cool/arxiv/2502.07411)] [[pdf](https://arxiv.org/pdf/2502.07411)]
> **Authors**: Sheng Zhou,Junbin Xiao,Qingyun Li,Yicong Li,Xun Yang,Dan Guo,Meng Wang,Tat-Seng Chua,Angela Yao
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,多媒体
- **Abstract**: We introduce EgoTextVQA, a novel and rigorously constructed benchmark for egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K ego-view videos and 7K scene-text aware questions that reflect real-user needs in outdoor driving and indoor house-keeping activities. The questions are designed to elicit identification and reasoning on scene text in an egocentric and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10 prominent multimodal large language models. Currently, all models struggle, and the best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the severe deficiency of these techniques in egocentric QA assistance. Our further investigations suggest that precise temporal grounding and multi-frame reasoning, along with high resolution and auxiliary scene-text inputs, are key for better performance. With thorough analyses and heuristic suggestions, we hope EgoTextVQA can serve as a solid testbed for research in egocentric scene-text QA assistance.

### MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification 
[[arxiv](https://arxiv.org/abs/2502.07409)] [[cool](https://papers.cool/arxiv/2502.07409)] [[pdf](https://arxiv.org/pdf/2502.07409)]
> **Authors**: Anh-Tien Nguyen,Duy Minh Ho Nguyen,Nghiem Tuong Diep,Trung Quoc Nguyen,Nhat Ho,Jacqueline Michelle Metsch,Miriam Cindy Maurer,Daniel Sonntag,Hanibal Bohnenberger,Anne-Christin Hauschild
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: first version
- **标题**: None
- **领域**: 计算机视觉和模式识别,机器学习
- **Abstract**: Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.

### FADE: Forecasting for Anomaly Detection on ECG 
[[arxiv](https://arxiv.org/abs/2502.07389)] [[cool](https://papers.cool/arxiv/2502.07389)] [[pdf](https://arxiv.org/pdf/2502.07389)]
> **Authors**: Paula Ruiz-Barroso,Francisco M. Castro,José Miranda,Denisa-Andreea Constantinescu,David Atienza,Nicolás Guil
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Cardiovascular diseases, a leading cause of noncommunicable disease-related deaths, require early and accurate detection to improve patient outcomes. Taking advantage of advances in machine learning and deep learning, multiple approaches have been proposed in the literature to address the challenge of detecting ECG anomalies. Typically, these methods are based on the manual interpretation of ECG signals, which is time consuming and depends on the expertise of healthcare professionals. The objective of this work is to propose a deep learning system, FADE, designed for normal ECG forecasting and anomaly detection, which reduces the need for extensive labeled datasets and manual interpretation. FADE has been trained in a self-supervised manner with a novel morphological inspired loss function. Unlike conventional models that learn from labeled anomalous ECG waveforms, our approach predicts the future of normal ECG signals, thus avoiding the need for extensive labeled datasets. Using a novel distance function to compare forecasted ECG signals with actual sensor data, our method effectively identifies cardiac anomalies. Additionally, this approach can be adapted to new contexts through domain adaptation techniques. To evaluate our proposal, we performed a set of experiments using two publicly available datasets: MIT-BIH NSR and MIT-BIH Arrythmia. The results demonstrate that our system achieves an average accuracy of 83.84% in anomaly detection, while correctly classifying normal ECG signals with an accuracy of 85.46%. Our proposed approach exhibited superior performance in the early detection of cardiac anomalies in ECG signals, surpassing previous methods that predominantly identify a limited range of anomalies. FADE effectively detects both abnormal heartbeats and arrhythmias, offering significant advantages in healthcare through cost reduction or processing of large-scale ECG data.

### USRNet: Unified Scene Recovery Network for Enhancing Traffic Imaging under Multiple Adverse Weather Conditions 
[[arxiv](https://arxiv.org/abs/2502.07372)] [[cool](https://papers.cool/arxiv/2502.07372)] [[pdf](https://arxiv.org/pdf/2502.07372)]
> **Authors**: Yuxu Lu,Ai Chen,Dong Yang,Ryan Wen Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Advancements in computer vision technology have facilitated the extensive deployment of intelligent transportation systems and visual surveillance systems across various applications, including autonomous driving, public safety, and environmental monitoring. However, adverse weather conditions such as haze, rain, snow, and more complex mixed degradation can significantly degrade image quality. The degradation compromises the accuracy and reliability of these systems across various scenarios. To tackle the challenge of developing adaptable models for scene restoration, we introduce the unified scene recovery network (USRNet), capable of handling multiple types of image degradation. The USRNet features a sophisticated architecture consisting of a scene encoder, an attention-driven node independent learning mechanism (NILM), an edge decoder, and a scene restoration module. The scene encoder, powered by advanced residual blocks, extracts deep features from degraded images in a progressive manner, ensuring thorough encoding of degradation information. To enhance the USRNet's adaptability in diverse weather conditions, we introduce NILM, which enables the network to learn and respond to different scenarios with precision, thereby increasing its robustness. The edge decoder is designed to extract edge features with precision, which is essential for maintaining image sharpness. Experimental results demonstrate that USRNet surpasses existing methods in handling complex imaging degradations, thereby improving the accuracy and reliability of visual systems across diverse scenarios. The code resources for this work can be accessed in https://github.com/LouisYxLu/USRNet.

### Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven Measurement Systems 
[[arxiv](https://arxiv.org/abs/2502.07351)] [[cool](https://papers.cool/arxiv/2502.07351)] [[pdf](https://arxiv.org/pdf/2502.07351)]
> **Authors**: Ai Chen,Yuxu Lu,Dong Yang,Junlin Zhou,Yan Fu,Duanbing Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MToIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MToIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather/imaging conditions illustrate that MToIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at https://github.com/Ai-Chen-Lab/MToIE.

### ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus Segmentation with Prototype Consistency Alignment and Conditional Self-Training 
[[arxiv](https://arxiv.org/abs/2502.07331)] [[cool](https://papers.cool/arxiv/2502.07331)] [[pdf](https://arxiv.org/pdf/2502.07331)]
> **Authors**: Siyue Li,Yongcheng Yao,Junru Zhong,Shutian Zhao,Yudong Zhang,Shuihua Wang,Jin Hong,Weitian Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Manual segmentation is labor-intensive, and automatic segmentation remains challenging due to the inherent variability in meniscal morphology, partial volume effects, and low contrast between the meniscus and surrounding tissues. To address these challenges, we propose ERANet, an innovative semi-supervised framework for meniscus segmentation that effectively leverages both labeled and unlabeled images through advanced augmentation and learning strategies. ERANet integrates three key components: edge replacement augmentation (ERA), prototype consistency alignment (PCA), and a conditional self-training (CST) strategy within a mean teacher architecture. ERA introduces anatomically relevant perturbations by simulating meniscal variations, ensuring that augmentations align with the structural context. PCA enhances segmentation performance by aligning intra-class features and promoting compact, discriminative feature representations, particularly in scenarios with limited labeled data. CST improves segmentation robustness by iteratively refining pseudo-labels and mitigating the impact of label noise during training. Together, these innovations establish ERANet as a robust and scalable solution for meniscus segmentation, effectively addressing key barriers to practical implementation. We validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and 3D Fast/Turbo Spin Echo (FSE/TSE) MRI sequences. The results demonstrate the superior performance of ERANet compared to state-of-the-art methods. The proposed framework achieves reliable and accurate segmentation of meniscus structures, even when trained on minimal labeled data. Extensive ablation studies further highlight the synergistic contributions of ERA, PCA, and CST, solidifying ERANet as a transformative solution for semi-supervised meniscus segmentation in medical imaging.

### Semantic to Structure: Learning Structural Representations for Infringement Detection 
[[arxiv](https://arxiv.org/abs/2502.07323)] [[cool](https://papers.cool/arxiv/2502.07323)] [[pdf](https://arxiv.org/pdf/2502.07323)]
> **Authors**: Chuanwei Huang,Zexi Jia,Hongyan Fei,Yeshuang Zhu,Zhiqiang Yuan,Jinchao Zhang,Jie Zhou
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Structural information in images is crucial for aesthetic assessment, and it is widely recognized in the artistic field that imitating the structure of other works significantly infringes on creators' rights. The advancement of diffusion models has led to AI-generated content imitating artists' structural creations, yet effective detection methods are still lacking. In this paper, we define this phenomenon as "structural infringement" and propose a corresponding detection method. Additionally, we develop quantitative metrics and create manually annotated datasets for evaluation: the SIA dataset of synthesized data, and the SIR dataset of real data. Due to the current lack of datasets for structural infringement detection, we propose a new data synthesis strategy based on diffusion models and LLM, successfully training a structural infringement detection model. Experimental results show that our method can successfully detect structural infringements and achieve notable improvements on annotated test sets.

### TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation 
[[arxiv](https://arxiv.org/abs/2502.07306)] [[cool](https://papers.cool/arxiv/2502.07306)] [[pdf](https://arxiv.org/pdf/2502.07306)]
> **Authors**: Navid Rajabi,Jana Kosecka
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能,计算语言学,机器学习,机器人技术
- **Abstract**: In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps \cite{vlmaps} on the complex R2R-Habitat \cite{r2r} instruction dataset and quantify in detail the effect of visual grounding on navigation performance.

### CASC-AI: Consensus-aware Self-corrective AI Agents for Noise Cell Segmentation 
[[arxiv](https://arxiv.org/abs/2502.07302)] [[cool](https://papers.cool/arxiv/2502.07302)] [[pdf](https://arxiv.org/pdf/2502.07302)]
> **Authors**: Ruining Deng,Yihe Yang,David J. Pisapia,Benjamin Liechty,Junchao Zhu,Juming Xiong,Junlin Guo,Zhengyi Lu,Jiacheng Wang,Xing Yao,Runxuan Yu,Rendong Zhang,Gaurav Rudravaram,Mengmeng Yin,Pinaki Sarder,Haichun Yang,Yuankai Huo,Mert R. Sabuncu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Multi-class cell segmentation in high-resolution gigapixel whole slide images (WSI) is crucial for various clinical applications. However, training such models typically requires labor-intensive, pixel-wise annotations by domain experts. Recent efforts have democratized this process by involving lay annotators without medical expertise. However, conventional non-agent-based approaches struggle to handle annotation noise adaptively, as they lack mechanisms to mitigate false positives (FP) and false negatives (FN) at both the image-feature and pixel levels. In this paper, we propose a consensus-aware self-corrective AI agent that leverages the Consensus Matrix to guide its learning process. The Consensus Matrix defines regions where both the AI and annotators agree on cell and non-cell annotations, which are prioritized with stronger supervision. Conversely, areas of disagreement are adaptively weighted based on their feature similarity to high-confidence agreement regions, with more similar regions receiving greater attention. Additionally, contrastive learning is employed to separate features of noisy regions from those of reliable agreement regions by maximizing their dissimilarity. This paradigm enables the AI to iteratively refine noisy labels, enhancing its robustness. Validated on one real-world lay-annotated cell dataset and two simulated noisy datasets, our method demonstrates improved segmentation performance, effectively correcting FP and FN errors and showcasing its potential for training robust models on noisy datasets. The official implementation and cell annotations are publicly available at https://github.com/ddrrnn123/CASC-AI.

### Learning Inverse Laplacian Pyramid for Progressive Depth Completion 
[[arxiv](https://arxiv.org/abs/2502.07289)] [[cool](https://papers.cool/arxiv/2502.07289)] [[pdf](https://arxiv.org/pdf/2502.07289)]
> **Authors**: Kun Wang,Zhiqiang Yan,Junkai Fan,Jun Li,Jian Yang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机视觉和模式识别
- **Abstract**: Depth completion endeavors to reconstruct a dense depth map from sparse depth measurements, leveraging the information provided by a corresponding color image. Existing approaches mostly hinge on single-scale propagation strategies that iteratively ameliorate initial coarse depth estimates through pixel-level message passing. Despite their commendable outcomes, these techniques are frequently hampered by computational inefficiencies and a limited grasp of scene context. To circumvent these challenges, we introduce LP-Net, an innovative framework that implements a multi-scale, progressive prediction paradigm based on Laplacian Pyramid decomposition. Diverging from propagation-based approaches, LP-Net initiates with a rudimentary, low-resolution depth prediction to encapsulate the global scene context, subsequently refining this through successive upsampling and the reinstatement of high-frequency details at incremental scales. We have developed two novel modules to bolster this strategy: 1) the Multi-path Feature Pyramid module, which segregates feature maps into discrete pathways, employing multi-scale transformations to amalgamate comprehensive spatial information, and 2) the Selective Depth Filtering module, which dynamically learns to apply both smoothness and sharpness filters to judiciously mitigate noise while accentuating intricate details. By integrating these advancements, LP-Net not only secures state-of-the-art (SOTA) performance across both outdoor and indoor benchmarks such as KITTI, NYUv2, and TOFDC, but also demonstrates superior computational efficiency. At the time of submission, LP-Net ranks 1st among all peer-reviewed methods on the official KITTI leaderboard.

### Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal Analysis 
[[arxiv](https://arxiv.org/abs/2502.07277)] [[cool](https://papers.cool/arxiv/2502.07277)] [[pdf](https://arxiv.org/pdf/2502.07277)]
> **Authors**: Amir Hosein Fadaei,Mohammad-Reza A. Dehaqani
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 29 pages, 25 figures
- **标题**: None
- **领域**: 计算机视觉和模式识别,人工智能
- **Abstract**: It's no secret that video has become the primary way we share information online. That's why there's been a surge in demand for algorithms that can analyze and understand video content. It's a trend going to continue as video continues to dominate the digital landscape. These algorithms will extract and classify related features from the video and will use them to describe the events and objects in the video. Deep neural networks have displayed encouraging outcomes in the realm of feature extraction and video description. This paper will explore the spatiotemporal features found in videos and recent advancements in deep neural networks in video understanding. We will review some of the main trends in video understanding models and their structural design, the main problems, and some offered solutions in this topic. We will also review and compare significant video understanding and action recognition datasets.

## 计算机与社会(cs.CY:Computers and Society)

### Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course 
[[arxiv](https://arxiv.org/abs/2502.07931)] [[cool](https://papers.cool/arxiv/2502.07931)] [[pdf](https://arxiv.org/pdf/2502.07931)]
> **Authors**: James Weichert,Hoda Eldardiry
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted at 2025 ASEE Annual Conference & Exposition
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: As artificial intelligence (AI) technologies begin to permeate diverse fields-from healthcare to education-consumers, researchers and policymakers are increasingly raising concerns about whether and how AI is regulated. It is therefore reasonable to anticipate that alignment with principles of 'ethical' or 'responsible' AI, as well as compliance with law and policy, will form an increasingly important part of AI development. Yet, for the most part, the conventional computer science curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to explore how new educational content related to AI ethics and AI policy can be integrated into both ethics- and technical-focused courses. This paper describes a two-lecture 'AI policy module' that was piloted in a graduate-level introductory machine learning course in 2024. The module, which includes an in-class active learning game, is evaluated using data from student surveys before and after the lectures, and pedagogical motivations and considerations are discussed. We find that the module is successful in engaging otherwise technically-oriented students on the topic of AI policy, increasing student awareness of the social impacts of a variety of AI technologies and developing student interest in the field of AI regulation.

### Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action 
[[arxiv](https://arxiv.org/abs/2502.07794)] [[cool](https://papers.cool/arxiv/2502.07794)] [[pdf](https://arxiv.org/pdf/2502.07794)]
> **Authors**: Jasmine Chiat Ling Ong,Yilin Ning,Mingxuan Liu,Yian Ma,Zhao Liang,Kuldev Singh,Robert T Chang,Silke Vogel,John CW Lim,Iris Siu Kwan Tan,Oscar Freyer,Stephen Gilbert,Danielle S Bitterman,Xiaoxuan Liu,Alastair K Denniston,Nan Liu
> **First submission**: 2025-01-27
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare.

### Can Generative AI be Egalitarian? 
[[arxiv](https://arxiv.org/abs/2502.07790)] [[cool](https://papers.cool/arxiv/2502.07790)] [[pdf](https://arxiv.org/pdf/2502.07790)]
> **Authors**: Philip Feldman,James R. Foulds,Shimei Pan
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-12
> **comment**: 14 pages, 5 figures
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: The recent explosion of "foundation" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism, while the potential for enormous profit has challenged technology organizations' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this "egalitarian" approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content.

### Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method 
[[arxiv](https://arxiv.org/abs/2502.07789)] [[cool](https://papers.cool/arxiv/2502.07789)] [[pdf](https://arxiv.org/pdf/2502.07789)]
> **Authors**: Alfredo Capozucca,Daniil Yampolskyi,Alexander Goldberg,Maximiliano Cristiá
> **First submission**: 2025-01-20
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications.

### Economics of Sourcing Human Data 
[[arxiv](https://arxiv.org/abs/2502.07732)] [[cool](https://papers.cool/arxiv/2502.07732)] [[pdf](https://arxiv.org/pdf/2502.07732)]
> **Authors**: Sebastin Santy,Prasanta Bhattacharya,Manoel Horta Ribeiro,Kelsey Allen,Sewoong Oh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机与社会,人工智能,计算语言学,计算机视觉和模式识别,人机交互,机器学习
- **Abstract**: Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content--it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations--rather than relying solely on external incentives--can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.

### SoK: A Classification for AI-driven Personalized Privacy Assistants 
[[arxiv](https://arxiv.org/abs/2502.07693)] [[cool](https://papers.cool/arxiv/2502.07693)] [[pdf](https://arxiv.org/pdf/2502.07693)]
> **Authors**: Victor Morel,Leonardo Iwaya,Simone Fischer-Hübner
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Work in progress
- **标题**: None
- **领域**: 计算机与社会,人工智能
- **Abstract**: To help users make privacy-related decisions, personalized privacy assistants based on AI technology have been developed in recent years. These AI-driven Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits for users, who may otherwise struggle to make decisions regarding their personal data in environments saturated with privacy-related decision requests. However, no study systematically inquired about the features of these AI-driven PPAs, their underlying technologies, or the accuracy of their decisions. To fill this gap, we present a Systematization of Knowledge (SoK) to map the existing solutions found in the scientific literature. We screened 1697 unique research papers over the last decade (2013-2023), constructing a classification from 39 included papers. As a result, this SoK reviews several aspects of existing research on AI-driven PPAs in terms of types of publications, contributions, methodological quality, and other quantitative insights. Furthermore, we provide a comprehensive classification for AI-driven PPAs, delving into their architectural choices, system contexts, types of AI used, data sources, types of decisions, and control over decisions, among other facets. Based on our SoK, we further underline the research gaps and challenges and formulate recommendations for the design and development of AI-driven PPAs as well as avenues for future research.

## 数据库(cs.DB:Databases)

### CREDAL: Close Reading of Data Models 
[[arxiv](https://arxiv.org/abs/2502.07943)] [[cool](https://papers.cool/arxiv/2502.07943)] [[pdf](https://arxiv.org/pdf/2502.07943)]
> **Authors**: George Fletcher,Olha Nahurna,Matvii Prytula,Julia Stoyanovich
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 数据库,人工智能,计算机与社会
- **Abstract**: Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model, every statistical model, and every database has an underlying data model without which the system would not be usable. Hence, data models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards this, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary artifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed nature, and the design of technical systems. While recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance for those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does not exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development process and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the critical study of data.

## 分布式、并行和集群计算(cs.DC:Distributed, Parallel, and Cluster Computing)

### General Coded Computing: Adversarial Settings 
[[arxiv](https://arxiv.org/abs/2502.08058)] [[cool](https://papers.cool/arxiv/2502.08058)] [[pdf](https://arxiv.org/pdf/2502.08058)]
> **Authors**: Parsa Moradi,Hanzaleh Akbarinodehi,Mohammad Ali Maddah-Ali
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 18 pages, 1 figure
- **标题**: None
- **领域**: 分布式、并行和集群计算,机器学习
- **Abstract**: Conventional coded computing frameworks are predominantly tailored for structured computations, such as matrix multiplication and polynomial evaluation. Such tasks allow the reuse of tools and techniques from algebraic coding theory to improve the reliability of distributed systems in the presence of stragglers and adversarial servers. This paper lays the foundation for general coded computing, which extends the applicability of coded computing to handle a wide class of computations. In addition, it particularly addresses the challenging problem of managing adversarial servers. We demonstrate that, in the proposed scheme, for a system with $N$ servers, where $\mathcal{O}(N^a)$, $a \in [0,1)$, are adversarial, the supremum of the average approximation error over all adversarial strategies decays at a rate of $N^{\frac{6}{5}(a-1)}$, under minimal assumptions on the computing tasks. Furthermore, we show that within a general framework, the proposed scheme achieves optimal adversarial robustness, in terms of maximum number of adversarial servers it can tolerate. This marks a significant step toward practical and reliable general coded computing. Implementation results further validate the effectiveness of the proposed method in handling various computations, including inference in deep neural networks.

### DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training 
[[arxiv](https://arxiv.org/abs/2502.07590)] [[cool](https://papers.cool/arxiv/2502.07590)] [[pdf](https://arxiv.org/pdf/2502.07590)]
> **Authors**: Xin Tan,Yuetao Chen,Yimin Jiang,Xing Chen,Kun Yan,Nan Duan,Yibo Zhu,Daxin Jiang,Hong Xu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 分布式、并行和集群计算,计算机视觉和模式识别
- **Abstract**: Diffusion Transformers (DiTs) have shown remarkable performance in modeling and generating high-quality videos. However, the quadratic computational complexity of 3D full attention mechanism presents significant challenges in scaling video DiT training, especially for high-definition and lengthy videos, where attention can dominate up to 95% of the end-to-end time and necessitate specialized communication paradigms to handle large input sizes. This paper introduces DSV, a novel framework designed to accelerate and scale the training of video DiTs by leveraging the inherent dynamic attention sparsity throughout the training process. DSV employs a two-stage training algorithm that exploits sparsity patterns, focusing on critical elements supported by efficient, tailored kernels. To accommodate the new sparsity dimension, we develop a hybrid sparsity-aware context parallelism that effectively scales to large inputs by addressing the heterogeneity of sparsity across attention heads and blocks, resulting in optimized sparse computation and communication. Extensive evaluations demonstrate that DSV achieves up to 3.02x gain in training throughput with nearly no quality degradation.

## 数据结构和算法(cs.DS:Data Structures and Algorithms)

### Polynomial-Time Approximability of Constrained Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07764)] [[cool](https://papers.cool/arxiv/2502.07764)] [[pdf](https://arxiv.org/pdf/2502.07764)]
> **Authors**: Jeremy McMahan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 数据结构和算法,人工智能,机器学习
- **Abstract**: We study the computational complexity of approximating general constrained Markov decision processes. Our primary contribution is the design of a polynomial time $(0,ε)$-additive bicriteria approximation algorithm for finding optimal constrained policies across a broad class of recursively computable constraints, including almost-sure, chance, expectation, and their anytime variants. Matching lower bounds imply our approximation guarantees are optimal so long as $P \neq NP$. The generality of our approach results in answers to several long-standing open complexity questions in the constrained reinforcement learning literature. Specifically, we are the first to prove polynomial-time approximability for the following settings: policies under chance constraints, deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes.

### Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations 
[[arxiv](https://arxiv.org/abs/2502.07657)] [[cool](https://papers.cool/arxiv/2502.07657)] [[pdf](https://arxiv.org/pdf/2502.07657)]
> **Authors**: Oren Mangoubi,Nisheeth K. Vishnoi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Published in Journal of the ACM. arXiv admin note: substantial text overlap with arXiv:2306.16648
- **标题**: None
- **领域**: 数据结构和算法,密码学和安全,机器学习,数值分析,可能性
- **Abstract**: We consider the problem of approximating a $d \times d$ covariance matrix $M$ with a rank-$k$ matrix under $(\varepsilon,δ)$-differential privacy. We present and analyze a complex variant of the Gaussian mechanism and obtain upper bounds on the Frobenius norm of the difference between the matrix output by this mechanism and the best rank-$k$ approximation to $M$. Our analysis provides improvements over previous bounds, particularly when the spectrum of $M$ satisfies natural structural assumptions. The novel insight is to view the addition of Gaussian noise to a matrix as a continuous-time matrix Brownian motion. This viewpoint allows us to track the evolution of eigenvalues and eigenvectors of the matrix, which are governed by stochastic differential equations discovered by Dyson. These equations enable us to upper bound the Frobenius distance between the best rank-$k$ approximation of $M$ and that of a Gaussian perturbation of $M$ as an integral that involves inverse eigenvalue gaps of the stochastically evolving matrix, as opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems. Subsequently, again using the Dyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$ perturbed by Gaussian noise have large gaps with high probability. These results also contribute to the analysis of low-rank approximations under average-case perturbations, and to an understanding of eigenvalue gaps for random matrices, both of which may be of independent interest.

## 图形(cs.GR:Graphics)

### MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization 
[[arxiv](https://arxiv.org/abs/2502.07754)] [[cool](https://papers.cool/arxiv/2502.07754)] [[pdf](https://arxiv.org/pdf/2502.07754)]
> **Authors**: Rafał Tobiasz,Grzegorz Wilczyński,Marcin Mazur,Sławomir Tadeja,Przemysław Spurek
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 图形,计算机视觉和模式识别
- **Abstract**: Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer graphics. GS-based algorithms almost always bypass classical methods such as ray tracing, which offers numerous inherent advantages for rendering. For example, ray tracing is able to handle incoherent rays for advanced lighting effects, including shadows and reflections. To address this limitation, we introduce MeshSplats, a method which converts GS to a mesh-like format. Following the completion of training, MeshSplats transforms Gaussian elements into mesh faces, enabling rendering using ray tracing methods with all their associated benefits. Our model can be utilized immediately following transformation, yielding a mesh of slightly reduced quality without additional training. Furthermore, we can enhance the reconstruction quality through the application of a dedicated optimization algorithm that operates on mesh faces rather than Gaussian components. The efficacy of our method is substantiated by experimental results, underscoring its extensive applications in computer graphics and image processing.

## 计算机科学与博弈论(cs.GT:Computer Science and Game Theory)

### Multi-Agent Performative Prediction Beyond the Insensitivity Assumption: A Case Study for Mortgage Competition 
[[arxiv](https://arxiv.org/abs/2502.08063)] [[cool](https://papers.cool/arxiv/2502.08063)] [[pdf](https://arxiv.org/pdf/2502.08063)]
> **Authors**: Guanghui Wang,Krishna Acharya,Lokranjan Lakshmikanthan,Vidya Muthukumar,Juba Ziani
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: Performative prediction models account for feedback loops in decision-making processes where predictions influence future data distributions. While existing work largely assumes insensitivity of data distributions to small strategy changes, this assumption usually fails in real-world competitive (i.e. multi-agent) settings. For example, in Bertrand-type competitions, a small reduction in one firm's price can lead that firm to capture the entire demand, while all others sharply lose all of their customers. We study a representative setting of multi-agent performative prediction in which insensitivity assumptions do not hold, and investigate the convergence of natural dynamics. To do so, we focus on a specific game that we call the ''Bank Game'', where two lenders compete over interest rates and credit score thresholds. Consumers act similarly as to in a Bertrand Competition, with each consumer selecting the firm with the lowest interest rate that they are eligible for based on the firms' credit thresholds. Our analysis characterizes the equilibria of this game and demonstrates that when both firms use a common and natural no-regret learning dynamic -- exponential weights -- with proper initialization, the dynamics always converge to stable outcomes despite the general-sum structure. Notably, our setting admits multiple stable equilibria, with convergence dependent on initial conditions. We also provide theoretical convergence results in the stochastic case when the utility matrix is not fully known, but each learner can observe sufficiently many samples of consumers at each time step to estimate it, showing robustness to slight mis-specifications. Finally, we provide experimental results that validate our theoretical findings.

### Sink equilibria and the attractors of learning in games 
[[arxiv](https://arxiv.org/abs/2502.07975)] [[cool](https://papers.cool/arxiv/2502.07975)] [[pdf](https://arxiv.org/pdf/2502.07975)]
> **Authors**: Oliver Biggar,Christos Papadimitriou
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,机器学习
- **Abstract**: Characterizing the limit behavior -- that is, the attractors -- of learning dynamics is one of the most fundamental open questions in game theory. In recent work in this front, it was conjectured that the attractors of the replicator dynamic are in one-to-one correspondence with the sink equilibria of the game -- the sink strongly connected components of a game's preference graph -- , and it was established that they do stand in at least one-to-many correspondence with them. We make threefold progress on the problem of characterizing attractors. First, we show through a topological construction that the one-to-one conjecture is false. Second, we make progress on the attractor characterization problem for two-player games by establishing that the one-to-one conjecture is true in the absence of a local pattern called a weak local source -- a pattern that is absent from zero-sum games. Finally, we look -- for the first time in this context -- at fictitious play, the longest-studied learning dynamic, and examine to what extent the conjecture generalizes there. We establish that under fictitious play, sink equilibria always contain attractors (sometimes strictly), and every attractor corresponds to a strongly connected set of nodes in the preference graph.

### Algorithmic Aspects of Strategic Trading 
[[arxiv](https://arxiv.org/abs/2502.07606)] [[cool](https://papers.cool/arxiv/2502.07606)] [[pdf](https://arxiv.org/pdf/2502.07606)]
> **Authors**: Michael Kearns,Mirah Shi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算机科学与博弈论,计算工程、金融和科学,机器学习
- **Abstract**: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.

## 人机交互(cs.HC:Human-Computer Interaction)

### Exploring Mobile Touch Interaction with Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07629)] [[cool](https://papers.cool/arxiv/2502.07629)] [[pdf](https://arxiv.org/pdf/2502.07629)]
> **Authors**: Tim Zindulka,Jannek Sekowski,Florian Lehmann,Daniel Buschek
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 21 pages, 16 figures, 3 tables, ACM CHI 2025
- **标题**: None
- **领域**: 人机交互,计算语言学
- **Abstract**: Interacting with Large Language Models (LLMs) for text editing on mobile devices currently requires users to break out of their writing environment and switch to a conversational AI interface. In this paper, we propose to control the LLM via touch gestures performed directly on the text. We first chart a design space that covers fundamental touch input and text transformations. In this space, we then concretely explore two control mappings: spread-to-generate and pinch-to-shorten, with visual feedback loops. We evaluate this concept in a user study (N=14) that compares three feedback designs: no visualisation, text length indicator, and length + word indicator. The results demonstrate that touch-based control of LLMs is both feasible and user-friendly, with the length + word indicator proving most effective for managing text generation. This work lays the foundation for further research into gesture-based interaction with LLMs on touch devices.

### SensPS: Sensing Personal Space Comfortable Distance between Human-Human Using Multimodal Sensors 
[[arxiv](https://arxiv.org/abs/2502.07441)] [[cool](https://papers.cool/arxiv/2502.07441)] [[pdf](https://arxiv.org/pdf/2502.07441)]
> **Authors**: Ko Watanabe,Nico Förster,Shoya Ishimaru
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: Personal space, also known as peripersonal space, is crucial in human social interaction, influencing comfort, communication, and social stress. Estimating and respecting personal space is essential for enhancing human-computer interaction (HCI) and smart environments. Personal space preferences vary due to individual traits, cultural background, and contextual factors. Advanced multimodal sensing technologies, including eye-tracking and wristband sensors, offer opportunities to develop adaptive systems that dynamically adjust to user comfort levels. Integrating physiological and behavioral data enables a deeper understanding of spatial interactions. This study develops a sensor-based model to estimate comfortable personal space and identifies key features influencing spatial preferences. Our findings show that multimodal sensors, particularly eye-tracking and physiological wristband data, can effectively predict personal space preferences, with eye-tracking data playing a more significant role. An experimental study involving controlled human interactions demonstrates that a Transformer-based model achieves the highest predictive accuracy (F1 score: 0.87) for estimating personal space. Eye-tracking features, such as gaze point and pupil diameter, emerge as the most significant predictors, while physiological signals from wristband sensors contribute marginally. These results highlight the potential for AI-driven personalization of social space in adaptive environments, suggesting that multimodal sensing can be leveraged to develop intelligent systems that optimize spatial arrangements in workplaces, educational institutions, and public settings. Future work should explore larger datasets, real-world applications, and additional physiological markers to enhance model robustness.

### Human-in-the-Loop Annotation for Image-Based Engagement Estimation: Assessing the Impact of Model Reliability on Annotation Accuracy 
[[arxiv](https://arxiv.org/abs/2502.07404)] [[cool](https://papers.cool/arxiv/2502.07404)] [[pdf](https://arxiv.org/pdf/2502.07404)]
> **Authors**: Sahana Yadnakudige Subramanya,Ko Watanabe,Andreas Dengel,Shoya Ishimaru
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 人机交互,人工智能,计算机视觉和模式识别
- **Abstract**: Human-in-the-loop (HITL) frameworks are increasingly recognized for their potential to improve annotation accuracy in emotion estimation systems by combining machine predictions with human expertise. This study focuses on integrating a high-performing image-based emotion model into a HITL annotation framework to evaluate the collaborative potential of human-machine interaction and identify the psychological and practical factors critical to successful collaboration. Specifically, we investigate how varying model reliability and cognitive framing influence human trust, cognitive load, and annotation behavior in HITL systems. We demonstrate that model reliability and psychological framing significantly impact annotators' trust, engagement, and consistency, offering insights into optimizing HITL frameworks. Through three experimental scenarios with 29 participants--baseline model reliability (S1), fabricated errors (S2), and cognitive bias introduced by negative framing (S3)--we analyzed behavioral and qualitative data. Reliable predictions in S1 yielded high trust and annotation consistency, while unreliable outputs in S2 led to increased critical evaluations but also heightened frustration and response variability. Negative framing in S3 revealed how cognitive bias influenced participants to perceive the model as more relatable and accurate, despite misinformation regarding its reliability. These findings highlight the importance of both reliable machine outputs and psychological factors in shaping effective human-machine collaboration. By leveraging the strengths of both human oversight and automated systems, this study establishes a scalable HITL framework for emotion annotation and lays the foundation for broader applications in adaptive learning and human-computer interaction.

### Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning 
[[arxiv](https://arxiv.org/abs/2502.07401)] [[cool](https://papers.cool/arxiv/2502.07401)] [[pdf](https://arxiv.org/pdf/2502.07401)]
> **Authors**: Johnny Chan,Yuming Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 9 pages, 4 figures, accepted and presented in the 2025 6th International Conference on Advances in Education and Information Technology (AEIT)
- **标题**: None
- **领域**: 人机交互,人工智能
- **Abstract**: This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies.

## 信息检索(cs.IR:Information Retrieval)

### ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval 
[[arxiv](https://arxiv.org/abs/2502.07971)] [[cool](https://papers.cool/arxiv/2502.07971)] [[pdf](https://arxiv.org/pdf/2502.07971)]
> **Authors**: Shubham Gupta,Zichao Li,Tianyi Chen,Cem Subakan,Siva Reddy,Perouz Taslakian,Valentina Zantedeschi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: :I.2; I.7; E.2; H.3
- **标题**: None
- **领域**: 信息检索,人工智能,机器学习
- **Abstract**: Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that ReTreever generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.

### Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos 
[[arxiv](https://arxiv.org/abs/2502.07327)] [[cool](https://papers.cool/arxiv/2502.07327)] [[pdf](https://arxiv.org/pdf/2502.07327)]
> **Authors**: Haowen Gao,Liang Pang,Shicheng Xu,Leigang Qu,Tat-Seng Chua,Huawei Shen,Xueqi Cheng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 信息检索,计算机视觉和模式识别
- **Abstract**: With the rapid development of AI-generated content (AIGC), the creation of high-quality AI-generated videos has become faster and easier, resulting in the Internet being flooded with all kinds of video content. However, the impact of these videos on the content ecosystem remains largely unexplored. Video information retrieval remains a fundamental approach for accessing video content. Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior. To explore this, we first construct a comprehensive benchmark dataset containing both real and AI-generated videos, along with a set of fair and rigorous metrics to assess bias. This benchmark consists of 13,000 videos generated by two state-of-the-art open-source video generation models. We meticulously design a suite of rigorous metrics to accurately measure this preference, accounting for potential biases arising from the limited frame rate and suboptimal quality of AIGC videos. We then applied three off-the-shelf video retrieval models to perform retrieval tasks on this hybrid dataset. Our findings reveal a clear preference for AI-generated videos in retrieval. Further investigation shows that incorporating AI-generated videos into the training set of retrieval models exacerbates this bias. Unlike the preference observed in image modalities, we find that video retrieval bias arises from both unseen visual and temporal information, making the root causes of video bias a complex interplay of these two factors. To mitigate this bias, we fine-tune the retrieval models using a contrastive learning approach. The results of this study highlight the potential implications of AI-generated videos on retrieval systems.

## 机器学习(cs.LG:Machine Learning)

### PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation 
[[arxiv](https://arxiv.org/abs/2502.08106)] [[cool](https://papers.cool/arxiv/2502.08106)] [[pdf](https://arxiv.org/pdf/2502.08106)]
> **Authors**: Ziyan Wang,Sizhe Wei,Xiaoming Huo,Hao Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.

### Out-of-Distribution Detection on Graphs: A Survey 
[[arxiv](https://arxiv.org/abs/2502.08105)] [[cool](https://papers.cool/arxiv/2502.08105)] [[pdf](https://arxiv.org/pdf/2502.08105)]
> **Authors**: Tingyi Cai,Yunliang Jiang,Yixin Liu,Ming Li,Changqin Huang,Shirui Pan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 9 pages, 6 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Graph machine learning has witnessed rapid growth, driving advancements across diverse domains. However, the in-distribution assumption, where training and testing data share the same distribution, often breaks in real-world scenarios, leading to degraded model performance under distribution shifts. This challenge has catalyzed interest in graph out-of-distribution (GOOD) detection, which focuses on identifying graph data that deviates from the distribution seen during training, thereby enhancing model robustness. In this paper, we provide a rigorous definition of GOOD detection and systematically categorize existing methods into four types: enhancement-based, reconstruction-based, information propagation-based, and classification-based approaches. We analyze the principles and mechanisms of each approach and clarify the distinctions between GOOD detection and related fields, such as graph anomaly detection, outlier detection, and GOOD generalization. Beyond methodology, we discuss practical applications and theoretical foundations, highlighting the unique challenges posed by graph data. Finally, we discuss the primary challenges and propose future directions to advance this emerging field. The repository of this survey is available at https://github.com/ca1man-2022/Awesome-GOOD-Detection.

### Rethinking Tokenized Graph Transformers for Node Classification 
[[arxiv](https://arxiv.org/abs/2502.08101)] [[cool](https://papers.cool/arxiv/2502.08101)] [[pdf](https://arxiv.org/pdf/2502.08101)]
> **Authors**: Jinsong Chen,Chenyang Li,GaiChao Li,John E. Hopcroft,Kun He
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Preprint version
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTs for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification.

### Unsupervised categorization of similarity measures 
[[arxiv](https://arxiv.org/abs/2502.08098)] [[cool](https://papers.cool/arxiv/2502.08098)] [[pdf](https://arxiv.org/pdf/2502.08098)]
> **Authors**: Yoshiyuki Ohmura,Wataru Shimaya,Yasuo Kuniyoshi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: arXiv admin note: substantial text overlap with arXiv:2306.00239
- **标题**: None
- **领域**: 机器学习,神经和进化计算
- **Abstract**: In general, objects can be distinguished on the basis of their features, such as color or shape. In particular, it is assumed that similarity judgments about such features can be processed independently in different metric spaces. However, the unsupervised categorization mechanism of metric spaces corresponding to object features remains unknown. Here, we show that the artificial neural network system can autonomously categorize metric spaces through representation learning to satisfy the algebraic independence between neural networks, and project sensory information onto multiple high-dimensional metric spaces to independently evaluate the differences and similarities between features. Conventional methods often constrain the axes of the latent space to be mutually independent or orthogonal. However, the independent axes are not suitable for categorizing metric spaces. High-dimensional metric spaces that are independent of each other are not uniquely determined by the mutually independent axes, because any combination of independent axes can form mutually independent spaces. In other words, the mutually independent axes cannot be used to naturally categorize different feature spaces, such as color space and shape space. Therefore, constraining the axes to be mutually independent makes it difficult to categorize high-dimensional metric spaces. To overcome this problem, we developed a method to constrain only the spaces to be mutually independent and not the composed axes to be independent. Our theory provides general conditions for the unsupervised categorization of independent metric spaces, thus advancing the mathematical theory of functional differentiation of neural networks.

### Mixture of Decoupled Message Passing Experts with Entropy Constraint for General Node Classification 
[[arxiv](https://arxiv.org/abs/2502.08083)] [[cool](https://papers.cool/arxiv/2502.08083)] [[pdf](https://arxiv.org/pdf/2502.08083)]
> **Authors**: Xuanze Chen,Jiajun Zhou,Jinsong Chen,Shanqing Yu,Qi Xuan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: arXiv admin note: text overlap with arXiv:2412.08193
- **标题**: None
- **领域**: 机器学习,社交和信息网络
- **Abstract**: The varying degrees of homophily and heterophily in real-world graphs persistently constrain the universality of graph neural networks (GNNs) for node classification. Adopting a data-centric perspective, this work reveals an inherent preference of different graphs towards distinct message encoding schemes: homophilous graphs favor local propagation, while heterophilous graphs exhibit preference for flexible combinations of propagation and transformation. To address this, we propose GNNMoE, a universal node classification framework based on the Mixture-of-Experts (MoE) mechanism. The framework first constructs diverse message-passing experts through recombination of fine-grained encoding operators, then designs soft and hard gating layers to allocate the most suitable expert networks for each node's representation learning, thereby enhancing both model expressiveness and adaptability to diverse graphs. Furthermore, considering that soft gating might introduce encoding noise in homophilous scenarios, we introduce an entropy constraint to guide sharpening of soft gates, achieving organic integration of weighted combination and Top-K selection. Extensive experiments demonstrate that GNNMoE significantly outperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both node classification performance and universality across diverse graph datasets.

### Cascading Bandits Robust to Adversarial Corruptions 
[[arxiv](https://arxiv.org/abs/2502.08077)] [[cool](https://papers.cool/arxiv/2502.08077)] [[pdf](https://arxiv.org/pdf/2502.08077)]
> **Authors**: Jize Xie,Cheng Chen,Zhiyong Wang,Shuai Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Online learning to rank sequentially recommends a small list of items to users from a large candidate set and receives the users' click feedback. In many real-world scenarios, users browse the recommended list in order and click the first attractive item without checking the rest. Such behaviors are usually formulated as the cascade model. Many recent works study algorithms for cascading bandits, an online learning to rank framework in the cascade model. However, the performance of existing methods may drop significantly if part of the user feedback is adversarially corrupted (e.g., click fraud). In this work, we study how to resist adversarial corruptions in cascading bandits. We first formulate the ``\textit{Cascading Bandits with Adversarial Corruptions}" (CBAC) problem, which assumes that there is an adaptive adversary that may manipulate the user feedback. Then we propose two robust algorithms for this problem, which assume the corruption level is known and agnostic, respectively. We show that both algorithms can achieve logarithmic regret when the algorithm is not under attack, and the regret increases linearly with the corruption level. The experimental results also verify the robustness of our methods.

### Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning 
[[arxiv](https://arxiv.org/abs/2502.08056)] [[cool](https://papers.cool/arxiv/2502.08056)] [[pdf](https://arxiv.org/pdf/2502.08056)]
> **Authors**: Zijian He,Reyna Abhyankar,Vikranth Srivatsa,Yiying Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.

### The Art of Misclassification: Too Many Classes, Not Enough Points 
[[arxiv](https://arxiv.org/abs/2502.08041)] [[cool](https://papers.cool/arxiv/2502.08041)] [[pdf](https://arxiv.org/pdf/2502.08041)]
> **Authors**: Mario Franco,Gerardo Febres,Nelson Fernández,Carlos Gershenson
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: Classification is a ubiquitous and fundamental problem in artificial intelligence and machine learning, with extensive efforts dedicated to developing more powerful classifiers and larger datasets. However, the classification task is ultimately constrained by the intrinsic properties of datasets, independently of computational power or model complexity. In this work, we introduce a formal entropy-based measure of classificability, which quantifies the inherent difficulty of a classification problem by assessing the uncertainty in class assignments given feature representations. This measure captures the degree of class overlap and aligns with human intuition, serving as an upper bound on classification performance for classification problems. Our results establish a theoretical limit beyond which no classifier can improve the classification accuracy, regardless of the architecture or amount of data, in a given problem. Our approach provides a principled framework for understanding when classification is inherently fallible and fundamentally ambiguous.

### Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning 
[[arxiv](https://arxiv.org/abs/2502.08024)] [[cool](https://papers.cool/arxiv/2502.08024)] [[pdf](https://arxiv.org/pdf/2502.08024)]
> **Authors**: Divyansh Jhunjhunwala,Pranay Sharma,Zheng Xu,Gauri Joshi
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,分布式、并行和集群计算
- **Abstract**: Initializing with pre-trained models when learning on downstream tasks is becoming standard practice in machine learning. Several recent works explore the benefits of pre-trained initialization in a federated learning (FL) setting, where the downstream training is performed at the edge clients with heterogeneous data distribution. These works show that starting from a pre-trained model can substantially reduce the adverse impact of data heterogeneity on the test performance of a model trained in a federated setting, with no changes to the standard FedAvg training algorithm. In this work, we provide a deeper theoretical understanding of this phenomenon. To do so, we study the class of two-layer convolutional neural networks (CNNs) and provide bounds on the training error convergence and test error of such a network trained with FedAvg. We introduce the notion of aligned and misaligned filters at initialization and show that the data heterogeneity only affects learning on misaligned filters. Starting with a pre-trained model typically results in fewer misaligned filters at initialization, thus producing a lower test error even when the model is trained in a federated setting with data heterogeneity. Experiments in synthetic settings and practical FL training on CNNs verify our theoretical findings.

### Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol 
[[arxiv](https://arxiv.org/abs/2502.08021)] [[cool](https://papers.cool/arxiv/2502.08021)] [[pdf](https://arxiv.org/pdf/2502.08021)]
> **Authors**: Pai Liu,Lingfeng Zhao,Shivangi Agarwal,Jinghan Liu,Audrey Huang,Philip Amortila,Nan Jiang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions ("model-free") or dynamics ("model-based") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance.

### An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.08008)] [[cool](https://papers.cool/arxiv/2502.08008)] [[pdf](https://arxiv.org/pdf/2502.08008)]
> **Authors**: Kasra Ahmadi,Rouzbeh Behnia,Reza Ebrahimi,Mehran Mozaffari Kermani,Jeremiah Birrell,Jason Pacheco,Attila A Yavuz
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,密码学和安全
- **Abstract**: Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $ε= 10$ and 1.9% for $ε= 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.

### The Role of Randomness in Stability 
[[arxiv](https://arxiv.org/abs/2502.08007)] [[cool](https://papers.cool/arxiv/2502.08007)] [[pdf](https://arxiv.org/pdf/2502.08007)]
> **Authors**: Max Hopkins,Shay Moran
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: :68Q32
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Stability is a central property in learning and statistics promising the output of an algorithm $A$ does not change substantially when applied to similar datasets $S$ and $S'$. It is an elementary fact that any sufficiently stable algorithm (e.g.\ one returning the same result with high probability, satisfying privacy guarantees, etc.) must be randomized. This raises a natural question: can we quantify how much randomness is needed for algorithmic stability? We study the randomness complexity of two influential notions of stability in learning: replicability, which promises $A$ usually outputs the same result when run over samples from the same distribution (and shared random coins), and differential privacy, which promises the output distribution of $A$ remains similar under neighboring datasets. The randomness complexity of these notions was studied recently in (Dixon et al. ICML 2024) and (Cannone et al. ITCS 2024) for basic $d$-dimensional tasks (e.g. estimating the bias of $d$ coins), but little is known about the measures more generally or in complex settings like classification. Toward this end, we prove a `weak-to-strong' boosting theorem for stability: the randomness complexity of a task $M$ (either under replicability or DP) is tightly controlled by the best replication probability of any deterministic algorithm solving the task, a weak measure called `global stability' that is universally capped at $\frac{1}{2}$ (Chase et al. FOCS 2023). Using this, we characterize the randomness complexity of PAC Learning: a class has bounded randomness complexity iff it has finite Littlestone dimension, and moreover scales at worst logarithmically in the excess error of the learner. This resolves a question of (Chase et al. STOC 2024) who asked for such a characterization in the equivalent language of (error-dependent) `list-replicability'.

### Greed is Good: Guided Generation from a Greedy Perspective 
[[arxiv](https://arxiv.org/abs/2502.08006)] [[cool](https://papers.cool/arxiv/2502.08006)] [[pdf](https://arxiv.org/pdf/2502.08006)]
> **Authors**: Zander W. Blasingame,Chen Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Initial preprint
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models. In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner. We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques. We show that a greedy guidance strategy makes good decisions and compare it to a guidance strategy using the ideal gradients found via the continuous adjoint equations. We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective.

### Towards Training One-Step Diffusion Models Without Distillation 
[[arxiv](https://arxiv.org/abs/2502.08005)] [[cool](https://papers.cool/arxiv/2502.08005)] [[pdf](https://arxiv.org/pdf/2502.08005)]
> **Authors**: Mingtian Zhang,Jiajun He,Wenlin Chen,Zijing Ou,José Miguel Hernández-Lobato,Bernhard Schölkopf,David Barber
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 13 pages, Technical Report
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Recent advances in one-step generative models typically follow a two-stage process: first training a teacher diffusion model and then distilling it into a one-step student model. This distillation process traditionally relies on both the teacher model's score function to compute the distillation loss and its weights for student initialization. In this paper, we explore whether one-step generative models can be trained directly without this distillation process. First, we show that the teacher's score function is not essential and propose a family of distillation methods that achieve competitive results without relying on score estimation. Next, we demonstrate that initialization from teacher weights is indispensable in successful training. Surprisingly, we find that this benefit is not due to improved ``input-output" mapping but rather the learned feature representations, which dominate distillation quality. Our findings provide a better understanding of the role of initialization in one-step model training and its impact on distillation quality.

### Heterogeneous Multi-agent Multi-armed Bandits on Stochastic Block Models 
[[arxiv](https://arxiv.org/abs/2502.08003)] [[cool](https://papers.cool/arxiv/2502.08003)] [[pdf](https://arxiv.org/pdf/2502.08003)]
> **Authors**: Mengfan Xu,Liren Shan,Fatemeh Ghaffari,Xuchuang Wang,Xutong Liu,Mohammad Hajiesmaili
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 55 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We study a novel heterogeneous multi-agent multi-armed bandit problem with a cluster structure induced by stochastic block models, influencing not only graph topology, but also reward heterogeneity. Specifically, agents are distributed on random graphs based on stochastic block models - a generalized Erdos-Renyi model with heterogeneous edge probabilities: agents are grouped into clusters (known or unknown); edge probabilities for agents within the same cluster differ from those across clusters. In addition, the cluster structure in stochastic block model also determines our heterogeneous rewards. Rewards distributions of the same arm vary across agents in different clusters but remain consistent within a cluster, unifying homogeneous and heterogeneous settings and varying degree of heterogeneity, and rewards are independent samples from these distributions. The objective is to minimize system-wide regret across all agents. To address this, we propose a novel algorithm applicable to both known and unknown cluster settings. The algorithm combines an averaging-based consensus approach with a newly introduced information aggregation and weighting technique, resulting in a UCB-type strategy. It accounts for graph randomness, leverages both intra-cluster (homogeneous) and inter-cluster (heterogeneous) information from rewards and graphs, and incorporates cluster detection for unknown cluster settings. We derive optimal instance-dependent regret upper bounds of order $\log{T}$ under sub-Gaussian rewards. Importantly, our regret bounds capture the degree of heterogeneity in the system (an additional layer of complexity), exhibit smaller constants, scale better for large systems, and impose significantly relaxed assumptions on edge probabilities. In contrast, prior works have not accounted for this refined problem complexity, rely on more stringent assumptions, and exhibit limited scalability.

### Adaptive kernel predictors from feature-learning infinite limits of neural networks 
[[arxiv](https://arxiv.org/abs/2502.07998)] [[cool](https://papers.cool/arxiv/2502.07998)] [[pdf](https://arxiv.org/pdf/2502.07998)]
> **Authors**: Clarissa Lauditi,Blake Bordelon,Cengiz Pehlevan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,无序系统和神经网络,机器学习
- **Abstract**: Previous influential work showed that infinite width limits of neural networks in the lazy training regime are described by kernel machines. Here, we show that neural networks trained in the rich, feature learning infinite-width regime in two different settings are also described by kernel machines, but with data-dependent kernels. For both cases, we provide explicit expressions for the kernel predictors and prescriptions to numerically calculate them. To derive the first predictor, we study the large-width limit of feature-learning Bayesian networks, showing how feature learning leads to task-relevant adaptation of layer kernels and preactivation densities. The saddle point equations governing this limit result in a min-max optimization problem that defines the kernel predictor. To derive the second predictor, we study gradient flow training of randomly initialized networks trained with weight decay in the infinite-width limit using dynamical mean field theory (DMFT). The fixed point equations of the arising DMFT defines the task-adapted internal representations and the kernel predictor. We compare our kernel predictors to kernels derived from lazy regime and demonstrate that our adaptive kernels achieve lower test loss on benchmark datasets.

### Learning Effective Dynamics across Spatio-Temporal Scales of Complex Flows 
[[arxiv](https://arxiv.org/abs/2502.07990)] [[cool](https://papers.cool/arxiv/2502.07990)] [[pdf](https://arxiv.org/pdf/2502.07990)]
> **Authors**: Han Gao,Sebastian Kaltenbach,Petros Koumoutsakos
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Conference on Parsimony andLearning(CPAL)
- **标题**: None
- **领域**: 机器学习,计算物理,流体动力学
- **Abstract**: Modeling and simulation of complex fluid flows with dynamics that span multiple spatio-temporal scales is a fundamental challenge in many scientific and engineering domains. Full-scale resolving simulations for systems such as highly turbulent flows are not feasible in the foreseeable future, and reduced-order models must capture dynamics that involve interactions across scales. In the present work, we propose a novel framework, Graph-based Learning of Effective Dynamics (Graph-LED), that leverages graph neural networks (GNNs), as well as an attention-based autoregressive model, to extract the effective dynamics from a small amount of simulation data. GNNs represent flow fields on unstructured meshes as graphs and effectively handle complex geometries and non-uniform grids. The proposed method combines a GNN based, dimensionality reduction for variable-size unstructured meshes with an autoregressive temporal attention model that can learn temporal dependencies automatically. We evaluated the proposed approach on a suite of fluid dynamics problems, including flow past a cylinder and flow over a backward-facing step over a range of Reynolds numbers. The results demonstrate robust and effective forecasting of spatio-temporal physics; in the case of the flow past a cylinder, both small-scale effects that occur close to the cylinder as well as its wake are accurately captured.

### CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs 
[[arxiv](https://arxiv.org/abs/2502.07980)] [[cool](https://papers.cool/arxiv/2502.07980)] [[pdf](https://arxiv.org/pdf/2502.07980)]
> **Authors**: Lejla Skelic,Yan Xu,Matthew Cox,Wenjie Lu,Tao Yu,Ruonan Han
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.

### A Survey of In-Context Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07978)] [[cool](https://papers.cool/arxiv/2502.07978)] [[pdf](https://arxiv.org/pdf/2502.07978)]
> **Authors**: Amir Moeini,Jiuqi Wang,Jacob Beck,Ethan Blaser,Shimon Whiteson,Rohan Chandra,Shangtong Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement learning (RL) agents typically optimize their policies by performing expensive backward passes to update their network parameters. However, some agents can solve new tasks without updating any parameters by simply conditioning on additional context such as their action-observation histories. This paper surveys work on such behavior, known as in-context reinforcement learning.

### RESIST: Resilient Decentralized Learning Using Consensus Gradient Descent 
[[arxiv](https://arxiv.org/abs/2502.07977)] [[cool](https://papers.cool/arxiv/2502.07977)] [[pdf](https://arxiv.org/pdf/2502.07977)]
> **Authors**: Cheng Fang,Rishabh Dixit,Waheed U. Bajwa,Mert Gurbuzbalaban
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: preprint of a journal paper; 100 pages and 17 figures
- **标题**: None
- **领域**: 机器学习,优化与控制,机器学习
- **Abstract**: Empirical risk minimization (ERM) is a cornerstone of modern machine learning (ML), supported by advances in optimization theory that ensure efficient solutions with provable algorithmic convergence rates, which measure the speed at which optimization algorithms approach a solution, and statistical learning rates, which characterize how well the solution generalizes to unseen data. Privacy, memory, computational, and communications constraints increasingly necessitate data collection, processing, and storage across network-connected devices. In many applications, these networks operate in decentralized settings where a central server cannot be assumed, requiring decentralized ML algorithms that are both efficient and resilient. Decentralized learning, however, faces significant challenges, including an increased attack surface for adversarial interference during decentralized learning processes. This paper focuses on the man-in-the-middle (MITM) attack, which can cause models to deviate significantly from their intended ERM solutions. To address this challenge, we propose RESIST (Resilient dEcentralized learning using conSensus gradIent deScenT), an optimization algorithm designed to be robust against adversarially compromised communication links. RESIST achieves algorithmic and statistical convergence for strongly convex, Polyak-Lojasiewicz, and nonconvex ERM problems. Experimental results demonstrate the robustness and scalability of RESIST for real-world decentralized learning in adversarial environments.

### Generative Risk Minimization for Out-of-Distribution Generalization on Graphs 
[[arxiv](https://arxiv.org/abs/2502.07968)] [[cool](https://papers.cool/arxiv/2502.07968)] [[pdf](https://arxiv.org/pdf/2502.07968)]
> **Authors**: Song Wang,Zhen Tan,Yaochen Zhu,Chuxu Zhang,Jundong Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: TMLR 02/2025
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.

### New tools for comparing classical and neural ODE models for tumor growth 
[[arxiv](https://arxiv.org/abs/2502.07964)] [[cool](https://papers.cool/arxiv/2502.07964)] [[pdf](https://arxiv.org/pdf/2502.07964)]
> **Authors**: Anthony D. Blaom,Samuel Okon
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 9 pages, 2 figures. Related software is archived at https://github.com/ablaom/TumorGrowth.jl
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: A new computational tool TumorGrowth$.$jl for modeling tumor growth is introduced. The tool allows the comparison of standard textbook models, such as General Bertalanffy and Gompertz, with some newer models, including, for the first time, neural ODE models. As an application, we revisit a human meta-study of non-small cell lung cancer and bladder cancer lesions, in patients undergoing two different treatment options, to determine if previously reported performance differences are statistically significant, and if newer, more complex models perform any better. In a population of examples with at least four time-volume measurements available for calibration, and an average of about 6.3, our main conclusion is that the General Bertalanffy model has superior performance, on average. However, where more measurements are available, we argue that more complex models, capable of capturing rebound and relapse behavior, may be better choices.

### ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport Plans 
[[arxiv](https://arxiv.org/abs/2502.07962)] [[cool](https://papers.cool/arxiv/2502.07962)] [[pdf](https://arxiv.org/pdf/2502.07962)]
> **Authors**: Ashkan Shahbazi,Elaheh Akbari,Darian Salehi,Xinran Liu,Navid Naderializadeh,Soheil Kolouri
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While self-attention has been instrumental in the success of Transformers, it can lead to over-concentration on a few tokens during training, resulting in suboptimal information flow. Enforcing doubly-stochastic constraints in attention matrices has been shown to improve structure and balance in attention distributions. However, existing methods rely on iterative Sinkhorn normalization, which is computationally costly. In this paper, we introduce a novel, fully parallelizable doubly-stochastic attention mechanism based on sliced optimal transport, leveraging Expected Sliced Transport Plans (ESP). Unlike prior approaches, our method enforces double stochasticity without iterative Sinkhorn normalization, significantly enhancing efficiency. To ensure differentiability, we incorporate a temperature-based soft sorting technique, enabling seamless integration into deep learning models. Experiments across multiple benchmark datasets, including image classification, point cloud classification, sentiment analysis, and neural machine translation, demonstrate that our enhanced attention regularization consistently improves performance across diverse applications.

### VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07949)] [[cool](https://papers.cool/arxiv/2502.07949)] [[pdf](https://arxiv.org/pdf/2502.07949)]
> **Authors**: Qingyuan Wu,Jianheng Liu,Jianye Hao,Jun Wang,Kun Shao
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision. However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency.

### Active Advantage-Aligned Online Reinforcement Learning with Offline Data 
[[arxiv](https://arxiv.org/abs/2502.07937)] [[cool](https://papers.cool/arxiv/2502.07937)] [[pdf](https://arxiv.org/pdf/2502.07937)]
> **Authors**: Xuefeng Liu,Hung T. C. Le,Siyu Chen,Rick Stevens,Zhuoran Yang,Matthew R. Walter,Yuxin Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Online reinforcement learning (RL) enhances policies through direct interactions with the environment, but faces challenges related to sample efficiency. In contrast, offline RL leverages extensive pre-collected data to learn policies, but often produces suboptimal results due to limited data coverage. Recent efforts have sought to integrate offline and online RL in order to harness the advantages of both approaches. However, effectively combining online and offline RL remains challenging due to issues that include catastrophic forgetting, lack of robustness and sample efficiency. In an effort to address these challenges, we introduce A3 RL , a novel method that actively selects data from combined online and offline sources to optimize policy improvement. We provide theoretical guarantee that validates the effectiveness our active sampling strategy and conduct thorough empirical experiments showing that our method outperforms existing state-of-the-art online RL techniques that utilize offline data. Our code will be publicly available at: https://github.com/xuefeng-cs/A3RL.

### TransMLA: Multi-Head Latent Attention Is All You Need 
[[arxiv](https://arxiv.org/abs/2502.07864)] [[cool](https://papers.cool/arxiv/2502.07864)] [[pdf](https://arxiv.org/pdf/2502.07864)]
> **Authors**: Fanxu Meng,Zengwei Yao,Muhan Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: https://github.com/fxmeng/TransMLA
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce TransMLA, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.

### ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources 
[[arxiv](https://arxiv.org/abs/2502.07862)] [[cool](https://papers.cool/arxiv/2502.07862)] [[pdf](https://arxiv.org/pdf/2502.07862)]
> **Authors**: Jason Wu,Kang Yang,Lance Kaplan,Mani Srivastava
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Multimodal deep learning systems are deployed in dynamic scenarios due to the robustness afforded by multiple sensing modalities. Nevertheless, they struggle with varying compute resource availability (due to multi-tenancy, device heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed corruption, environmental noise, etc.). Current multimodal systems employ static resource provisioning and cannot easily adapt when compute resources change over time. Additionally, their reliance on processing sensor data with fixed feature extractors is ill-equipped to handle variations in modality quality. Consequently, uninformative modalities, such as those with high noise, needlessly consume resources better allocated towards other modalities. We propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of tackling both challenges - it adjusts the total number of active layers across all modalities to meet compute resource constraints, and continually reallocates layers across input modalities according to their modality quality. Our evaluations showcase ADMN can match the accuracy of state-of-the-art networks while reducing up to 75% of their floating-point operations.

### BalanceKV: KV Cache Compression through Discrepancy Theory 
[[arxiv](https://arxiv.org/abs/2502.07861)] [[cool](https://papers.cool/arxiv/2502.07861)] [[pdf](https://arxiv.org/pdf/2502.07861)]
> **Authors**: Insu Han,Michael Kapralov,Ekaterina Kochetkova,Kshiteej Sheth,Amir Zandieh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,数据结构和算法
- **Abstract**: Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods.

### MAAT: Mamba Adaptive Anomaly Transformer with association discrepancy for time series 
[[arxiv](https://arxiv.org/abs/2502.07858)] [[cool](https://papers.cool/arxiv/2502.07858)] [[pdf](https://arxiv.org/pdf/2502.07858)]
> **Authors**: Abdellah Zakaria Sellam,Ilyes Benaissa,Abdelmalik Taleb-Ahmed,Luigi Patrono,Cosimo Distante
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Anomaly detection in time series is essential for industrial monitoring and environmental sensing, yet distinguishing anomalies from complex patterns remains challenging. Existing methods like the Anomaly Transformer and DCdetector have progressed, but they face limitations such as sensitivity to short-term contexts and inefficiency in noisy, non-stationary environments. To overcome these issues, we introduce MAAT, an improved architecture that enhances association discrepancy modeling and reconstruction quality. MAAT features Sparse Attention, efficiently capturing long-range dependencies by focusing on relevant time steps, thereby reducing computational redundancy. Additionally, a Mamba-Selective State Space Model is incorporated into the reconstruction module, utilizing a skip connection and Gated Attention to improve anomaly localization and detection performance. Extensive experiments show that MAAT significantly outperforms previous methods, achieving better anomaly distinguishability and generalization across various time series applications, setting a new standard for unsupervised time series anomaly detection in real-world scenarios.

### Advancing Heat Demand Forecasting with Attention Mechanisms: Opportunities and Challenges 
[[arxiv](https://arxiv.org/abs/2502.07854)] [[cool](https://papers.cool/arxiv/2502.07854)] [[pdf](https://arxiv.org/pdf/2502.07854)]
> **Authors**: Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Andreas Maier,Siming Bayer
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Global leaders and policymakers are unified in their unequivocal commitment to decarbonization efforts in support of Net-Zero agreements. District Heating Systems (DHS), while contributing to carbon emissions due to the continued reliance on fossil fuels for heat production, are embracing more sustainable practices albeit with some sense of vulnerability as it could constrain their ability to adapt to dynamic demand and production scenarios. As demographic demands grow and renewables become the central strategy in decarbonizing the heating sector, the need for accurate demand forecasting has intensified. Advances in digitization have paved the way for Machine Learning (ML) based solutions to become the industry standard for modeling complex time series patterns. In this paper, we focus on building a Deep Learning (DL) model that uses deconstructed components of independent and dependent variables that affect heat demand as features to perform multi-step ahead forecasting of head demand. The model represents the input features in a time-frequency space and uses an attention mechanism to generate accurate forecasts. The proposed method is evaluated on a real-world dataset and the forecasting performance is assessed against LSTM and CNN-based forecasting models. Across different supply zones, the attention-based models outperforms the baselines quantitatively and qualitatively, with an Mean Absolute Error (MAE) of 0.105 with a standard deviation of 0.06kW h and a Mean Absolute Percentage Error (MAPE) of 5.4% with a standard deviation of 2.8%, in comparison the second best model with a MAE of 0.10 with a standard deviation of 0.06kW h and a MAPE of 5.6% with a standard deviation of 3%.

### Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations 
[[arxiv](https://arxiv.org/abs/2502.07849)] [[cool](https://papers.cool/arxiv/2502.07849)] [[pdf](https://arxiv.org/pdf/2502.07849)]
> **Authors**: Krunoslav Lehman Pavasovic,Jakob Verbeek,Giulio Biroli,Marc Mezard
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Recent studies have raised concerns about the effectiveness of Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it can lead to overshooting the target distribution and reducing sample diversity. In this work, we demonstrate that in infinite and sufficiently high-dimensional contexts CFG effectively reproduces the target distribution, revealing a blessing-of-dimensionality result. Additionally, we explore finite-dimensional effects, precisely characterizing overshoot and variance reduction. Based on our analysis, we introduce non-linear generalizations of CFG. Through numerical simulations on Gaussian mixtures and experiments on class-conditional and text-to-image diffusion models, we validate our analysis and show that our non-linear CFG offers improved flexibility and generation quality without additional computation cost.

### Emotional EEG Classification using Upscaled Connectivity Matrices 
[[arxiv](https://arxiv.org/abs/2502.07843)] [[cool](https://papers.cool/arxiv/2502.07843)] [[pdf](https://arxiv.org/pdf/2502.07843)]
> **Authors**: Chae-Won Lee,Jong-Seok Lee
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In recent studies of emotional EEG classification, connectivity matrices have been successfully employed as input to convolutional neural networks (CNNs), which can effectively consider inter-regional interaction patterns in EEG. However, we find that such an approach has a limitation that important patterns in connectivity matrices may be lost during the convolutional operations in CNNs. To resolve this issue, we propose and validate an idea to upscale the connectivity matrices to strengthen the local patterns. Experimental results demonstrate that this simple idea can significantly enhance the classification performance.

### SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters 
[[arxiv](https://arxiv.org/abs/2502.07832)] [[cool](https://papers.cool/arxiv/2502.07832)] [[pdf](https://arxiv.org/pdf/2502.07832)]
> **Authors**: Yiping Wang,Hanxian Huang,Yifang Chen,Jishen Zhao,Simon Shaolei Du,Yuandong Tian
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: 24 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.

### Implicit Language Models are RNNs: Balancing Parallelization and Expressivity 
[[arxiv](https://arxiv.org/abs/2502.07827)] [[cool](https://papers.cool/arxiv/2502.07827)] [[pdf](https://arxiv.org/pdf/2502.07827)]
> **Authors**: Mark Schöne,Babak Rahmani,Heiner Kremer,Fabian Falck,Hitesh Ballani,Jannes Gladrow
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: 25 pages, 12 figures, 7 tables
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.

### Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution 
[[arxiv](https://arxiv.org/abs/2502.07814)] [[cool](https://papers.cool/arxiv/2502.07814)] [[pdf](https://arxiv.org/pdf/2502.07814)]
> **Authors**: Siwei Tu,Ben Fei,Weidong Yang,Fenghua Ling,Hao Chen,Zili Liu,Kun Chen,Hang Fan,Wanli Ouyang,Lei Bai
> **First submission**: 2025-02-08
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,大气和海洋物理
- **Abstract**: Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km.

### Curvature Tuning: Provable Training-free Model Steering From a Single Parameter 
[[arxiv](https://arxiv.org/abs/2502.07783)] [[cool](https://papers.cool/arxiv/2502.07783)] [[pdf](https://arxiv.org/pdf/2502.07783)]
> **Authors**: Leyang Hu,Randall Balestriero
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The scaling of model size and data size has reshaped the paradigm of AI. As a result, the common protocol to leverage the latest models is to steer them towards a specific downstream task of interest through {\em fine-tuning}. Despite its importance, the main methods for fine-tuning remain limited to full or low-rank adapters--containing countless hyper-parameters and lacking interpretability. In this paper, we take a step back and demonstrate how novel and explainable post-training steering solutions can be derived theoretically from {\em spline operators}, a rich mathematical framing of Deep Networks that was recently developed. Our method--coined \textbf{Curvature Tuning (CT)}--has a single parameter that provably modulates the curvature of the model's decision boundary henceforth allowing training-free steering. This makes CT both more efficient and interpretable than conventional fine-tuning methods. We empirically validate its effectiveness in improving generalization and robustness of pretrained models. For example, CT improves out-of-distribution transfer performances of ResNet-18/50 by 2.57\%/1.74\% across seventeen downstream datasets, and improves RobustBench robust accuracy by 11.76\%/348.44\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving their generalization on nine downstream datasets by 2.43\%/3.33\%. Our code is available at \href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.

### DarwinLM: Evolutionary Structured Pruning of Large Language Models 
[[arxiv](https://arxiv.org/abs/2502.07780)] [[cool](https://papers.cool/arxiv/2502.07780)] [[pdf](https://arxiv.org/pdf/2502.07780)]
> **Authors**: Shengkun Tang,Oliver Sieberling,Eldar Kurtic,Zhiqiang Shen,Dan Alistarh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Code: https://github.com/IST-DASLab/DarwinLM
- **标题**: None
- **领域**: 机器学习,计算语言学
- **Abstract**: Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for \emph{non-uniform} model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for \emph{training-aware} structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring $5\times$ less training data during post-compression training. Code is at: https://github.com/IST-DASLab/DarwinLM

### Optimistic Interior Point Methods for Sequential Hypothesis Testing by Betting 
[[arxiv](https://arxiv.org/abs/2502.07774)] [[cool](https://papers.cool/arxiv/2502.07774)] [[pdf](https://arxiv.org/pdf/2502.07774)]
> **Authors**: Can Chen,Jun-Kun Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The technique of "testing by betting" frames nonparametric sequential hypothesis testing as a multiple-round game, where a player bets on future observations that arrive in a streaming fashion, accumulates wealth that quantifies evidence against the null hypothesis, and rejects the null once the wealth exceeds a specified threshold while controlling the false positive error. Designing an online learning algorithm that achieves a small regret in the game can help rapidly accumulate the bettor's wealth, which in turn can shorten the time to reject the null hypothesis under the alternative $H_1$. However, many of the existing works employ the Online Newton Step (ONS) to update within a halved decision space to avoid a gradient explosion issue, which is potentially conservative for rapid wealth accumulation. In this paper, we introduce a novel strategy utilizing interior-point methods in optimization that allows updates across the entire interior of the decision space without the risk of gradient explosion. Our approach not only maintains strong statistical guarantees but also facilitates faster null hypothesis rejection in critical scenarios, overcoming the limitations of existing approaches.

### Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension 
[[arxiv](https://arxiv.org/abs/2502.07752)] [[cool](https://papers.cool/arxiv/2502.07752)] [[pdf](https://arxiv.org/pdf/2502.07752)]
> **Authors**: Wenbo Gong,Meyer Scetbon,Chao Ma,Edward Meeds
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory.

### PFedDST: Personalized Federated Learning with Decentralized Selection Training 
[[arxiv](https://arxiv.org/abs/2502.07750)] [[cool](https://papers.cool/arxiv/2502.07750)] [[pdf](https://arxiv.org/pdf/2502.07750)]
> **Authors**: Mengchen Fan,Keren Li,Tianyun Zhang,Qing Tian,Baocheng Geng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.

### HiPoNet: A Topology-Preserving Multi-View Neural Network For High Dimensional Point Cloud and Single-Cell Data 
[[arxiv](https://arxiv.org/abs/2502.07746)] [[cool](https://papers.cool/arxiv/2502.07746)] [[pdf](https://arxiv.org/pdf/2502.07746)]
> **Authors**: Siddharth Viswanath,Hiren Madhu,Dhananjay Bhaskar,Jake Kovalic,Dave Johnson,Rex Ying,Christopher Tape,Ian Adelstein,Michael Perlmutter,Smita Krishnaswamy
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,代数拓扑
- **Abstract**: In this paper, we propose HiPoNet, an end-to-end differentiable neural network for regression, classification, and representation learning on high-dimensional point clouds. Single-cell data can have high dimensionality exceeding the capabilities of existing methods point cloud tailored for 3D data. Moreover, modern single-cell and spatial experiments now yield entire cohorts of datasets (i.e. one on every patient), necessitating models that can process large, high-dimensional point clouds at scale. Most current approaches build a single nearest-neighbor graph, discarding important geometric information. In contrast, HiPoNet forms higher-order simplicial complexes through learnable feature reweighting, generating multiple data views that disentangle distinct biological processes. It then employs simplicial wavelet transforms to extract multi-scale features - capturing both local and global topology. We empirically show that these components preserve topological information in the learned representations, and that HiPoNet significantly outperforms state-of-the-art point-cloud and graph-based models on single cell. We also show an application of HiPoNet on spatial transcriptomics datasets using spatial co-ordinates as one of the views. Overall, HiPoNet offers a robust and scalable solution for high-dimensional data analysis.

### Advancing climate model interpretability: Feature attribution for Arctic melt anomalies 
[[arxiv](https://arxiv.org/abs/2502.07741)] [[cool](https://papers.cool/arxiv/2502.07741)] [[pdf](https://arxiv.org/pdf/2502.07741)]
> **Authors**: Tolulope Ale,Nicole-Jeanne Schlegel,Vandana P. Janeja
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 9 pages
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The focus of our work is improving the interpretability of anomalies in climate models and advancing our understanding of Arctic melt dynamics. The Arctic and Antarctic ice sheets are experiencing rapid surface melting and increased freshwater runoff, contributing significantly to global sea level rise. Understanding the mechanisms driving snowmelt in these regions is crucial. ERA5, a widely used reanalysis dataset in polar climate studies, offers extensive climate variables and global data assimilation. However, its snowmelt model employs an energy imbalance approach that may oversimplify the complexity of surface melt. In contrast, the Glacier Energy and Mass Balance (GEMB) model incorporates additional physical processes, such as snow accumulation, firn densification, and meltwater percolation/refreezing, providing a more detailed representation of surface melt dynamics. In this research, we focus on analyzing surface snowmelt dynamics of the Greenland Ice Sheet using feature attribution for anomalous melt events in ERA5 and GEMB models. We present a novel unsupervised attribution method leveraging counterfactual explanation method to analyze detected anomalies in ERA5 and GEMB. Our anomaly detection results are validated using MEaSUREs ground-truth data, and the attributions are evaluated against established feature ranking methods, including XGBoost, Shapley values, and Random Forest. Our attribution framework identifies the physics behind each model and the climate features driving melt anomalies. These findings demonstrate the utility of our attribution method in enhancing the interpretability of anomalies in climate models and advancing our understanding of Arctic melt dynamics.

### HRP: High-Rank Preheating for Superior LoRA Initialization 
[[arxiv](https://arxiv.org/abs/2502.07739)] [[cool](https://papers.cool/arxiv/2502.07739)] [[pdf](https://arxiv.org/pdf/2502.07739)]
> **Authors**: Yuzhu Chen,Yingjie Wang,Shi Fu,Li Shen,Yongcheng Jing,Xinmei Tian,Dacheng Tao
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: This paper studies the crucial impact of initialization on the convergence properties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that random initialization, a widely used schema, will likely lead LoRA to random low-rank results, rather than the best low-rank result. While this issue can be mitigated by adjusting initialization towards a well-informed direction, it relies on prior knowledge of the target, which is typically unknown in real-world scenarios. To approximate this well-informed initial direction, we propose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few steps and uses the singular value decomposition of the preheated result as a superior initialization. HRP initialization is theory-supported to combine the convergence strengths of high-rank LoRA and the generalization strengths of low-rank LoRA. Extensive experiments demonstrate that HRP significantly enhances LoRA's effectiveness across various models and tasks, achieving performance comparable to full-parameter fine-tuning and outperforming other initialization strategies.

### Revisiting Non-Acyclic GFlowNets in Discrete Environments 
[[arxiv](https://arxiv.org/abs/2502.07735)] [[cool](https://papers.cool/arxiv/2502.07735)] [[pdf](https://arxiv.org/pdf/2502.07735)]
> **Authors**: Nikita Morozov,Ian Maksimov,Daniil Tiapkin,Sergey Samsonov
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects from a given probability distribution, potentially known up to a normalizing constant. Instead of working in the object space, GFlowNets proceed by sampling trajectories in an appropriately constructed directed acyclic graph environment, greatly relying on the acyclicity of the graph. In our paper, we revisit the theory that relaxes the acyclicity assumption and present a simpler theoretical framework for non-acyclic GFlowNets in discrete environments. Moreover, we provide various novel theoretical insights related to training with fixed backward policies, the nature of flow functions, and connections between entropy-regularized RL and non-acyclic GFlowNets, which naturally generalize the respective concepts and theoretical results from the acyclic setting. In addition, we experimentally re-examine the concept of loss stability in non-acyclic GFlowNet training, as well as validate our own theoretical findings.

### TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning 
[[arxiv](https://arxiv.org/abs/2502.07721)] [[cool](https://papers.cool/arxiv/2502.07721)] [[pdf](https://arxiv.org/pdf/2502.07721)]
> **Authors**: Mengyang Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: The prevalence of noisy labels in real-world datasets poses a significant impediment to the effective deployment of deep learning models. While meta-learning strategies have emerged as a promising approach for addressing this challenge, existing methods often suffer from limited transferability and task-specific designs. This paper introduces TMLC-Net, a novel Transferable Meta-Learner for Correcting Noisy Labels, designed to overcome these limitations. TMLC-Net learns a general-purpose label correction strategy that can be readily applied across diverse datasets and model architectures without requiring extensive retraining or fine-tuning. Our approach integrates three core components: (1) Normalized Noise Perception, which captures and normalizes training dynamics to handle distribution shifts; (2) Time-Series Encoding, which models the temporal evolution of sample statistics using a recurrent neural network; and (3) Subclass Decoding, which predicts a corrected label distribution based on the learned representations. We conduct extensive experiments on benchmark datasets with various noise types and levels, demonstrating that TMLC-Net consistently outperforms state-of-the-art methods in terms of both accuracy and robustness to label noise. Furthermore, we analyze the transferability of TMLC-Net, showcasing its adaptability to new datasets and noise conditions, and establishing its potential as a broadly applicable solution for robust deep learning in noisy environments.

### Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07715)] [[cool](https://papers.cool/arxiv/2502.07715)] [[pdf](https://arxiv.org/pdf/2502.07715)]
> **Authors**: Aya Kayal,Sattar Vakili,Laura Toni,Alberto Bernacchia
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted at AISTATS 2025
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Reinforcement Learning (RL) problems are being considered under increasingly more complex structures. While tabular and linear models have been thoroughly explored, the analytical study of RL under nonlinear function approximation, especially kernel-based models, has recently gained traction for their strong representational capacity and theoretical tractability. In this context, we examine the question of statistical efficiency in kernel-based RL within the reward-free RL framework, specifically asking: how many samples are required to design a near-optimal policy? Existing work addresses this question under restrictive assumptions about the class of kernel functions. We first explore this question by assuming a generative model, then relax this assumption at the cost of increasing the sample complexity by a factor of H, the length of the episode. We tackle this fundamental problem using a broad class of kernels and a simpler algorithm compared to prior work. Our approach derives new confidence intervals for kernel ridge regression, specific to our RL setting, which may be of broader applicability. We further validate our theoretical findings through simulations.

### Partial-Label Learning with Conformal Candidate Cleaning 
[[arxiv](https://arxiv.org/abs/2502.07661)] [[cool](https://papers.cool/arxiv/2502.07661)] [[pdf](https://arxiv.org/pdf/2502.07661)]
> **Authors**: Tobias Fuchs,Florian Kalinke
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Real-world data is often ambiguous; for example, human annotation produces instances with multiple conflicting class labels. Partial-label learning (PLL) aims at training a classifier in this challenging setting, where each instance is associated with a set of candidate labels and one correct, but unknown, class label. A multitude of algorithms targeting this setting exists and, to enhance their prediction quality, several extensions that are applicable across a wide range of PLL methods have been introduced. While many of these extensions rely on heuristics, this article proposes a novel enhancing method that incrementally prunes candidate sets using conformal prediction. To work around the missing labeled validation set, which is typically required for conformal prediction, we propose a strategy that alternates between training a PLL classifier to label the validation set, leveraging these predicted class labels for calibration, and pruning candidate labels that are not part of the resulting conformal sets. In this sense, our method alternates between empirical risk minimization and candidate set pruning. We establish that our pruning method preserves the conformal validity with respect to the unknown ground truth. Our extensive experiments on artificial and real-world data show that the proposed approach significantly improves the test set accuracies of several state-of-the-art PLL classifiers.

### A Unifying Framework for Causal Imitation Learning with Hidden Confounders 
[[arxiv](https://arxiv.org/abs/2502.07656)] [[cool](https://papers.cool/arxiv/2502.07656)] [[pdf](https://arxiv.org/pdf/2502.07656)]
> **Authors**: Daqian Shao,Thomas Kleine Buening,Marta Kwiatkowska
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We propose a general and unifying framework for causal Imitation Learning (IL) with hidden confounders that subsumes several existing confounded IL settings from the literature. Our framework accounts for two types of hidden confounders: (a) those observed by the expert, which thus influence the expert's policy, and (b) confounding noise hidden to both the expert and the IL algorithm. For additional flexibility, we also introduce a confounding noise horizon and time-varying expert-observable hidden variables. We show that causal IL in our framework can be reduced to a set of Conditional Moment Restrictions (CMRs) by leveraging trajectory histories as instruments to learn a history-dependent policy. We propose DML-IL, a novel algorithm that uses instrumental variable regression to solve these CMRs and learn a policy. We provide a bound on the imitation gap for DML-IL, which recovers prior results as special cases. Empirical evaluation on a toy environment with continues state-action spaces and multiple Mujoco tasks demonstrate that DML-IL outperforms state-of-the-art causal IL algorithms.

### Causal Additive Models with Unobserved Causal Paths and Backdoor Paths 
[[arxiv](https://arxiv.org/abs/2502.07646)] [[cool](https://papers.cool/arxiv/2502.07646)] [[pdf](https://arxiv.org/pdf/2502.07646)]
> **Authors**: Thong Pham,Takashi Nicholas Maeda,Shohei Shimizu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 14 pages
- **标题**: None
- **领域**: 机器学习,方法论,机器学习
- **Abstract**: Causal additive models have been employed as tractable yet expressive frameworks for causal discovery involving hidden variables. State-of-the-art methodologies suggest that determining the causal relationship between a pair of variables is infeasible in the presence of an unobserved backdoor or an unobserved causal path. Contrary to this assumption, we theoretically show that resolving the causal direction is feasible in certain scenarios by incorporating two novel components into the theory. The first component introduces a novel characterization of regression sets within independence between regression residuals. The second component leverages conditional independence among the observed variables. We also provide a search algorithm that integrates these innovations and demonstrate its competitive performance against existing methods.

### Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving 
[[arxiv](https://arxiv.org/abs/2502.07640)] [[cool](https://papers.cool/arxiv/2502.07640)] [[pdf](https://arxiv.org/pdf/2502.07640)]
> **Authors**: Yong Lin,Shange Tang,Bohan Lyu,Jiayun Wu,Hongzhou Lin,Kaiyu Yang,Jia Li,Mengzhou Xia,Danqi Chen,Sanjeev Arora,Chi Jin
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. Despite using only supervised fine-tuning, our final prover significantly outperforms the previous best open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning. On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32), surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.

### Consistency Training with Physical Constraints 
[[arxiv](https://arxiv.org/abs/2502.07636)] [[cool](https://papers.cool/arxiv/2502.07636)] [[pdf](https://arxiv.org/pdf/2502.07636)]
> **Authors**: Che-Chia Chang,Chen-Yang Dai,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We propose a physics-aware Consistency Training (CT) method that accelerates sampling in Diffusion Models with physical constraints. Our approach leverages a two-stage strategy: (1) learning the noise-to-data mapping via CT, and (2) incorporating physics constraints as a regularizer. Experiments on toy examples show that our method generates samples in a single step while adhering to the imposed constraints. This approach has the potential to efficiently solve partial differential equations (PDEs) using deep generative modeling.

### Distributed Value Decomposition Networks with Networked Agents 
[[arxiv](https://arxiv.org/abs/2502.07635)] [[cool](https://papers.cool/arxiv/2502.07635)] [[pdf](https://arxiv.org/pdf/2502.07635)]
> **Authors**: Guilherme S. Varela,Alberto Sardinha,Francisco S. Melo
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 21 pages, 15 figures, to be published in Proceedings of the 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS
- **标题**: None
- **领域**: 机器学习,人工智能,多代理系统
- **Abstract**: We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL tasks in three standard environments.

### Efficient Distributed Training through Gradient Compression with Sparsification and Quantization Techniques 
[[arxiv](https://arxiv.org/abs/2502.07634)] [[cool](https://papers.cool/arxiv/2502.07634)] [[pdf](https://arxiv.org/pdf/2502.07634)]
> **Authors**: Shruti Singh,Shantanu Kumar
> **First submission**: 2024-12-07
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,多媒体
- **Abstract**: This study investigates the impact of gradient compression on distributed training performance, focusing on sparsification and quantization techniques, including top-k, DGC, and QSGD. In baseline experiments, random-k compression results in severe performance degradation, highlighting its inefficacy. In contrast, using top-k and DGC at 50 times compression yields performance improvements, reducing perplexity by up to 0.06 compared to baseline. Experiments across 1, 2, and 4 workers demonstrate that conservative sparsification can have a regularizing effect, especially for smaller models, while compression ratios above 5000 times impair performance, particularly for DGC. Communication times are reduced across all compression methods, with top-k and DGC decreasing communication to negligible levels at high compression ratios. However, increased computation times offset this efficiency for top-k due to sorting demands, making it less scalable than DGC or QSGD. In convergence tests, sparsification techniques show accelerated convergence, requiring fewer epochs than the baseline, which has implications for computational savings. Although precision trade-offs emerge, floating point errors are mitigated by compression. This study's findings underscore the need to tune hyperparameters specifically for each compression technique to achieve optimal model performance, especially in distributed training systems.

### Causal-Informed Contrastive Learning: Towards Bias-Resilient Pre-training under Concept Drift 
[[arxiv](https://arxiv.org/abs/2502.07620)] [[cool](https://papers.cool/arxiv/2502.07620)] [[pdf](https://arxiv.org/pdf/2502.07620)]
> **Authors**: Xiaoyu Yang,Jie Lu,En Yu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 17pages, 3 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: The evolution of large-scale contrastive pre-training propelled by top-tier datasets has reached a transition point in the scaling law. Consequently, sustaining and enhancing a model's pre-training capabilities in drift environments have surfaced as a notable challenge. In this paper, we initially uncover that contrastive pre-training methods are significantly impacted by concept drift wherein distributions change unpredictably, resulting in notable biases in the feature space of the pre-trained model. Empowered by causal inference, we construct a structural causal graph to analyze the impact of concept drift to contrastive pre-training systemically, and propose the causal interventional contrastive objective. Upon achieving this, we devise a resilient contrastive pre-training approach to accommodate the data stream of concept drift, with simple and scalable implementation. Extensive experiments on various downstream tasks demonstrate our resilient contrastive pre-training effectively mitigates the bias stemming from the concept drift data stream. Codes are available at https://anonymous.4open.science/r/ResilientCL/.

### Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models and Large Language Models for Health Sensing 
[[arxiv](https://arxiv.org/abs/2502.07608)] [[cool](https://papers.cool/arxiv/2502.07608)] [[pdf](https://arxiv.org/pdf/2502.07608)]
> **Authors**: Arvind Pillai,Dimitris Spathis,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 17 pages, 7 figures
- **标题**: None
- **领域**: 机器学习,人机交互
- **Abstract**: Large language models (LLMs) show promise for health applications when combined with behavioral sensing data. Traditional approaches convert sensor data into text prompts, but this process is prone to errors, computationally expensive, and requires domain expertise. These challenges are particularly acute when processing extended time series data. While time series foundation models (TFMs) have recently emerged as powerful tools for learning representations from temporal data, bridging TFMs and LLMs remains challenging. Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM representations without intermediate text conversion. Our approach first trains on synthetic data using periodicity prediction as a pretext task, followed by evaluation on mental health classification tasks. We validate Time2Lang on two longitudinal wearable and mobile sensing datasets: daily depression prediction using step count data (17,251 days from 256 participants) and flourishing classification based on conversation duration (46 participants over 10 weeks). Time2Lang maintains near constant inference times regardless of input length, unlike traditional prompting methods. The generated embeddings preserve essential time-series characteristics such as auto-correlation. Our results demonstrate that TFMs and LLMs can be effectively integrated while minimizing information loss and enabling performance transfer across these distinct modeling paradigms. To our knowledge, we are the first to integrate a TFM and an LLM for health, thus establishing a foundation for future research combining general-purpose large models for complex healthcare tasks.

### DMWM: Dual-Mind World Model with Long-Term Imagination 
[[arxiv](https://arxiv.org/abs/2502.07591)] [[cool](https://papers.cool/arxiv/2502.07591)] [[pdf](https://arxiv.org/pdf/2502.07591)]
> **Authors**: Lingyi Wang,Rashed Shelim,Walid Saad,Naren Ramakrishnan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models.

### SEMU: Singular Value Decomposition for Efficient Machine Unlearning 
[[arxiv](https://arxiv.org/abs/2502.07587)] [[cool](https://papers.cool/arxiv/2502.07587)] [[pdf](https://arxiv.org/pdf/2502.07587)]
> **Authors**: Marcin Sendera,Łukasz Struski,Kamil Książek,Kryspin Musiol,Jacek Tabor,Dawid Rymarczyk
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: While the capabilities of generative foundational models have advanced rapidly in recent years, methods to prevent harmful and unsafe behaviors remain underdeveloped. Among the pressing challenges in AI safety, machine unlearning (MU) has become increasingly critical to meet upcoming safety regulations. Most existing MU approaches focus on altering the most significant parameters of the model. However, these methods often require fine-tuning substantial portions of the model, resulting in high computational costs and training instabilities, which are typically mitigated by access to the original training dataset. In this work, we address these limitations by leveraging Singular Value Decomposition (SVD) to create a compact, low-dimensional projection that enables the selective forgetting of specific data points. We propose Singular Value Decomposition for Efficient Machine Unlearning (SEMU), a novel approach designed to optimize MU in two key aspects. First, SEMU minimizes the number of model parameters that need to be modified, effectively removing unwanted knowledge while making only minimal changes to the model's weights. Second, SEMU eliminates the dependency on the original training dataset, preserving the model's previously acquired knowledge without additional data requirements. Extensive experiments demonstrate that SEMU achieves competitive performance while significantly improving efficiency in terms of both data usage and the number of modified parameters.

### Generative Modeling with Bayesian Sample Inference 
[[arxiv](https://arxiv.org/abs/2502.07580)] [[cool](https://papers.cool/arxiv/2502.07580)] [[pdf](https://arxiv.org/pdf/2502.07580)]
> **Authors**: Marten Lienen,Marcel Kollovieh,Stephan Günnemann
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We derive a novel generative model from the simple act of Gaussian posterior inference. Treating the generated sample as an unknown variable to infer lets us formulate the sampling process in the language of Bayesian probability. Our model uses a sequence of prediction and posterior update steps to narrow down the unknown sample from a broad initial belief. In addition to a rigorous theoretical analysis, we establish a connection between our model and diffusion models and show that it includes Bayesian Flow Networks (BFNs) as a special case. In our experiments, we demonstrate improved performance over both BFNs and Variational Diffusion Models, achieving competitive likelihood scores on CIFAR10 and ImageNet.

### Single-Step Consistent Diffusion Samplers 
[[arxiv](https://arxiv.org/abs/2502.07579)] [[cool](https://papers.cool/arxiv/2502.07579)] [[pdf](https://arxiv.org/pdf/2502.07579)]
> **Authors**: Pascal Jutras-Dubé,Patrick Pynadath,Ruqi Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs that limit their practicality in time-sensitive or resource-constrained settings. In this work, we introduce consistent diffusion samplers, a new class of samplers designed to generate high-fidelity samples in a single step. We first develop a distillation algorithm to train a consistent diffusion sampler from a pretrained diffusion model without pre-collecting large datasets of samples. Our algorithm leverages incomplete sampling trajectories and noisy intermediate states directly from the diffusion process. We further propose a method to train a consistent diffusion sampler from scratch, fully amortizing exploration by training a single model that both performs diffusion sampling and skips intermediate steps using a self-consistency loss. Through extensive experiments on a variety of unnormalized distributions, we show that our approach yields high-fidelity samples using less than 1% of the network evaluations required by traditional diffusion samplers.

### Automated Capability Discovery via Model Self-Exploration 
[[arxiv](https://arxiv.org/abs/2502.07577)] [[cool](https://papers.cool/arxiv/2502.07577)] [[pdf](https://arxiv.org/pdf/2502.07577)]
> **Authors**: Cong Lu,Shengran Hu,Jeff Clune
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at https://github.com/conglu1997/ACD.

### LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid 
[[arxiv](https://arxiv.org/abs/2502.07563)] [[cool](https://papers.cool/arxiv/2502.07563)] [[pdf](https://arxiv.org/pdf/2502.07563)]
> **Authors**: Weigao Sun,Disen Lan,Yiran Zhong,Xiaoye Qu,Yu Cheng
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Technical report, 17 pages
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学
- **Abstract**: Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.

### Attention Learning is Needed to Efficiently Learn Parity Function 
[[arxiv](https://arxiv.org/abs/2502.07553)] [[cool](https://papers.cool/arxiv/2502.07553)] [[pdf](https://arxiv.org/pdf/2502.07553)]
> **Authors**: Yaomengxi Han,Debarghya Ghoshdastidar
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Transformers, with their attention mechanisms, have emerged as the state-of-the-art architectures of sequential modeling and empirically outperform feed-forward neural networks (FFNNs) across many fields, such as natural language processing and computer vision. However, their generalization ability, particularly for low-sensitivity functions, remains less studied. We bridge this gap by analyzing transformers on the $k$-parity problem. Daniely and Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7 \log k)$ parameters can learn $k$-parity, where the input length $n$ is typically much larger than $k$. In this paper, we prove that FFNNs require at least $Ω(n)$ parameters to learn $k$-parity, while transformers require only $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs. We further prove that this parameter efficiency cannot be achieved with fixed attention heads. Our work establishes transformers as theoretically superior to FFNNs in learning parity function, showing how their attention mechanisms enable parameter-efficient generalization in functions with low sensitivity.

### Early Stopping Against Label Noise Without Validation Data 
[[arxiv](https://arxiv.org/abs/2502.07551)] [[cool](https://papers.cool/arxiv/2502.07551)] [[pdf](https://arxiv.org/pdf/2502.07551)]
> **Authors**: Suqin Yuan,Lei Feng,Tongliang Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted by ICLR 2024
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient validation data could result in a sub-optimal selection of the desired model. In this paper, we propose a novel early stopping method called Label Wave, which does not require validation data for selecting the desired model in the presence of label noise. It works by tracking the changes in the model's predictions on the training set during the training process, aiming to halt training before the model unduly fits mislabeled data. This method is empirically supported by our observation that minimum fluctuations in predictions typically occur at the training epoch before the model excessively fits mislabeled data. Through extensive experiments, we show both the effectiveness of the Label Wave method across various settings and its capability to enhance the performance of existing methods for learning with noisy labels.

### HGTUL: A Hypergraph-based Model For Trajectory User Linking 
[[arxiv](https://arxiv.org/abs/2502.07549)] [[cool](https://papers.cool/arxiv/2502.07549)] [[pdf](https://arxiv.org/pdf/2502.07549)]
> **Authors**: Fengjie Chang,Xinning Zhu,Zheng Hu,Yang Qin
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 11 pages, 4 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Trajectory User Linking (TUL), which links anonymous trajectories with users who generate them, plays a crucial role in modeling human mobility. Despite significant advancements in this field, existing studies primarily neglect the high-order inter-trajectory relationships, which represent complex associations among multiple trajectories, manifested through multi-location co-occurrence patterns emerging when trajectories intersect at various Points of Interest (POIs). Furthermore, they also overlook the variable influence of POIs on different trajectories, as well as the user class imbalance problem caused by disparities in user activity levels and check-in frequencies. To address these limitations, we propose a novel HyperGraph-based multi-perspective Trajectory User Linking model (HGTUL). Our model learns trajectory representations from both relational and spatio-temporal perspectives: (1) it captures high-order associations among trajectories by constructing a trajectory hypergraph and leverages a hypergraph attention network to learn the variable impact of POIs on trajectories; (2) it models the spatio-temporal characteristics of trajectories by incorporating their temporal and spatial information into a sequential encoder. Moreover, we design a data balancing method to effectively address the user class imbalance problem and experimentally validate its significance in TUL. Extensive experiments on three real-world datasets demonstrate that HGTUL outperforms state-of-the-art baselines, achieving improvements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics, respectively.

### Instance-dependent Early Stopping 
[[arxiv](https://arxiv.org/abs/2502.07547)] [[cool](https://papers.cool/arxiv/2502.07547)] [[pdf](https://arxiv.org/pdf/2502.07547)]
> **Authors**: Suqin Yuan,Runqi Lin,Lei Feng,Bo Han,Tongliang Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted by ICLR 2025 (Spotlight)
- **标题**: None
- **领域**: 机器学习
- **Abstract**: In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.

### Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with Diffusion 
[[arxiv](https://arxiv.org/abs/2502.07532)] [[cool](https://papers.cool/arxiv/2502.07532)] [[pdf](https://arxiv.org/pdf/2502.07532)]
> **Authors**: Erik Larsson,Joel Oskarsson,Tomas Landelius,Fredrik Lindsten
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,大气和海洋物理
- **Abstract**: Machine learning methods have been shown to be effective for weather forecasting, based on the speed and accuracy compared to traditional numerical models. While early efforts primarily concentrated on deterministic predictions, the field has increasingly shifted toward probabilistic forecasting to better capture the forecast uncertainty. Most machine learning-based models have been designed for global-scale predictions, with only limited work targeting regional or limited area forecasting, which allows more specialized and flexible modeling for specific locations. This work introduces Diffusion-LAM, a probabilistic limited area weather model leveraging conditional diffusion. By conditioning on boundary data from surrounding regions, our approach generates forecasts within a defined area. Experimental results on the MEPS limited area dataset demonstrate the potential of Diffusion-LAM to deliver accurate probabilistic forecasts, highlighting its promise for limited-area weather prediction.

### Training Deep Learning Models with Norm-Constrained LMOs 
[[arxiv](https://arxiv.org/abs/2502.07529)] [[cool](https://papers.cool/arxiv/2502.07529)] [[pdf](https://arxiv.org/pdf/2502.07529)]
> **Authors**: Thomas Pethick,Wanyun Xie,Kimon Antonakopoulos,Zhenyu Zhu,Antonio Silveti-Falls,Volkan Cevher
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,优化与控制
- **Abstract**: In this work, we study optimization methods that leverage the linear minimization oracle (LMO) over a norm-ball. We propose a new stochastic family of algorithms that uses the LMO to adapt to the geometry of the problem and, perhaps surprisingly, show that they can be applied to unconstrained problems. The resulting update rule unifies several existing optimization methods under a single framework. Furthermore, we propose an explicit choice of norm for deep architectures, which, as a side benefit, leads to the transferability of hyperparameters across model sizes. Experimentally, we demonstrate significant speedups on nanoGPT training without any reliance on Adam. The proposed method is memory-efficient, requiring only one set of model weights and one set of gradients, which can be stored in half-precision.

### Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization 
[[arxiv](https://arxiv.org/abs/2502.07523)] [[cool](https://papers.cool/arxiv/2502.07523)] [[pdf](https://arxiv.org/pdf/2502.07523)]
> **Authors**: Daniel Palenicek,Florian Vogt,Jan Peters
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics, which are emphasized by higher UTD ratios. To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive performance across 25 challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning.

### A Near-optimal, Scalable and Corruption-tolerant Framework for Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond 
[[arxiv](https://arxiv.org/abs/2502.07514)] [[cool](https://papers.cool/arxiv/2502.07514)] [[pdf](https://arxiv.org/pdf/2502.07514)]
> **Authors**: Zicheng Hu,Cheng Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We investigate various stochastic bandit problems in the presence of adversarial corruption. A seminal contribution to this area is the BARBAR~\citep{gupta2019better} algorithm, which is both simple and efficient, tolerating significant levels of corruption with nearly no degradation in performance. However, its regret upper bound exhibits a complexity of $O(KC)$, while the lower bound is $Ω(C)$. In this paper, we enhance the BARBAR algorithm by proposing a novel framework called BARBAT, which eliminates the factor of $K$ and achieves an optimal regret bound up to a logarithmic factor. We also demonstrate how BARBAT can be extended to various settings, including graph bandits, combinatorial semi-bandits, batched bandits and multi-agent bandits. In comparison to the Follow-The-Regularized-Leader (FTRL) family of methods, which provide a best-of-both-worlds guarantee, our approach is more efficient and parallelizable. Notably, FTRL-based methods face challenges in scaling to batched and multi-agent settings.

### Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein Marginal Penalization 
[[arxiv](https://arxiv.org/abs/2502.07510)] [[cool](https://papers.cool/arxiv/2502.07510)] [[pdf](https://arxiv.org/pdf/2502.07510)]
> **Authors**: Florian Beier,Moritz Piening,Robert Beinert,Gabriele Steidl
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: We propose a new approach for unsupervised alignment of heterogeneous datasets, which maps data from two different domains without any known correspondences to a common metric space. Our method is based on an unbalanced optimal transport problem with Gromov-Wasserstein marginal penalization. It can be seen as a counterpart to the recently introduced joint multidimensional scaling method. We prove that there exists a minimizer of our functional and that for penalization parameters going to infinity, the corresponding sequence of minimizers converges to a minimizer of the so-called embedded Wasserstein distance. Our model can be reformulated as a quadratic, multi-marginal, unbalanced optimal transport problem, for which a bi-convex relaxation admits a numerical solver via block-coordinate descent. We provide numerical examples for joint embeddings in Euclidean as well as non-Euclidean spaces.

### Unified Graph Networks (UGN): A Deep Neural Framework for Solving Graph Problems 
[[arxiv](https://arxiv.org/abs/2502.07500)] [[cool](https://papers.cool/arxiv/2502.07500)] [[pdf](https://arxiv.org/pdf/2502.07500)]
> **Authors**: Rudrajit Dawn,Madhusudan Ghosh,Partha Basuchowdhuri,Sudip Kumar Naskar
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Deep neural networks have enabled researchers to create powerful generalized frameworks, such as transformers, that can be used to solve well-studied problems in various application domains, such as text and image. However, such generalized frameworks are not available for solving graph problems. Graph structures are ubiquitous in many applications around us and many graph problems have been widely studied over years. In recent times, there has been a surge in deep neural network based approaches to solve graph problems, with growing availability of graph structured datasets across diverse domains. Nevertheless, existing methods are mostly tailored to solve a specific task and lack the capability to create a generalized model leading to solutions for different downstream tasks. In this work, we propose a novel, resource-efficient framework named \emph{U}nified \emph{G}raph \emph{N}etwork (UGN) by leveraging the feature extraction capability of graph convolutional neural networks (GCN) and 2-dimensional convolutional neural networks (Conv2D). UGN unifies various graph learning tasks, such as link prediction, node classification, community detection, graph-to-graph translation, knowledge graph completion, and more, within a cohesive framework, while exercising minimal task-specific extensions (e.g., formation of supernodes for coarsening massive networks to increase scalability, use of \textit{mean target connectivity matrix} (MTCM) representation for achieving scalability in graph translation task, etc.) to enhance the generalization capability of graph learning and analysis. We test the novel UGN framework for six uncorrelated graph problems, using twelve different datasets. Experimental results show that UGN outperforms the state-of-the-art baselines by a significant margin on ten datasets, while producing comparable results on the remaining dataset.

### On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals 
[[arxiv](https://arxiv.org/abs/2502.07497)] [[cool](https://papers.cool/arxiv/2502.07497)] [[pdf](https://arxiv.org/pdf/2502.07497)]
> **Authors**: Rudi Coppola,Manuel Mazo Jr
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Estimating the expectation of a Bernoulli random variable based on N independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks-such as certifying the statistical safety of dynamical systems-can be formulated as BPCI problems. Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as training-conditional CP was recently employed to tackle the problem of safety certification. In this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification.

### Exploring Patterns Behind Sports 
[[arxiv](https://arxiv.org/abs/2502.07491)] [[cool](https://papers.cool/arxiv/2502.07491)] [[pdf](https://arxiv.org/pdf/2502.07491)]
> **Authors**: Chang Liu,Chengcheng Ma,XuanQi Zhou
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,信息检索
- **Abstract**: This paper presents a comprehensive framework for time series prediction using a hybrid model that combines ARIMA and LSTM. The model incorporates feature engineering techniques, including embedding and PCA, to transform raw data into a lower-dimensional representation while retaining key information. The embedding technique is used to convert categorical data into continuous vectors, facilitating the capture of complex relationships. PCA is applied to reduce dimensionality and extract principal components, enhancing model performance and computational efficiency. To handle both linear and nonlinear patterns in the data, the ARIMA model captures linear trends, while the LSTM model models complex nonlinear dependencies. The hybrid model is trained on historical data and achieves high accuracy, as demonstrated by low RMSE and MAE scores. Additionally, the paper employs the run test to assess the randomness of sequences, providing insights into the underlying patterns. Ablation studies are conducted to validate the roles of different components in the model, demonstrating the significance of each module. The paper also utilizes the SHAP method to quantify the impact of traditional advantages on the predicted results, offering a detailed understanding of feature importance. The KNN method is used to determine the optimal prediction interval, further enhancing the model's accuracy. The results highlight the effectiveness of combining traditional statistical methods with modern deep learning techniques for robust time series forecasting in Sports.

### Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time Series Forecasting Based on Biological ODEs 
[[arxiv](https://arxiv.org/abs/2502.07489)] [[cool](https://papers.cool/arxiv/2502.07489)] [[pdf](https://arxiv.org/pdf/2502.07489)]
> **Authors**: Christian Klötergens,Vijaya Krishna Yalavarthi,Randolf Scholz,Maximilian Stubbemann,Stefan Born,Lars Schmidt-Thieme
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: State-of-the-art methods for forecasting irregularly sampled time series with missing values predominantly rely on just four datasets and a few small toy examples for evaluation. While ordinary differential equations (ODE) are the prevalent models in science and engineering, a baseline model that forecasts a constant value outperforms ODE-based models from the last five years on three of these existing datasets. This unintuitive finding hampers further research on ODE-based models, a more plausible model family. In this paper, we develop a methodology to generate irregularly sampled multivariate time series (IMTS) datasets from ordinary differential equations and to select challenging instances via rejection sampling. Using this methodology, we create Physiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting of 50 individual datasets, derived from real-world ordinary differential equations from research in biology. Physiome-ODE is the first benchmark for IMTS forecasting that we are aware of and an order of magnitude larger than the current evaluation setting of four datasets. Using our benchmark Physiome-ODE, we show qualitatively completely different results than those derived from the current four datasets: on Physiome-ODE ODE-based models can play to their strength and our benchmark can differentiate in a meaningful way between different IMTS forecasting models. This way, we expect to give a new impulse to research on ODE-based time series modeling.

### Improving Adaptive Moment Optimization via Preconditioner Diagonalization 
[[arxiv](https://arxiv.org/abs/2502.07488)] [[cool](https://papers.cool/arxiv/2502.07488)] [[pdf](https://arxiv.org/pdf/2502.07488)]
> **Authors**: Son Nguyen,Bo Liu,Lizhang Chen,Qiang Liu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 19 pages, 13 figures
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Modern adaptive optimization methods, such as Adam and its variants, have emerged as the most widely used tools in deep learning over recent years. These algorithms offer automatic mechanisms for dynamically adjusting the update step based on estimates of gradient statistics. Compared to traditional algorithms like Stochastic Gradient Descent, these adaptive methods are typically more robust to model scale and hyperparameter tuning. However, the gradient statistics employed by these methods often do not leverage sufficient gradient covariance information, leading to suboptimal updates in certain directions of the parameter space and potentially slower convergence. In this work, we keep track of such covariance statistics in the form of a structured preconditioner matrix. Unlike other works, our approach does not apply direct approximations to estimate this matrix. We instead implement an invertible transformation that maps the preconditioner matrix into a new space where it becomes approximately diagonal. This enables a diagonal approximation of the preconditioner matrix in the transformed space, offering several computational advantages. Empirical results show that our approach can substantially enhance the convergence speed of modern adaptive optimizers. Notably, for large language models like LLaMA, we can achieve a speedup of 2x compared to the baseline Adam. Additionally, our method can be integrated with memory-efficient optimizers like Adafactor to manage computational overhead.

### Overfitting Regimes of Nadaraya-Watson Interpolators 
[[arxiv](https://arxiv.org/abs/2502.07480)] [[cool](https://papers.cool/arxiv/2502.07480)] [[pdf](https://arxiv.org/pdf/2502.07480)]
> **Authors**: Daniel Barzilai,Guy Kornowski,Ohad Shamir
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 26 pages
- **标题**: None
- **领域**: 机器学习,统计理论,机器学习
- **Abstract**: In recent years, there has been much interest in understanding the generalization behavior of interpolating predictors, which overfit on noisy training data. Whereas standard analyses are concerned with whether a method is consistent or not, recent observations have shown that even inconsistent predictors can generalize well. In this work, we revisit the classic interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method), and study its generalization capabilities through this modern viewpoint. In particular, by varying a single bandwidth-like hyperparameter, we prove the existence of multiple overfitting behaviors, ranging non-monotonically from catastrophic, through benign, to tempered. Our results highlight how even classical interpolating methods can exhibit intricate generalization behaviors. Numerical experiments complement our theory, demonstrating the same phenomena.

### Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models 
[[arxiv](https://arxiv.org/abs/2502.07465)] [[cool](https://papers.cool/arxiv/2502.07465)] [[pdf](https://arxiv.org/pdf/2502.07465)]
> **Authors**: Li Mao,Wei Du,Shuo Wen,Qi Li,Tong Zhang,Wei Zhong
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: The paper was submitted without the consent of all co-authors. The content of the paper is incomplete and requires substantial additional work before it can be considered a complete and coherent submission
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This study uses deep-learning models to predict city partition crime counts on specific days. It helps police enhance surveillance, gather intelligence, and proactively prevent crimes. We formulate crime count prediction as a spatiotemporal sequence challenge, where both input data and prediction targets are spatiotemporal sequences. In order to improve the accuracy of crime forecasting, we introduce a new model that combines Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a comparative analysis to access the effects of various data sequences, including raw and binned data, on the prediction errors of four deep learning forecasting models. Directly inputting raw crime data into the forecasting model causes high prediction errors, making the model unsuitable for real - world use. The findings indicate that the proposed CNN-LSTM model achieves optimal performance when crime data is categorized into 10 or 5 groups. Data binning can enhance forecasting model performance, but poorly defined intervals may reduce map granularity. Compared to dividing into 5 bins, binning into 10 intervals strikes an optimal balance, preserving data characteristics and surpassing raw data in predictive modelling efficacy.

### Logarithmic Regret for Online KL-Regularized Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07460)] [[cool](https://papers.cool/arxiv/2502.07460)] [[pdf](https://arxiv.org/pdf/2502.07460)]
> **Authors**: Heyang Zhao,Chenlu Ye,Wei Xiong,Quanquan Gu,Tong Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\mathcal{O}\big(η\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ logarithmic regret bound, where $η, N_{\mathcal R},T,d_{\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.

### FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for Federated Learning on Heterogeneous Data 
[[arxiv](https://arxiv.org/abs/2502.07456)] [[cool](https://papers.cool/arxiv/2502.07456)] [[pdf](https://arxiv.org/pdf/2502.07456)]
> **Authors**: Yuxia Sun,Aoxiang Sun,Siyi Pan,Zhixiao Fu,Jingcai Guo
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 11 pages, 2 figures
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: Personalized federated learning (PFL) tailors models to clients' unique data distributions while preserving privacy. However, existing aggregation-weight-based PFL methods often struggle with heterogeneous data, facing challenges in accuracy, computational efficiency, and communication overhead. We propose FedAPA, a novel PFL method featuring a server-side, gradient-based adaptive aggregation strategy to generate personalized models, by updating aggregation weights based on gradients of client-parameter changes with respect to the aggregation weights in a centralized manner. FedAPA guarantees theoretical convergence and achieves superior accuracy and computational efficiency compared to 10 PFL competitors across three datasets, with competitive communication overhead.

### CapyMOA: Efficient Machine Learning for Data Streams in Python 
[[arxiv](https://arxiv.org/abs/2502.07432)] [[cool](https://papers.cool/arxiv/2502.07432)] [[pdf](https://arxiv.org/pdf/2502.07432)]
> **Authors**: Heitor Murilo Gomes,Anton Lee,Nuwan Gunasekara,Yibin Sun,Guilherme Weigert Cassales,Justin Liu,Marco Heyden,Vitor Cerqueira,Maroua Bahri,Yun Sing Koh,Bernhard Pfahringer,Albert Bifet
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: CapyMOA is an open-source library designed for efficient machine learning on streaming data. It provides a structured framework for real-time learning and evaluation, featuring a flexible data representation. CapyMOA includes an extensible architecture that allows integration with external frameworks such as MOA and PyTorch, facilitating hybrid learning approaches that combine traditional online algorithms with deep learning techniques. By emphasizing adaptability, scalability, and usability, CapyMOA allows researchers and practitioners to tackle dynamic learning challenges across various domains.

### Towards a Foundation Model for Physics-Informed Neural Networks: Multi-PDE Learning with Active Sampling 
[[arxiv](https://arxiv.org/abs/2502.07425)] [[cool](https://papers.cool/arxiv/2502.07425)] [[pdf](https://arxiv.org/pdf/2502.07425)]
> **Authors**: Keon Vin Park
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws into neural network training. However, traditional PINN models are typically designed for single PDEs, limiting their generalizability across different physical systems. In this work, we explore the potential of a foundation PINN model capable of solving multiple PDEs within a unified architecture. We investigate the efficacy of a single PINN framework trained on four distinct PDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave Equation, and the 2D Laplace Equation, demonstrating its ability to learn diverse physical dynamics. To enhance sample efficiency, we incorporate Active Learning (AL) using Monte Carlo (MC) Dropout-based uncertainty estimation, selecting the most informative training samples iteratively. We evaluate different active learning strategies, comparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset, and analyze their impact on solution accuracy. Our results indicate that targeted uncertainty sampling significantly improves performance with fewer training samples, leading to efficient learning across multiple PDEs. This work highlights the feasibility of a generalizable PINN-based foundation model, capable of adapting to different physics-based problems without redesigning network architectures. Our findings suggest that multi-PDE PINNs with active learning can serve as an effective approach for reducing computational costs while maintaining high accuracy in physics-based deep learning applications.

### MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.07422)] [[cool](https://papers.cool/arxiv/2502.07422)] [[pdf](https://arxiv.org/pdf/2502.07422)]
> **Authors**: Lotfi Abdelkrim Mecharbat,Alberto Marchisio,Muhammad Shafique,Mohammad M. Ghassemi,Tuka Alhanai
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算机视觉和模式识别
- **Abstract**: There has been a surge in optimizing edge Deep Neural Networks (DNNs) for accuracy and efficiency using traditional optimization techniques such as pruning, and more recently, employing automatic design methodologies. However, the focus of these design techniques has often overlooked critical metrics such as fairness, robustness, and generalization. As a result, when evaluating SOTA edge DNNs' performance in image classification using the FACET dataset, we found that they exhibit significant accuracy disparities (14.09%) across 10 different skin tones, alongside issues of non-robustness and poor generalizability. In response to these observations, we introduce Mixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic design technique that navigates through a space of mixture of experts to discover accurate, fair, robust, and general edge DNNs. MoENAS improves the accuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy disparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and minimizing overfitting to 0.21%, all while keeping model size close to state-of-the-art models average size (+0.4M). With these improvements, MoENAS establishes a new benchmark for edge DNN design, paving the way for the development of more inclusive and robust edge DNNs.

### Sample Weight Averaging for Stable Prediction 
[[arxiv](https://arxiv.org/abs/2502.07414)] [[cool](https://papers.cool/arxiv/2502.07414)] [[pdf](https://arxiv.org/pdf/2502.07414)]
> **Authors**: Han Yu,Yue He,Renzhe Xu,Dongbai Li,Jiayin Zhang,Wenchao Zou,Peng Cui
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: The challenge of Out-of-Distribution (OOD) generalization poses a foundational concern for the application of machine learning algorithms to risk-sensitive areas. Inspired by traditional importance weighting and propensity weighting methods, prior approaches employ an independence-based sample reweighting procedure. They aim at decorrelating covariates to counteract the bias introduced by spurious correlations between unstable variables and the outcome, thus enhancing generalization and fulfilling stable prediction under covariate shift. Nonetheless, these methods are prone to experiencing an inflation of variance, primarily attributable to the reduced efficacy in utilizing training samples during the reweighting process. Existing remedies necessitate either environmental labels or substantially higher time costs along with additional assumptions and supervised information. To mitigate this issue, we propose SAmple Weight Averaging (SAWA), a simple yet efficacious strategy that can be universally integrated into various sample reweighting algorithms to decrease the variance and coefficient estimation error, thus boosting the covariate-shift generalization and achieving stable prediction across different environments. We prove its rationality and benefits theoretically. Experiments across synthetic datasets and real-world datasets consistently underscore its superiority against covariate shift.

### No Data, No Optimization: A Lightweight Method To Disrupt Neural Networks With Sign-Flips 
[[arxiv](https://arxiv.org/abs/2502.07408)] [[cool](https://papers.cool/arxiv/2502.07408)] [[pdf](https://arxiv.org/pdf/2502.07408)]
> **Authors**: Ido Galil,Moshe Kimhi,Ran El-Yaniv
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping only a handful of sign bits in their parameters. We introduce Deep Neural Lesion (DNL), a data-free, lightweight method that locates these critical parameters and triggers massive accuracy drops. We validate its efficacy on a wide variety of computer vision models and datasets. The method requires no training data or optimization and can be carried out via common exploits software, firmware or hardware based attack vectors. An enhanced variant that uses a single forward and backward pass further amplifies the damage beyond DNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet reduces accuracy by 99.8\%. We also show that selectively protecting a small fraction of vulnerable sign bits provides a practical defense against such attacks.

### Interpretable Rules for Online Failure Prediction: A Case Study on the Metro do Porto dataset 
[[arxiv](https://arxiv.org/abs/2502.07394)] [[cool](https://papers.cool/arxiv/2502.07394)] [[pdf](https://arxiv.org/pdf/2502.07394)]
> **Authors**: Matthias Jakobs,Bruno Veloso,Joao Gama
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Under submission at Information Fusion
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Due to their high predictive performance, predictive maintenance applications have increasingly been approached with Deep Learning techniques in recent years. However, as in other real-world application scenarios, the need for explainability is often stated but not sufficiently addressed. This study will focus on predicting failures on Metro trains in Porto, Portugal. While recent works have found high-performing deep neural network architectures that feature a parallel explainability pipeline, the generated explanations are fairly complicated and need help explaining why the failures are happening. This work proposes a simple online rule-based explainability approach with interpretable features that leads to straightforward, interpretable rules. We showcase our approach on MetroPT2 and find that three specific sensors on the Metro do Porto trains suffice to predict the failures present in the dataset with simple rules.

### EvoFlow: Evolving Diverse Agentic Workflows On The Fly 
[[arxiv](https://arxiv.org/abs/2502.07373)] [[cool](https://papers.cool/arxiv/2502.07373)] [[pdf](https://arxiv.org/pdf/2502.07373)]
> **Authors**: Guibin Zhang,Kaijie Chen,Guancheng Wan,Heng Chang,Hong Cheng,Kun Wang,Shuyue Hu,Lei Bai
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,计算语言学,多代理系统,神经和进化计算
- **Abstract**: The past two years have witnessed the evolution of large language model (LLM)-based multi-agent systems from labor-intensive manual design to partial automation (\textit{e.g.}, prompt engineering, communication topology) and eventually to fully automated design. However, existing agentic automation pipelines often lack LLM heterogeneity and focus on single-objective performance optimization, limiting their potential to combine weaker models for more customized and cost-effective solutions. To address this challenge, we propose EvoFlow, a niching evolutionary algorithm-based framework to automatically search a population of heterogeneous and complexity-adaptive agentic workflows, rather than a single homogeneous, complex workflow. Technically, EvoFlow performs \textit{(1) tag-based retrieval} to extract parent workflows from an agentic population, evolves new workflows through \textit{(2) crossover} and \textit{(3) mutation}, and employs \textit{(4) niching-based selection} to maintain population diversity and quality. Extensive evaluations across seven benchmarks demonstrate that EvoFlow is: \textbf{(I) diverse}, evolving a population of workflows ranging from simple I/O tasks to complex multi-turn interactions; \textbf{(II) high-performing}, outperforming previous handcrafted and automated workflows by $1.23\%\sim29.86\%$; \textbf{(III) economical}, surpassing powerful \llmname{o1-preview} at $12.4\%$ of its inference cost using weaker open-source models.

### Effects of Random Edge-Dropping on Over-Squashing in Graph Neural Networks 
[[arxiv](https://arxiv.org/abs/2502.07364)] [[cool](https://papers.cool/arxiv/2502.07364)] [[pdf](https://arxiv.org/pdf/2502.07364)]
> **Authors**: Jasraj Singh,Keyue Jiang,Brooks Paige,Laura Toni
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 24 pages, 7 figures, 2 tables
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks (GNNs) that leverage the graph topology to propagate messages across increasingly larger neighborhoods. The message-passing scheme leads to two distinct challenges: over-smoothing and over-squashing. While several algorithms, e.g. DropEdge and its variants -- DropNode, DropAgg and DropGNN -- have successfully addressed the over-smoothing problem, their impact on over-squashing remains largely unexplored. This represents a critical gap in the literature as failure to mitigate over-squashing would make these methods unsuitable for long-range tasks. In this work, we take the first step towards closing this gap by studying the aforementioned algorithms in the context of over-squashing. We present novel theoretical results that characterize the negative effects of DropEdge on sensitivity between distant nodes, suggesting its unsuitability for long-range tasks. Our findings are easily extended to its variants, allowing us to build a comprehensive understanding of how they affect over-squashing. We evaluate these methods using real-world datasets, demonstrating their detrimental effects. Specifically, we show that while DropEdge-variants improve test-time performance in short range tasks, they deteriorate performance in long-range ones. Our theory explains these results as follows: random edge-dropping lowers the effective receptive field of GNNs, which although beneficial for short-range tasks, misaligns the models on long-range ones. This forces the models to overfit to short-range artefacts in the training set, resulting in poor generalization. Our conclusions highlight the need to re-evaluate various methods designed for training deep GNNs, with a renewed focus on modelling long-range interactions.

### Integrating Physics and Data-Driven Approaches: An Explainable and Uncertainty-Aware Hybrid Model for Wind Turbine Power Prediction 
[[arxiv](https://arxiv.org/abs/2502.07344)] [[cool](https://papers.cool/arxiv/2502.07344)] [[pdf](https://arxiv.org/pdf/2502.07344)]
> **Authors**: Alfonso Gijón,Simone Eiraudo,Antonio Manjavacas,Daniele Salvatore Schiera,Miguel Molina-Solana,Juan Gómez-Romero
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能,计算工程、金融和科学
- **Abstract**: The rapid growth of the wind energy sector underscores the urgent need to optimize turbine operations and ensure effective maintenance through early fault detection systems. While traditional empirical and physics-based models offer approximate predictions of power generation based on wind speed, they often fail to capture the complex, non-linear relationships between other input variables and the resulting power output. Data-driven machine learning methods present a promising avenue for improving wind turbine modeling by leveraging large datasets, enhancing prediction accuracy but often at the cost of interpretability. In this study, we propose a hybrid semi-parametric model that combines the strengths of both approaches, applied to a dataset from a wind farm with four turbines. The model integrates a physics-inspired submodel, providing a reasonable approximation of power generation, with a non-parametric submodel that predicts the residuals. This non-parametric submodel is trained on a broader range of variables to account for phenomena not captured by the physics-based component. The hybrid model achieves a 37% improvement in prediction accuracy over the physics-based model. To enhance interpretability, SHAP values are used to analyze the influence of input features on the residual submodel's output. Additionally, prediction uncertainties are quantified using a conformalized quantile regression method. The combination of these techniques, alongside the physics grounding of the parametric submodel, provides a flexible, accurate, and reliable framework. Ultimately, this study opens the door for evaluating the impact of unmodeled variables on wind turbine power generation, offering a basis for potential optimization.

### Neural Flow Samplers with Shortcut Models 
[[arxiv](https://arxiv.org/abs/2502.07337)] [[cool](https://papers.cool/arxiv/2502.07337)] [[pdf](https://arxiv.org/pdf/2502.07337)]
> **Authors**: Wuhao Chen,Zijing Ou,Yingzhen Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Sampling from unnormalized densities is a fundamental task across various domains. Flow-based samplers generate samples by learning a velocity field that satisfies the continuity equation, but this requires estimating the intractable time derivative of the partition function. While importance sampling provides an approximation, it suffers from high variance. To mitigate this, we introduce a velocity-driven Sequential Monte Carlo method combined with control variates to reduce variance. Additionally, we incorporate a shortcut model to improve efficiency by minimizing the number of sampling steps. Empirical results on both synthetic datasets and $n$-body system targets validate the effectiveness of our approach.

### Long-term simulation of physical and mechanical behaviors using curriculum-transfer-learning based physics-informed neural networks 
[[arxiv](https://arxiv.org/abs/2502.07325)] [[cool](https://papers.cool/arxiv/2502.07325)] [[pdf](https://arxiv.org/pdf/2502.07325)]
> **Authors**: Yuan Guo,Zhuojia Fu,Jian Min,Shiyu Lin,Xiaoting Liu,Youssef F. Rashed,Xiaoying Zhuang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 31 pages, 18 figures
- **标题**: None
- **领域**: 机器学习,数值分析
- **Abstract**: This paper proposes a Curriculum-Transfer-Learning based physics-informed neural network (CTL-PINN) for long-term simulation of physical and mechanical behaviors. The main innovation of CTL-PINN lies in decomposing long-term problems into a sequence of short-term subproblems. Initially, the standard PINN is employed to solve the first sub-problem. As the simulation progresses, subsequent time-domain problems are addressed using a curriculum learning approach that integrates information from previous steps. Furthermore, transfer learning techniques are incorporated, allowing the model to effectively utilize prior training data and solve sequential time domain transfer problems. CTL-PINN combines the strengths of curriculum learning and transfer learning, overcoming the limitations of standard PINNs, such as local optimization issues, and addressing the inaccuracies over extended time domains encountered in CL-PINN and the low computational efficiency of TL-PINN. The efficacy and robustness of CTL-PINN are demonstrated through applications to nonlinear wave propagation, Kirchhoff plate dynamic response, and the hydrodynamic model of the Three Gorges Reservoir Area, showcasing its superior capability in addressing long-term computational challenges.

### Learnable Residual-based Latent Denoising in Semantic Communication 
[[arxiv](https://arxiv.org/abs/2502.07319)] [[cool](https://papers.cool/arxiv/2502.07319)] [[pdf](https://arxiv.org/pdf/2502.07319)]
> **Authors**: Mingkai Xu,Yongpeng Wu,Yuxuan Shi,Xiang-Gen Xia,Wenjun Zhang,Ping Zhang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: This paper has been accepted by IEEE Wireless Communications Letters
- **标题**: None
- **领域**: 机器学习,信息论
- **Abstract**: A latent denoising semantic communication (SemCom) framework is proposed for robust image transmission over noisy channels. By incorporating a learnable latent denoiser into the receiver, the received signals are preprocessed to effectively remove the channel noise and recover the semantic information, thereby enhancing the quality of the decoded images. Specifically, a latent denoising mapping is established by an iterative residual learning approach to improve the denoising efficiency while ensuring stable performance. Moreover, channel signal-to-noise ratio (SNR) is utilized to estimate and predict the latent similarity score (SS) for conditional denoising, where the number of denoising steps is adapted based on the predicted SS sequence, further reducing the communication latency. Finally, simulations demonstrate that the proposed framework can effectively and efficiently remove the channel noise at various levels and reconstruct visual-appealing images.

### OpenGrok: Enhancing SNS Data Processing with Distilled Knowledge and Mask-like Mechanisms 
[[arxiv](https://arxiv.org/abs/2502.07312)] [[cool](https://papers.cool/arxiv/2502.07312)] [[pdf](https://arxiv.org/pdf/2502.07312)]
> **Authors**: Lumen AI,Zaozhuang No. 28 Middle School,Shihao Ji,Zihui Song,Fucheng Zhong,Jisen Jia,Zhaobo Wu,Zheyi Cao,Tianhao Xu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 7 pages
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: This report details Lumen Labs' novel approach to processing Social Networking Service (SNS) data. We leverage knowledge distillation, specifically a simple distillation method inspired by DeepSeek-R1's CoT acquisition, combined with prompt hacking, to extract valuable training data from the Grok model. This data is then used to fine-tune a Phi-3-mini model, augmented with a mask-like mechanism specifically designed for handling the nuances of SNS data. Our method demonstrates state-of-the-art (SOTA) performance on several SNS data processing tasks, outperforming existing models like Grok, Phi-3, and GPT-4. We provide a comprehensive analysis of our approach, including mathematical formulations, engineering details, ablation studies, and comparative evaluations.

### Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification 
[[arxiv](https://arxiv.org/abs/2502.07299)] [[cool](https://papers.cool/arxiv/2502.07299)] [[pdf](https://arxiv.org/pdf/2502.07299)]
> **Authors**: Zicheng Liu,Siyuan Li,Zhiyuan Chen,Lei Xin,Fang Wu,Chang Yu,Qirong Yang,Yucheng Guo,Yujie Yang,Stan Z. Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 12 pages main text with 6 pages Appendix
- **标题**: None
- **领域**: 机器学习,人工智能,计算语言学,基因组学
- **Abstract**: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. While modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains under-explored. In this paper, we follow the guidance of the central dogma to redesign both the data and model pipeline and offer a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions of both coding and non-coding regions with masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive Experiments show that Life-Code achieves state-of-the-art performance on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.

### Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical Trials 
[[arxiv](https://arxiv.org/abs/2502.07297)] [[cool](https://papers.cool/arxiv/2502.07297)] [[pdf](https://arxiv.org/pdf/2502.07297)]
> **Authors**: Qian Shao,Bang Du,Zepeng Li,Qiyuan Chen,Hongxia Xu,Jimeng Sun,Jian Wu,Jintai Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Under review
- **标题**: None
- **领域**: 机器学习,定量方法
- **Abstract**: Clinical trials are pivotal in cardiac drug development, yet they often fail due to inadequate efficacy and unexpected safety issues, leading to significant financial losses. Using in-silico trials to replace a part of physical clinical trials, e.g., leveraging advanced generative models to generate drug-influenced electrocardiograms (ECGs), seems an effective method to reduce financial risk and potential harm to trial participants. While existing generative models have demonstrated progress in ECG generation, they fall short in modeling drug reactions due to limited fidelity and inability to capture individualized drug response patterns. In this paper, we propose a Drug-Aware Diffusion Model (DADM), which could simulate individualized drug reactions while ensuring fidelity. To ensure fidelity, we construct a set of ordinary differential equations to provide external physical knowledge (EPK) of the realistic ECG morphology. The EPK is used to adaptively constrain the morphology of the generated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore, we propose an extension of ControlNet to incorporate demographic and drug data, simulating individual drug reactions. We compare DADM with the other eight state-of-the-art ECG generative models on two real-world databases covering 8 types of drug regimens. The results demonstrate that DADM can more accurately simulate drug-induced changes in ECGs, improving the accuracy by at least 5.79% and recall by 8%.

### Treatment Effect Estimation for Exponential Family Outcomes using Neural Networks with Targeted Regularization 
[[arxiv](https://arxiv.org/abs/2502.07295)] [[cool](https://papers.cool/arxiv/2502.07295)] [[pdf](https://arxiv.org/pdf/2502.07295)]
> **Authors**: Jiahong Li,Zeqin Yang,Jiayi Dan,Jixing Xu,Zhichao Zou,Peng Zhen,Jiecheng Guo
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Neural Networks (NNs) have became a natural choice for treatment effect estimation due to their strong approximation capabilities. Nevertheless, how to design NN-based estimators with desirable properties, such as low bias and doubly robustness, still remains a significant challenge. A common approach to address this is targeted regularization, which modifies the objective function of NNs. However, existing works on targeted regularization are limited to Gaussian-distributed outcomes, significantly restricting their applicability in real-world scenarios. In this work, we aim to bridge this blank by extending this framework to the boarder exponential family outcomes. Specifically, we first derive the von-Mises expansion of the Average Dose function of Canonical Functions (ADCF), which inspires us how to construct a doubly robust estimator with good properties. Based on this, we develop a NN-based estimator for ADCF by generalizing functional targeted regularization to exponential families, and provide the corresponding theoretical convergence rate. Extensive experimental results demonstrate the effectiveness of our proposed model.

### Supervised Contrastive Block Disentanglement 
[[arxiv](https://arxiv.org/abs/2502.07281)] [[cool](https://papers.cool/arxiv/2502.07281)] [[pdf](https://arxiv.org/pdf/2502.07281)]
> **Authors**: Taro Makino,Ji Won Park,Natasa Tagasovska,Takamasa Kudo,Paula Coelho,Jan-Christian Huetter,Heming Yao,Burkhard Hoeckendorf,Ana Carolina Leote,Stephen Ra,David Richmond,Kyunghyun Cho,Aviv Regev,Romain Lopez
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习
- **Abstract**: Real-world datasets often combine data collected under different experimental conditions. This yields larger datasets, but also introduces spurious correlations that make it difficult to model the phenomena of interest. We address this by learning two embeddings to independently represent the phenomena of interest and the spurious correlations. The embedding representing the phenomena of interest is correlated with the target variable $y$, and is invariant to the environment variable $e$. In contrast, the embedding representing the spurious correlations is correlated with $e$. The invariance to $e$ is difficult to achieve on real-world datasets. Our primary contribution is an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that effectively enforces this invariance. It is based purely on Supervised Contrastive Learning, and applies to real-world data better than existing approaches. We empirically validate SCBD on two challenging problems. The first problem is domain generalization, where we achieve strong performance on a synthetic dataset, as well as on Camelyon17-WILDS. We introduce a single hyperparameter $α$ to control the degree of invariance to $e$. When we increase $α$ to strengthen the degree of invariance, out-of-distribution performance improves at the expense of in-distribution performance. The second problem is batch correction, in which we apply SCBD to preserve biological signal and remove inter-well batch effects when modeling single-cell perturbations from 26 million Optical Pooled Screening images.

### MIGT: Memory Instance Gated Transformer Framework for Financial Portfolio Management 
[[arxiv](https://arxiv.org/abs/2502.07280)] [[cool](https://papers.cool/arxiv/2502.07280)] [[pdf](https://arxiv.org/pdf/2502.07280)]
> **Authors**: Fengchen Gu,Angelos Stefanidis,Ángel García-Fernández,Jionglong Su,Huakang Li
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Deep reinforcement learning (DRL) has been applied in financial portfolio management to improve returns in changing market conditions. However, unlike most fields where DRL is widely used, the stock market is more volatile and dynamic as it is affected by several factors such as global events and investor sentiment. Therefore, it remains a challenge to construct a DRL-based portfolio management framework with strong return capability, stable training, and generalization ability. This study introduces a new framework utilizing the Memory Instance Gated Transformer (MIGT) for effective portfolio management. By incorporating a novel Gated Instance Attention module, which combines a transformer variant, instance normalization, and a Lite Gate Unit, our approach aims to maximize investment returns while ensuring the learning process's stability and reducing outlier impacts. Tested on the Dow Jones Industrial Average 30, our framework's performance is evaluated against fifteen other strategies using key financial metrics like the cumulative return and risk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight MIGT's advantage, showcasing at least a 9.75% improvement in cumulative returns and a minimum 2.36% increase in risk-return ratios over competing strategies, marking a significant advancement in DRL for portfolio management.

### Exploratory Diffusion Policy for Unsupervised Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07279)] [[cool](https://papers.cool/arxiv/2502.07279)] [[pdf](https://arxiv.org/pdf/2502.07279)]
> **Authors**: Chengyang Ying,Huayu Chen,Xinning Zhou,Zhongkai Hao,Hang Su,Jun Zhu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Unsupervised reinforcement learning (RL) aims to pre-train agents by exploring states or skills in reward-free environments, facilitating the adaptation to downstream tasks. However, existing methods often overlook the fitting ability of pre-trained policies and struggle to handle the heterogeneous pre-training data, which are crucial for achieving efficient exploration and fast fine-tuning. To address this gap, we propose Exploratory Diffusion Policy (EDP), which leverages the strong expressive ability of diffusion models to fit the explored data, both boosting exploration and obtaining an efficient initialization for downstream tasks. Specifically, we estimate the distribution of collected data in the replay buffer with the diffusion policy and propose a score intrinsic reward, encouraging the agent to explore unseen states. For fine-tuning the pre-trained diffusion policy on downstream tasks, we provide both theoretical analyses and practical algorithms, including an alternating method of Q function optimization and diffusion policy distillation. Extensive experiments demonstrate the effectiveness of EDP in efficient exploration during pre-training and fast adaptation during fine-tuning.

### Dataset Ownership Verification in Contrastive Pre-trained Models 
[[arxiv](https://arxiv.org/abs/2502.07276)] [[cool](https://papers.cool/arxiv/2502.07276)] [[pdf](https://arxiv.org/pdf/2502.07276)]
> **Authors**: Yuechen Xie,Jie Song,Mengqi Xue,Haofei Zhang,Xingen Wang,Bingde Hu,Genlang Chen,Mingli Song
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted by ICLR2025
- **标题**: None
- **领域**: 机器学习,人工智能,计算机视觉和模式识别
- **Abstract**: High-quality open-source datasets, which necessitate substantial efforts for curation, has become the primary catalyst for the swift progress of deep learning. Concurrently, protecting these datasets is paramount for the well-being of the data owner. Dataset ownership verification emerges as a crucial method in this domain, but existing approaches are often limited to supervised models and cannot be directly extended to increasingly popular unsupervised pre-trained models. In this work, we propose the first dataset ownership verification method tailored specifically for self-supervised pre-trained models by contrastive learning. Its primary objective is to ascertain whether a suspicious black-box backbone has been pre-trained on a specific unlabeled dataset, aiding dataset owners in upholding their rights. The proposed approach is motivated by our empirical insights that when models are trained with the target dataset, the unary and binary instance relationships within the embedding space exhibit significant variations compared to models trained without the target dataset. We validate the efficacy of this approach across multiple contrastive pre-trained models including SimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our method rejects the null hypothesis with a $p$-value markedly below $0.05$, surpassing all previous methodologies. Our code is available at https://github.com/xieyc99/DOV4CL.

### Cost-Efficient Continual Learning with Sufficient Exemplar Memory 
[[arxiv](https://arxiv.org/abs/2502.07274)] [[cool](https://papers.cool/arxiv/2502.07274)] [[pdf](https://arxiv.org/pdf/2502.07274)]
> **Authors**: Dongkyu Cho,Taesup Moon,Rumi Chunara,Kyunghyun Cho,Sungmin Cha
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 12 pages, 5 figures
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: Continual learning (CL) research typically assumes highly constrained exemplar memory resources. However, in many real-world scenarios-especially in the era of large foundation models-memory is abundant, while GPU computational costs are the primary bottleneck. In this work, we investigate CL in a novel setting where exemplar memory is ample (i.e., sufficient exemplar memory). Unlike prior methods designed for strict exemplar memory constraints, we propose a simple yet effective approach that directly operates in the model's weight space through a combination of weight resetting and averaging techniques. Our method achieves state-of-the-art performance while reducing the computational cost to a quarter or third of existing methods. These findings challenge conventional CL assumptions and provide a practical baseline for computationally efficient CL applications.

### Variational Learning Induces Adaptive Label Smoothing 
[[arxiv](https://arxiv.org/abs/2502.07273)] [[cool](https://papers.cool/arxiv/2502.07273)] [[pdf](https://arxiv.org/pdf/2502.07273)]
> **Authors**: Sin-Han Yang,Zhedong Liu,Gian Maria Marconi,Mohammad Emtiyaz Khan
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,人工智能
- **Abstract**: We show that variational learning naturally induces an adaptive label smoothing where label noise is specialized for each example. Such label-smoothing is useful to handle examples with labeling errors and distribution shifts, but designing a good adaptivity strategy is not always easy. We propose to skip this step and simply use the natural adaptivity induced during the optimization of a variational objective. We show empirical results where a variational algorithm called IVON outperforms traditional label smoothing and yields adaptivity strategies similar to those of an existing approach. By connecting Bayesian methods to label smoothing, our work provides a new way to handle overconfident predictions.

## 多代理系统(cs.MA:Multiagent Systems)

### Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths of Large and Small LLMs 
[[arxiv](https://arxiv.org/abs/2502.07942)] [[cool](https://papers.cool/arxiv/2502.07942)] [[pdf](https://arxiv.org/pdf/2502.07942)]
> **Authors**: Ruichen Zhang,Mufan Qiu,Zhen Tan,Mohan Zhang,Vincent Lu,Jie Peng,Kaidi Xu,Leandro Z. Agudelo,Peter Qian,Tianlong Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 多代理系统,机器学习
- **Abstract**: Web browsing agents powered by large language models (LLMs) have shown tremendous potential in automating complex web-based tasks. Existing approaches typically rely on large LLMs (e.g., GPT-4o) to explore web environments and generate trajectory data, which is then used either for demonstration retrieval (for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that remains decoupled from the exploration. In this paper, we propose AgentSymbiotic, an iterative framework that couples data synthesis with task-performance, yielding a "symbiotic improvement" for both large and small LLMs. Our study uncovers a complementary dynamic between LLM types: while large LLMs excel at generating high-quality trajectories for distillation, the distilled small LLMs-owing to their distinct reasoning capabilities-often choose actions that diverge from those of their larger counterparts. This divergence drives the exploration of novel trajectories, thereby enriching the synthesized data. However, we also observe that the performance of small LLMs becomes a bottleneck in this iterative enhancement process. To address this, we propose two innovations in LLM distillation: a speculative data synthesis strategy that mitigates off-policy bias, and a multi-task learning approach designed to boost the reasoning capabilities of the student LLM. Furthermore, we introduce a Hybrid Mode for Privacy Preservation to address user privacy concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA performance with both LLM types. Our best Large LLM agent reaches 52%, surpassing the previous best of 45%, while our 8B distilled model demonstrates a competitive 49%, exceeding the prior best of 28%. Code will be released upon acceptance.

## 网络和互联网架构(cs.NI:Networking and Internet Architecture)

### LLM-Sketch: Enhancing Network Sketches with LLM 
[[arxiv](https://arxiv.org/abs/2502.07495)] [[cool](https://papers.cool/arxiv/2502.07495)] [[pdf](https://arxiv.org/pdf/2502.07495)]
> **Authors**: Yuanpeng Li,Zhen Xu,Zongwei Lv,Yannan Hu,Yong Cui,Tong Yang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 网络和互联网架构,机器学习
- **Abstract**: Network stream mining is fundamental to many network operations. Sketches, as compact data structures that offer low memory overhead with bounded accuracy, have emerged as a promising solution for network stream mining. Recent studies attempt to optimize sketches using machine learning; however, these approaches face the challenges of lacking adaptivity to dynamic networks and incurring high training costs. In this paper, we propose LLM-Sketch, based on the insight that fields beyond the flow IDs in packet headers can also help infer flow sizes. By using a two-tier data structure and separately recording large and small flows, LLM-Sketch improves accuracy while minimizing memory usage. Furthermore, it leverages fine-tuned large language models (LLMs) to reliably estimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and the results demonstrate that LLM-Sketch outperforms state-of-the-art methods by achieving a $7.5\times$ accuracy improvement.

## 表现(cs.PF:Performance)

### Memory Analysis on the Training Course of DeepSeek Models 
[[arxiv](https://arxiv.org/abs/2502.07846)] [[cool](https://papers.cool/arxiv/2502.07846)] [[pdf](https://arxiv.org/pdf/2502.07846)]
> **Authors**: Ping Zhang,Lei Su
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 表现,机器学习
- **Abstract**: We present a theoretical analysis of GPU memory consumption during the training of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary objective is to clarify the device-level memory requirements associated with various distributed training configurations. Specifically, we examine critical factors influencing memory usage, including micro-batch size, activation recomputation policies, 3D parallelism, and ZeRO optimizations. It is important to emphasize that the training policies discussed in this report are not representative of DeepSeek's official configurations. Instead, they are explored to provide a deeper understanding of memory dynamics in training of large-scale mixture-of-experts model.

## 机器人技术(cs.RO:Robotics)

### COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping 
[[arxiv](https://arxiv.org/abs/2502.08054)] [[cool](https://papers.cool/arxiv/2502.08054)] [[pdf](https://arxiv.org/pdf/2502.08054)]
> **Authors**: Jun Yamada,Alexander L. Mitchell,Jack Collins,Ingmar Posner
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 14 pages, 11 figures, https://combo-grasp.github.io/
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Traditional robot manipulation approaches struggle with the complexity of non-prehensile or bimanual strategies commonly used by humans in these circumstances. State-of-the-art reinforcement learning (RL) methods are unsuitable due to the inherent complexity of the task. In contrast, learning from demonstration requires collecting a significant number of expert demonstrations, which is often infeasible. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), a learning-based approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination. Specifically, during RL training for the grasping policy, the constraint policy's output is refined through gradients from a jointly trained value function, improving bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy point cloud-based policies in real-world environments. Empirical evaluations demonstrate that COMBO-Grasp significantly improves task success rates compared to competitive baseline approaches, with successful generalisation to unseen objects in both simulated and real-world environments.

### End-to-End Predictive Planner for Autonomous Driving with Consistency Models 
[[arxiv](https://arxiv.org/abs/2502.08033)] [[cool](https://papers.cool/arxiv/2502.08033)] [[pdf](https://arxiv.org/pdf/2502.08033)]
> **Authors**: Anjian Li,Sangjae Bae,David Isele,Ryne Beeson,Faizan M. Tariq
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Trajectory prediction and planning are fundamental components for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditionally, these components have often been treated as separate modules, limiting the ability to perform interactive planning and leading to computational inefficiency in multi-agent scenarios. In this paper, we present a novel unified and data-driven framework that integrates prediction and planning with a single consistency model. Trained on real-world human driving datasets, our consistency model generates samples from high-dimensional, multimodal joint trajectory distributions of the ego and multiple surrounding agents, enabling end-to-end predictive planning. It effectively produces interactive behaviors, such as proactive nudging and yielding to ensure both safe and efficient interactions with other road users. To incorporate additional planning constraints on the ego vehicle, we propose an alternating direction method for multi-objective guidance in online guided sampling. Compared to diffusion models, our consistency model achieves better performance with fewer sampling steps, making it more suitable for real-time deployment. Experimental results on Waymo Open Motion Dataset (WOMD) demonstrate our method's superiority in trajectory quality, constraint satisfaction, and interactive behavior compared to various existing approaches.

### Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning 
[[arxiv](https://arxiv.org/abs/2502.07839)] [[cool](https://papers.cool/arxiv/2502.07839)] [[pdf](https://arxiv.org/pdf/2502.07839)]
> **Authors**: Pengyu Wang,Jialu Li,Ling Shi
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: Accepted in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Workshop
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: With the increasing prevalence of autonomous vehicles (AVs), their vulnerability to various types of attacks has grown, presenting significant security challenges. In this paper, we propose a reinforcement learning (RL)-based approach for designing optimal stealthy integrity attacks on AV actuators. We also analyze the limitations of state-of-the-art RL-based secure controllers developed to counter such attacks. Through extensive simulation experiments, we demonstrate the effectiveness and efficiency of our proposed method.

### RoboBERT: An End-to-end Multimodal Robotic Manipulation Model 
[[arxiv](https://arxiv.org/abs/2502.07837)] [[cool](https://papers.cool/arxiv/2502.07837)] [[pdf](https://arxiv.org/pdf/2502.07837)]
> **Authors**: Sicheng Wang,Jianhua Shan,Jianwei Zhang,Haozhang Gao,Hailiang Han,Yipeng Chen,Kang Wei,Chengkun Zhang,Kairos Wong,Jie Zhao,Lei Zhao,Bin Fang
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器人技术,机器学习
- **Abstract**: Embodied intelligence integrates multiple modalities, enabling agents to understand images, language, and actions simultaneously. However, existing models always depend on additional datasets or extensive pre-training to maximize performance improvements, consuming abundant training time and expensive hardware cost. To tackle this issue, we present RoboBERT, a novel end-to-end robotic manipulation model integrated with a unique training strategy. This model utilizes a CNN-based diffusion policy, enhancing and stabilizing the effectiveness of this model by separating training processes for different modalities. It also underscores the importance of data augmentation, verifying various techniques to significantly boost performance. Unlike models that depend on extra data or large foundation models, RoboBERT achieves a highly competitive success rate while using only language-labeled expert demonstrations and maintaining a relatively smaller model size. Specifically, RoboBERT achieves an average length of 4.52 on the CALVIN benchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art (SOTA) record. Furthermore, when tested on a real robot, the model demonstrates superior performance, achieving a higher success rate than other methods trained with the same data. We propose that these concepts and methodologies of RoboBERT demonstrate extensive versatility and compatibility, contributing significantly to the development of lightweight multimodal robotic models. The code can be accessed on https://github.com/PeterWangsicheng/RoboBERT

## 声音(cs.SD:Sound)

### JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed Metadata 
[[arxiv](https://arxiv.org/abs/2502.07461)] [[cool](https://papers.cool/arxiv/2502.07461)] [[pdf](https://arxiv.org/pdf/2502.07461)]
> **Authors**: Abhinaba Roy,Renhang Liu,Tongyu Lu,Dorien Herremans
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 8 pages, 5 figures
- **标题**: None
- **领域**: 声音,人工智能
- **Abstract**: We introduce JamendoMaxCaps, a large-scale music-caption dataset featuring over 200,000 freely licensed instrumental tracks from the renowned Jamendo platform. The dataset includes captions generated by a state-of-the-art captioning model, enhanced with imputed metadata. We also introduce a retrieval system that leverages both musical features and metadata to identify similar songs, which are then used to fill in missing metadata using a local large language model (LLLM). This approach allows us to provide a more comprehensive and informative dataset for researchers working on music-language understanding tasks. We validate this approach quantitatively with five different measurements. By making the JamendoMaxCaps dataset publicly available, we provide a high-quality resource to advance research in music-language understanding tasks such as music retrieval, multimodal representation learning, and generative music models.

### Music for All: Exploring Multicultural Representations in Music Generation Models 
[[arxiv](https://arxiv.org/abs/2502.07328)] [[cool](https://papers.cool/arxiv/2502.07328)] [[pdf](https://arxiv.org/pdf/2502.07328)]
> **Authors**: Atharva Mehta,Shivam Chauhan,Amirbek Djanibekov,Atharva Kulkarni,Gus Xia,Monojit Choudhury
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 17 pages, 5 figures, accepted to NAACL'25
- **标题**: None
- **领域**: 声音,人工智能,计算语言学,机器学习,多媒体
- **Abstract**: The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.

## 软件工程(cs.SE:Software Engineering)

### Generative AI and Empirical Software Engineering: A Paradigm Shift 
[[arxiv](https://arxiv.org/abs/2502.08108)] [[cool](https://papers.cool/arxiv/2502.08108)] [[pdf](https://arxiv.org/pdf/2502.08108)]
> **Authors**: Christoph Treude,Margaret-Anne Storey
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field. This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.

### From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems 
[[arxiv](https://arxiv.org/abs/2502.07974)] [[cool](https://papers.cool/arxiv/2502.07974)] [[pdf](https://arxiv.org/pdf/2502.07974)]
> **Authors**: Yining Hong,Christopher S. Timperley,Christian Kästner
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted for publication at the International Conference onAIEngineering (CAIN) 2025
- **标题**: None
- **领域**: 软件工程,人工智能,机器学习
- **Abstract**: Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.

### Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights 
[[arxiv](https://arxiv.org/abs/2502.07835)] [[cool](https://papers.cool/arxiv/2502.07835)] [[pdf](https://arxiv.org/pdf/2502.07835)]
> **Authors**: Ahilan Ayyachamy Nadar Ponnusamy
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security. AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub

### Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization 
[[arxiv](https://arxiv.org/abs/2502.07786)] [[cool](https://papers.cool/arxiv/2502.07786)] [[pdf](https://arxiv.org/pdf/2502.07786)]
> **Authors**: Pedro Orvalho,Mikoláš Janota,Vasco Manquinho
> **First submission**: 2024-12-19
> **First announcement**: 2025-02-12
> **comment**: Accepted at AAAI 2025. 11 pages, 4 listings, 2 figures and 5 tables
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools.

### Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK 
[[arxiv](https://arxiv.org/abs/2502.07728)] [[cool](https://papers.cool/arxiv/2502.07728)] [[pdf](https://arxiv.org/pdf/2502.07728)]
> **Authors**: Marcos Cramer,Lucian McIntyre
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: Large language models (LLMs) have demonstrated remarkable code generation capabilities, but the correctness of the generated code cannot be inherently trusted. This paper explores the feasibility of using formal software verification, specifically the SPARK framework for Ada, to ensure the reliability of LLM-generated code. We present Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code. The tool is benchmarked on a curated set of SPARK programs, with annotations selectively removed to test specific capabilities. The performance of Marmaragan with GPT-4o on the benchmark is promising, with correct annotations having been generated for 50.7% of the benchmark cases. The results establish a foundation for future work on combining the power of LLMs with the reliability of formal software verification.

### On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o 
[[arxiv](https://arxiv.org/abs/2502.07399)] [[cool](https://papers.cool/arxiv/2502.07399)] [[pdf](https://arxiv.org/pdf/2502.07399)]
> **Authors**: Rundong Liu,Andre Frade,Amal Vaidya,Maxime Labonne,Marcus Kaiser,Bismayan Chakrabarti,Jonathan Budd,Sean Moran
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 软件工程,人工智能
- **Abstract**: This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluator's feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The framework's evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at: https://github.com/jpmorganchase/CodeQuest.

## 理论经济学(econ.TH:Theoretical Economics)

### NDAI Agreements 
[[arxiv](https://arxiv.org/abs/2502.07924)] [[cool](https://papers.cool/arxiv/2502.07924)] [[pdf](https://arxiv.org/pdf/2502.07924)]
> **Authors**: Matthew Stephenson,Andrew Miller,Xyn Sun,Bhargav Annem,Rohan Parikh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 21 pages, 1 figure
- **标题**: None
- **领域**: 理论经济学,人工智能
- **Abstract**: We study a fundamental challenge in the economics of innovation: an inventor must reveal details of a new idea to secure compensation or funding, yet such disclosure risks expropriation. We present a model in which a seller (inventor) and buyer (investor) bargain over an information good under the threat of hold-up. In the classical setting, the seller withholds disclosure to avoid misappropriation, leading to inefficiency. We show that trusted execution environments (TEEs) combined with AI agents can mitigate and even fully eliminate this hold-up problem. By delegating the disclosure and payment decisions to tamper-proof programs, the seller can safely reveal the invention without risking expropriation, achieving full disclosure and an efficient ex post transfer. Moreover, even if the invention's value exceeds a threshold that TEEs can fully secure, partial disclosure still improves outcomes compared to no disclosure. Recognizing that real AI agents are imperfect, we model "agent errors" in payments or disclosures and demonstrate that budget caps and acceptance thresholds suffice to preserve most of the efficiency gains. Our results imply that cryptographic or hardware-based solutions can function as an "ironclad NDA," substantially mitigating the fundamental disclosure-appropriation paradox first identified by Arrow (1962) and Nelson (1959). This has far-reaching policy implications for fostering R&D, technology transfer, and collaboration.

## 音频和语音处理(eess.AS:Audio and Speech Processing)

### Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss 
[[arxiv](https://arxiv.org/abs/2502.07575)] [[cool](https://papers.cool/arxiv/2502.07575)] [[pdf](https://arxiv.org/pdf/2502.07575)]
> **Authors**: Fu-An Chao,Berlin Chen
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted to NAACL 2025 main conference
- **标题**: None
- **领域**: 音频和语音处理,计算语言学
- **Abstract**: Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts: the former aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while the latter focuses instead on pinpointing the precise phonetic pronunciation errors made by non-native language learners. However, it is generally expected that a full-fledged CAPT system should perform both functionalities simultaneously and efficiently. In response to this surging demand, we in this work first propose HMamba, a novel CAPT approach that seamlessly integrates APA and MDD tasks in parallel. In addition, we introduce a novel loss function, decoupled cross-entropy loss (deXent), specifically tailored for MDD to facilitate better-supervised learning for detecting mispronounced phones, thereby enhancing overall performance. A comprehensive set of empirical results on the speechocean762 benchmark dataset demonstrates the effectiveness of our approach on APA. Notably, our proposed approach also yields a considerable improvement in MDD performance over a strong baseline, achieving an F1-score of 63.85%. Our codes are made available at https://github.com/Fuann/hmamba

## 图像和视频处理(eess.IV:Image and Video Processing)

### Automatic Prostate Volume Estimation in Transabdominal Ultrasound Images 
[[arxiv](https://arxiv.org/abs/2502.07859)] [[cool](https://papers.cool/arxiv/2502.07859)] [[pdf](https://arxiv.org/pdf/2502.07859)]
> **Authors**: Tiziano Natali,Liza M. Kurucz,Matteo Fusaglia,Laura S. Mertens,Theo J. M. Ruers,Pim J. van Leeuwen,Behdad Dashtbozorg
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Prostate cancer is a leading health concern among men, requiring accurate and accessible methods for early detection and risk stratification. Prostate volume (PV) is a key parameter in multivariate risk stratification for early prostate cancer detection, commonly estimated using transrectal ultrasound (TRUS). While TRUS provides precise prostate volume measurements, its invasive nature often compromises patient comfort. Transabdominal ultrasound (TAUS) provides a non-invasive alternative but faces challenges such as lower image quality, complex interpretation, and reliance on operator expertise. This study introduces a new deep-learning-based framework for automatic PV estimation using TAUS, emphasizing its potential to enable accurate and non-invasive prostate cancer risk stratification. A dataset of TAUS videos from 100 individual patients was curated, with manually delineated prostate boundaries and calculated diameters by an expert clinician as ground truth. The introduced framework integrates deep-learning models for prostate segmentation in both axial and sagittal planes, automatic prostate diameter estimation, and PV calculation. Segmentation performance was evaluated using Dice correlation coefficient (%) and Hausdorff distance (mm). Framework's volume estimation capabilities were evaluated on volumetric error (mL). The framework demonstrates that it can estimate PV from TAUS videos with a mean volumetric error of -5.5 mL, which results in an average relative error between 5 and 15%. The introduced framework for automatic PV estimation from TAUS images, utilizing deep learning models for prostate segmentation, shows promising results. It effectively segments the prostate and estimates its volume, offering potential for reliable, non-invasive risk stratification for early prostate detection.

### The establishment of static digital humans and the integration with spinal models 
[[arxiv](https://arxiv.org/abs/2502.07844)] [[cool](https://papers.cool/arxiv/2502.07844)] [[pdf](https://arxiv.org/pdf/2502.07844)]
> **Authors**: Fujiao Ju,Yuxuan Wang,Shuo Wang,Chengyin Wang,Yinbo Chen,Jianfeng Li,Mingjie Dong,Bin Fang,Qianyu Zhuang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,计算机视觉和模式识别
- **Abstract**: Adolescent idiopathic scoliosis (AIS), a prevalent spinal deformity, significantly affects individuals' health and quality of life. Conventional imaging techniques, such as X - rays, computed tomography (CT), and magnetic resonance imaging (MRI), offer static views of the spine. However, they are restricted in capturing the dynamic changes of the spine and its interactions with overall body motion. Therefore, developing new techniques to address these limitations has become extremely important. Dynamic digital human modeling represents a major breakthrough in digital medicine. It enables a three - dimensional (3D) view of the spine as it changes during daily activities, assisting clinicians in detecting deformities that might be missed in static imaging. Although dynamic modeling holds great potential, constructing an accurate static digital human model is a crucial initial step for high - precision simulations. In this study, our focus is on constructing an accurate static digital human model integrating the spine, which is vital for subsequent dynamic digital human research on AIS. First, we generate human point - cloud data by combining the 3D Gaussian method with the Skinned Multi - Person Linear (SMPL) model from the patient's multi - view images. Then, we fit a standard skeletal model to the generated human model. Next, we align the real spine model reconstructed from CT images with the standard skeletal model. We validated the resulting personalized spine model using X - ray data from six AIS patients, with Cobb angles (used to measure the severity of scoliosis) as evaluation metrics. The results indicate that the model's error was within 1 degree of the actual measurements. This study presents an important method for constructing digital humans.

### The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation 
[[arxiv](https://arxiv.org/abs/2502.07516)] [[cool](https://papers.cool/arxiv/2502.07516)] [[pdf](https://arxiv.org/pdf/2502.07516)]
> **Authors**: Raman Dutt
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 图像和视频处理,人工智能,计算机视觉和模式识别,机器学习
- **Abstract**: Generative models, particularly text-to-image (T2I) diffusion models, play a crucial role in medical image analysis. However, these models are prone to training data memorization, posing significant risks to patient privacy. Synthetic chest X-ray generation is one of the most common applications in medical image analysis with the MIMIC-CXR dataset serving as the primary data repository for this task. This study presents the first systematic attempt to identify prompts and text tokens in MIMIC-CXR that contribute the most to training data memorization. Our analysis reveals two unexpected findings: (1) prompts containing traces of de-identification procedures (markers introduced to hide Protected Health Information) are the most memorized, and (2) among all tokens, de-identification markers contribute the most towards memorization. This highlights a broader issue with the standard anonymization practices and T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time memorization mitigation strategies are ineffective and fail to sufficiently reduce the model's reliance on memorized text tokens. On this front, we propose actionable strategies for different stakeholders to enhance privacy and improve the reliability of generative models in medical imaging. Finally, our results provide a foundation for future work on developing and benchmarking memorization mitigation techniques for synthetic chest X-ray generation using the MIMIC-CXR dataset. The anonymized code is available at https://anonymous.4open.science/r/diffusion_memorization-8011/

## 数值分析(math.NA:Numerical Analysis)

### What is a Sketch-and-Precondition Derivation for Low-Rank Approximation? Inverse Power Error or Inverse Power Estimation? 
[[arxiv](https://arxiv.org/abs/2502.07993)] [[cool](https://papers.cool/arxiv/2502.07993)] [[pdf](https://arxiv.org/pdf/2502.07993)]
> **Authors**: Ruihan Xu,Yiping Lu
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 数值分析,计算复杂度,机器学习,计算,机器学习
- **Abstract**: Randomized sketching accelerates large-scale numerical linear algebra by reducing computational complexity. While the traditional sketch-and-solve approach reduces the problem size directly through sketching, the sketch-and-precondition method leverages sketching to construct a computational friendly preconditioner. This preconditioner improves the convergence speed of iterative solvers applied to the original problem, maintaining accuracy in the full space. Furthermore, the convergence rate of the solver improves at least linearly with the sketch size. Despite its potential, developing a sketch-and-precondition framework for randomized algorithms in low-rank matrix approximation remains an open challenge. We introduce the Error-Powered Sketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for the Lagrange form as a sketch-and-precondition variant for randomized low-rank approximation. Our method achieves theoretical guarantees, including a convergence rate that improves at least linearly with the sketch size.

## 优化与控制(math.OC:Optimization and Control)

### Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle 
[[arxiv](https://arxiv.org/abs/2502.07923)] [[cool](https://papers.cool/arxiv/2502.07923)] [[pdf](https://arxiv.org/pdf/2502.07923)]
> **Authors**: Nikita Kornilov,Philip Zmushko,Andrei Semenov,Alexander Gasnikov,Alexander Beznosikov
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 优化与控制,机器学习
- **Abstract**: The growing popularity of AI optimization problems involving severely corrupted data has increased the demand for methods capable of handling heavy-tailed noise, i.e., noise with bounded $κ$-th moment, $κ\in (1,2]$. For the widely used clipping technique, effectiveness heavily depends on the careful tuning of clipping levels throughout training. In this paper, we demonstrate that using only the sign of the input, without introducing additional hyperparameters, is sufficient to cope with heavy-tailed noise effectively. For smooth non-convex functions, we prove that SignSGD achieves optimal sample complexity $\tilde{O}\left(\varepsilon^{-\frac{3κ- 2}{κ- 1}}\right)$ with high probability for attaining an average gradient norm accuracy of $\varepsilon$. Under the assumption of symmetric noise, we use SignSGD with Majority Voting to extend this bound to the distributed optimization or reduce the sample complexity to $\tilde{O}(\varepsilon^{-4})$ in the case of a single worker with arbitrary parameters. Furthermore, we explore the application of the sign operator in zeroth-order optimization with an oracle that can only compare function values at two different points. We propose a novel method, MajorityVote-CompsSGD, and provide the first-known high-probability bound $\tilde{O}(\varepsilon^{-6})$ for the number of comparisons under symmetric noise assumption. Our theoretical findings are supported by the superior performance of sign-based methods in training Large Language Models.

## 仪器仪表和探测器(physics.ins-det:Instrumentation and Detectors)

### Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF Corrections 
[[arxiv](https://arxiv.org/abs/2502.07630)] [[cool](https://papers.cool/arxiv/2502.07630)] [[pdf](https://arxiv.org/pdf/2502.07630)]
> **Authors**: Stephan Naunheim,Luis Lopes de Paiva,Vanessa Nadig,Yannick Kuhl,Stefan Gundacker,Florian Mueller,Volkmar Schulz
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 仪器仪表和探测器,机器学习
- **Abstract**: PET is a functional imaging method that visualizes metabolic processes. TOF information can be derived from coincident detector signals and incorporated into image reconstruction to enhance the SNR. PET detectors are typically assessed by their CTR, but timing performance is degraded by various factors. Research on timing calibration seeks to mitigate these degradations and restore accurate timing information. While many calibration methods use analytical approaches, machine learning techniques have recently gained attention due to their flexibility. We developed a residual physics-based calibration approach that combines prior domain knowledge with the power of machine learning models. This approach begins with an initial analytical calibration addressing first-order skews. The remaining deviations, regarded as residual effects, are used to train machine learning models to eliminate higher-order skews. The key advantage is that the experimenter guides the learning process through the definition of timing residuals. In earlier studies, we developed models that directly predicted the expected time difference, which offered corrections only implicitly (implicit correction models). In this study, we introduce a new definition for timing residuals, enabling us to train models that directly predict correction values (explicit correction models). The explicit correction approach significantly simplifies data acquisition, improves linearity, and enhances timing performance from $371 \pm 6$ ps to $281 \pm 5$ ps for coincidences from 430 keV to 590 keV. Additionally, the new definition reduces model size, making it suitable for high-throughput applications like PET scanners. Experiments were conducted using two detector stacks composed of $4 \times 4$ LYSO:Ce,Ca crystals ($3.8\times 3.8\times 20$ mm$^{3}$) coupled to $4 \times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.

## 等离子体物理(physics.plasm-ph:Plasma Physics)

### 5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma Turbulence 
[[arxiv](https://arxiv.org/abs/2502.07469)] [[cool](https://papers.cool/arxiv/2502.07469)] [[pdf](https://arxiv.org/pdf/2502.07469)]
> **Authors**: Gianluca Galletti,Fabian Paischer,Paul Setinek,William Hornsby,Lorenzo Zanisi,Naomi Carey,Stanislas Pamela,Johannes Brandstetter
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 6 pages (+ references and appendix)
- **标题**: None
- **领域**: 等离子体物理,人工智能,机器学习,机器学习
- **Abstract**: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to achieving commercially viable fusion power is understanding plasma turbulence, which can significantly degrade plasma confinement. Modelling turbulence is crucial to design performing plasma scenarios for next-generation reactor-class devices and current experimental machines. The nonlinear gyrokinetic equation underpinning turbulence modelling evolves a 5D distribution function over time. Solving this equation numerically is extremely expensive, requiring up to weeks for a single run to converge, making it unfeasible for iterative optimisation and control studies. In this work, we propose a method for training neural surrogates for 5D gyrokinetic simulations. Our method extends a hierarchical vision transformer to five dimensions and is trained on the 5D distribution function for the adiabatic electron approximation. We demonstrate that our model can accurately infer downstream physical quantities such as heat flux time trace and electrostatic potentials for single-step predictions two orders of magnitude faster than numerical codes. Our work paves the way towards neural surrogates for plasma turbulence simulations to accelerate deployment of commercial energy production via nuclear fusion.

## 基因组学(q-bio.GN:Genomics)

### Whole-Genome Phenotype Prediction with Machine Learning: Open Problems in Bacterial Genomics 
[[arxiv](https://arxiv.org/abs/2502.07749)] [[cool](https://papers.cool/arxiv/2502.07749)] [[pdf](https://arxiv.org/pdf/2502.07749)]
> **Authors**: Tamsin James,Ben Williamson,Peter Tino,Nicole Wheeler
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 13 pages
- **标题**: None
- **领域**: 基因组学,机器学习
- **Abstract**: How can we identify causal genetic mechanisms that govern bacterial traits? Initial efforts entrusting machine learning models to handle the task of predicting phenotype from genotype return high accuracy scores. However, attempts to extract any meaning from the predictive models are found to be corrupted by falsely identified "causal" features. Relying solely on pattern recognition and correlations is unreliable, significantly so in bacterial genomics settings where high-dimensionality and spurious associations are the norm. Though it is not yet clear whether we can overcome this hurdle, significant efforts are being made towards discovering potential high-risk bacterial genetic variants. In view of this, we set up open problems surrounding phenotype prediction from bacterial whole-genome datasets and extending those to learning causal effects, and discuss challenges that impact the reliability of a machine's decision-making when faced with datasets of this nature.

## 神经元和认知(q-bio.NC:Neurons and Cognition)

### Some things to know about achieving artificial general intelligence 
[[arxiv](https://arxiv.org/abs/2502.07828)] [[cool](https://papers.cool/arxiv/2502.07828)] [[pdf](https://arxiv.org/pdf/2502.07828)]
> **Authors**: Herbert Roitblat
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 神经元和认知,人工智能
- **Abstract**: Current and foreseeable GenAI models are not capable of achieving artificial general intelligence because they are burdened with anthropogenic debt. They depend heavily on human input to provide well-structured problems, architecture, and training data. They cast every problem as a language pattern learning problem and are thus not capable of the kind of autonomy needed to achieve artificial general intelligence. Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent. Another barrier is the need to recognize that there are multiple kinds of problems, some of which cannot be solved by available computational methods (for example, "insight problems"). Current methods for evaluating models (benchmarks and tests) are not adequate to identify the generality of the solutions, because it is impossible to infer the means by which a problem was solved from the fact of its solution. A test could be passed, for example, by a test-specific or a test-general method. It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success.

### neuro2voc: Decoding Vocalizations from Neural Activity 
[[arxiv](https://arxiv.org/abs/2502.07800)] [[cool](https://papers.cool/arxiv/2502.07800)] [[pdf](https://arxiv.org/pdf/2502.07800)]
> **Authors**: Fei Gao
> **First submission**: 2025-02-02
> **First announcement**: 2025-02-12
> **comment**: Master Thesis
- **标题**: None
- **领域**: 神经元和认知,机器学习,音频和语音处理
- **Abstract**: Accurate decoding of neural spike trains and relating them to motor output is a challenging task due to the inherent sparsity and length in neural spikes and the complexity of brain circuits. This master project investigates experimental methods for decoding zebra finch motor outputs (in both discrete syllables and continuous spectrograms), from invasive neural recordings obtained from Neuropixels. There are three major achievements: (1) XGBoost with SHAP analysis trained on spike rates revealed neuronal interaction patterns crucial for syllable classification. (2) Novel method (tokenizing neural data with GPT2) and architecture (Mamba2) demonstrated potential for decoding of syllables using spikes. (3) A combined contrastive learning-VAE framework successfully generated spectrograms from binned neural data. This work establishes a promising foundation for neural decoding of complex motor outputs and offers several novel methodological approaches for processing sparse neural data.

## 定量方法(q-bio.QM:Quantitative Methods)

### Advancing Precision Oncology Through Modeling of Longitudinal and Multimodal Data 
[[arxiv](https://arxiv.org/abs/2502.07836)] [[cool](https://papers.cool/arxiv/2502.07836)] [[pdf](https://arxiv.org/pdf/2502.07836)]
> **Authors**: Luoting Zhuang,Stephen H. Park,Steven J. Skates,Ashley E. Prosper,Denise R. Aberle,William Hsu
> **First submission**: 2025-02-10
> **First announcement**: 2025-02-12
> **comment**: This work has been submitted to the IEEE RBME for potential publication
- **标题**: None
- **领域**: 定量方法,机器学习
- **Abstract**: Cancer evolves continuously over time through a complex interplay of genetic, epigenetic, microenvironmental, and phenotypic changes. This dynamic behavior drives uncontrolled cell growth, metastasis, immune evasion, and therapy resistance, posing challenges for effective monitoring and treatment. However, today's data-driven research in oncology has primarily focused on cross-sectional analysis using data from a single modality, limiting the ability to fully characterize and interpret the disease's dynamic heterogeneity. Advances in multiscale data collection and computational methods now enable the discovery of longitudinal multimodal biomarkers for precision oncology. Longitudinal data reveal patterns of disease progression and treatment response that are not evident from single-timepoint data, enabling timely abnormality detection and dynamic treatment adaptation. Multimodal data integration offers complementary information from diverse sources for more precise risk assessment and targeting of cancer therapy. In this review, we survey methods of longitudinal and multimodal modeling, highlighting their synergy in providing multifaceted insights for personalized care tailored to the unique characteristics of a patient's cancer. We summarize the current challenges and future directions of longitudinal multimodal analysis in advancing precision oncology.

### Supervised contrastive learning for cell stage classification of animal embryos 
[[arxiv](https://arxiv.org/abs/2502.07360)] [[cool](https://papers.cool/arxiv/2502.07360)] [[pdf](https://arxiv.org/pdf/2502.07360)]
> **Authors**: Yasmine Hachani,Patrick Bouthemy,Elisa Fromont,Sylvie Ruffini,Ludivine Laffont,Alline de Paula Reis
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 定量方法,计算机视觉和模式识别
- **Abstract**: Video microscopy, when combined with machine learning, offers a promising approach for studying the early development of in vitro produced (IVP) embryos. However, manually annotating developmental events, and more specifically cell divisions, is time-consuming for a biologist and cannot scale up for practical applications. We aim to automatically classify the cell stages of embryos from 2D time-lapse microscopy videos with a deep learning approach. We focus on the analysis of bovine embryonic development using video microscopy, as we are primarily interested in the application of cattle breeding, and we have created a Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1) low-quality images and bovine dark cells that make the identification of cell stages difficult, (2) class ambiguity at the boundaries of developmental stages, and (3) imbalanced data distribution. To address these challenges, we introduce CLEmbryo, a novel method that leverages supervised contrastive learning combined with focal loss for training, and the lightweight 3D neural network CSN-50 as an encoder. We also show that our method generalizes well. CLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset and the publicly available NYU Mouse Embryos dataset.

## 计算金融(q-fin.CP:Computational Finance)

### Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis 
[[arxiv](https://arxiv.org/abs/2502.07806)] [[cool](https://papers.cool/arxiv/2502.07806)] [[pdf](https://arxiv.org/pdf/2502.07806)]
> **Authors**: Rath Minati,Date Hema
> **First submission**: 2025-02-06
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 计算金融,人工智能,机器学习
- **Abstract**: The integration of Quantum Deep Learning (QDL) techniques into the landscape of financial risk analysis presents a promising avenue for innovation. This study introduces a framework for credit risk assessment in the banking sector, combining quantum deep learning techniques with adaptive modeling for Row-Type Dependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed approach tailors predictive models to different loan categories, aiming to enhance the accuracy and efficiency of credit risk evaluation. While this work explores the potential of integrating quantum methods with classical deep learning for risk assessment, it focuses on the feasibility and performance of this hybrid framework rather than claiming transformative industry-wide impacts. The findings offer insights into how quantum techniques can complement traditional financial analysis, paving the way for further advancements in predictive modeling for credit risk.

## 量子物理学(quant-ph:Quantum Physics)

### A unifying account of warm start guarantees for patches of quantum landscapes 
[[arxiv](https://arxiv.org/abs/2502.07889)] [[cool](https://papers.cool/arxiv/2502.07889)] [[pdf](https://arxiv.org/pdf/2502.07889)]
> **Authors**: Hela Mhiri,Ricard Puig,Sacha Lerch,Manuel S. Rudolph,Thiparat Chotibut,Supanut Thanasilp,Zoë Holmes
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 量子物理学,机器学习,机器学习
- **Abstract**: Barren plateaus are fundamentally a statement about quantum loss landscapes on average but there can, and generally will, exist patches of barren plateau landscapes with substantial gradients. Previous work has studied certain classes of parameterized quantum circuits and found example regions where gradients vanish at worst polynomially in system size. Here we present a general bound that unifies all these previous cases and that can tackle physically-motivated ansätze that could not be analyzed previously. Concretely, we analytically prove a lower-bound on the variance of the loss that can be used to show that in a non-exponentially narrow region around a point with curvature the loss variance cannot decay exponentially fast. This result is complemented by numerics and an upper-bound that suggest that any loss function with a barren plateau will have exponentially vanishing gradients in any constant radius subregion. Our work thus suggests that while there are hopes to be able to warm-start variational quantum algorithms, any initialization strategy that cannot get increasingly close to the region of attraction with increasing problem size is likely inadequate.

## 应用领域(stat.AP:Applications)

### Forecasting the future development in quality and value of professional football players for applications in team management 
[[arxiv](https://arxiv.org/abs/2502.07528)] [[cool](https://papers.cool/arxiv/2502.07528)] [[pdf](https://arxiv.org/pdf/2502.07528)]
> **Authors**: Koen W. van Arem,Floris Goes-Smit,Jakob Söhl
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: The article itself is on the pages 1-27. The data set used in this article is described in the appendix at the pages 28-35
- **标题**: None
- **领域**: 应用领域,机器学习
- **Abstract**: Transfers in professional football (soccer) are risky investments because of the large transfer fees and high risks involved. Although data-driven models can be used to improve transfer decisions, existing models focus on describing players' historical progress, leaving their future performance unknown. Moreover, recent developments have called for the use of explainable models combined with uncertainty quantification of predictions. This paper assesses explainable machine learning models based on predictive accuracy and uncertainty quantification methods for the prediction of the future development in quality and transfer value of professional football players. Using a historical data set of data-driven indicators describing player quality and the transfer value of a football player, the models are trained to forecast player quality and player value one year ahead. These two prediction problems demonstrate the efficacy of tree-based models, particularly random forest and XGBoost, in making accurate predictions. In general, the random forest model is found to be the most suitable model because it provides accurate predictions as well as an uncertainty quantification method that naturally arises from the bagging procedure of the random forest model. Additionally, our research shows that the development of player performance contains nonlinear patterns and interactions between variables, and that time series information can provide useful information for the modeling of player performance metrics. Our research provides models to help football clubs make more informed, data-driven transfer decisions by forecasting player quality and transfer value.

## 机器学习(stat.ML:Machine Learning)

### Optimizing Likelihoods via Mutual Information: Bridging Simulation-Based Inference and Bayesian Optimal Experimental Design 
[[arxiv](https://arxiv.org/abs/2502.08004)] [[cool](https://papers.cool/arxiv/2502.08004)] [[pdf](https://arxiv.org/pdf/2502.08004)]
> **Authors**: Vincent D. Zaballa,Elliot E. Hui
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Preprint. Under Review
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Simulation-based inference (SBI) is a method to perform inference on a variety of complex scientific models with challenging inference (inverse) problems. Bayesian Optimal Experimental Design (BOED) aims to efficiently use experimental resources to make better inferences. Various stochastic gradient-based BOED methods have been proposed as an alternative to Bayesian optimization and other experimental design heuristics to maximize information gain from an experiment. We demonstrate a link via mutual information bounds between SBI and stochastic gradient-based variational inference methods that permits BOED to be used in SBI applications as SBI-BOED. This link allows simultaneous optimization of experimental designs and optimization of amortized inference functions. We evaluate the pitfalls of naive design optimization using this method in a standard SBI task and demonstrate the utility of a well-chosen design distribution in BOED. We compare this approach on SBI-based models in real-world simulators in epidemiology and biology, showing notable improvements in inference.

### Discrete Markov Probabilistic Models 
[[arxiv](https://arxiv.org/abs/2502.07939)] [[cool](https://papers.cool/arxiv/2502.07939)] [[pdf](https://arxiv.org/pdf/2502.07939)]
> **Authors**: Le-Tuyet-Nhi Pham,Dario Shariatian,Antonio Ocello,Giovanni Conforti,Alain Durmus
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel algorithm for discrete data generation. The algorithm operates in the space of bits $\{0,1\}^d$, where the noising process is a continuous-time Markov chain that can be sampled exactly via a Poissonian clock that flips labels uniformly at random. The time-reversal process, like the forward noise process, is a jump process, with its intensity governed by a discrete analogue of the classical score function. Crucially, this intensity is proven to be the conditional expectation of a function of the forward process, strengthening its theoretical alignment with score-based generative models while ensuring robustness and efficiency. We further establish convergence bounds for the algorithm under minimal assumptions and demonstrate its effectiveness through experiments on low-dimensional Bernoulli-distributed datasets and high-dimensional binary MNIST data. The results highlight its strong performance in generating discrete structures. This work bridges theoretical foundations and practical applications, advancing the development of effective and theoretically grounded discrete generative modeling.

### The Observational Partial Order of Causal Structures with Latent Variables 
[[arxiv](https://arxiv.org/abs/2502.07891)] [[cool](https://papers.cool/arxiv/2502.07891)] [[pdf](https://arxiv.org/pdf/2502.07891)]
> **Authors**: Marina Maciel Ansanelli,Elie Wolfe,Robert W. Spekkens
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 48 pages, 30 figures; acknowledgements added
- **标题**: None
- **领域**: 机器学习,机器学习,量子物理学
- **Abstract**: For two causal structures with the same set of visible variables, one is said to observationally dominate the other if the set of distributions over the visible variables realizable by the first contains the set of distributions over the visible variables realizable by the second. Knowing such dominance relations is useful for adjudicating between these structures given observational data. We here consider the problem of determining the partial order of equivalence classes of causal structures with latent variables relative to observational dominance. We provide a complete characterization of the dominance order in the case of three visible variables, and a partial characterization in the case of four visible variables. Our techniques also help to identify which observational equivalence classes have a set of realizable distributions that is characterized by nontrivial inequality constraints, analogous to Bell inequalities and instrumental inequalities. We find evidence that as one increases the number of visible variables, the equivalence classes satisfying nontrivial inequality constraints become ubiquitous. (Because such classes are the ones for which there can be a difference in the distributions that are quantumly and classically realizable, this implies that the potential for quantum-classical gaps is also ubiquitous.) Furthermore, we find evidence that constraint-based causal discovery algorithms that rely solely on conditional independence constraints have a significantly weaker distinguishing power among observational equivalence classes than algorithms that go beyond these (i.e., algorithms that also leverage nested Markov constraints and inequality constraints).

### SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph 
[[arxiv](https://arxiv.org/abs/2502.07857)] [[cool](https://papers.cool/arxiv/2502.07857)] [[pdf](https://arxiv.org/pdf/2502.07857)]
> **Authors**: Mátyás Schubert,Tom Claassen,Sara Magliacane
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Accepted at AISTATS 2025
- **标题**: None
- **领域**: 机器学习,人工智能,机器学习
- **Abstract**: Causal discovery can be computationally demanding for large numbers of variables. If we only wish to estimate the causal effects on a small subset of target variables, we might not need to learn the causal graph for all variables, but only a small subgraph that includes the targets and their adjustment sets. In this paper, we focus on identifying causal effects between target variables in a computationally and statistically efficient way. This task combines causal discovery and effect estimation, aligning the discovery objective with the effects to be estimated. We show that definite non-ancestors of the targets are unnecessary to learn causal relations between the targets and to identify efficient adjustments sets. We sequentially identify and prune these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP) framework, which can be used either as a preprocessing step to standard causal discovery methods, or as a standalone sound and complete causal discovery algorithm. Our results on synthetic and real data show that both approaches substantially reduce the number of independence tests and the computation time without compromising the quality of causal effect estimations.

### Guiding Time-Varying Generative Models with Natural Gradients on Exponential Family Manifold 
[[arxiv](https://arxiv.org/abs/2502.07650)] [[cool](https://papers.cool/arxiv/2502.07650)] [[pdf](https://arxiv.org/pdf/2502.07650)]
> **Authors**: Song Liu,Leyang Wang,Yakun Wang
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Optimising probabilistic models is a well-studied field in statistics. However, its connection with the training of generative models remains largely under-explored. In this paper, we show that the evolution of time-varying generative models can be projected onto an exponential family manifold, naturally creating a link between the parameters of a generative model and those of a probabilistic model. We then train the generative model by moving its projection on the manifold according to the natural gradient descent scheme. This approach also allows us to approximate the natural gradient of the KL divergence efficiently without relying on MCMC for intractable models. Furthermore, we propose particle versions of the algorithm, which feature closed-form update rules for any parametric model within the exponential family. Through toy and real-world experiments, we validate the effectiveness of the proposed algorithms.

### Understanding the Generalization Error of Markov algorithms through Poissonization 
[[arxiv](https://arxiv.org/abs/2502.07584)] [[cool](https://papers.cool/arxiv/2502.07584)] [[pdf](https://arxiv.org/pdf/2502.07584)]
> **Authors**: Benjamin Dupuis,Maxime Haddouche,George Deligiannidis,Umut Simsekli
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Using continuous-time stochastic differential equation (SDE) proxies to stochastic optimization algorithms has proven fruitful for understanding their generalization abilities. A significant part of these approaches are based on the so-called ``entropy flows'', which greatly simplify the generalization analysis. Unfortunately, such well-structured entropy flows cannot be obtained for most discrete-time algorithms, and the existing SDE approaches remain limited to specific noise and algorithmic structures. We aim to alleviate this issue by introducing a generic framework for analyzing the generalization error of Markov algorithms through `Poissonization', a continuous-time approximation of discrete-time processes with formal approximation guarantees. Through this approach, we first develop a novel entropy flow, which directly leads to PAC-Bayesian generalization bounds. We then draw novel links to modified versions of the celebrated logarithmic Sobolev inequalities (LSI), identify cases where such LSIs are satisfied, and obtain improved bounds. Beyond its generality, our framework allows exploiting specific properties of learning algorithms. In particular, we incorporate the noise structure of different algorithm types - namely, those with additional noise injections (noisy) and those without (non-noisy) - through various technical tools. This illustrates the capacity of our methods to achieve known (yet, Poissonized) and new generalization bounds.

### Quantification of model error for inverse problems in the Weak Neural Variational Inference framework 
[[arxiv](https://arxiv.org/abs/2502.07415)] [[cool](https://papers.cool/arxiv/2502.07415)] [[pdf](https://arxiv.org/pdf/2502.07415)]
> **Authors**: Vincent C. Scholz,P. S. Koutsourelakis
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 15 pages, 6 figures
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: We present a novel extension of the Weak Neural Variational Inference (WNVI) framework for probabilistic material property estimation that explicitly quantifies model errors in PDE-based inverse problems. Traditional approaches assume the correctness of all governing equations, including potentially unreliable constitutive laws, which can lead to biased estimates and misinterpretations. Our proposed framework addresses this limitation by distinguishing between reliable governing equations, such as conservation laws, and uncertain constitutive relationships. By treating all state variables as latent random variables, we enforce these equations through separate sets of residuals, leveraging a virtual likelihood approach with weighted residuals. This formulation not only identifies regions where constitutive laws break down but also improves robustness against model uncertainties without relying on a fully trustworthy forward model. We demonstrate the effectiveness of our approach in the context of elastography, showing that it provides a structured, interpretable, and computationally efficient alternative to traditional model error correction techniques. Our findings suggest that the proposed framework enhances the accuracy and reliability of material property estimation by offering a principled way to incorporate uncertainty in constitutive modeling.

### Bandit Optimal Transport 
[[arxiv](https://arxiv.org/abs/2502.07397)] [[cool](https://papers.cool/arxiv/2502.07397)] [[pdf](https://arxiv.org/pdf/2502.07397)]
> **Authors**: Lorenzo Croissant
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习
- **Abstract**: Despite the impressive progress in statistical Optimal Transport (OT) in recent years, there has been little interest in the study of the \emph{sequential learning} of OT. Surprisingly so, as this problem is both practically motivated and a challenging extension of existing settings such as linear bandits. This article considers (for the first time) the stochastic bandit problem of learning to solve generic Kantorovich and entropic OT problems from repeated interactions when the marginals are known but the cost is unknown. We provide $\tilde{\mathcal O}(\sqrt{T})$ regret algorithms for both problems by extending linear bandits on Hilbert spaces. These results provide a reduction to infinite-dimensional linear bandits. To deal with the dimension, we provide a method to exploit the intrinsic regularity of the cost to learn, yielding corresponding regret bounds which interpolate between $\tilde{\mathcal O}(\sqrt{T})$ and $\tilde{\mathcal O}(T)$.

### Uniform Kernel Prober 
[[arxiv](https://arxiv.org/abs/2502.07369)] [[cool](https://papers.cool/arxiv/2502.07369)] [[pdf](https://arxiv.org/pdf/2502.07369)]
> **Authors**: Soumya Mukherjee,Bharath K. Sriperumbudur
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: 34 pages, 10 figures
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: The ability to identify useful features or representations of the input data based on training data that achieves low prediction error on test data across multiple prediction tasks is considered the key to multitask learning success. In practice, however, one faces the issue of the choice of prediction tasks and the availability of test data from the chosen tasks while comparing the relative performance of different features. In this work, we develop a class of pseudometrics called Uniform Kernel Prober (UKP) for comparing features or representations learned by different statistical models such as neural networks when the downstream prediction tasks involve kernel ridge regression. The proposed pseudometric, UKP, between any two representations, provides a uniform measure of prediction error on test data corresponding to a general class of kernel ridge regression tasks for a given choice of a kernel without access to test data. Additionally, desired invariances in representations can be successfully captured by UKP only through the choice of the kernel function and the pseudometric can be efficiently estimated from $n$ input data samples with $O(\frac{1}{\sqrt{n}})$ estimation error. We also experimentally demonstrate the ability of UKP to discriminate between different types of features or representations based on their generalization performance on downstream kernel ridge regression tasks.

### Negative Dependence as a toolbox for machine learning : review and new developments 
[[arxiv](https://arxiv.org/abs/2502.07285)] [[cool](https://papers.cool/arxiv/2502.07285)] [[pdf](https://arxiv.org/pdf/2502.07285)]
> **Authors**: Hoang-Son Tran,Vladimir Petrovic,Remi Bardenet,Subhroshekhar Ghosh
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: Dedicated to the memory of Prof K.R. Parthasarathy: visionary, guru, and scientist par excellence
- **标题**: None
- **领域**: 机器学习,机器学习,可能性
- **Abstract**: Negative dependence is becoming a key driver in advancing learning capabilities beyond the limits of traditional independence. Recent developments have evidenced support towards negatively dependent systems as a learning paradigm in a broad range of fundamental machine learning challenges including optimization, sampling, dimensionality reduction and sparse signal recovery, often surpassing the performance of current methods based on statistical independence. The most popular negatively dependent model has been that of determinantal point processes (DPPs), which have their origins in quantum theory. However, other models, such as perturbed lattice models, strongly Rayleigh measures, zeros of random functions have gained salience in various learning applications. In this article, we review this burgeoning field of research, as it has developed over the past two decades or so. We also present new results on applications of DPPs to the parsimonious representation of neural networks. In the limited scope of the article, we mostly focus on aspects of this area to which the authors contributed over the recent years, including applications to Monte Carlo methods, coresets and stochastic gradient descent, stochastic networks, signal processing and connections to quantum computation. However, starting from basics of negative dependence for the uninitiated reader, extensive references are provided to a broad swath of related developments which could not be covered within our limited scope. While existing works and reviews generally focus on specific negatively dependent models (e.g. DPPs), a notable feature of this article is that it addresses negative dependence as a machine learning methodology as a whole. In this vein, it covers within its span an array of negatively dependent models and their applications well beyond DPPs, thereby putting forward a very general and rather unique perspective.

### Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds 
[[arxiv](https://arxiv.org/abs/2502.07265)] [[cool](https://papers.cool/arxiv/2502.07265)] [[pdf](https://arxiv.org/pdf/2502.07265)]
> **Authors**: Yunrui Guan,Krishnakumar Balasubramanian,Shiqian Ma
> **First submission**: 2025-02-11
> **First announcement**: 2025-02-12
> **comment**: No comments
- **标题**: None
- **领域**: 机器学习,机器学习,统计理论
- **Abstract**: We introduce the Riemannian Proximal Sampler, a method for sampling from densities defined on Riemannian manifolds. The performance of this sampler critically depends on two key oracles: the Manifold Brownian Increments (MBI) oracle and the Riemannian Heat-kernel (RHK) oracle. We establish high-accuracy sampling guarantees for the Riemannian Proximal Sampler, showing that generating samples with $\varepsilon$-accuracy requires $O(\log(1/\varepsilon))$ iterations in Kullback-Leibler divergence assuming access to exact oracles and $O(\log^2(1/\varepsilon))$ iterations in the total variation metric assuming access to sufficiently accurate inexact oracles. Furthermore, we present practical implementations of these oracles by leveraging heat-kernel truncation and Varadhan's asymptotics. In the latter case, we interpret the Riemannian Proximal Sampler as a discretization of the entropy-regularized Riemannian Proximal Point Method on the associated Wasserstein space. We provide preliminary numerical results that illustrate the effectiveness of the proposed methodology.

## 其他论文

- [Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems Deployment](https://arxiv.org/abs/2502.08115)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [From Clicks to Conversations: Evaluating the Effectiveness of Conversational Agents in Statistical Analysis](https://arxiv.org/abs/2502.08114)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,stat.CO in whitelist
- [Machine Learning-Driven Volumetric Cloud Rendering: Procedural Shader Optimization and Dynamic Lighting in Unreal Engine for Realistic Atmospheric Simulation](https://arxiv.org/abs/2502.08107)
  - **标题**: None
  - **Filtered Reason**: none of cs.GR in whitelist
- [Large language models perpetuate bias in palliative care: development and analysis of the Palliative Care Adversarial Dataset (PCAD)](https://arxiv.org/abs/2502.08073)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [Collaborative Filtering Meets Spectrum Shift: Connecting User-Item Interaction with Graph-Structured Side Information](https://arxiv.org/abs/2502.08071)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?](https://arxiv.org/abs/2502.08050)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Hierarchical Manifold Projection for Ransomware Detection: A Novel Geometric Approach to Identifying Malicious Encryption Patterns](https://arxiv.org/abs/2502.08013)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Welzijn.AI: A Conversational AI System for Monitoring Mental Well-being and a Use Case for Responsible AI Development](https://arxiv.org/abs/2502.07983)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [AI Humor Generation: Cognitive, Social and Creative Skills for Effective Humor](https://arxiv.org/abs/2502.07981)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants](https://arxiv.org/abs/2502.07956)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Distributed Approach to Haskell Based Applications Refactoring with LLMs Based Multi-Agent Systems](https://arxiv.org/abs/2502.07928)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment](https://arxiv.org/abs/2502.07903)
  - **标题**: None
  - **Filtered Reason**: none of cs.DC in whitelist
- [Sentiment Analysis Tools in Software Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2502.07893)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Automatic Robot Task Planning by Integrating Large Language Model with Genetic Programming](https://arxiv.org/abs/2502.07772)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO,eess.SY in whitelist
- [Great Power Brings Great Responsibility: Personalizing Conversational AI for Diverse Problem-Solvers](https://arxiv.org/abs/2502.07763)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [DeepVL: Dynamics and Inertial Measurements-based Deep Velocity Learning for Underwater Odometry](https://arxiv.org/abs/2502.07726)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
- [Pluto: Authoring Semantically Aligned Text and Charts for Data-Driven Communication](https://arxiv.org/abs/2502.07725)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [OpenCat: Improving Interoperability of ADS Testing](https://arxiv.org/abs/2502.07719)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Mock Deep Testing: Toward Separate Development of Data and Models for Deep Learning](https://arxiv.org/abs/2502.07712)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [RenderBox: Expressive Performance Rendering with Text Control](https://arxiv.org/abs/2502.07711)
  - **标题**: None
  - **Filtered Reason**: none of eess.AS,cs.MM in whitelist
- [A Framework for LLM-powered Design Assistants](https://arxiv.org/abs/2502.07698)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC,cs.CY in whitelist
- [HarmonyCut: Supporting Creative Chinese Paper-cutting Design with Form and Connotation Harmony](https://arxiv.org/abs/2502.07628)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Optimizing Datasets for Code Summarization: Is Code-Comment Coherence Enough?](https://arxiv.org/abs/2502.07611)
  - **标题**: None
  - **Filtered Reason**: none of cs.SE in whitelist
- [Towards spatial computing: recent advances in multimodal natural interaction for XR headsets](https://arxiv.org/abs/2502.07598)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference](https://arxiv.org/abs/2502.07578)
  - **标题**: None
  - **Filtered Reason**: none of cs.AR in whitelist
- [Efficient Sparsification of Simplicial Complexes via Local Densities of States](https://arxiv.org/abs/2502.07558)
  - **标题**: None
  - **Filtered Reason**: none of math.NA,cs.SI,cs.DM,cs.CG,stat.ML in whitelist
- [JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation](https://arxiv.org/abs/2502.07557)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [Visual-based spatial audio generation system for multi-speaker environments](https://arxiv.org/abs/2502.07538)
  - **标题**: None
  - **Filtered Reason**: none of cs.SD,eess.AS,cs.MM in whitelist
- [Decentralized Entropy-Driven Ransomware Detection Using Autonomous Neural Graph Embeddings](https://arxiv.org/abs/2502.07498)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [ETimeline: An Extensive Timeline Generation Dataset based on Large Language Model](https://arxiv.org/abs/2502.07474)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Quantitative Analysis of Objects in Prisoner Artworks](https://arxiv.org/abs/2502.07440)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Optimality in importance sampling: a gentle survey](https://arxiv.org/abs/2502.07396)
  - **标题**: None
  - **Filtered Reason**: none of cs.CE,stat.CO,stat.ML in whitelist
- [UAV-assisted Joint Mobile Edge Computing and Data Collection via Matching-enabled Deep Reinforcement Learning](https://arxiv.org/abs/2502.07388)
  - **标题**: None
  - **Filtered Reason**: none of cs.NE in whitelist
- [EMERALD: Evidence Management for Continuous Certification as a Service in the Cloud](https://arxiv.org/abs/2502.07330)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR in whitelist
- [White Hat Search Engine Optimization using Large Language Models](https://arxiv.org/abs/2502.07315)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR,cs.GT in whitelist
- [Explicit Codes approaching Generalized Singleton Bound using Expanders](https://arxiv.org/abs/2502.07308)
  - **标题**: None
  - **Filtered Reason**: none of cs.CC,cs.IT in whitelist
- [CreAgent: Towards Long-Term Evaluation of Recommender System under Platform-Creator Information Asymmetry](https://arxiv.org/abs/2502.07307)
  - **标题**: None
  - **Filtered Reason**: none of cs.IR in whitelist
- [Investigating Creativity in Humans and Generative AI Through Circles Exercises](https://arxiv.org/abs/2502.07292)
  - **标题**: None
  - **Filtered Reason**: none of cs.HC in whitelist
- [Diverse Perspectives on AI: Examining People's Acceptability and Reasoning of Possible AI Use Cases](https://arxiv.org/abs/2502.07287)
  - **标题**: None
  - **Filtered Reason**: none of cs.CY in whitelist
- [VLWE: Variety-based Learning with Errors for Vector Encryption through Algebraic Geometry](https://arxiv.org/abs/2502.07284)
  - **标题**: None
  - **Filtered Reason**: none of cs.CR,cs.CG in whitelist
- [Leader-follower formation enabled by pressure sensing in free-swimming undulatory robotic fish](https://arxiv.org/abs/2502.07282)
  - **标题**: None
  - **Filtered Reason**: none of cs.RO in whitelist
